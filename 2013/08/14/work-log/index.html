
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>Work Log - Background ML bugs; Why is Foreground Noise Variance so large? &larr; </title>
   <meta name="author" content="Kyle Simek" />

   <link rel="start" href="/ksimek/research/" />

	
	
    <link rel="shortcut icon" href="/ksimek/research/favicon.ico">

	
	

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="/ksimek/research/assets/themes/mark-reid/css/syntax.css" type="text/css" />

   <!-- Jquery UI CSS --!>
   <link media="screen" rel="stylesheet" href="/ksimek/research/css/ui-smoothness/jquery-ui-1.8.22.custom.css" type="text/css" />

   <!-- Homepage CSS -->
   <link media="screen" rel="stylesheet" href="/ksimek/research/assets/themes/mark-reid/css/screen.css" type="text/css" />

   <!-- Handheld CSS -->
   <link media="handheld, only screen and (max-width: 480px), only screen and (max-device-width: 480px)" href="/ksimek/research/assets/themes/mark-reid/css/handheld.css" type="text/css" rel="stylesheet" />


<!--[if IEMobile]>
<link rel="stylesheet" type="text/css" href="/ksimek/research/assets/themes/mark-reid/css/handheld.css" media="screen" />
<![endif]-->

   <!-- Mathjax Javascript -->
   <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
   </script>

   <!-- three.js Javascript -->
    <script src="/ksimek/research/js/jquery.js"></script>
    <script src="/ksimek/research/js/jquery-ui-1.8.22.custom.min.js"></script>

    

    <script type="text/javascript">
        $(document).ready(function(){
                $('#javascript_error').hide();
        });
    </script>



</head>
<body id="">
<div id="site">
  
  <div id="header">
    <h1>
    	<a href="/ksimek/research/" title="KLS Research Blog">KLS Research Blog</a>
    	<span class="byline">&larr; <a href="/ksimek/research/">Nothing to see here...</a></span>
    </h1>
    <ul class="nav">
      <li><a class="home" href="/ksimek/research/">Home</a></li>
      <li><a href="/ksimek/research/about.html">About</a></li>
      <li><a href="/ksimek/research/contact.html">Contact</a></li>
      <li><a  href="/ksimek/research/archive.html">Archive</a></li>
      <li><a  href="/ksimek/research/feeds.html"><img src="/ksimek/research/img/feed-icon.gif" /></a></li> 

    </ul>
  </div>


  
<div id="page" class="article">
	
  
  
  <h1 class="title">Work Log - Background ML bugs; Why is Foreground Noise Variance so large?</h1>

  <div class="date emphnext">August 14, 2013</div>
    


  
    

<div class="meta-info">
<table>
    <tr>
        <th>Project</th>
        <td><a href="/ksimek/research/projects/tulips.html">Tulips</a></td>
    </tr>
    <tr>
        <th>Subproject</th>
        <td>Data Association v2</td>
    </tr>
    <tr>
        <th>Working path</th>
        <td>projects/&#8203;tulips/&#8203;trunk/&#8203;src/&#8203;matlab/&#8203;data_association_2</td>
    </tr>


</table>

    Unless otherwise noted, all filesystem paths are relative to the "Working path" named above.
</div>


<h1>ML Validity Testing</h1>

<p>In yesterday's test, I hadn't realized that the training ML didn't include all of the curves, only the foreground curves were included.  Rerunning:</p>

<pre><code>Reference ML: -5.1552e+04
Training ML: -5.0786e+04
</code></pre>

<p>Okay, we're back in the ballpark -- Within 1.5% of the reference.</p>

<p>Found the other issue: the roundabout way I was using to generate the training covariance matrices was ignoring the user-specified position_variance_2d.  Results now match:</p>

<pre><code>Reference ML: -5.1552e+04
Training ML: -5.1552e+04
</code></pre>

<p>Unfortunately, none of this explains why training is causing noise variance to collapse so low.  All discovered problems were merely bugs in the validation logic.  At least we know the training ML logic is valid.</p>

<p>...</p>

<p>Quick inspection shows that 2D curves are, indeed, pre-smoothed.  This means that noise variance can collapse to near-zero when fitting 2D curve.</p>

<p>The new question becomes: why doesn't it occur in the foreground (3D) model?</p>

<p>...</p>

<p>Try re-indexing...</p>

<pre><code>% update params related to smoothing variance
params_2 = tbc_.params;
params_2.smoothing_variance = training_results{1}.smoothing_variance;
params_2.noise_variance = training_results{1}.noise_variance;
% re-construct likelihood (including indicies)
test_Corrs_ll_2 = tr_prep_likelihood(test_Corrs, data_, params_2);
% re-run training
train_params_done = tr_train(test_Corrs_ll_2, training_results{2}, 400, 3);
</code></pre>

<p>Results</p>

<pre><code>        smoothing_variance: 0.0011
            noise_variance: 0.6846
         position_variance: 1.3597e+04
             rate_variance: 0.3042
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.3013e-06
 perturb_position_variance: 0.6621
             perturb_scale: 2.9795

Final ML: -7840.140322
</code></pre>

<p>Compare against results prior to re-indexing: version:</p>

<pre><code>        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654

Final ML: -8049.9e+03
</code></pre>

<p>So we've improved, but nowhere near the background model's ML of 4611.9.  Note that curves got smoother and less noisy.  More correlation, more variance pushed into the perturbations.  (why the f*** perturb_smoothing_variance is just sitting there like an idiot is still beyond me).</p>

<h2>Miscellaneous thoughts</h2>

<p>Need to visualize ll_means against smoothed curve.  The perturbations should be correlated.  Maybe even plot them?  Note the perturbations need to be considered only in the directions parallel to the image plane.  <strong> This has never been done, and is necessary to validate the index-estimation in <code>correspondence/corr_to_likelihood</code>.</strong> Can we visualize after removing rate and offset components?  Yes: difference between ll_means and per-view reconstructed curve.</p>

<p>Consider smarter smoothing in <code>corr_to_likelihood</code> -- using posterior max instead of <code>csaps</code>.    Could give better index estimation if the visualization test shows problems.</p>

<p>What if we were using a 3D likelhood for background curves too? Could we still expect the BG ML to be insanely high, and the noise variance to be insanely peaked?  Backproject, estimate 3d noise variance, estimate index set.  The farther away it gets, the more variance in the correlated points.  Which means lower ML, right?  But training will push variance lower.</p>

<p>Note that larger noise variance in FG model will explain away bad triangulations.  Perturb variances also explain to some extent, but maybe they aren't sufficient to explain enough of it.</p>

<p>Why is perturb_smoothing_variance basically zero?  If it was higher, it could explain more of the traingulation error, and allow noise variance to drop.  Should we be using a different pertubation model?  Maybe Brownian motion instead of integrated brownian motion?  Visualizing perturbations would be informative here.</p>

<p>Do smooth perturbations follow a different dynamics model than linear and offset perturbations?  Can we force it to be larger?  what if it was the only option for modelling perturbations?   It's true that a small amount of smoothing variance can result in a huge amount of marginal point variance, and large variances kill ML's.  Probably a mean-reverting model is more sensible -- Ornstein Ulenbeck process, perhaps?  Or SqExp?  I avoided these in the past, because it changes the form of the marginal curve covariances -- they're no longer purely cubic-spline processes.  But I never considered the fact that we need to model triangulation error.</p>

<p>Observations: setting perturb_smoothing_variance to exactly zero has no change in ML.</p>

<p><strong>Consider tying foreground and background noise variance during training.</strong>  &lt;---  This is the most pragmatic solution.  Avoids getting mired in details, and acknowledges what we know to be true: image noise arises from the same process in foreground and background models.</p>

<p>Possibly the fact that we allow a nonzero position_mean in 2D but not in 3D is the issue?</p>

<h1>Finer-grained index estimation</h1>

<p>Got it!  In <code>corr_to_likelihood.m</code>, we have two parameters that determine how fine-grained the sampling is along the smoothed curve.  Each observed curve is then matched against the sampled curve.  The sampling period is 2 pixels, which means there's an average error of about 1 pixel in each index estimate.</p>

<p>Reducing the sampling period to 1 pixel and re-training gives:</p>

<pre><code>        smoothing_variance: 0.0014
            noise_variance: 0.3986
         position_variance: 1.3555e+04
             rate_variance: 0.3100
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 3.3992e-04
 perturb_position_variance: 0.6874
             perturb_scale: 2.3823

Final ML: -6357.585946
</code></pre>

<p>Although we only slightly changed the sampling period, the final ML improved significantly.  The noise variance dropped from 0.7 to 0.4, too.  Perturb rate variance changed by 2 orders of magnitude!</p>

<p>Reducing sampling period to 0.5:</p>

<pre><code>        smoothing_variance: 0.0018
            noise_variance: 0.3060
         position_variance: 1.3499e+04
             rate_variance: 0.3098
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 4.3411e-04
 perturb_position_variance: 0.8152
             perturb_scale: 2.4095

Final ML: -5664.35
</code></pre>

<p>The upward ML trend continues, but the noise variance appears to be flattening out.  The perturb_position_variance jumped upward unexpectedly.</p>

<p>This might explain all of the disparity between the 2D and 3D noise variances.  Unfortunately, we can't reduce the sampling period to 0.0004, because the runtime complexity of the matching  is O(N<sup>2),</sup> where N is the number of sampled points.</p>

<p>Better idea: after finding the optimal region in the matching algorithm, improve it by projecting the point onto the line segments neighboring the matched point.  Constant-time, and significantly better!</p>

<p>Another thought: if the rasterization error was approx. 1 pixel, the sampling error could be reduced to the point where the rasterization error dominated (possibly a sampling period of  0.5 or 0.01 would achieve this).  That way, both 2D and 3D noise sigma would be dominated by rasterization error, and would train to similar values.</p>

<h1>TODO</h1>

<ul>
<li>implement post-match index improvement.</li>
<li>Plot reconstruction residuals, look for correlation model.

<ul>
<li>Goal: determine if residuals are truely independent, and belong in the noise bucket.</li>
</ul>
</li>
<li>Try training BG and FG together, with the same noise variance.</li>
</ul>


      <address class="signature">
        Posted by
        <a class="author" href="/ksimek/research/">Kyle Simek</a> 
      </address>
  


  
  
  <div class="prev-next">
    <a href="/ksimek/research/2013/08/15/work-log" class="next" title="Work Log">Work Log &rarr;</a>
  
  
    <a href="/ksimek/research/2013/08/13/work-log" class="prev" title="Work Log - Theoretical Rate variance bug; Training background curve model">&larr; Work Log - Theoretical Rate variance bug; Training background curve model</a>
  
  </div>
  <div class="clearer"> </div>

<div class="post-sharing">
 

  
  		<div id="fb-root"></div>

<div class="post-share ulno mob">

<!-- Hacker News -->
<span class="hn"><span id="hnews"></span></span>

<!-- Twitter -->
<span class="tw"><a href="https://twitter.com/share" class="twitter-share-button" data-text="Work Log - Background ML bugs; Why is Foreground Noise Variance so large?" data-via="" data-related="" data-count="" data-size="">Tweet</a></span>

<!-- Google+ -->
<span class="gp"><div class="g-plusone" data-size="medium" data-annotation="bubble" data-width=""></div></span>

<!-- Facebook -->
<!--
<span class="fb"><div class="fb-like" data-send="false" data-layout="button_count" data-width="90" data-show-faces="false" data-font=""></div></span>
-->

<!-- Reddit -->
<script type="text/javascript" src="http://www.reddit.com/static/button/button1.js"></script>
</div>

<script>
  
(function(doc, script) {
 	
	//Async Social Buttons
    var js, 
        fjs = doc.getElementsByTagName(script)[0],
        add = function(url, id) {
            if (doc.getElementById(id)) {return;}
            js = doc.createElement(script);
            js.src = url;
            id && (js.id = id);
            fjs.parentNode.insertBefore(js, fjs);
        };
    
    // Twitter SDK
    add('//platform.twitter.com/widgets.js', 'twitter-wjs');

    // Google+ button
    add('https://apis.google.com/js/plusone.js');
    
    // Facebook SDK
    add('//connect.facebook.net/en_GB/all.js#xfbml=1&appId=123', 'facebook-jssdk');
    
    //Hacker News Button 	
 	var hn_like = document.createElement('iframe');
      hn_like.frameborder="no";
      hn_like.scrolling="no";
      hn_like.height="28px";
      hn_like.width="110px";
      hn_like.style="border:0px;";
      hn_like.src = "http://hnlike.com/upvote.php?link="
                    + encodeURIComponent(document.location)
                    + "&title="
                    + encodeURIComponent("Work Log - Background ML bugs; Why is Foreground Noise Variance so large?");
      hn_like.innerHTML="iframes not supported by your browser";
      
      var where = document.getElementById("hnews");
      where.parentNode.insertBefore(
        hn_like,
        where
      );
}(document, 'script'));

</script>

  



</div>




  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'klsresearch'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




  
</div><!-- End Page -->



  
  <div id="footer">
  	<address>
  		<span class="copyright">
  			Content by <a href="/ksimek/research/about.html">Kyle Simek</a>. Original design by 
  			<a href="http://mark.reid.name/">Mark Reid</a>
  			<br/>
  			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
  		</span>
  		<span class="engine">
  			Powered by <a href="http://github.com/mojombo/jekyll/" title="A static, minimalist CMS">Jekyll</a>
  		</span>
  	</address>
  </div>
  
</div>

<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->

  


  <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-33692744-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>




</body>
</html>

