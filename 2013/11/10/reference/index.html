
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>Gradient w.r.t. Indices &larr; </title>
   <meta name="author" content="Kyle Simek" />

   <link rel="start" href="/ksimek/research/" />

	
	
    <link rel="shortcut icon" href="/ksimek/research/favicon.ico">

	
	

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="/ksimek/research/assets/themes/mark-reid/css/syntax.css" type="text/css" />

   <!-- Jquery UI CSS --!>
   <link media="screen" rel="stylesheet" href="/ksimek/research/css/ui-smoothness/jquery-ui-1.8.22.custom.css" type="text/css" />

   <!-- Homepage CSS -->
   <link media="screen" rel="stylesheet" href="/ksimek/research/assets/themes/mark-reid/css/screen.css" type="text/css" />

   <!-- Handheld CSS -->
   <link media="handheld, only screen and (max-width: 480px), only screen and (max-device-width: 480px)" href="/ksimek/research/assets/themes/mark-reid/css/handheld.css" type="text/css" rel="stylesheet" />


<!--[if IEMobile]>
<link rel="stylesheet" type="text/css" href="/ksimek/research/assets/themes/mark-reid/css/handheld.css" media="screen" />
<![endif]-->

   <!-- Mathjax Javascript -->
   <script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
   </script>

   <!-- three.js Javascript -->
    <script src="/ksimek/research/js/jquery.js"></script>
    <script src="/ksimek/research/js/jquery-ui-1.8.22.custom.min.js"></script>

    

    <script type="text/javascript">
        $(document).ready(function(){
                $('#javascript_error').hide();
        });
    </script>



</head>
<body id="">
<div id="site">
  
  <div id="header">
    <h1>
    	<a href="/ksimek/research/" title="KLS Research Blog">KLS Research Blog</a>
    	<span class="byline">&larr; <a href="/ksimek/research/">Nothing to see here...</a></span>
    </h1>
    <ul class="nav">
      <li><a class="home" href="/ksimek/research/">Home</a></li>
      <li><a href="/ksimek/research/about.html">About</a></li>
      <li><a href="/ksimek/research/contact.html">Contact</a></li>
      <li><a  href="/ksimek/research/archive.html">Archive</a></li>
      <li><a  href="/ksimek/research/categories.html">Categories</a></li>
      <li><a  href="/ksimek/research/projects">Projects</a></li>
      <li><a  href="/ksimek/research/events">Events</a></li>
      <li><a  href="/ksimek/research/feeds.html"><img src="/ksimek/research/img/feed-icon.gif" /></a></li> 

    </ul>
  </div>


  
<div id="page" class="article">
	
  
  
    


  <h1 class="title">
        [Reference] Gradient w.r.t. Indices
    </h1>

  <div class="date emphnext">November 10, 2013</div>
    


  
    <p>To optimize indices, we'll need to compute the derivative of the marginal log-likelihood w.r.t. changing indices.</p>

<p>I first tried to derive this using the generalization of the chain rule to matrix expressions (see matrix cookbook, section 2.8.1), but the computation exploded.  Since ultimately, the derivative is a simple single-input, single output function, we can use differentials to derive the solution.</p>

<p>Let the marginal likelihood as a function of indices be \(g(x)\):</p>

<div>
\[
    \frac{\partial g(x)}{\partial x_i} = \frac{\partial}{\partial x_i} 
        \frac{1}{2}(-y^\top S^\top ( I + S K(x) S^\top)^{-1} S y)
\]

Let \(U = I + S K(x) S^\top\), and \(V = U^{-1}\).  Working inside out, lets find \(\frac{\partial U}{\partial x_i}\).

\[
\begin{align}
    U + dU  &= I + S (K + dK) S ^\top \\
            &= I + S K S^\top + S dK S ^\top \\
        dU  &= S \, dK\, S^\top \\
        U'  &= S K' S^\top
\end{align}
\]

Where \(M'\) is the derivative of the elements of \(M\) w.r.t. \(x_i\).  Next, \(\frac{\partial V}{\partial x_i}\), which comes from the matrix cookbook, equation (36).

\[
    dV = -U^{-1} \, dU \, U^{-1} \\
    V' = -U^{-1} U' U^{-1}
\]

Finally,  \(\frac{\partial g(x)}{\partial x_i}\):
    
\[
\begin{align}
    g + dg  &= -\frac{1}{2}y^\top S^\top (V + dV) S y \\
    g + dg  &= -\frac{1}{2}y^\top S^\top \, V \, S y + y^\top S^\top \,dV \,S y \\
        dg  &= -\frac{1}{2}y^\top S^\top \,dV \,S y \\
        g'  &= -\frac{1}{2}y^\top S^\top \, V' \,S y \\
\end{align}
\]

Expanding \(V\) gives the final formula:
\[
\begin{align}
        g'  &= \frac{1}{2}y^\top S^\top U^{-1} S K' S^\top U^{-1} S y \\
        g'  &= \frac{1}{2}y^\top M K' M y \\
        g'  &= \frac{1}{2}z^\top K' z \tag{1}\\
\end{align}
\]

<p>
Here, \(M = S^\top U^{-1} S \), (which is symmetric), and \(z = M y\).  
</p>

<p>
This equation gives us a single element of the gradient, namely \(d g(x)/dx_i\).  However, once \(z\) is computed, we can reuse it  when recomputing (1) for all other \(x_j\)'s.  The cost of each subsequent gradient element becomes \(O(n^2)\), making the total gradient \(O(n^3)\), which is pretty good. (This assumes the K's can be computed efficiently, which is true; see below.)  However, we also observe that \(K'\) is sparse with size \(O(n)\), so we can do sparse multiplication to reduce the running time to linear, and <strong>the full gradient takes \(O(n^2)\)</strong>, assuming \(z\) is precomputed.  Cool! 
</p>

</div>


<p></p>


<h2>Derivatives of K(x)</h2>

<p>Below we derive the derivative of each of the three covariance expresssions, which combine to give \(K'\).</p>

<p><em>Cubic covariance</em></p>

<p>Recall the cubic covariance expression:</p>

<div>
\[
k(x_1, x_2) = (x_a - x_b) x_b^2 / 2 + x_b^3/3
\]

Where \(x_b = min(x_1, x_2)\) and \(x_a = max(x_1, x_2)\).
</div>


<p>Taking the derivative w.r.t. (x_2) gives:</p>

<div>
\[
\begin{align}
\frac{\partial k(x_1, x_2)}{\partial x_2} &= 
    \begin{cases}
         x_1^2 / 2 & \text{if } x_2 >= x_1 \\
         x_1 x_2 - x_2^2/2 & \text{if } x_2 < x_1 
    \end{cases} \\
            &= 
    \begin{cases}
         x_b^2 / 2 & \text{if } x_2 >= x_1 \\
         x_a x_b - x_b^2/2 & \text{if } x_2 < x_1 
    \end{cases} \\
\end{align}
\]
</div>


<p>Or equivalently</p>

<div>
\[
\frac{\partial k(x_1, x_2)}{\partial x_2} = 
         x_b \left ( x_1  - x_b/2 \right ) \tag{2}
\]
</div>


<p>Note that if the goal is to find \(K' = \frac{\partial{K}}{\partial{x_i}}\), the on-diagonal element \(k'_{i i}\) needs a slightly different formula, because the kernel is a function of a single variable, \(x_i\).</p>

<div>
\[
\begin{align}
\frac{\partial k(x_i)}{\partial x_i} &= 
    \frac{\partial}{\partial x_i} x_i^3/3 \\
        &= x_i^2 \tag{3}
\end{align}
\]


Compare this with the general formula: if we plugged-in \(x_i'\) to both inputs of equation (2), we'd get \(x_i^2/2\), which underestimates the derivative by half.  We'll see in the next section that when computing \(g'\) the "wrong" expression for \(k'_{ii}\) is actually more useful than the correct one, because we'll need to scale it by 0.5 anyway.

</div>




<p></p>


<p><em>Linear Covariance</em></p>

<p>Recall the cubic covariance expression:</p>

<div>
\[
k(x_1, x_2) = x_1 x_2
\]

The derivative w.r.t. \(x_2\) is simply \(x_1\).
</div>


<p><em>Offset Covariance</em></p>

<p>Recall the offset covariance expression:</p>

<div>
\[
k(x_1, x_2) = k
\]

The derivative w.r.t. \(x_2\) is zero.
</div>


<p><em>Implementation</em></p>

<p>Implemented end-to-end version in <code>kernel/get_model_kernel_derivative.m</code>; see also components in <code>kernel/get_spacial_kernel_derivative.m</code> and <code>kernel/cubic_kernel_derivative.m</code>.</p>

<p>These functions return all of the partial derivatives of the matrix with respect to the first input.   The i-th row of the result make up the nonzero values in \(\frac{\partial K}{\partial x_i}\).  Below is example code that computes all of the partial derivative matrices.</p>

<pre><code>N = 100;
% construct indices
x = linspace(0, 10, N);
% construct derivative rows
d_kernel = get_model_kernel_derivative(...);
d_K = eval_kernel(d_kernel, x, x);
% construct dK/dx_i, for each i = 1..N
d_K_d_x = dcell(1,N);
for i = 1:N
    tmp = sparse(N, N);
    tmp(i,:) = d_K(i,:);
    tmp(:,i) = d_K(i,:)';
    d_K_d_x{i} = tmp;
end
</code></pre>

<p><em>Directional Derivatives</em></p>

<p>I think we can get directional derivatives of \(K\) by taking the weighted sum of partial derivatives, where the weights are the component lengths of the direction vector.  I have yet to confirm this beyond a hand-wavy hunch, and in practice, this might not even be needed, since computing the full gradient is so efficient.</p>

<h2>Sparsity of K'</h2>

<p>The sparsity of \(\frac{\partial K}{\partial x_i}\) actually allows us to further simplify formula (1) for \(g'\), ultimately allowing us to compute the entire gradient in a single matrix multiplication.</p>

<p>First observe that K' is only nonzero on the i-th row and column:</p>

<div>
\[
    k'_{i,j} = 
    \begin{cases}
        \delta_{ij} & \text{if } i = j \\
        0 & \text{otherwise}
    \end{cases}
\]

where \(\delta_{ij} = \frac{\partial k_{ij}}{\partial x_i} \).

For convenience, we'll define the vector \(\Delta_i = [\delta_{i1}, ..., \delta_{in}]^\top\).


Let \(g_i'\) be the partial derivative of \(g\) w.r.t. \(x_i\), with the entire gradient denoted by \(\nabla g = [g'_1, ..., g'_n]^\top\).  Using sparsity, eq. (1) can be rewritten as

\[
    g_i' = \frac{1}{2} z_i (\delta_{i1} z_1 + ... + \delta_{i(i-1)} z_{i-1} + \sum_j \delta_ij z_j + \delta_{i(i+1)} z_{i+1} + ... + \delta_{in} z_n)
\]

The expression in the parentheses is almost a dot product of z and \(\Delta_i\), but with the i-th term replaced with the dot product of z and \(\Delta_i\).  We can re-write the expression in terms of dot products, minus a correction.

\[
\begin{align}
    g_i' = \frac{1}{2} z_i (2 * z \cdot \Delta_i - z_i \delta_i) \\ 
       = z_i \, z \cdot \Delta_i' \\
\end{align}
\]

Where \(\Delta_i'\)  is equal to \(\Delta_i\) in all elements except the i-th, which is equal to \(0.5 \lambda_i\).  Note that we can get \(\Delta_i'\) by using equation (2) above instead of equation (3), which allows us to avoid having a separate implementation for on-diagonal elements.

Since each \(g_i\) arises from a dot product, we can compute \(\nabla g\) using matrix multiplication.  Let \(\Delta' = [\Delta'_1, ..., \Delta'_n] \), i.e. the matrix whose i-th column is \(\Delta'_i\).  The gradient expression becomes

\[
\begin{align}
    \nabla g = z \odot (\Delta' z) \tag{4}
\end{align}
\]

where \(\odot\) denotes element-wise multiplication.
<br /><br />
To handle multiple dimensions, simply apply to each dimension independently and sum the results.

</div>




      <address class="signature">
        Posted by
        <a class="author" href="/ksimek/research/">Kyle Simek</a> 
      </address>
  


  
  
  <div class="prev-next">
    <a href="/ksimek/research/2013/11/10/work-log" class="next" title="Work Log">Work Log &rarr;</a>
  
  
    <a href="/ksimek/research/2013/11/08/meeting-notes" class="prev" title="iPlant Literature Review Planning Meetings">&larr; iPlant Literature Review Planning Meetings</a>
  
  </div>
  <div class="clearer"> </div>

<div class="post-sharing">
 

</div>




  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'klsresearch'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




  
</div><!-- End Page -->



  
  <div id="footer">
  	<address>
  		<span class="copyright">
  			Content by <a href="/ksimek/research/about.html">Kyle Simek</a>. Original design by 
  			<a href="http://mark.reid.name/">Mark Reid</a>
  			<br/>
  			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
  		</span>
  		<span class="engine">
  			Powered by <a href="http://github.com/mojombo/jekyll/" title="A static, minimalist CMS">Jekyll</a>
  		</span>
  	</address>
  </div>
  
</div>

<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->

  


  <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-33692744-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>




</body>
</html>

