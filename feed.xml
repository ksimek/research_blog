<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>iPlant Literature Review Planning Meetings</title>
				<description>&lt;h2&gt;Document discussion&lt;/h2&gt;

&lt;p&gt;Some random notes about issues to bring up in the final document.&lt;/p&gt;

&lt;p&gt;Talk about representation vs. &quot;features&quot; (characteristics).  Clarify &quot;features&quot;, don't use indescriminitively (call them high-level features, topological features, as opposed to image features).&lt;/p&gt;

&lt;p&gt;Two ways to go about finding &quot;features&quot;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;look directly for charactersitics/features&quot;&lt;/li&gt;
&lt;li&gt;&quot;look for model, find the other stuff&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;i.e. looking for representations vs. characteristics&lt;/p&gt;

&lt;p&gt;Bisque vs. Nonbisque?  Unclear what the final application will be, but if Bisque is the goal, some discussion of it's feasibility is probably warranted.  For example, interactive systems like the Clark paper might not be feasible for Bisque.&lt;/p&gt;

&lt;h2&gt;Taxonomy of curve-extraction&lt;/h2&gt;

&lt;p&gt;Kobus mentioned the different &quot;dimensions&quot;  that the problem can be split into.  Some might be&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3D vs 2D&lt;/li&gt;
&lt;li&gt;imaging system type (microscope, MRI, camera, etc)&lt;/li&gt;
&lt;li&gt;temporal vs. nontemporal&lt;/li&gt;
&lt;li&gt;branching vs non branching&lt;/li&gt;
&lt;li&gt;biological vs. nonbiological&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Research Topics&lt;/h2&gt;

&lt;p&gt;Recommend splitting time between bio literature and CV literature.&lt;/p&gt;

&lt;p&gt;Several topics to look into&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image Processing&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;snakes&quot; - active contours&lt;/li&gt;
&lt;li&gt;Medial axis filtering&lt;/li&gt;
&lt;li&gt;curve modelling (GP,&lt;/li&gt;
&lt;li&gt;finding branches&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pollen tube tracking&lt;/li&gt;
&lt;li&gt;root tracking&lt;/li&gt;
&lt;li&gt;Blood vessel literature / vascular segmentation&lt;/li&gt;
&lt;li&gt;Neuron tracing&lt;/li&gt;
&lt;li&gt;Alternaria / Fungus?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Data Inventory &lt;/h2&gt;

&lt;p&gt;We currently have data sets from a handful of different applications/domains.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pollen tubes&lt;/li&gt;
&lt;li&gt;&quot;Clark&quot; root image (2D, one image)&lt;/li&gt;
&lt;li&gt;&quot;Max Plank&quot; root images&lt;/li&gt;
&lt;li&gt;Neurons&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Other stuff&lt;/h2&gt;

&lt;p&gt;Other topics that might be worth looking into, but maybe of speculative value.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rhizotron - nondestructive underground root imaging system&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Any questions for Martha?&lt;/p&gt;

&lt;p&gt;Reading for next week:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;* citations from Clark and Hypotrace
    * related work
    * image processing
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Fri, 08 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/08/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/08/work-log</guid>
			</item>
		
			<item>
				<title>WACV reconstruction (revisited)</title>
				<description>&lt;p&gt;Addressing issues that came up last time we ran WACV.&lt;/p&gt;

&lt;p&gt;Currently working on dataset #8, which has some real problems.&lt;/p&gt;

&lt;p&gt;Spent some time tweaking the ground truth, adding missing curves, and correcting one or two obvious mistakes.&lt;/p&gt;

&lt;p&gt;Still getting terrible results; one problem might be the &quot;rough&quot; reconstruction, which doesn't take into account anisotropic data uncertainty.  Fixed.&lt;/p&gt;

&lt;p&gt;False-positive reversal has been corrected, but still getting terrible results; it seems like the camera calibration is waaaay off.&lt;/p&gt;

&lt;p&gt;I dont have the calibration data for this dataset available, locally re-syncing full dataset.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Are these calibrated cameras ordered in counter-clockwise direction? No.  but the calibration_data.mat file in for this dataset is.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Inspected correspondence diagrams.  Possibly some of the ground truth curves are reversed?  Dataset #8, curve #7, view #9, only corresponds to very end of other points:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-07-bad_correspondence.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tweaking ground truth tracing program to show start-point of curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Ground truth program doesn't compile.  Clang++ for mountain lion is complaining.&lt;/p&gt;

&lt;p&gt;Apparently clang no longer uses gnu stdc++ library, and some library components use proprietary gnu symbols (I admit, it was my code).  Fixed.&lt;/p&gt;

&lt;p&gt;Getting some new errors regarding some forward declarations of STL pair.  Was hard to debug, because errors weren't local to the problematic code, and errors were cryptic.  Removed forward declarations; fixed.&lt;/p&gt;

&lt;p&gt;Found a template function I hand't ported from my experimental branch of KJB.&lt;/p&gt;

&lt;p&gt;Linker errors -- libXmu and libXi not found.  Tweaked init_compile's logic for OpenGL on Macs. Hopefully I didn't break anyone else's builds...&lt;/p&gt;

&lt;p&gt;More linker errors. Apparently i need to recompile &lt;strong&gt;everything&lt;/strong&gt; to use clang's c++ libraries?? rebuilt boost, next is casadi, but I can't find the source on my machine.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;recalibrate cameras&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 07 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/07/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/07/work-log</guid>
			</item>
		
			<item>
				<title>CVPR cleanup, documentation</title>
				<description>&lt;p&gt;Cleaned up blog, added &lt;a href=&quot;/ksimek/research/projects&quot;&gt;project pages&lt;/a&gt;, &lt;a href=&quot;/ksimek/research/events&quot;&gt;events page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Added to &lt;a href=&quot;/ksimek/research/events/CVPR2014/&quot;&gt;CVPR 2012 page&lt;/a&gt;, created &lt;a href=&quot;/ksimek/research/events/CVPR2014/summary.html&quot;&gt;summary of &quot;done&quot; and &quot;TODO&quot;&lt;/a&gt; post-CVPR.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;WACV - extend to multi-view/tracking; fix reconstruction errors.&lt;/p&gt;

&lt;p&gt;Working, but need to remove translation and scale perturb component.&lt;/p&gt;

&lt;h2&gt;Testing branch index and start index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;negative start index causes gaps&lt;/li&gt;
&lt;li&gt;positive start index causes overshot (pre-tails)&lt;/li&gt;
&lt;li&gt;tree base is shifting between view&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO (new) &lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Testing branch index and start index, and reversal&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Wed, 06 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/06/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/06/work-log</guid>
			</item>
		
			<item>
				<title>iPlant </title>
				<description>&lt;p&gt;Doing background research inre: forwarded email from Kobus, &quot;Fwd: What we really need from your students&quot;.&lt;/p&gt;

&lt;h1&gt;Literature review&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Paper 1:&lt;/strong&gt; &lt;em&gt;Three-Dimensional root Phenotyping with a Novel Imaging and Software Platform&lt;/em&gt;, by Clark, et. al.&lt;/p&gt;

&lt;p&gt;Introduces a semi-automated method for semantic reconstruction of root structures from turntable images, using our lab's definition of semantic reconstruction, i.e. 3D curves with topology and part labels.  It looks like they're using standard voxel-carving to identify foreground and background voxels (i.e. root and non-root voxels).  This approach uses calibrated cameras  to backproject silhouette images and take the intersection--in other words, visual hull but with voxels instead of polygonal meshes.   Then the skeleton of the foreground voxels is extracted using a median filter method to extract 3D curves.  Skeleton branches are then manually labeled by domain experts as one of several root types.  There also seems to be some functionality to manually correct errors during backprojection and skeleton extraction phase.&lt;/p&gt;

&lt;p&gt;They key contribution seems to be a list of 27 features derived from the resulting 3D data.  Bushiness, centroid and volume distribution seem to be discriminative for classifying a specimen between the two different species under study.    They also  measure the amount of helical curvature and how much gravity affects growth, which has apparently not been studied in rice plants prior to this?.&lt;/p&gt;

&lt;p&gt;Extracting most of the interesting features requires full semantic reconstruction, which is very difficult to obtain using known fully-automatic methods.  Further, this approach requires a calibrated camera, which likely precludes us from using it for post-hoc analysis of existing datasets that might exist in Bisque, unless calibration data is available.&lt;/p&gt;

&lt;p&gt;The &quot;Clark Rice root&quot; image provided on the wiki is a high resolution 2D image, which appears to be different from those used in the Clark paper, so it's unclear what its relevance is in this context.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Other notes&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Its unclear how silhouettes are extracted, but likely just intensity thresholding with manually-chosen threshold value.  They're using a lightbox background, so this approach seems sensible.   There's probably some tunable parameters for the skeleton-extraction phase, but these aren't discussed.&lt;/p&gt;

&lt;!--
Bisque thoughts
===================

* 3D approaches might be unrealistic in Bisque
    1. Most datasets won't have calibrated cameras.
        * Could try auto-calibration, but I double their effectiveness in the wild.
        * Could have user click corresponding points, between views, but this leads us to...
    2. Most datasets won't have multiple views.
        * could try bayesian approaches to solving the inverse problem, but this is an open research problem.

    3. If 1. and 2. are satisfied, we still need significant user interaction to implement Clarke; unlikely to be possible in Bisque?
        * Maybe Bisque could receive the files output by Clarke's software?  But at that point, the analysis is already done, what is Bisque being used for?
* Most effective 3D systems will collect data under a process specifically tailored to 3D data.  How does Bisque fit into this scenario?




iPlant recognizes that most image-processing software developed by biologists is heavily tuned to a specific application, and doesn't generalize to other datasets.  This is a problem endemic to the entire field of computer vision; even today many approaches work well on the dataset they were trained on and fail on others.  Even the Deva/Feltzenswalb human detectors--arguably one of the most robust object detectors in the field -- performs poorly on new datasets, as we saw when we applied it to Mind's Eye.  Only after rescaling images, tuning thresholds, and retraining did we get reasonable performance.

This highlights a very CV-centric mindset, which I claim hinders the progress toward the goal of obtaining the best possible plant informatics.  The problem is a misalignment of goals.  For CV researchers, the goal is to do the best CV possible with as little human interaction possible.     In contrast, biologists' goals are to do the best biology possible, which hinges on obtaining the best possible data at the least possible cost. As a result, CV researchers judge our algorithms on &quot;percent correct&quot; assuming zero human interaction, whereas biologists are more likely to judge an approach by the number of human-hours needed to get to 100% accuracy or close to it.   The Clark paper is a good example, where humans manually make corrections to the computer vision algorithm and manually label the parts of a model; only then do they obtain the 27 features of plant roots that make their research novel.  We should recognize that our roles as iPLant &quot;consultants&quot; may conflict with our natural instincts as CV researchers and try to embrace strategies that leverage our CV expertise but ultimately serve the goals of biologists.

One such strategy is to relax the &quot;fully automatic&quot; constraint and look toward semi-automatic systems that use human input to improve robustness and achieve broad applicability across datasets.  Clark is a perfect example of such a method that uses some human interaction in exchange for drastically better results.  I've seen similar successes in the area of plant modeling, where interactive CAD interfaces are combined with CV algorithms from 20 years ago (e.g. snakes) to create a method that is efficient and provides precise models.

This not to say that advanced CV methods have not place in iPlant, but if Martha says our current charges is to make recommendations as to broadly-applicable methods that are currently &quot;doable,&quot; my answer is &quot;semi-automatic methods&quot;.  Putting this in the context of CV, even the best fully automatic methods aren't really fully automatic, because they require training on the part of the user, which requires some form of interactive system.

So the overall theme of my argument is using CV to construct better interactive GUIs.  At the moment, the greatest challenge to this strategy are the constraints posed by the current Bisque system, which provides only a minimal amount of HTML-esqe interactivity (e.g. click points, type in &quot;tags&quot;), in a painfully &quot;modal&quot; framework (e.g. submit user's response to server, wait for server do some analysis, repeat).  One solution is to push hard in improviing the Bisque framework, for example exploiting HTML5 and javascript to provide rich client-side user interfaces.  However, not only would this be an expensive undertaking, but the most effective GUIs will need to be specific to the task at hand, and building an interactive system for building interactive system would be a herculean task in both design and implementation.  

Maybe the answer lies outside of Bisque itself.  Bisque's real value is as a database of images and metadata.  Let's recognize this and push more toward promoting scientists adding Bisque-compatible &quot;export&quot; functionality to existing data-analysis systems (like Clark, et al.), so Bisque can become the world-standard repository for rich image metadata.  

Alternatively, if iPlant wants to continue to push in the direction of performing analysis inside the system, it needs to put significant effort into interfaces that are dynamic, responsive, and powerful.  This means leaning heavily on javascript, using canvas, webGL, and client-side image analysis so users can respond in real-time to the results of their interactions.  For example, imagine an interface in which users can &quot;paint&quot; some foregound and background pixels to train a classifier, quickly see the result of the classifier, and re-train by painting misclassified pixels.   Or an automatic curve-tracing algorithm where users can drag incorrect curves onto their correct path.  These are approaches that are proven and can provide excellent results, but require human-hours to get there.  Minimizing human hours will be a combination of good UI to fix CV errors quickly and good CV to minimize the UI time.  



The alternative is to focus on the features we can extract in the absence of near 100% accuracy.  

--&gt;

</description>
				<pubDate>Tue, 05 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/05/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/05/work-log</guid>
			</item>
		
			<item>
				<title>Post-CVPR-deadline; 2-part likelihood efficiency, 2-pass sampling</title>
				<description>&lt;p&gt;First day after CVPR deadline.&lt;/p&gt;

&lt;p&gt;Comp exam - committee, planning.&lt;/p&gt;

&lt;p&gt;Computer issues - upgrade, crashing, keyboard?&lt;/p&gt;

&lt;h1&gt;2-Pass sampling.&lt;/h1&gt;

&lt;p&gt;Evaluating the second likelihood term is still very slow, even after a 100x speedup.&lt;/p&gt;

&lt;p&gt;First do MH using single-term likelihood.  Then treat that as the proposal for the two-term likelihood.  Since the first step satisfies detailed balance, we have:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \hat p(\theta) q(\theta' | \theta) &amp;=
    \hat p(\theta') q(\theta | \theta') \\
    \frac{\hat p(\theta)}{\hat p(\theta')} &amp;=
    \frac{q(\theta | \theta')}{q(\theta' | \theta)} 
\end{align}

\]
&lt;/div&gt;


&lt;p&gt;Where \(\hat p(\theta)\) is the surrogate posterior, using only the single-term likelihood.  Substituting this identity into the full MCMC acceptance term, we get:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\alpha &amp;= \left \{ \frac{p(\theta') q(\theta | \theta')}{p(\theta)q(\theta' | \theta)}  \right \} \\
    &amp;= \left \{ \frac{p(\theta') \hat p(\theta)}{p(\theta)\hat p(\theta')}  \right \} \\
    &amp;= \frac{L_2(\theta')}{L_2(\theta)}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;This obviously isn't applicable for traditional gibbs sampling, but gibbs could be used for proposal, and MH used to accept/reject&lt;/p&gt;
</description>
				<pubDate>Mon, 04 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/04/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/04/work-log</guid>
			</item>
		
			<item>
				<title>(Two day) Markov Sampling (ctd).  Implementing, testing, optimizing</title>
				<description>&lt;p&gt;Note: changes to &lt;a href=&quot;/ksimek/research/events/CVPR2014/params.html&quot;&gt;params&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Resuming from yesterday's discussion about Markov sampling.  I was concerned about piecewise sampling of interpolated values with insufficient data being used.  After some thought, realized there's a better approach: construct blocks from input indices, not output indices.&lt;/p&gt;

&lt;p&gt;Secondly: only use the markov approach when there's too much data to eat at once.  Those cases are also the ones with the least probability of poor-data issues.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Re-implemented (&lt;code&gt;curve_tree_ml_5.ml&lt;/code&gt;), geting weird results.  Huge negative eigenvalues&lt;/p&gt;

&lt;p&gt;symptom: 30 output indices, 1000 output indices
symptom: K has 26k elements!&lt;/p&gt;

&lt;p&gt;This code is a mess, proving hard to debug.  I'm going to roll back to version 4, use ideas developed in v5 to implement Markov sampling.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Done.  No errors, but results aren't great.  Is it possible the markov blanket is wrong, or maybe we're misusing previous data?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;I think I found it... wasn't conditioning on previous sampled values.&lt;/p&gt;

&lt;p&gt;Seems to be fixed now.  Next on to timing and tuning&lt;/p&gt;

&lt;h2&gt;Profiling / Optimizing&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;attachment covariance&lt;/li&gt;
&lt;li&gt;markov order&lt;/li&gt;
&lt;li&gt;block size&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;strong&gt;Constructing Attachment covariance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;construct_attachment_covariance_3 is still a bottleneck.  Need to investigate some of the suggestions from a few days ago.&lt;/p&gt;

&lt;p&gt;Only computing &lt;code&gt;Cov_star_star&lt;/code&gt; once (exploiting stationarity in temporal GP) helps somewhat.&lt;/p&gt;

&lt;p&gt;Grouping &quot;sibling&quot; object construction should help a lot too.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Markov order&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Markov order as low as 10 doesn't seem to negatively affect results.&lt;/p&gt;

&lt;p&gt;Need to crop observations before the earliest sampled point we're conditioning on.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Timing
Before: 17.0s (total,  7.5s on inversion)
After: 16.5 (7.2s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiny improvement, but at no cost so who's complaining?&lt;/p&gt;

&lt;h2&gt;Block Size&lt;/h2&gt;

&lt;p&gt;Not much to say here...  100 seems to be optimal.&lt;/p&gt;

&lt;h2&gt;Markov Order&lt;/h2&gt;

&lt;p&gt;Plotted maximum posterior for various markov orders to compare reconstruction quality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-01-mls_vs_mo.png&quot; alt=&quot;maximum liklehood vs. markov order&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;range: 1.40e3
(unweighted) mean: 4.0156
std deviation: 510.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the mean should be taken with a grain of salt,
Some chaos below 500, but the scale of the fluctuations is actually relatively small, 0.035%.  We should note that in MCMC, its the absolute fluctuations that are significant, so percent error can be deceptive.  But even so, I think these results are pretty good.&lt;/p&gt;

&lt;p&gt;It's interesting that markov order of 10 is only slightly better than markov order of zero!    Also surprising that between 10 and 500, error increases.&lt;/p&gt;

&lt;p&gt;Should compare variance here vs. variance of log-likelihood w.r.t. posterior.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Result of 20 posterior samples (low markov order)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markov order = 10
block size =  100
ll std deviation = 2.33e3
ll mean: 4.0153e6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Result of 20 posterior samples (high markov order)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markov order = 500
block size =  100
ll std deviation = 2.17e3
ll mean: 4.0132e6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The log-likelihood standard deviation are high relative to the error range in the graph above.   This is weak evidence that the variance in the graph above is dues to small-scale instabilities in the pixel likelihood.  That is to say, tiny insignificant changes to the 3D model can cause noticible fluctuations to the pixel likelihod, due to it's nonlinearities.  To some extent, what we really want is the expected value of the likelihood over the entire set insignificant 3D perturbations.  So if the deviations we see due to markov-order are simply observations of this phenomenon, they can be safely ignored, because they are in any scenario.&lt;/p&gt;

&lt;p&gt;Let's test by re-running the low-markov order test with lots of samples (200 instead of 20) and see if the ll mean approaches that of the high-markov-order test.  This is also a decent stress test for the likelihood server.&lt;/p&gt;

&lt;p&gt;Result of 200 posterior samples (low markov order)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markov order = 10
block size =  100
ll std deviation = 2.5603e3
ll mean: 4.0133e6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep, we're getting closer to the high markov order results for ll mean.  I think we can drop this issue for now and be okay with a markov order of 10-20.  It is still surprising that markov order of zero seems to work so well.  I guess future data doesn't add too much if the present data is sufficient.&lt;/p&gt;

&lt;h2&gt;Bottlenecks&lt;/h2&gt;

&lt;p&gt;Inversion: 7.1s
buildling covariance:  2.5s
one_d_to_three_d: 1.5s
blkdiag: 1.2s
other: ~3.5s&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inversion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Only need to invert once for all views... Actually, not true because inversion contains view-specific sample data.  But a large block of the matrix is unchanged between views.  Implementing optimization...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Down to &lt;strong&gt;14.7 seconds&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Using sparsity and suppressing output, down to &lt;strong&gt;11.5s&lt;/strong&gt;. (8s when not in profile mode)&lt;/p&gt;

&lt;p&gt;However, getting some numerical issues (small-magnitude negative eigenvlaues); probably will do better if we use a symmetric equation and invert using cholesky.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;construct_attachment_covariance is now the bottlneck, taking a full 35% of run time.&lt;br/&gt;
Some options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;cache intermediate values here and return them as described in the previous post.&lt;/li&gt;
&lt;li&gt;group together calls to &quot;build_sibling_object&quot;&lt;/li&gt;
&lt;li&gt;avoid computing self-covariance when its available in prior_K&lt;/li&gt;
&lt;li&gt;implement one-pass version for symmetric matrices&lt;/li&gt;
&lt;li&gt;precompute self-covariance for all views in one call.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Implemented 2, 3, and 4, reduced running time to &lt;strong&gt;8.7s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Implemented 1, reduced to  &lt;strong&gt;8.4s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Implemented 5, now construct_attachment_covariance is not a bottleneck.  &lt;strong&gt;8.3s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Not many strong bottlenecks now.  Some opportunities  to avoid build_cov_ and one_d_to_three_d like we did in &lt;code&gt;curve_tree_ml_5_bak&lt;/code&gt;.  Also, using a symmetric formula for posterior might help here.&lt;/p&gt;

&lt;p&gt;Tweaked build_cov_ to skip an inner loop if it would be a no-op.  Down to &lt;strong&gt;7.8s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Set block size to 100 (had forgotten to change back yesterday, was 120)   &lt;strong&gt;7.6s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Rearranged matrix multiplication.  &lt;strong&gt;7.4s&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mex'd &lt;code&gt;one_d_to_three_d&lt;/code&gt; utility function.  &lt;strong&gt;6.9s&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Problem.  Samples look bad.  Overall shape is retained, but rough, with discontinuities.&lt;/p&gt;

&lt;p&gt;Switched off block matrix inversion, results look good, but back to &lt;strong&gt;8.3s&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found bug.  Indexing problem was causing previous samples to be ignored.  Fixed, and its better than before, but still getting some non-negligible eigenvalues and occasional discontinuities in samples.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Originally took 275s, now 8.3s in profile mode, 5.x s in regular mode.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;running time vs. number of samples to determine affect of likelihood server bottleneck (use GPU likelihood for real-world estimate)&lt;/li&gt;
&lt;li&gt;profile/optimize likelihood server directly?&lt;/li&gt;
&lt;li&gt;Symmetrize formula and use cholesky for inversion.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 31 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/31/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/31/work-log</guid>
			</item>
		
			<item>
				<title>Optimizing posterior-sampling for pixel likelihood</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting issues in &quot;optimized&quot; code in &lt;code&gt;curve_tree_ml_4&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Biggest change was in how K, K_star, K_star_star were computed.
Comparing against the reference implementation &lt;code&gt;curve_tree_m_3&lt;/code&gt; shows the &quot;parent&quot; indices aren't correct.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Solved.  When incorporating noise-free sampled values, was using indices and covariance from noisy observed values.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Timing

Old speed:    20.7 s
New runtime:  75.7 s
Speedup: 3.6x
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Still rendering black.  why?&lt;/p&gt;

&lt;p&gt;Apparently this is an issue with OSX's AMD driver -- geometry shaders fail after returning from sleep.  Restarting program solves it.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Profiling v4&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Only 19 calls to &lt;code&gt;construct_attachment_covariance_3&lt;/code&gt;, down from ~500.&lt;/p&gt;

&lt;p&gt;78% bottleneck in matrix multiplication / inversion.&lt;/p&gt;

&lt;p&gt;Main problem is the 2500x2500 matrix from the main stem.  Other curves are in the 200 to 500 dimension range, and are fairly fast.&lt;/p&gt;

&lt;p&gt;Nystrom (2x) on curves with dimension greater than 1000 brings bottlenect from 18s to 11s, but as before, results are highly sensitive to the nystrom factor and using it without careful tuning / heursitics is risky.&lt;/p&gt;

&lt;p&gt;Probably expoiting the Markov nature of the curve GP  is the answer.  Linear runtime vs. cubic.&lt;/p&gt;

&lt;h2&gt;Markov within-curve sampling&lt;/h2&gt;

&lt;p&gt;break output indices into blocks.  For each block, get observation markov blanket.  Combine observed data and ancestor data into covarance matrix and data vector, then sample.&lt;/p&gt;

&lt;p&gt;Needed three covariance matrices: data vs. data, model vs. model, and model vs. data.&lt;/p&gt;

&lt;p&gt;Opportunity for optimizing construct_attachment_covariance_3 - sample all simpling-pair covariances at once.  289 total combinations (i.e. calls to eval_kernel) per dataset, could reduce to one.&lt;/p&gt;

&lt;p&gt;Seems like we can save computation in model covariance matrices by (a) exploiting the fact that it's always between points in the same view, and (b) only view changes between calls, spacial indices stay the same.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(A) compute K_star_star only once.&lt;/li&gt;
&lt;li&gt;(B) cache prior_K between constructions of K_star.&lt;/li&gt;
&lt;li&gt;(*) Cache &lt;code&gt;obj&lt;/code&gt;s and 'status' between calls to construct_attachment_covariance.  K_star reuses the lower triangular elements of K, K_star_star simply mirrors the upper-triangular  elements of K_star.&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Implementing...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Theoretical issues with the within-curve markov assumption.  If the markov blanket is chosen crudely, your samples could drift aimlessly until they reach observations in an unexpected place.  In other words, extrapolation, or sampling from weak data is a mistake.  Need a good criterion for when to stop takin on more evidence.&lt;/p&gt;
</description>
				<pubDate>Wed, 30 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/30/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/30/work-log</guid>
			</item>
		
			<item>
				<title>Ancestral Sampling from poserior (Markov-aware sampling)</title>
				<description>&lt;p&gt;Realization: nystrom may be tricky, because the same thinning factor probably won't be appropriate for all scenarios (e.g. short curves).&lt;/p&gt;

&lt;p&gt;Possible approach: always take first and last inidices of a curve; thin longer curves more aggressively.&lt;/p&gt;

&lt;p&gt;Better idea: use markov structure.&lt;/p&gt;

&lt;h2&gt;Exploiting Markov structure for Sampling plants &lt;/h2&gt;

&lt;p&gt;Main ideas: Use inheritence sampling to sample fully tree.  Two rules (1) Parent curve posterior should only depend on it's observed points and a small number of points on its children.  (2) Child curves depend also on the sampled parent points.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parent points&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The main question here is how many and which observed points of  child-curves should be included to compute posterior?  Heuristic approach: start with child points nearest to branch-point and work outward until information gain is negligible.&lt;/p&gt;

&lt;p&gt;Information gain is the change in entropy of the branch-point posterior, conditioned on the active set.  Entropy is proportional to the determinant of the covariance matrix.&lt;/p&gt;

&lt;p&gt;Let bp be the branch point, and I be the active set.&lt;/p&gt;

&lt;div&gt;
\[
Entropy(bp | I) \propto K(bp, bp) - K(bp, np) [K(np, np)  + \Sigma_y(np)]^{-1} K(np, bp)
\]
&lt;/div&gt;


&lt;p&gt;Where Sigma_y is the observation covariance.  We may choose to use the prior conditional entropy, which is a simpler stand-in for the posterior conditional entropy.&lt;/p&gt;

&lt;div&gt;
\[
Entropy(bp | I) \propto K(bp, bp) - K(bp, np) [K(np, np)]^{-1} K(np, bp)
\]
&lt;/div&gt;


&lt;p&gt;Note that the entropy is now only a function of the indices of the observations, not the observations themselves.  Thus, each observation only contribues one dimension to the matrix inversion, instead of three, making the computation 3&lt;sup&gt;3&lt;/sup&gt; times faster.  This is only sensible if the observation variance is relatively uniform between observations, which isn't necessarilly true, but it may be servicable heuristic if thresholds are set appropriately high.&lt;/p&gt;

&lt;p&gt;Opportunities exist to do rank-one updates to inverse, but probably not necessary, as the matrices should be small.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Child curves&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Note that child-curves can also be parent-curves, so the previous approach for determining the active set should be followed.&lt;/p&gt;

&lt;p&gt;We need to estimate the conditional prior probability for the curve, conditioned on its sampled parent.  If the sampled parent contains the branch point, bp, the conditional prior covariance is already available in the track's pre-computed prior_K field, and the mean is bp.  In general, bp isn't sampled, so we must use the nearby points on the parent to find the conditional prior of bp first and sample it explicitly.&lt;/p&gt;

&lt;p&gt;Let bp be a branch point, cp be the child point, and nbp be the &quot;neighbors&quot; of bp on the parent curve, i.e. the markov blanket of bp on the parent.&lt;/p&gt;

&lt;p&gt;Given bp, the child points are distributed by&lt;/p&gt;

&lt;div&gt;
\[
cp \mid bp \sim \mathcal{N}(bp, K_{prior})
\]
&lt;/div&gt;


&lt;p&gt;The branch point is distributed by&lt;/p&gt;

&lt;div&gt;
\[
bp \mid pnp \sim \mathcal{N}(\mu, \Sigma)
\]
&lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt;
\[
\mu = k(bp, nbp) k(nbp, nbp)^{-1} nbp \\
\Sigma = k(bp, bp) - k(bp, pnp) K(pnp, pnp)^{-1} K(pnp, bp)
\]
&lt;/div&gt;


&lt;p&gt;The a neighborhood size of four points around bp should be sufficient, since this model is piecewise cubic, but it may be worthwhile to experiment with more, since we aren't in bottleneck territory.&lt;/p&gt;

&lt;p&gt;Now that we have conditional priors, we can construct the conditional posterior using the standard formula.&lt;/p&gt;

&lt;h2&gt;Mixing noisy and noise-free data in Posterior&lt;/h2&gt;

&lt;p&gt;Ran into a snag when trying to incorporate the noise-free conditional values that arise during ancestral sampling.  Derived an elegant solution; see the &lt;a href=&quot;/ksimek/research/2013/10/29/reference&quot;&gt;writeup here&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Implementing markov-aware pixel likelihood&lt;/h2&gt;

&lt;p&gt;For now, using a constant number of points on parent, instead of using the entropy method above.  Could add later; basic approach: precompute all covariances and add/remove one at a time.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Seems to be working, but slow as hell (slower than nystrom method). profiling&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Construct_attachment_covariance_3 is called 486 times!&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Construct non view-specific matrices outside of view loop.&lt;/li&gt;
&lt;li&gt;nystrom for large curves?&lt;/li&gt;
&lt;li&gt;do all covariances at once?  saves the computation of sibling and parent objecst?&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;implemented a first attempt at speedup; producing faulty results.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish optimized ancestral sampling&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Tue, 29 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/29/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/29/work-log</guid>
			</item>
		
			<item>
				<title>Mixing Noisy and Noise-free values in GP Posterior</title>
				<description>&lt;p&gt;When doing ancestral sampling of the posterior, each curve's conditional posterior depends on (a) it's relevant (noisy) observation and (b) the sampled (noise-free) values of the parent.  We can incorporate both in the posterior by treating the noise-free values as observations with zero variance in the likelihood.&lt;/p&gt;

&lt;p&gt;In practice, theres a minor issue implementing this.  Recall that because 3D observations are degenerate in one direction (namely the backprojection direction), we prefer to work with the precision matrix, \(\Lambda\), instead of a covariance matrix.  Under this representation, noise-free values have infinite precision, so operations on \(\Lambda\) are invalid.  Instead, we take a hybrid appraoch, using both precisions and covariances.&lt;/p&gt;

&lt;p&gt;Recall the standard formulation for the posterior mean:&lt;/p&gt;

&lt;div&gt;
\[
    \mu = K_* \left[ K + \Sigma \right]^{-1} y
\]
&lt;/div&gt;


&lt;p&gt;Without loss of generality, assume zero-noise observations appear after noisy observations.  We can rewrite the posterior in terms of the precision matrix \(\Lambda\) of the noisy values:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \mu &amp;= K_* \left[ K + \left( 
                \begin{array}{cc}
                    \Lambda^{-1} &amp; 0 \\
                    0 &amp; 0
                \end{array}
            \right) \right]^{-1} y \\
        &amp;= K_* \left[
        
            \left( 
                \begin{array}{cc}
                    \Lambda &amp; 0 \\
                    0 &amp; I
                \end{array}
             \right) 
        K + 
            \left( 
                \begin{array}{cc}
                    I &amp; 0 \\
                    0 &amp; 0
                \end{array}
             \right) 

            \right]^{-1} 
            \left( 
                \begin{array}{cc}
                    \Lambda &amp; 0 \\
                    0 &amp; I
                \end{array}
             \right) 
            y \\
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;We can implement this by slightly modifying our code for the precision-matrix formulation of posterior.  First, give all noise-free values a precision of 1.0 in \(\Lambda\), and then modify the \(I\) inside the parentheses by zeroing-out elements corresponding to noise-free values.  Using prime symbol to denote the modified matricies, the result is&lt;/p&gt;

&lt;div&gt;
\[
    \mu = K_* \left[ \Lambda' K + I' \right]^{-1} \Lambda' y
\]
&lt;/div&gt;


&lt;p&gt;The expression for posterior covariance is derived in the same way, and has similar form.&lt;/p&gt;
</description>
				<pubDate>Tue, 29 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/29/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/29/reference</guid>
			</item>
		
			<item>
				<title>Implementing Nystrom method; bugs in posterior sampling code</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Re-ran with method 2.  Still getting negative eigenvalues.&lt;/p&gt;

&lt;p&gt;Possibly non-symmetry issue.&lt;/p&gt;

&lt;p&gt;Change gears -- work on fast implementation, then resume debugging.&lt;/p&gt;

&lt;h2&gt;Reduced rank approximation of data-covariance&lt;/h2&gt;

&lt;div&gt;
&lt;p&gt;
Goal: approximate \(\left(K + \Sigma_D \right)^{-1}\), using a low-rank approximation of K;  where \(\Sigma_D\) is the likelihood covariance.
&lt;/p&gt;&lt;p&gt;
In our case \(\Sigma_D\) has infinite covariance, so this inverse doesn't exist. However, if we work with precision matrix \(\Lambda = \Sigma_D^{-1}\), and use the decomposition \( S' S = \Lambda \) we can replace this expression with the equivalent

\[
    S' \left( S K S' + I \right)^{-1} S
\]

Even though S is rank-deficient, the inverse in this expression does exist, thanks to the finite positive values being added to the diagonal.  
&lt;/p&gt;&lt;p&gt;
We approximate the above expression as follows.  Let \(K\) be an \(N\) by \(N\) matrix. We can take the eigendecomposition \(K = V D V'\) and approximate it with \(\tilde K = \tilde V \tilde D \tilde V'\), where \(\tilde V\) and \(\tilde D\) consist of the first \(n\) eigenvalues and eigenvectors of \(K\) respectively.   We can use this low-rank approximation with the [Woodbury matrix identity](http://en.wikipedia.org/wiki/Woodbury_matrix_identity) to approximate the above inverse in \(O(n^3)\) time, rather than \(O(N^3)\).  The Woodbury identity is
        
\[
    (A + U C V )^{-1} = A^{-1} - A^{-1} U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}
\]

Setting \(A = I\), \(U = V' = S \tilde V \) and \(C = \tilde D \), we get:
    
\[
    \left( S K S' + I \right)^{-1} = I - S \tilde V (\tilde D^{-1} + \tilde V' S' S \tilde V)^{-1} \tilde V' S'
\]
&lt;/p&gt;&lt;p&gt;
It remains to find \(\tilde V\) and \(\tilde D\) efficiently.  Naive eigenvalue decomposition takes \(O(N^3)\) time, which isn't any better than direct inversion.  Sections 4.3.2 and 8.1 of Williams and Rasmussen show how to approximate \(n\) eigenfunctions and eigenvalues from \(n\) data points in \(O(n^3)\) time.  Eigenvectors \(\tilde V\) arise by evaluating the approximate eigenfunctions at the appropriate indices.
&lt;/p&gt;
&lt;p&gt;

Substitutuing back into the original expression by surrounding with (\(S'\) and \(S\)), we get the final expression:
    
\[
\begin{align}
    \approx &amp; S' \left ( I - S \tilde V (\tilde D^{-1} + \tilde V' S' S \tilde V)^{-1} \tilde V' S' \right) S \\
    =&amp;\Lambda - \Lambda \tilde V (\tilde D^{-1} + \tilde V' \Lambda \tilde V)^{-1} \tilde V' \Lambda
    \end{align}

\]
&lt;/p&gt;
&lt;/div&gt;


&lt;h2&gt;Work log&lt;/h2&gt;

&lt;p&gt;Implemented two different implementations of &quot;nystrom solve&quot; (&lt;code&gt;tools/nystrom_solve.m&lt;/code&gt;).  Crude testing shows big speedup, but stalling on a big eigenvalue decomposition.  Replacing with a call to &quot;chol&quot;, waiting on visual results.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&quot;chol&quot; gives much faster results. (no need to symmetrize)  But results are junk -- blank screen.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, bug in call to chol; fixed.&lt;/p&gt;

&lt;p&gt;still getting black.  Posterior covariance is not positive definite; eigenvalues are negative.&lt;/p&gt;

&lt;p&gt;When I force covariance to zero (to view the mean value), it's still in the ballpark but wonky (same as before).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Clue&lt;/strong&gt;:   When re-using (thinned) input covariance matrix and indices, result looks sensible.  Clearly, we have a problem with how K_star is computed (and likelihood K_star_star, too).&lt;/p&gt;

&lt;p&gt;Comparing different K_star's...&lt;/p&gt;

&lt;p&gt;Looks like on-diagonal elements differ, off-diagonals are okay.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Got it!  Wacv was trained using no-perturb model, but is being tested with OU-perturb.&lt;/p&gt;

&lt;p&gt;TODO: edit get_all_Wacv and/or get_wacv_result to receive a model-type... done.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;reducing test to block (1,1)&lt;/p&gt;

&lt;p&gt;possible sources of disparity: indices, function&lt;/p&gt;

&lt;p&gt;checked indices -- same.&lt;/p&gt;

&lt;p&gt;check params -- model-type doesnt match.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;First view now looks good in matlab.&lt;/p&gt;

&lt;p&gt;Running all views in likelihood_server.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Means look great!.  Now on to random perturbed.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Cholesky claims posterior covariance isn't positive definite.&lt;/p&gt;

&lt;p&gt;Possibly a bug in construct_attachment_covariance_3?&lt;/p&gt;

&lt;p&gt;Re-running with K_star_star from original covariance matrix.  Still gives &quot;not positive definite&quot; errors.&lt;/p&gt;

&lt;p&gt;Resorting to eigenvalue decomposition method.&lt;/p&gt;

&lt;p&gt;Results look good!  perturbation from mode is &lt;em&gt;TINY&lt;/em&gt;, at least in the ground truthed WACV dataset.  If this is true for hyptoheses in the full system, we can probably get away with simply taking the mean and saving significant computation of the covariacne matrix.&lt;/p&gt;

&lt;h2&gt;Tuning Nystrom factor&lt;/h2&gt;

&lt;p&gt;nystrom factor: 50&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-28-nystrom_50.png&quot; alt=&quot;nystrom factor = 50&quot; /&gt;&lt;/p&gt;

&lt;p&gt;nystrom factor: 20&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-28-nystrom_20.png&quot; alt=&quot;nystrom factor = 20&quot; /&gt;&lt;/p&gt;

&lt;p&gt;nystrom factor: 10&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-28-nystrom_10.png&quot; alt=&quot;nystrom factor = 10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nystrom factor apparently has a huge affect on reconstruction quality.  Possibly better choice of thinned index set (compared to uniform sampling) will improve sensitivity, but a brief attempt at this (grouping xyz indices ) had net negative effect.&lt;/p&gt;

&lt;p&gt;Needs more thought, possibly should resort to subset of data (SD) or subset of regressors (SR) method, or exploit markov structure.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Nystrom method is implemented and provides huge speedups without obvious in rendered results.&lt;/li&gt;
&lt;li&gt;Spent many hours debugging issues with covariance matrix.  Turned out to not be bug in theory or implementation of the math, but from an unexpected hard-coded value in wacvin wacv results.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Nystrom thinning: 10
ll2 spacing: 3
Unclear&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;what is best thinning amount for Nystrom method?&lt;/li&gt;
&lt;li&gt;what is best spacing for output index set&lt;/li&gt;
&lt;li&gt;are perturbations always virtually nil?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;refactor construct_attachment_covariance_* into a single implementation.&lt;/li&gt;
&lt;li&gt;optimize &lt;code&gt;curve_tree_ml_2.m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 28 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/28/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/28/work-log</guid>
			</item>
		
	</channel>
</rss>
