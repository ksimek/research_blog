<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Optimizing &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Attempting to use sparsity to speed up &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First attempt&lt;/strong&gt;: try building only the relevant elements of the prior matrix, K.&lt;br/&gt;
&lt;strong&gt;Resut&lt;/strong&gt;: Gains in multiplication are lost in construction of K.  Insignificant speedup.  Rolling back.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second attempt&lt;/strong&gt;: convert block-diagonal matrix &lt;em&gt;S&lt;/em&gt; to a sparse matrix.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Significant gains in multiplication; tolerable losses when constructing &lt;em&gt;S&lt;/em&gt;.  ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third Attempt&lt;/strong&gt;: Convert the individual blocks &lt;em&gt;S_i&lt;/em&gt; to sparse, &lt;em&gt;then&lt;/em&gt; construct &lt;em&gt;S&lt;/em&gt; from them.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Further speedup of ~2x&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fourth Attempt&lt;/strong&gt;:  Change &lt;code&gt;(eye(size(K)) + S * K * S')&lt;/code&gt; to &lt;code&gt;(speye(size(K)) + S * K * S')&lt;/code&gt; ; i.e. changing &lt;code&gt;eye&lt;/code&gt; to &lt;code&gt;speye&lt;/code&gt;.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: moderate speedup, ~1.5x.&lt;/p&gt;

&lt;p&gt;I think we've squeezed all we can from this function.  It's now 10x faster than the naive method on a problem with 4000-dimensions.&lt;/p&gt;

&lt;h2&gt;Results: &lt;/h2&gt;

&lt;p&gt;Running &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6362.9 ms)
Old ML: 207.041742

Computing marginal likelihood (new way, legacy correspondence)...
Done. (7655.2 ms)
New ML (Legacy corr): 207.700616

Computing marginal likelihood (new way)...
Done. (6537.7 ms)
New ML (New corr): 793.585038

Computing marginal likelihood (new way, no MIL)...
Done. (6330.3 ms)
New ML (New corr, no MIL): 793.600835

Computing marginal likelihood (direct method)...
Done. (667.9 ms)  &amp;lt;-------------------  10x speedup over previous
New ML (direct method): 793.300407 &amp;lt;--  0.04% error!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note speedup and negligible error compared the previous method.&lt;/p&gt;

&lt;h2&gt;General Observations Regarding &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;I've noticed that &lt;code&gt;markov_order&lt;/code&gt; must be much larger than I expected to avoid approximation error.&lt;/p&gt;

&lt;p&gt;Recall in &lt;code&gt;curve_ml2.m&lt;/code&gt;, we use the Markov assumption to break-down the prior covariance, and then combine them with the likelihood cliques.  In that case, we could use a Markov order between 2 and 5 without significant approximation error.&lt;/p&gt;

&lt;p&gt;In &lt;code&gt;curve_ml5.m&lt;/code&gt;, using a Markov order less than 100 results in unacceptable error.&lt;/p&gt;

&lt;p&gt;There are two reasons for this.  First, we're decomposing the marginal likelihood Gaussian, not the prior.  The prior is explicitly Markovian, whereas the marginal likelihood is not.  The ML Guassian adds extra uncertainty to every point, which means we need to consider more nearby points to avoid erroneous conclusions.&lt;/p&gt;

&lt;p&gt;Second, because the new approach creates a distinct index set for each view, indices are often repeated (multiple views of the same point) and considering them doesn't tell you anything about what direction the curve is heading.&lt;/p&gt;

&lt;p&gt;The first point will likely be mitigated when we start using the new curve model, which will allow the likelihood variance to decrease dramatically.  However, the markov assumption in the prior is less likely to hold under these models, so some experimentation will be needed.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The code for the new likelihood, &lt;code&gt;curve_ml5.m&lt;/code&gt; is now stable.  It is fast and accurate in its current implementation, assuming a reasonable value for &lt;code&gt;markov_order&lt;/code&gt;.&lt;/p&gt;
</description>
				<pubDate>Wed, 17 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing new direct method for marginal likelihood.&lt;/h2&gt;

&lt;p&gt;Implemented in &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After initial attempt, the new method produced inconsistent results.&lt;/p&gt;

&lt;p&gt;Re-derived the result two more ways and the math comes out the same.  The theory looks right.&lt;/p&gt;

&lt;p&gt;Finally found the bug -- was computing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;instead of&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = 2 * log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the Cholesky decomposition is the square-root of the matrix Sigma, I was forgetting to double it's determinant to get the determinant of Sigma.&lt;/p&gt;

&lt;p&gt;Now getting the correct results; now time to make it fast.&lt;/p&gt;

&lt;h2&gt;Optimizing direct method for ML&lt;/h2&gt;

&lt;p&gt;Need to get O(n) runtime instead of O(n&lt;sup&gt;3).&lt;/sup&gt;  Will use existing code for markov-decomposed PDF evaluation.&lt;/p&gt;

&lt;p&gt;Bottleneck is 100% the Cholesky decomposition.  The direct method doesn't remove redundant dimensions, so it's slower than &lt;code&gt;curve_ml4.m&lt;/code&gt;, which does.  Recall that this fact makes &lt;code&gt;curvE_ml4.m&lt;/code&gt; not general enough to use the new foreground models with.&lt;/p&gt;

&lt;p&gt;...........&lt;/p&gt;

&lt;p&gt;After some investagation, the previous statement appears to be untrue.  The bottlenect is dense matrix multiplication, which is fixed by using sparse matrices.&lt;/p&gt;

&lt;p&gt;Now, cholesky &lt;strong&gt;is&lt;/strong&gt; the bottlenect, but not as huge as before.  The markov decompose method only gives a ~30% speedup at best, while introducing ~2.5% error and significant extra complexity. Is it worth it?&lt;/p&gt;
</description>
				<pubDate>Tue, 16 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log</guid>
			</item>
		
			<item>
				<title>Direct Evaluation of the Marginal Likelihood</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Previously, we've always used hacks to evaluate the marginal likelihood, because evaluating it directly was apparently impossible because of an infinite normalization constant.  I've discovered that this isn't actually true; the infinite normalization constant arose due to our approximation of the likelihood.  The trick in correctly computing the ML is to replace the usual normalization constant with a corrected one.&lt;/p&gt;

&lt;p&gt;Our old method required knowledge of the maximum posterior solution, which was costly to compute.  The new approach is straightforward to describe, more accurate than our previous method, and can be evaluated in linear time by exploiting its Markovian structure.&lt;/p&gt;

&lt;p&gt;Below is a rough-draft writeup of my derivation of the marginal likelihood function.&lt;/p&gt;

&lt;h2&gt;Direct Evaluation of the Marginal likelihood&lt;/h2&gt;

&lt;p&gt;The 3D marginal likelihood function arises as the sum of a 3D curve process and a 3D perturbation process.  Both are gaussian (the second is approximate), this the result is a Gaussian function, the convolution of two Gaussians.  However, the perturbation process has infinite variance in the direction of backprojection, which arises from the fact that perturbation actually occurs in 2D, we are just backprojecting it to 3D for tractibility.  In other words, the ML isn't a distribution in 3D, it's a distribution in 2D, and we need to determine the appropriate normalization constant for the 3D function to it generates 2D ML densities.&lt;/p&gt;

&lt;p&gt;The likelihood function is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y | x) &amp;= \prod p(y_i | x_i) \\
         &amp;= \prod \mathcal{N}(y_i; \rho_I(x_i), \sigma_n^2 I) \\
         &amp;\approx \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \mathcal{G}(Y_i; x_i, S_i^{-1}) \\
         &amp;= \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \exp\{(Y_i - x_i)^\top S_i (Y_i - x_i)\} \\
         &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}^{2n}} \exp\{(Y - x)^\top S (Y - x)\} \\
         &amp;= \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\}
\end{align}
\]&lt;/div&gt;


&lt;p&gt;Where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\rho_i(x)\) is the projection of x into the I-th view,&lt;/li&gt;
&lt;li&gt;\(Y_i\) is the estimated backprojected position of observation \(y_i\),&lt;/li&gt;
&lt;li&gt;\(S_i\) is the curvature of the 3d likelihood function w.r.t. \(x_i\) evaluated at $Y_i$,&lt;/li&gt;
&lt;li&gt;\(Y\), \(S\) and \(x\) are the concatenation of \(Y_i\), and \(x\)&lt;/li&gt;
&lt;li&gt;\(S\) is the block-diagonal matrix of \(S_i\)'s&lt;/li&gt;
&lt;li&gt;\(\mathcal{G}\) is a Gaussian function (an unnormalized normal distribution).&lt;/li&gt;
&lt;li&gt;\(Z_l\) is a normalization constant.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We have transformed the likelihood into a log-linear function of x, by rewriting the PDF in terms of only 3D entities.  Note that \(S_i\) has zero curvature in the backprojection direction, resulting in infinite variance in the Gaussian function.   Also note that the normalization constant isn't the standard one for a 3D gaussian distribution, because this isn't a distribution over x.  The normalization constant is chosen so the approximate likelihood in 3D agrees with the exact 2D likelihood when perturbation is zero (i.e. when \(Y_i\) lies anywhere on the backprojection line).&lt;/p&gt;

&lt;p&gt;This approximation ignores the nonlinearity of projection; the likelihood function as a function of \(x_i\) would actually look like a cone whose axis-perpendicular slices are Gaussians, whereas our function is cylinder-shaped.  In practice, the approximation error has minimal effect on the marginal likelihood computation, assuming $Y_i$ is a good estimate of the posterior depth, and the posterior is reasonably peaked.  (A graphic to illustrate would be good here).&lt;/p&gt;

&lt;p&gt;The prior is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(x) &amp;= \mathcal{N}(x; \mathbf{0}, K) \\
     &amp;= \frac{1}{Z_p} \exp\{x^\top K^{-1} x\}
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;where \(K\) is the covariance matrix arising from the Gaussian process kernel, and Z_p is the standard Gaussian normalization constant.&lt;/p&gt;

&lt;p&gt;The marginal likelihood is defined as&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \int p(y|x) p(x) dx \\
     &amp;= \int \left ( \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\} \right ) \left ( \frac{1}{Z} \exp\{x^\top K^{-1} x\} \right ) \\
     &amp;= \frac{1}{Z_l Z_p}\int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;The expression within the integral is the convolution of two unnormalized zero-mean Gaussians, so the integral is a zero-mean Gaussian whose covariance is the sum of the inputs' covariances.  If the input Gaussians &lt;strong&gt;were&lt;/strong&gt; properly normalized, the normalization constant would be \(1/Z\);  since they are not, the normalization constant is \((Z_1 Z_2 / Z)\), where \(1/Z_1\) and \(1/Z_2\) are the would-be normalization constants for the first and second Gaussian, respectively.  Note that the second Gaussian's normalization constant is \(1/Z_p\), so this results in the cancellation we see below&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \frac{1}{Z_l Z_p} \int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     &amp;= \frac{1}{Z_l Z_p} \left ( \frac{Z_{l,3d} Z_p}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \} \right ) \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \}
     \end{align}
\]&lt;/div&gt;




&lt;div&gt;
    Where \( Z_{l,3d} \) is the would-be 3D normalization constant for our likelihood Gaussian.  Since \(S\) is rank-deficient, the normalization constants based on \(S^{-1}\) will be infinite (i.e. \(Z_{l,3d}\) and \(Z\)).  However, the terms involving determinants of \(S^{-1}\) should cancel in the ratio, resulting in a gaussian with infinite variance, but non-infinite normalization constant. **(Need to show this mathematically next time)** 
    &lt;/div&gt;

</description>
				<pubDate>Fri, 12 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Markov-decomposed ML&lt;/h2&gt;

&lt;p&gt;The original plan was to markov-decompose the prior and posterior, but soon remembered that I also need to compute the mean of the posterior, which isn't as straighforward to compute.&lt;/p&gt;

&lt;p&gt;After some deliberation and whiteboarding, I decided to implement Bishop's forward/backward algorithm to simultaneously marginalize and compute the maximum.  Will test this against the clique-tree implementation to ensure correctness.&lt;/p&gt;

&lt;p&gt;........&lt;/p&gt;

&lt;p&gt;During testing I realized a fundamental problem with the current likelihood: there is no obvious way to consistently handle the &quot;redundant&quot; dimensions that arise from duplicated indices.  In some kernels, these are handled naturally, namely the kernels that treat different views as different indices.  The old kernel, however, does not, and the duplicated indices result in degenerate posterior distributions unless handled using hacks.&lt;/p&gt;

&lt;p&gt;We could handle this on a kernel-by-kernel basis, but it would be unmaintainable, and could hinder further research into new kernels.  It also isn't clear that the hacks I have in mind would actually give correct results.&lt;/p&gt;

&lt;p&gt;This problem is inherent to the &quot;candidates estimator&quot; of the marginal likelihood, because it involves a ratio of the posterior and prior, both of which are degenerate in these cases.&lt;/p&gt;

&lt;p&gt;The actual marginal likelihood function has no such degeneracies, because each observation is independent, given the underlying curve.  However, until recently it was unclear how to evaluate the marginal likelihood using the approximated likelihood function.  The approximate likelihood has a rank-deficient precision matrix, and its normalization constant is non-standard due to the transformation from 2D to 3D.  However, I think I've developed a way to evaluate it, which I describe in the next article.&lt;/p&gt;
</description>
				<pubDate>Thu, 11 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Cleanup and speedup of curve_ml.&lt;/p&gt;

&lt;h2&gt;Task 1: Version without matrix-inversion lemma&lt;/h2&gt;

&lt;p&gt;Making a simpler version that doesn't exploit the matrix inversion lemma (MIL).&lt;/p&gt;

&lt;p&gt;The benefits of the MIL are likely mitigated by markov-decomposition (next task), and MIL
complicates the code and API design.&lt;/p&gt;

&lt;p&gt;Results (small test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (108.2 ms)
Old ML: 62.183969
Computing marginal likelihood (new way, legacy correspondence)...
Done. (120.8 ms)
New ML (Legacy corr): 63.908784
Computing marginal likelihood (new way)...
Done. (88.9 ms)
New ML (New corr): 72.304718           &amp;lt;----------  OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (97.3 ms)
New ML (New corr, no MIL): 72.304666   &amp;lt;----------  NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference is about 0.00007% -- seems good.  Slightly slower, possibly just noise.&lt;/p&gt;

&lt;p&gt;Results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6447.3 ms)
Old ML: 207.324848
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7665.1 ms)
New ML (Legacy corr): 209.199676
Computing marginal likelihood (new way)...
Done. (6205.6 ms)
New ML (New corr): 457529.406731  &amp;lt;---------- ?????
Computing marginal likelihood (new way, no MIL)...
Done. (5946.7 ms)
New ML (New corr, no MIL): 778723.327102  &amp;lt;---------- ?????
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both versions give CRAZY high results; significantly different from the &lt;a href=&quot;/ksimek/research/2013/07/05/work-log/&quot;&gt;same test on Friday&lt;/a&gt;.  Need to investigate...&lt;/p&gt;

&lt;h2&gt;Investigating new results&lt;/h2&gt;

&lt;p&gt;Observations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All results differ from friday, including old ML (slightly)&lt;/li&gt;
&lt;li&gt;New ML using legacy correspondence is still reasonable.&lt;/li&gt;
&lt;li&gt;Both crazy results are using new correspondence.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Look at pieces (log posterior, likelihood, prior)&lt;/li&gt;
&lt;li&gt;Inspect new correspondence&lt;/li&gt;
&lt;li&gt;Think about what changed since Friday?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:10:19 PM&lt;/p&gt;

&lt;p&gt;Ran old test, &lt;code&gt;test_ml_end_to_end.m&lt;/code&gt; and compared to old results in new test, &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt;.  Old test still gives old results, so I'll compare the two tests to determine what has changed.&lt;/p&gt;

&lt;p&gt;12:12:48 PM&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;mix&lt;/code&gt; parameter was set to 0.5 instead of 0.0.  I had forgotten I changed it at the end if Friday.&lt;/p&gt;

&lt;p&gt;Now getting &quot;good&quot; results again.  It raises a question that needs to be answered: why is ML sooo sensitive to evalaution position when using new correspondence?&lt;/p&gt;

&lt;h2&gt;Resuming&lt;/h2&gt;

&lt;p&gt;New results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6009.5 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7286.3 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6146.6 ms)
New ML (New corr): 793.585038             &amp;lt;---------- OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (6008.3 ms)
New ML (New corr, no MIL): 793.600835     &amp;lt;---------- NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.002% error, slightly faster.  Good.&lt;/p&gt;

&lt;h2&gt;Investigating Anomaly in new ML &lt;/h2&gt;

&lt;p&gt;So why is new ML so sensitive to where it is evaluated?&lt;/p&gt;

&lt;p&gt;Note that it's only extreme when using new correspondence.  Old correspondence is okay.&lt;/p&gt;

&lt;p&gt;Inspect difference between max likelihood and max posterior.  Maybe it's just more extreme than with old correspondence.&lt;/p&gt;

&lt;p&gt;12:20:59 PM&lt;/p&gt;

&lt;p&gt;Idea: bug in non-0.0 case?  Nope.&lt;/p&gt;

&lt;p&gt;Tried mixes: 0.0, 0.01, 0.1. Steady and dramatic increase in ML for new correspondence.  Old correspondence is nearly constant.  What is different between these correspodnences (other than the correspondences themselves).&lt;/p&gt;

&lt;p&gt;12:57:08 PM&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;: The max likelihood in the new correspondences is waaaaay into the tails of the prior.&lt;/p&gt;

&lt;p&gt;Since no real triangulation is done to ensure agreement between views, the maximum likelihood triangulation is very rough  (infinite likelihood variance allows this to happen without penalty).  This places the curve far into the tails of the prior, where evaluation is highly unstable.  Both the prior and likelihood are then extremely low (-1e-8 in log space), which should cancel, but don't due to numerical instability.  Hence, marginal likelihoods with huge magnitude.&lt;/p&gt;

&lt;p&gt;This is great news, since it means everything is working mostly as expected.  The numerical instability issue is easilly solved by always evaluating at the posterior, not the maximum likelihood.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The version of the new ML that doesn't use the matrix inversion lemma is accurate, so we can proceed to the markov-decomposed version next.&lt;/p&gt;
</description>
				<pubDate>Mon, 08 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still trying to troubleshoot the difference between old and new likelihood implementations.&lt;/p&gt;

&lt;p&gt;Reviewing what is known&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prior is same&lt;/li&gt;
&lt;li&gt;Likelihood is same&lt;/li&gt;
&lt;li&gt;Posterior evaluates same in two different ways.&lt;/li&gt;
&lt;li&gt;mean curve is different&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Try using mean curve from old alg. in new alg.&lt;/p&gt;

&lt;p&gt;10:13:35 AM&lt;/p&gt;

&lt;p&gt;SOLVED.&lt;/p&gt;

&lt;p&gt;Yesterday I noticed that setting mix to 1.0 give decent results, but 0.0 was terrible.  Now I know why: 0.0 is a special case, in which a simplified calculation is used.  There was a bug in that special case, where the normalization constant didn't take into account the redundant dimensions that we eliminated.&lt;/p&gt;

&lt;h2&gt;ML Test&lt;/h2&gt;

&lt;p&gt;11:02:29 AM&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ml_test_end_to_end_2.m&lt;/code&gt; is now running&lt;/p&gt;

&lt;p&gt;Running on subset of correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...  
Done. (96.9 ms)  
Old ML: 62.176782

Computing marginal likelihood (new way, legacy correspondence)...  
Done. (112.3 ms)  
New ML (Legacy corr): 63.859940

Computing marginal likelihood (new way)...  
Done. (83.5 ms)  
New ML (New corr): 71.964026
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised the magnitude of difference between &quot;Old ML&quot; and &quot;New ML (Legacy corr)&quot;.  I guess the posterior variance is larger in this case, which means the posterior means can differ more.&lt;/p&gt;

&lt;p&gt;Running on full correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...
Done. (6400.1 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8328.4 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6349.2 ms)
New ML (New corr): 793.585038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that &quot;New ML (New Corr)&quot; is dramatiacally higher than the old and legacy ML's.  I suppose I should have been expected this, since this entire approach &lt;a href=&quot;/ksimek/research/2013/06/28/work-summary/&quot;&gt;was motivated by the terrible correspondences arising from the legacy correspondence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It's nice to see the ML for good correspondences getting even better.  Hopefully we'll see even more improvement as we try new curve models.&lt;/p&gt;

&lt;p&gt;Taking a break for lunch...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Next steps:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;fast ml_curve3

&lt;ul&gt;
&lt;li&gt;linear eval&lt;/li&gt;
&lt;li&gt;try without matrix inversion lemma&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;try new models (check that they're nonsingular; don't crash)&lt;/li&gt;
&lt;li&gt;set up training framework&lt;/li&gt;
&lt;li&gt;consider version without inversion lemma&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Fri, 05 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Continuing debugging of new likelihood&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Test:&lt;/strong&gt; &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;07:11:52 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Trouble with NaN error in &lt;code&gt;correspondence/corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;08:22:43 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Narrowed it down to gap-handling code-branch.&lt;/p&gt;

&lt;p&gt;08:53:34 AM&lt;/p&gt;

&lt;p&gt;Fixed. Arose from a nasty indexing scheme in the original &lt;code&gt;clean_correspondence.m&lt;/code&gt;, which I didn't handle well when adapting for &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;09:29:35 AM&lt;/p&gt;

&lt;p&gt;Nonsingular prior covariance matrix causing crash.  Apparently caused by same index appearing multiple times.&lt;/p&gt;

&lt;p&gt;In the past, we always merged such points.&lt;/p&gt;

&lt;p&gt;The new idea is different views of the same point would make the posterior covariance singular.  Either this isn't true, or there's a bug...&lt;/p&gt;

&lt;p&gt;Debugging this will take some serious coding and math brainpower and I fell debugging fatigue coming on.  Taking a brief break to refresh, then will tackle...&lt;/p&gt;

&lt;p&gt;11:56:21 AM&lt;/p&gt;

&lt;p&gt;Using a simple toy example, I (partially) confirmed my intuition that a degenerate prior matrix is okay, as long as its nullspace is spanned by the likelihood matrix.&lt;/p&gt;

&lt;p&gt;In the context of this bug, it means that two points sharing the same index value is fine as long as they arose from different views (assuming non-degenerate camera configuration, which is true in this case).&lt;/p&gt;

&lt;p&gt;Possible causes&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Numerical instability -- near-singular matrix.&lt;/li&gt;
&lt;li&gt;Error in Linear Algebra logic (seems unlikely, same code worked in old likelihood)&lt;/li&gt;
&lt;li&gt;Some bug earlier in the pipeline, causing invalid matrices here.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I'm guessing its 3; if not that, then maybe 2.&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;inspect the shared indices -- same view?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:48:49 PM&lt;/p&gt;

&lt;p&gt;Reversing my previous stance:  Degenerate prior with shared index will always result in a degenerate posterior.  The prior has two variables that are 100% correlated, and the posterior will also be 100% correlated.  This implies a degenerate covariance matrix.&lt;/p&gt;

&lt;p&gt;In such a case, the posterior's implied dimensionality is lower than it's apparent dimensionality.  The prior and posterior will be degenerate in the same way; this symmetry is aesthetically appealing, because the Candidates estimator for the marginal likelihood involves their ratio.  This suggests that eliminating the redundant dimensions for both pdfs is a sensible thing to do.&lt;/p&gt;

&lt;p&gt;How to resolve this?  Possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detect redundant indices early, eliminate them in the prior and posterior.  USe bookkeeping to remember which observations refer to the same model point.&lt;/li&gt;
&lt;li&gt;Handle reduntancies later, after degenerate posterior is formed, before evaluating posterior or prior .  The mean will automatically contain redundancies, which will avoid bookkeeping when comparing the mean to the data points (when &lt;code&gt;mix&lt;/code&gt; is != 0).&lt;/li&gt;
&lt;li&gt;Try to determine how identical indices arose in the first place.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Decided on 2 for now.  In retrospect, approach 1 may have had slightly less code and faster.&lt;/p&gt;

&lt;p&gt;01:45:00 PM&lt;/p&gt;

&lt;p&gt;ML is now running on legacy correspondence.  Results are significantly higher than the legacy ML algorithm (~ 2.25 times).  Not yet sure why yet.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6665.8 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8853.1 ms)
New ML (Legacy corr): 439.273126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Noticible slow-down, probably because of extra indices arising from not merging duplicate observations.&lt;/p&gt;

&lt;p&gt;possible causes of discrepancy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In old code, re-triangulation used simple averaging, not accounting for differing precisions of the points&lt;/li&gt;
&lt;li&gt;index sets differ between old and new code.  (update: checked, they are same)&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Plotting means for both to view differences...&lt;/p&gt;

&lt;p&gt;Investigation into ML discrepancy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Posterior means are similar, but obviously differ.&lt;/li&gt;
&lt;li&gt;both means are very non-smooth (need to re-address the smoothness issue)&lt;/li&gt;
&lt;li&gt;new mean has signficantly more points (~7%, 80 pts).  Could this alone affect ML?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;almost all difference is in posterior&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Maybe posterior kernel is different?  but prior is same
It's just the data covariance thats different.
but why??  There's more of them.&lt;/p&gt;

&lt;p&gt;New ml changes significantly when evaluation point changes.  This implies that the posterior curvature differs from the curvature of the joint distribution, which shouldn't happen.  A small amount of difference can arise naturally due to the approximation of the likelihood, but this is too large to be explained in that way.  Most noticibly, the new results look very similar to the old ones &lt;strong&gt;when evaluating at the likelihood mean&lt;/strong&gt;.  The largest difference occurs when evaluating at the posterior mean.  Not clear yet what conclusions to draw from this, or if this is just a coincidence.&lt;/p&gt;

&lt;p&gt;Maybe likelihood curvatures are differing.&lt;/p&gt;

&lt;p&gt;07:35:28 PM&lt;/p&gt;

&lt;p&gt;Was able to get identical likelihood value using matrix-form as I did with the original 2D geometric error form.  Seems like the precision matrices are good.&lt;/p&gt;

&lt;p&gt;Priors appear good too, since it evaluates to the same value as the reference implementation.&lt;/p&gt;

&lt;h2&gt;Reference implementation for new ML&lt;/h2&gt;

&lt;p&gt;Idea: direct implementation for quadform, but with hacked nomralization constant&lt;/p&gt;

&lt;h2&gt;In progress&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnosing errors in new likelihood &lt;code&gt;curve_ml_3.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;getting &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt; running.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 04 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Implementing covariance functions&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood&lt;/li&gt;
&lt;li&gt;Implement fast generalized marginal likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implement generalized marginal likelihood&lt;/h2&gt;

&lt;p&gt;Rewriting &lt;code&gt;curve_ml3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Based on &lt;code&gt;curve_ml.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Kernel now takes two sets of indices, curve-index and view-index.&lt;/p&gt;

&lt;p&gt;Kernel is now passed-in as a lambda.&lt;/p&gt;

&lt;p&gt;Need to decide whether to use matrix inversion lemma for global offset and linear prior.  Currently being used;  not using would be simpler; try both and compare results.&lt;/p&gt;

&lt;h2&gt;Testing new marginal likelihood&lt;/h2&gt;

&lt;p&gt;Creating new version of &lt;code&gt;test/test_ml_end_to_end.m&lt;/code&gt;, named &lt;code&gt;test/test_ml_end_to_end2.m&lt;/code&gt;.  This version uses the new likelihood format, and compares against the old.&lt;/p&gt;

&lt;h2&gt;New curve_likelihood&lt;/h2&gt;

&lt;p&gt;Need to write new version of &lt;code&gt;curve_likelihood.m&lt;/code&gt; to handle new likelihood format.&lt;/p&gt;

&lt;p&gt;It's going to be difficult to compare to older version, since we've changed the correspondence method...&lt;/p&gt;

&lt;p&gt;I'll have to add a flag that simulates the old correspondence method, but stores it in the new likelihood format...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;01:33:52 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I decided to write a separate function for the old correspondence method, called &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.  Moved &lt;code&gt;clean_correspondence3.m&lt;/code&gt; to &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The legacy function isn't exactly identical to the old function.  In particular, the index values might be slightly different, since it depends on the old function's global mean curve, which we don't compute in the new function.  If it's important, we can call the old function, and then call the new one.  This is a debugging function, so speed doesn't matter. For the moment, we'll accept the small difference, and use this function simply to confirm that we're in the ballpark.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;03:06:24 PM&lt;/em&gt;
Test is implemented.  Troubleshooting syntax errors...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;04:11:17 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Elusive NaN error in &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;
</description>
				<pubDate>Wed, 03 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing marginal likelihood &lt;/h2&gt;

&lt;p&gt;Working on generalizing marginal likelihood for all three foreground models.&lt;/p&gt;

&lt;p&gt;Considered briefly how to use the matrix inversion lemma to improve numerical stability of all models.  Decided it could be difficult to build in a general way;  will proceed with the direct method for now, until there is evidence of numerical instability.&lt;/p&gt;

&lt;p&gt;Generalized how covariance is generated.  Wrote several new functions for generating differenct covariance matrices.  Will need to rework some due to the developments I describe later.&lt;/p&gt;

&lt;h2&gt;Theoretical Developments&lt;/h2&gt;

&lt;p&gt;Thought extensively about perturbation models I described yesterday.  I realized there is a serious problem with modeling motion using brownian motion -- the prior variance grows without bound as time approaches infinity.  Thus, the marginal prior for the curve in view 36 has much greater prior variance than the curve one in view 1.  This doesn't make sense -- ideally, they should all have the same marginal prior.&lt;/p&gt;

&lt;p&gt;This led to a reading session in Williams and Rasmussen, which led me to develop two new motion models, which I describe extensively in today's accompanying post.&lt;/p&gt;

&lt;h2&gt;Tomorrow&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish implementing new covariance functions.&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood.&lt;/li&gt;
&lt;li&gt;implement &lt;em&gt;fast&lt;/em&gt; generalized marginal likelihood (approximation)

&lt;ul&gt;
&lt;li&gt;Test against existing ML for old models.&lt;/li&gt;
&lt;li&gt;Test with new models.  Are the ML's higher? (will probably need training).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build training data set from hand-traced ground-truth.&lt;/li&gt;
&lt;li&gt;Write training code.  Train all three candidate models.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Tue, 02 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log</guid>
			</item>
		
			<item>
				<title>Rethinking covariance functions</title>
				<description>&lt;p&gt;Noticed a problem with the cubic spline perturbation model.  As the view number increases, the variance marginal prior of the curve in that view approaches infinity.  This means that the &quot;smoothness&quot; prior is ignored more and more in later views.&lt;/p&gt;

&lt;h2&gt;Squared Exponential perturbation model&lt;/h2&gt;

&lt;p&gt;A better model would have the same marginal prior for all views, but with correlation between nearby views.  This allows curves to revert to the prior as temporally correlated evidence becomes less informative.  I believe the squared exponential covariance function has this property.  Instead of adding the sq-exp covariance to the existing covariance function, we should multiply it, so self-covariance is unchanged, but pairwise correlation is non-zero.&lt;/p&gt;

&lt;p&gt;An added benefit of this is that it only has one tunable parameter.&lt;/p&gt;

&lt;p&gt;It should be easy to incorporate into our test-bed, along with the other two newly proposed foreground models.&lt;/p&gt;

&lt;h2&gt;Ornstein Uhlenbeck perturbation model&lt;/h2&gt;

&lt;p&gt;Digging deeper into Williams and Rasmussen, I found precisely the GP I was looking for a few days ago:  the Ornsteinâ€“Uhlenbeck (OU) process.  This   process describes brownian motion under the influence of a regularizing force that pulls toward the mean.&lt;/p&gt;

&lt;p&gt;In other words, I can model correlation between views without affecting the marginal prior of the curve any particular view.  This is also accomplished by the squared-exponential model, but the OU process is probably more realistic, because the plant's motion looks non-smooth.&lt;/p&gt;

&lt;h2&gt;Modelling the mean or not?&lt;/h2&gt;

&lt;p&gt;I'm struggling with whether or not to explicitly model a &quot;mean&quot; curve with the SE and the OU processes.&lt;/p&gt;

&lt;p&gt;If I did model the mean, each curve's covariance function would be the sum of a standard covariance plus a perturbation covariance.  The standard covariance models the &quot;mean&quot; curve, and it would be the same for all views (100% correlated).  The perturbation covariance would be partially correlated between the views, using the SE or the OU process.  The bayes net has the following structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        O   &amp;lt;-- mean curve
     / /\ \
   /  /  \  \
 /   |    |   \
O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The alternative is to model the per-view curves directly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Under this model, each view's curve has a cubic-spline marginal distribution, and the SE or UO process controls the correlation between them.&lt;/p&gt;

&lt;p&gt;What isn't clear is whether the perturbations in the latter model will be independent between points.  We need to model within-view perturbations as correlated, otherwise the marginal likelihood will drop too low.  There is no explicit description of how perturbations of adjacent points correlate.&lt;/p&gt;

&lt;h2&gt;What follows is me thinking out-loud...&lt;/h2&gt;

&lt;p&gt;Intuitively, points nearby in curve-space (have similar \(\tau\)'s) can never be more correlated than they are when they appear in the same view.  Separating them in time (view) will decreases their correlation, until finally, there is no correlation; the only remaining correlation is between points in the same view.  The SE kernel modulates that correlation.  The SE kernel doesn't explicitly model correlation between perturbations, but this doesn't mean the correlation doesn't exist -- it is implicit in the original kernel.&lt;/p&gt;

&lt;p&gt;There is an analogy here with the classic krigging example.  In there models, the squared exponential over a 2D landscape (i.e. joint function over \(\mathbb{R}\)&lt;sup&gt;2&lt;/sup&gt; space) is equal to the product of 1D squared exponentials (i.e. two functions over \(\mathbb{R}\)&lt;sup&gt;1&lt;/sup&gt; space).  In other words, the 2D kernel is constructed by the product of 1D kernels.  There is no worry that the delta between nearby &quot;slices&quot; of the surface are uncorrelated, because the marginal covariance within that slice will enforce that smoothness.&lt;/p&gt;

&lt;p&gt;In our case, we also have a product of 1D covariance functions constructing a 2D covariance function.  The difference is that one of the kernels (the curve-space kernel) is a cubic-spline process, while the other (the time-dimension kernel) is squared exponential (or Ornstein-Uhlenbeck).  Despite this difference, my intuition is that the conclusions are the same - the deltas will be smooth (i.e. correlated), because they will be the difference between two smooth curves.&lt;/p&gt;

&lt;p&gt;Considering the marginals in both directions further illustrates why this works.  Obviously, a slice in the curve-direction will always look curve-like, since the marginals are all the same cubic-covariance GP prior.  In the time-direction, a single point will follow a GP or OU process, with initial conditions dictated by the first curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;...BUT...&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;...in the absence of data, each individual point will drift over time to the prior mean, I.e. &lt;strong&gt;zero&lt;/strong&gt;.  In other words, the 3D curve observed in view infinity gains no information from curve observed at time zero.&lt;/p&gt;

&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; realistic.  In reality, these points shouldn't drift far from some &quot;sensible&quot; center curve, implying that a mean curve exists.&lt;/p&gt;

&lt;p&gt;The time-slice of any particular point &lt;em&gt;should&lt;/em&gt; look like either a OU or SE process, but the mean needs to be explicit.  This implies an additive model, with distinct models for the unobserved &quot;mean curve&quot; and the deviations from it, which are summed to get the 3D curve seen in each view.&lt;/p&gt;

&lt;h2&gt;Modeling perturbation&lt;/h2&gt;

&lt;p&gt;So if we must model perturbation, how should we model it?&lt;/p&gt;

&lt;p&gt;One thing is clear: the marginal prior for the curve in any view &lt;strong&gt;must&lt;/strong&gt; still be a cubic-spline process.&lt;/p&gt;

&lt;p&gt;This implies that the perturbation must be a cubic-spline process, too.&lt;/p&gt;

&lt;p&gt;However, the variances for each component (offset, linear, and cubic) are likely to be different for the perturbation model, compared to the mean-curve model.  Most importantly, the magnitude of the offset variance must be large in the mean-curve model but will be &lt;em&gt;much&lt;/em&gt; lower (relative to linear and cubic variance) in the perturbation model.&lt;/p&gt;

&lt;p&gt;I was hoping to avoid adding four extra parameters to our model (perturbation offset, linear and cubic variance, plus perturbation scale-length).  The mean-free model only adds one parameter - scale-length.  I guess this is the price we pay for a better model -- and for higher marginal likelihoods.  Ideally, the occam's razor quality of the marginal likelihood will allow us to avoid overfitting this many parameters (7 total).  Any parameters that are superfluous should become near-zero during training.&lt;/p&gt;

&lt;p&gt;...I hope.&lt;/p&gt;
</description>
				<pubDate>Tue, 02 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions</guid>
			</item>
		
	</channel>
</rss>
