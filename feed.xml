<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Work Log - Attachment ML Math (ctd); Implementing Attach()/Detach()</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Note new working path: &lt;code&gt;data_association_3&lt;/code&gt;&lt;/strong&gt;&lt;br/&gt;
Fixed corrupt svn working copy.&lt;/p&gt;

&lt;h1&gt;Computing \(\mu_b\) and \(\Sigma_b\)&lt;/h1&gt;

&lt;div&gt; Since \(S\) is usually singular, we can't invert it, so we need to tweak yesterday's equations.  In what follows, the precision matrix \(S\) is decomposed into \(s s^\top\), which will allow us to keep the covariance matrix symmetric. 

&lt;div&gt;
The updated forumula for \(mu_b\) is:&lt;/div&gt;
\[
    \begin{align}
    \mu_b &amp;= K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} y_{MB}  \\
          &amp;= K_*^\top s^\top \left(s K_{\mathcal{MB}} s^\top + I\right)^{-1} s y_{MB}  \\
    \end{align}
\]

The formulate for \(\Sigma_b\) is:

\[
    \Sigma_b = K_b - K_*^\top s^\top \left(s K_{\mathcal{MB}} s^\top + I\right)^{-1} s K_*
\]

Recall that \(K_b\) is the prior covariance of the branch point.
&lt;/div&gt;


&lt;br /&gt;


&lt;h2&gt;Predictive covariance question&lt;/h2&gt;

&lt;div&gt;
&lt;p&gt;
I realized during implementation that it isn't clear what the predictive covariance, \(K_*\), should be.  I know it's the covariance between the true branch point and the observed points in the markov blanket, but the observed points have associated view-indices, while the branch point does not.  After some thought, I realized that the covariance arising from view-indices is essentially *likelihood* variance in this context (i.e. imaging noise), and according to Williams and Rasmussen, these parts of the likelihood should be omitted when computing the covariance between data and true unobserved points.  (See equation (2.21), notice that off-diagonal elements don't include the noise variance, \(\sigma_n I\)).
&lt;/p&gt;
&lt;p&gt;
To accomodate the use-case where you want to compute the covariance while ignoring view-index, I tweaked `kernel/get_model_kernel`; now, if you pass-in a `model_index` of zero, it returns a two-parameter no-perturb kernel where only spacial-indices are received.
&lt;/p&gt;
&lt;/div&gt;


&lt;h2&gt;&lt;code&gt;ll_indices_flat&lt;/code&gt; vs. &lt;code&gt;prior_indices&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;There's some ambiguity in these two fields of a curve-track.  The difference is simply that in &lt;code&gt;ll_indices_flat&lt;/code&gt;, indices &lt;em&gt;always&lt;/em&gt; start at zero, whereas in &lt;code&gt;prior_indices&lt;/code&gt; the offset &lt;code&gt;start_index&lt;/code&gt; is added to all values.  In practice, there are rules for where these two fields should be used:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;when passing indices to a kernel, always use &lt;code&gt;prior_indices&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;When dealing with geometry (e.g. referring to a point along a curve), always use ll_indices.  That way, the position of the referred point is unchanged if &lt;code&gt;start_index&lt;/code&gt; ever changes.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Task progress&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Finish &lt;code&gt;attachment/att_set_branch_index.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;rough test of attachment stuff - (does it parse? does it run?)

&lt;ul&gt;
&lt;li&gt;It runs, but with buggy output for \(\mu_b\) and \(\Sigma_b\).&lt;/li&gt;
&lt;li&gt;Found bug - incorrect re-indexing (xxyyzz to xyzxyz)&lt;/li&gt;
&lt;li&gt;Now giving apparently good results.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Implement efficient tree ML&lt;/li&gt;
&lt;li&gt;implement naive ML&lt;/li&gt;
&lt;li&gt;write tests

&lt;ol&gt;
&lt;li&gt;compare against &lt;code&gt;curve_ml5.m&lt;/code&gt; for independent curves&lt;/li&gt;
&lt;li&gt;compare against naive ML using full-covariance-matrix.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;invalidate children after calling attach/detach&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Misc notes&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;should place a condition on \(\Sigma_b\).  If it's variance is too high, need to expand markov-blanket.&lt;/li&gt;
&lt;li&gt;Add &lt;code&gt;model_type to params&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Have &quot;make_correspondence&quot; initialize start_index, parent_index, branch_index.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;chain of affects&lt;/h1&gt;

&lt;p&gt;How are children affected when properties of the parent change?&lt;/p&gt;

&lt;div&gt;
&lt;table border=&quot;1&quot;&gt;
&lt;tr&gt;
&lt;th width=&quot;33%&quot;&gt;parent property&lt;/th&gt;
&lt;th width=&quot;33%&quot;&gt;self affects&lt;/th&gt;
&lt;th&gt;child affects&lt;/th&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Assoc/Correspondence&lt;/td&gt;
&lt;td&gt;start_index (and inherited)&lt;/td&gt;
&lt;td&gt;Branch_index (and inherited.  easy to re-estimate deterministically), marginal likelihood, (not start index)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Start index&lt;/td&gt;
&lt;td&gt;prior_K, prior_indices, ML&lt;/td&gt;
&lt;td&gt;\(\mu_b\)*, \(\Sigma_b\)*, ML*&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Branch index&lt;/td&gt;
&lt;td&gt;\(\mu_b\), \(\Sigma_b\), &lt;/td&gt;
&lt;td&gt;none.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;
* Updating these might not be necessary
&lt;/div&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Implement Tree ML (2 ways, &quot;naive&quot; and &quot;fast&quot;)&lt;/li&gt;
&lt;li&gt;Test&lt;/li&gt;
&lt;li&gt;Get attachment for ground truth&lt;/li&gt;
&lt;li&gt;Re-train using attachment&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 22 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/22/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/22/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Clique-tree math (ctd)</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Conditional clique node&lt;/h1&gt;

&lt;p&gt;Continuing from yesterday, let's convert our conditional gaussian distribution to a gaussian function over \(x_c\) and \(\mu_b\).&lt;/p&gt;

&lt;p&gt;Let \(II\) represent a stack of identity matrices:&lt;/p&gt;

&lt;div&gt;
\[
II = \left( \begin{array}{c} I\\I\\...\\I \end{array}\right)
\]

The exponential expression for the conditional Gaussian distribution is

    \[
    -\frac{1}{2} (x_C - II \mu_b)^\top \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}(x_C - II \mu_b)
\]

Let's convert this to a linear function over \((x_C, \mu_b)\), instead of just \(x_C\):
        
    \[
    -\frac{1}{2} \left(\begin{array}{c}x_C \\ \mu_b \end{array} \right )^\top \left ( \begin{array}{cc}I &amp; -II\end{array}\right )^\top \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}\left ( \begin{array}{cc}I &amp; -II\end{array}\right )\left(\begin{array}{c}x_C \\ \mu_b \end{array}\right )
\]
&lt;/div&gt;


&lt;p&gt;Recall that \(\mu_b\) is a linear function of the data-points in the Markov-blanket, \(y_\mathcal{MB}\) (reproduced from &lt;a href=&quot;/ksimek/research/2013/08/19/work-log/&quot;&gt;yesterday's post&lt;/a&gt;)&lt;/p&gt;

&lt;div&gt;
\[
    \begin{align}
    \mu_b &amp;= K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} y_{MB}  \\
        &amp;= K_*^\top K^{-1}_{y_\mathcal{MB}}  y_\mathcal{MB}  \\
    \end{align}
\]

Rewriting the expression as a function of \((x_C, y_\mathcal{MB})\):

    \[
    \begin{align}
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\ K_* K^{-1}_{y_\mathcal{MB}}  y_\mathcal{MB} \end{array} \right )^\top
    \left ( \begin{array}{cc}I &amp; -II\end{array}\right )^\top 
    \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}
    \left ( \begin{array}{cc}I &amp; -II\end{array}\right )
    \left(\begin{array}{c}x_C \\ K_* K^{-1}_{y_\mathcal{MB}}  y_\mathcal{MB} \end{array}\right )
    \\
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\   y_\mathcal{MB} \end{array} \right )^\top
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}} \end{array}\right )^\top 
    \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}}\end{array}\right )
    \left(\begin{array}{c}x_C \\ y_\mathcal{MB} \end{array}\right )
    \end{align}
\]

And expanding \(\Sigma_b\):
 
\[
    \begin{align}
    \Sigma_b &amp;= K_b - K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} K_* \\
             &amp;= K_b - K_*^\top K^{-1}_{y_\mathcal{MB}} K_*

    \end{align}
\]

The final expression is:

    \[
    \begin{align}
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\   y_\mathcal{MB} \end{array} \right )^\top
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}} \end{array}\right )^\top 
    \left( \Sigma_C + II \left(K_b - K_*^\top K^{-1}_{y_\mathcal{MB}} K_* \right)  II^\top \right )^{-1}
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}}\end{array}\right )
    \left(\begin{array}{c}x_C \\ y_\mathcal{MB} \end{array}\right ) \\
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\   y_\mathcal{MB} \end{array} \right )^\top
    \Lambda_{C \mid \mathcal{MB}}
    \left(\begin{array}{c}x_C \\ y_\mathcal{MB} \end{array}\right )
    \end{align}
\]

the normalization constant is:
    
\[
    Z = 
    (2 \pi)^\frac{k}{2} \left | \Sigma_C + II \left(K_b - K_*^\top K^{-1}_{y_\mathcal{MB}} K_* \right)  II^\top \right |^\frac{1}{2}
\]
&lt;/div&gt;


&lt;h2&gt;Test experiment&lt;/h2&gt;

&lt;p&gt;See  &lt;code&gt;exp_2013_08_21_clique_tree_test.m&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;sample N points for curve 1, \(C_1\)&lt;/li&gt;
&lt;li&gt;sample N points for curve 2, \(C_2\)&lt;/li&gt;
&lt;li&gt;offset curve 1: \(C_2 = C_2 + C_1(:,5)\)&lt;/li&gt;
&lt;li&gt;add noise to \(C_1\) and \(C_2\) to get data \(y_1\),\(y_2\)&lt;/li&gt;
&lt;li&gt;add noise to \(C_1\) and \(C_2\) to get data \(y_1\), \(y_2\).&lt;/li&gt;
&lt;li&gt;Construct full prior, \(\Sigma\) (see below for definition).&lt;/li&gt;
&lt;li&gt;Evaluate ML directly from \(p(y_1, y_2) = \mathcal{N}(0, (\Sigma + \sigm_n I))\).&lt;/li&gt;
&lt;li&gt;Construct ML decomposed: \(p(y_2 | y_1) p(y_1) \)&lt;/li&gt;
&lt;li&gt;(didn't implement) Construct clique tree using

&lt;ul&gt;
&lt;li&gt;Node 1: \((0, \Sigma_1)\)&lt;/li&gt;
&lt;li&gt;Node 2: \((0, inv(\Lambda_{C\mid \mathcal{MB}}))\)&lt;/li&gt;
&lt;li&gt;multiply by corresponding noise nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(didn't implement) Marginalize clique tree.  compare against result in 7.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I originally thought the result from 8 was only an approximation (mainly because I hadn't originally written it out that way).  In fact, it's an exact computation, but it isn't very useful in this form, because deep trees still exhibit linear growth of the condition-set, meaning cubic growth in running time.  In practice, we can replace \(y_1\) to the data markov-blanket, \(y_{1,\mathcal{MB}}\).  The data markov-blanket is naturally larger than the prior m.b., which is a single point, but if the noise is low relative to the point-spacing, the data m.b. should still be relatively small.  Thus, we can approximate the ML with a decomposed form that avoids cubic growth.&lt;/p&gt;

&lt;p&gt;The alternative is to use the clique tree from step 9., but the former is simpler to implement, because we don't have to use the crazy linear form we developed above, and we don't have to do any message-passing.  We just need \(\Sigma_b\) and \(\mu_b\).&lt;/p&gt;

&lt;h2&gt;Computing max posterior with attachments&lt;/h2&gt;

&lt;p&gt;Since this operation won't be run during production, we can just implement it the naive way.&lt;/p&gt;

&lt;p&gt;But if we wanted a fast version, we could to a forward-pass approximation, i.e. find the max for the root curve, and then pass that as data to the child curves.&lt;/p&gt;

&lt;p&gt;A full forward-backward algorithm probably wouldn't be too hard, but probably not worth the trouble at the moment.&lt;/p&gt;

&lt;h1&gt;Build Tasks&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;apply attacment to trackset&lt;/li&gt;
&lt;li&gt;guess branch spacing&lt;/li&gt;
&lt;li&gt;construct independent cliques, given branch spacing.&lt;/li&gt;
&lt;li&gt;Guess branch point&lt;/li&gt;
&lt;li&gt;construct markov-blanket, \(\mathcal{MB}\).&lt;/li&gt;
&lt;li&gt;construct \(sigma_b\) and \(\mu_b\) from parent \(\mathcal{MB}\).&lt;/li&gt;
&lt;/ol&gt;


&lt;h1&gt;TODO &amp;amp; In-progress&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/attach.m&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_start_index.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_branch_index.m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Next

&lt;ul&gt;
&lt;li&gt;pre-flatten and pre_sort ll_*.

&lt;ul&gt;
&lt;li&gt;remove all calls to flatten_inputs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;precompute likelihood covariance matrix.

&lt;ul&gt;
&lt;li&gt;remove logic from &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;compute full branching ML using cached values.

&lt;ul&gt;
&lt;li&gt;identify connected components&lt;/li&gt;
&lt;li&gt;topological sort&lt;/li&gt;
&lt;li&gt;root-to-leaf evaluation over each CC.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Wed, 21 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/21/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/21/work-log</guid>
			</item>
		
			<item>
				<title>cleanup</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Note new working path: &lt;code&gt;data_association_3&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Started realizing that there are a huge number of dead/obsolete files in the &lt;code&gt;data_association_2&lt;/code&gt; directory.  It's time to migrate to a clean code-base, &lt;code&gt;data_association_3&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Reorganization&lt;/h1&gt;

&lt;h2&gt;Setups&lt;/h2&gt;

&lt;p&gt;Realized that &lt;code&gt;tmp_setup_workspace.m&lt;/code&gt; is very valuable, but will probably need to change and evolve over time.  Created new directory called &lt;code&gt;setups/&lt;/code&gt;, where setup scripts will be stored, organized by date.  Related files that the setup scrip needs will be stored in the same directory, with a similar name to the script itself.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tmp_setup_workspace.m&lt;/code&gt; is now in &lt;code&gt;setups/setup_workspace_2013_08_21.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Format is: &lt;code&gt;setup_&amp;lt;type&amp;gt;_&amp;lt;date&amp;gt;.m&lt;/code&gt;.  Currently only type is &quot;workspace&quot;.
Related &quot;load&quot; files have format: &lt;code&gt;setup_&amp;lt;type&amp;gt;_&amp;lt;date&amp;gt;.mat&lt;/code&gt;   or if multiple files, &lt;code&gt;setup_&amp;lt;type&amp;gt;_&amp;lt;date&amp;gt;.&amp;lt;N&amp;gt;.mat&lt;/code&gt;, where N is an increasing integer starting at 1.&lt;/p&gt;

&lt;h2&gt;Mex files&lt;/h2&gt;

&lt;p&gt;Propose creating a new file &lt;code&gt;compile_mex_scripts.m&lt;/code&gt;.  If called with no arguments, it will compile all scripts that are uncompiled.  Optional &quot;force_recompile&quot; parameter.&lt;/p&gt;

&lt;h1&gt;Misc Notes/Issues&lt;/h1&gt;

&lt;h2&gt;split_correspondence&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;split_correspondence.m&lt;/code&gt; is no longer available.  It became obsolete some time ago, around when we moved from &lt;code&gt;clean_correspondence.m&lt;/code&gt; to &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When we return to end-to-end sampling, we'll need something like it.  Alternatively,  maybe full-rebuilding of the split curves is fast enough, now that we've mexed the bottlenecks?&lt;/p&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Not totally done, but we can at least run the workspace setup file.&lt;/p&gt;

&lt;p&gt;Will save the current workspace, re-open matlab, and see what issues arise as we continue to work toward the short-term goal of implementing branching-curve marginal likelihood.&lt;/p&gt;

&lt;p&gt;Workspace saved to &lt;code&gt;tmp/workspace_2013_08_21.mat&lt;/code&gt;.&lt;/p&gt;
</description>
				<pubDate>Wed, 21 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/21/cleanup</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/21/cleanup</guid>
			</item>
		
			<item>
				<title>Work Log - Branching curve clique-tree</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Thinking about attachments.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Organize into connected components.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Reviewing ML code, with considerations to generalizing for attachments.&lt;/p&gt;

&lt;p&gt;Can we exploit the structure in the branching-prior covariance matrix to speed-up inversion in the marginal likelihood?  No obvious way.&lt;/p&gt;

&lt;p&gt;Why did we abandon the clique-tree method?  Because it wasn't clear that the perturb-models would be compatible with it.  Now, it seems like it might be, as long as we set the markov-order high enough.  More importantly, it might be &lt;em&gt;necessary&lt;/em&gt;, because after adding attachment, the covariance matrix is growing too large to allow direct evaluation.&lt;/p&gt;

&lt;p&gt;Construct clique tree per-track, conditioned on parent point.  In case of base-curves, the parent point is the prior.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;New fields&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;branch parent

&lt;ul&gt;
&lt;li&gt;curve index&lt;/li&gt;
&lt;li&gt;point index&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;branch distance

&lt;ul&gt;
&lt;li&gt;i.e. starting index of self&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gaussian structure

&lt;ul&gt;
&lt;li&gt;in canonical form&lt;/li&gt;
&lt;li&gt;no position variance&lt;/li&gt;
&lt;li&gt;no branching variance&lt;/li&gt;
&lt;li&gt;yes branch distance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Inferring branch distance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The reconstructed lateral branch curves are usually have gaps of several millimeters from their parent branch.  Assigning an index of zero to the first point will almost certainly result in a worse marginal likelihood, because these gaps are not well-modelled.&lt;/p&gt;

&lt;p&gt;Instead, we should attempt to infer the branch distance(or better yet, try to marginalize over it).&lt;/p&gt;

&lt;p&gt;Spent some time trying to derive a &quot;fast update&quot; for covariance matrices for inferring branch distance.  In this scheme, the matrices are computed assuming branch distance is zero, and then a easily computed delta matrix is added to account for non-zero branch distance.&lt;/p&gt;

&lt;p&gt;After some work, it looks like the smoothing variance kernel is too complicated to permit such a method.  It's just easier to recompute the entire covariance matrix.&lt;/p&gt;

&lt;h2&gt;Constructing branching clique-tree&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Construct individual cliques trees, assuming zero position variance (but including branch distance).  Call this the &quot;raw prior.&quot;&lt;/li&gt;
&lt;li&gt;Given a branch-point-index, compute the markov-blanket of the data points on the parent.&lt;/li&gt;
&lt;li&gt;Given raw prior and markov-blanket, construct conditional clique  node from raw clique node.&lt;/li&gt;
&lt;li&gt;Given clique tree, multiply and marginalize from tips to root.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;All cliques will now be stored in track structure (currently named &lt;code&gt;Corrs&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to construct conditional prior?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\mathcal{MB}\) - indices of the parent curve markov blanket, centered at branch point.&lt;/li&gt;
&lt;li&gt;\(y_\mathcal{MB}\) - observations in the markov blanket&lt;/li&gt;
&lt;li&gt;\(x_b\) - branch point&lt;/li&gt;
&lt;li&gt;\(\Sigma_b\) - branch point predictive covariance, conditioned on markov blanket&lt;/li&gt;
&lt;li&gt;\(x_c\) - child curve points, relative to branch point.&lt;/li&gt;
&lt;li&gt;\(x_C\) - child curve points, relative to world origin&lt;/li&gt;
&lt;li&gt;\(N\) - number of points in child curve.&lt;/li&gt;
&lt;li&gt;\(\Sigma_c\) - curve predictive covariance, conditioned on branch point&lt;/li&gt;
&lt;li&gt;\(\Sigma_C\) - curve predictive covariance, conditioned on markov blanket&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;div&gt;
The branch point distribution is given by the GP predictive distribution:
    
\[
p(x_b | y_\mathcal{MB}) = \mathcal{N}(\mu_b, \Sigma_b) \]

where 

\[
\begin{align}
    \mu_b &amp;= K_* \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} y_{MB} \\
    \Sigma_b &amp;= K_b - K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} K_*

\end{align}
\]

Here, \(K_*\) is the kernel of the branch index vs. the markov blanket indices, \(k(t_\mathcal{MB}, t_b)\).
&lt;/div&gt;


&lt;p&gt;The conditional child curve distribution is&lt;/p&gt;

&lt;div&gt;
\[ p(x_C \mid x_b) = p(x_c) = \mathcal{N}(x_b, \Sigma_c) \]

Here, \(Sigma_c\) is the raw curve covariance, whatever it may be.
&lt;br /&gt;
The marginal over the child curve arises due to the linear combination of random variables \(x_c\) and \(x_b\):

\[
x_C = x_c + \left ( \begin{array}{c}I\\I\\ \ldots \\I \end{array}\right) x_b
\]

The corresponding distribution is:

\[
p(x_C | y_\mathcal{MB}) = \mathcal{N}(\mu_C, \Sigma_C)
\]

where

\[
\begin{align}
\mu_C &amp;= \left( \begin{array}{c}I\\I\\ \ldots \\I \end{array}\right) \mu_b \\
\Sigma_C &amp;= \Sigma_c + \left( \begin{array}{c}I\\I\\ \ldots \\I \end{array}\right) \Sigma_b \left ( I \; I \ldots I \right )
\end{align}
\]
&lt;/div&gt;



</description>
				<pubDate>Mon, 19 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/19/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/19/work-log</guid>
			</item>
		
			<item>
				<title>Saturday Thoughts - Enabling Non-gaussian models by using Gaussian models as proposal distributions</title>
				<description>&lt;p&gt;The different between the models we're fitting and the models we'd really like to use is that our model doesn't have a way to prefer certain branch angles or internode-distance.&lt;/p&gt;

&lt;p&gt;Unfortunately, adding those features would break the Gaussian-ness of our model.  So we couldn't use it to evaluate marginal likelhoods, which we need to evaluate curve-fragemnt correspondences, i.e. triangulation.&lt;/p&gt;

&lt;p&gt;However, what's notable is that is that our models are invriably more permissive than the models we'd prefer.  So any model allowed under our preferred models would certainly be permitted by our &lt;em&gt;worse-but-Gaussian&lt;/em&gt; model.&lt;/p&gt;

&lt;p&gt;So even though we can't distinguish between different species with our weaker model, we can triangulate with it.  And once we have a triangulation, we can switch to a stronger model to do classification.  Since the data is so strong, the posterior under the weak model is still extremely peaked.  We can do importance sampling to marginalize the strong model, using the weak model as a proposal distribution.&lt;/p&gt;

&lt;p&gt;This also gives us an approach to using non-gaussian likelihoods (e.g. pixel-based likelihoods).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;use matlab to construct gaussian model.&lt;/li&gt;
&lt;li&gt;save and read into C++ code&lt;/li&gt;
&lt;li&gt;Use C++ code for sampling full (non-Gaussian) model&lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Sat, 17 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/17/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/17/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Improved indexing; Retraining; Distinguishing between camera and plant motion</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Improved Indexing (ctd)&lt;/h1&gt;

&lt;p&gt;Finished debugging changes to &lt;code&gt;corr_to_likelihood&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Recap: since projected model curves are discretely sampled at coarse intervals, multiple observed points may correspond to the same model point.  The results below show this. The projected model curve is sampled every 2 pixels (&lt;code&gt;index_delta_2d&lt;/code&gt;), so each model point has between 2 and 3 corresponding data points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-16-bad-indexing.png&quot; alt=&quot;old indexing results in aliasing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The new scheme post-processes the indexes by linearly interpolating the model curve and projecting the data point onto the neighboring line segments.  Resulting indices are much improved:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-16-better-indexing.png&quot; alt=&quot;new indexing scheme permits continuous (between-point) correspondences, which results in better indexing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that viewing angle distorts the correspondence angles somewhat.  Non-perpendicular correspondence lines may be simply due to non-orthogonal viewing direction.&lt;/p&gt;

&lt;p&gt;Since coarse sampling is no longer an issue, we can increase the 2D sample period and still get good results.  Below is the result after increasing 2D sampling period from 2 to 5:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-16-better-indexing-in-spite.png&quot; alt=&quot;new indexing scheme permits continuous (between-point) correspondences, which results in better indexing&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Improved training&lt;/h2&gt;

&lt;p&gt;This has implications on training results.  Re-running training using &lt;code&gt;exp_2013_08_11_train_all&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0030
            noise_variance: 1.0805
         position_variance: 1.6270e+04
             rate_variance: 0.2904
perturb_smoothing_variance: 1
     perturb_rate_variance: 1
 perturb_position_variance: 1
             perturb_scale: 2.5000

Final ML: -9.094636e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Ind-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0034
            noise_variance: 0.3472
         position_variance: 1.6458e+04
             rate_variance: 0.2605
perturb_smoothing_variance: 1.4186e-06
     perturb_rate_variance: 3.0555e-04
 perturb_position_variance: 0.5467
             perturb_scale: 2.5000

Final ML: -6.203953e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;OU-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0035
            noise_variance: 0.3486
         position_variance: 1.6440e+04
             rate_variance: 0.2587
perturb_smoothing_variance: 1.4874e-06
     perturb_rate_variance: 3.6269e-04
 perturb_position_variance: 0.7241
             perturb_scale: 2.3364

Final ML: -6.156721e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;SqExp-Perurb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0035
            noise_variance: 0.3479
         position_variance: 1.6246e+04
             rate_variance: 0.2745
perturb_smoothing_variance: 1.5495e-06
     perturb_rate_variance: 4.1614e-04
 perturb_position_variance: 0.6613
             perturb_scale: 0.9654

Final ML: -6.159716e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Awesome news:  perturb smoothing variance is now non-negligible!&lt;/strong&gt;  There must have been so much IID noise resulting from bad indexing that it totally masked the perturb smoothing variance.&lt;/p&gt;

&lt;p&gt;The totally validates our efforts to fix indexing.  Before, the model was fundamentally broken; bad indexing was preventing us from making any correct inferences beyond a certain level of granularity.  By fixing indexing, we're suddenly able to everything clearly, whereas before we were squinting through a noisy haze.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other observations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ML is much lower compared to the badly indexed results, which were one the order of -8000 (&lt;a href=&quot;/ksimek/research/2013/08/14/work-log/&quot;&gt;according to this post&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Noise variance dropped from 0.68 to 0.35.&lt;/li&gt;
&lt;li&gt;Smoothing variance has increased, probably because we attribute fewer deviations to IID noise.  Great!&lt;/li&gt;
&lt;li&gt;Global rate variance is lower, while perturb rate variance roughly tripled.&lt;/li&gt;
&lt;li&gt;Perturb scale dropped slightly.  Since noise variance can't explain independent deviations, the perturb-model takes over, becomes closer to independent.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Lets see if anything interesting comes out of our reconstructions...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;OU-perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az = 24;
el = 16;
axis_ = [ 70.0000  110.0000   50.0000  110.0000   47.8040  224.0467 ]

exp_2013_08_11_reconstruct_for_web(test_Corrs_ll_2, retraining_results{3}, 3, axis_, el, az, num_views, '/Users/ksimek/src/research_blog/img/2013-08-16-ou-model-%d.png', '/ksimek/research/img/2013-08-16-ou-model-%d.png', 'ou-reconstruct-anim', true)
&lt;/code&gt;&lt;/pre&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-16-ou-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-anim&quot; style=&quot;width:137px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;SqExp-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-9.png&quot;
        ]

    construct_animation($(&quot;#sqexp-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;sqexp-reconstruct-anim&quot; style=&quot;width:136px&quot;&gt; &lt;/div&gt;


&lt;h2&gt;Removing camera-based motion&lt;/h2&gt;

&lt;p&gt;We can remove perturbations that arise from poor camera calibration by assuming it is captured in the linear and offset perturbations; under this assumption, the remaining cubic-spline smooth perturbations capture the true plant motion.&lt;/p&gt;

&lt;div&gt;Removing linear and offset perturbations is as simple as removing their contributions to \(K^*\) in yesterday's equation for the mean of the predictive distribution.&lt;/div&gt;


&lt;p&gt;Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reverse = false(1,num_tracks);
reverse([1 2 4 5 6 8 9 10 11 12 14 15]) = true;
% error above, should omit 11:
% reverse([1 2 4 5 6 8 9 10 12 14 15]) = true;
exp_2013_08_16_visualize_smooth_perturbations( ...
        test_Corrs_ll_2, ...
        retraining_results{3},  ...
        3, axis_, el, az, num_views,  ...
        '/Users/ksimek/src/research_blog/img/2013-08-16-ou-model-smooth-%d.png', ...
        '/ksimek/research/img/2013-08-16-ou-model-smooth-%d.png', ...
        'ou-reconstruct-smooth-anim', reverse)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;removing-camera-perturbation&quot;&gt;&lt;/a&gt;
&lt;em&gt;OU-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-smooth-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-smooth-anim&quot; style=&quot;width:250px&quot;&gt; &lt;/div&gt;


&lt;p&gt;It's notable that the little curves at the top don't move.  Attaching them to the large main step will allow them to move, which should improve ML.&lt;/p&gt;

&lt;p&gt;It should also significantly affect training if we train with attachments in place.  Perturb_position_variance should be responsible for less of the variance, and perturb_smoothing_variance should explain more.&lt;/p&gt;

&lt;h1&gt;Attachments&lt;/h1&gt;

&lt;p&gt;Tasks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Create new attachments structure.  Indicates which curve and index the curve is attached to.&lt;/li&gt;
&lt;li&gt;Algorithm to convert N attachment to M track-sets, where M is the number of connected components in the attachment structure.  (in practice, attachment structure might be irrelevant, given the track-set structure).&lt;/li&gt;
&lt;li&gt;New function to construct prior matrix for track-sets, as opposed to individual tracks.&lt;/li&gt;
&lt;li&gt;New function to evaluate ML over track-sets.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Add attachments before training.&lt;/li&gt;
&lt;li&gt;Complete end-to-end training

&lt;ul&gt;
&lt;li&gt;input: path of training and curves&lt;/li&gt;
&lt;li&gt;automatically add attachments&lt;/li&gt;
&lt;li&gt;automatically reverse curves as needed&lt;/li&gt;
&lt;li&gt;save trained parameters somewhere&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Fri, 16 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/16/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/16/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Index Refinement; Mean-curve Reconstruction</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing from yesterday.&lt;/p&gt;

&lt;p&gt;Tried changing index_delta from 0.5 to 0.25.  ML suddenly dropped a &lt;em&gt;lot&lt;/em&gt;.  Realized that oversampling the smoothed curves causes significant degree of many-to-one, and our code prevents skipping more than one or two points during correspondences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Idea:&lt;/strong&gt; Sampling period should be equal to (or close to) the data point-spacing.&lt;/p&gt;

&lt;h1&gt;Index-refinement&lt;/h1&gt;

&lt;p&gt;Allow indices to correspond to continuous values between the samples.&lt;/p&gt;

&lt;p&gt;Modified &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt; to post-process indices to search the neighboring line segments for a better value. Re-ran &lt;code&gt;tr_prep_likelihood&lt;/code&gt; to reconstruct Corrs collection.  ML improves after fix.&lt;/p&gt;

&lt;p&gt;Need to confirm improved indexing by visualizing results.  See &lt;code&gt;experiments/exp_2013_08_15_visualize_indices.m&lt;/code&gt;, still in progress.  Some apparent bugs in aforementioned changes, causing bad results.  Still investigating...&lt;/p&gt;

&lt;h1&gt;Mean-curve reconstruction&lt;/h1&gt;

&lt;p&gt;Last week, I derived the equation for finding the maximum posterior per-view reconstruction.  Now we have a formula for the unobserved mean curve, i.e. the curve that each view is perturbed from.  Iterestingly, they both have the same form:&lt;/p&gt;

&lt;div&gt;
\[
    \mu = K^* (S K + I)^{-1} S y
\]
&lt;/div&gt;


&lt;p&gt;Note that this is slightly different from the form used in a previous post.  This form is equivalent, and better reflects the form used by Williams and Rasmussen.&lt;/p&gt;

&lt;div&gt;Here, \(K\) is the prior covariance, \(S\) is the likelihood precision matrix, and \(y\) are the virtual observations in 3D.  \(K^*\) is the covariance between the observed points (columns) and the points to be predicted (rows). The difference between the per-view and the mean reconstruction is the form of \(K^*\):  for the per-view reconstruction, \(K^*\) uses the full prior covariance, whereas with the mean reconstruction, the perturb covariances are set to zero.&lt;/div&gt;


&lt;h2&gt;Usage&lt;/h2&gt;

&lt;p&gt;Modified &lt;code&gt;curve_max_posterior_2.m&lt;/code&gt; by adding an extra optional parameter, &lt;code&gt;kernel_2&lt;/code&gt;.  If set, it assumes you want a mean reconstruction instead of a per-view reconstruction, and uses &lt;code&gt;kernel_2&lt;/code&gt; to build \(K*\).&lt;/p&gt;

&lt;p&gt;Added an extra flag to &lt;code&gt;experiments/exp_2013_08_11_reconstruct_for_web.m&lt;/code&gt;, which if set to true, will also plot the mean curve.&lt;/p&gt;
</description>
				<pubDate>Thu, 15 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/15/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/15/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Background ML bugs; Why is Foreground Noise Variance so large?</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;ML Validity Testing&lt;/h1&gt;

&lt;p&gt;In yesterday's test, I hadn't realized that the training ML didn't include all of the curves, only the foreground curves were included.  Rerunning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference ML: -5.1552e+04
Training ML: -5.0786e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Okay, we're back in the ballpark -- Within 1.5% of the reference.&lt;/p&gt;

&lt;p&gt;Found the other issue: the roundabout way I was using to generate the training covariance matrices was ignoring the user-specified position_variance_2d.  Results now match:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference ML: -5.1552e+04
Training ML: -5.1552e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, none of this explains why training is causing noise variance to collapse so low.  All discovered problems were merely bugs in the validation logic.  At least we know the training ML logic is valid.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Quick inspection shows that 2D curves are, indeed, pre-smoothed.  This means that noise variance can collapse to near-zero when fitting 2D curve.&lt;/p&gt;

&lt;p&gt;The new question becomes: why doesn't it occur in the foreground (3D) model?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Try re-indexing...&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% update params related to smoothing variance
params_2 = tbc_.params;
params_2.smoothing_variance = training_results{1}.smoothing_variance;
params_2.noise_variance = training_results{1}.noise_variance;
% re-construct likelihood (including indicies)
test_Corrs_ll_2 = tr_prep_likelihood(test_Corrs, data_, params_2);
% re-run training
train_params_done = tr_train(test_Corrs_ll_2, training_results{2}, 400, 3);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0011
            noise_variance: 0.6846
         position_variance: 1.3597e+04
             rate_variance: 0.3042
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.3013e-06
 perturb_position_variance: 0.6621
             perturb_scale: 2.9795

Final ML: -7840.140322
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare against results prior to re-indexing: version:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654

Final ML: -8049.9e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we've improved, but nowhere near the background model's ML of 4611.9.  Note that curves got smoother and less noisy.  More correlation, more variance pushed into the perturbations.  (why the f*** perturb_smoothing_variance is just sitting there like an idiot is still beyond me).&lt;/p&gt;

&lt;h2&gt;Miscellaneous thoughts&lt;/h2&gt;

&lt;p&gt;Need to visualize ll_means against smoothed curve.  The perturbations should be correlated.  Maybe even plot them?  Note the perturbations need to be considered only in the directions parallel to the image plane.  &lt;strong&gt; This has never been done, and is necessary to validate the index-estimation in &lt;code&gt;correspondence/corr_to_likelihood&lt;/code&gt;.&lt;/strong&gt; Can we visualize after removing rate and offset components?  Yes: difference between ll_means and per-view reconstructed curve.&lt;/p&gt;

&lt;p&gt;Consider smarter smoothing in &lt;code&gt;corr_to_likelihood&lt;/code&gt; -- using posterior max instead of &lt;code&gt;csaps&lt;/code&gt;.    Could give better index estimation if the visualization test shows problems.&lt;/p&gt;

&lt;p&gt;What if we were using a 3D likelhood for background curves too? Could we still expect the BG ML to be insanely high, and the noise variance to be insanely peaked?  Backproject, estimate 3d noise variance, estimate index set.  The farther away it gets, the more variance in the correlated points.  Which means lower ML, right?  But training will push variance lower.&lt;/p&gt;

&lt;p&gt;Note that larger noise variance in FG model will explain away bad triangulations.  Perturb variances also explain to some extent, but maybe they aren't sufficient to explain enough of it.&lt;/p&gt;

&lt;p&gt;Why is perturb_smoothing_variance basically zero?  If it was higher, it could explain more of the traingulation error, and allow noise variance to drop.  Should we be using a different pertubation model?  Maybe Brownian motion instead of integrated brownian motion?  Visualizing perturbations would be informative here.&lt;/p&gt;

&lt;p&gt;Do smooth perturbations follow a different dynamics model than linear and offset perturbations?  Can we force it to be larger?  what if it was the only option for modelling perturbations?   It's true that a small amount of smoothing variance can result in a huge amount of marginal point variance, and large variances kill ML's.  Probably a mean-reverting model is more sensible -- Ornstein Ulenbeck process, perhaps?  Or SqExp?  I avoided these in the past, because it changes the form of the marginal curve covariances -- they're no longer purely cubic-spline processes.  But I never considered the fact that we need to model triangulation error.&lt;/p&gt;

&lt;p&gt;Observations: setting perturb_smoothing_variance to exactly zero has no change in ML.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Consider tying foreground and background noise variance during training.&lt;/strong&gt;  &amp;lt;---  This is the most pragmatic solution.  Avoids getting mired in details, and acknowledges what we know to be true: image noise arises from the same process in foreground and background models.&lt;/p&gt;

&lt;p&gt;Possibly the fact that we allow a nonzero position_mean in 2D but not in 3D is the issue?&lt;/p&gt;

&lt;h1&gt;Finer-grained index estimation&lt;/h1&gt;

&lt;p&gt;Got it!  In &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;, we have two parameters that determine how fine-grained the sampling is along the smoothed curve.  Each observed curve is then matched against the sampled curve.  The sampling period is 2 pixels, which means there's an average error of about 1 pixel in each index estimate.&lt;/p&gt;

&lt;p&gt;Reducing the sampling period to 1 pixel and re-training gives:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0014
            noise_variance: 0.3986
         position_variance: 1.3555e+04
             rate_variance: 0.3100
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 3.3992e-04
 perturb_position_variance: 0.6874
             perturb_scale: 2.3823

Final ML: -6357.585946
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Although we only slightly changed the sampling period, the final ML improved significantly.  The noise variance dropped from 0.7 to 0.4, too.  Perturb rate variance changed by 2 orders of magnitude!&lt;/p&gt;

&lt;p&gt;Reducing sampling period to 0.5:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0018
            noise_variance: 0.3060
         position_variance: 1.3499e+04
             rate_variance: 0.3098
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 4.3411e-04
 perturb_position_variance: 0.8152
             perturb_scale: 2.4095

Final ML: -5664.35
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The upward ML trend continues, but the noise variance appears to be flattening out.  The perturb_position_variance jumped upward unexpectedly.&lt;/p&gt;

&lt;p&gt;This might explain all of the disparity between the 2D and 3D noise variances.  Unfortunately, we can't reduce the sampling period to 0.0004, because the runtime complexity of the matching  is O(N&lt;sup&gt;2),&lt;/sup&gt; where N is the number of sampled points.&lt;/p&gt;

&lt;p&gt;Better idea: after finding the optimal region in the matching algorithm, improve it by projecting the point onto the line segments neighboring the matched point.  Constant-time, and significantly better!&lt;/p&gt;

&lt;p&gt;Another thought: if the rasterization error was approx. 1 pixel, the sampling error could be reduced to the point where the rasterization error dominated (possibly a sampling period of  0.5 or 0.01 would achieve this).  That way, both 2D and 3D noise sigma would be dominated by rasterization error, and would train to similar values.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;implement post-match index improvement.&lt;/li&gt;
&lt;li&gt;Plot reconstruction residuals, look for correlation model.

&lt;ul&gt;
&lt;li&gt;Goal: determine if residuals are truely independent, and belong in the noise bucket.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try training BG and FG together, with the same noise variance.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Wed, 14 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/14/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/14/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Theoretical Rate variance bug; Training background curve model</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Curve reversing thoughts&lt;/h1&gt;

&lt;p&gt;The reversed curve issue only really matters during training.  Our tests show that curve-flip moves will mix well, even if the maximum isn't always correct.  Adding connections between parent and child curves should resolve these issues.&lt;/p&gt;

&lt;p&gt;During training, derive curve direction from ground-truth.&lt;/p&gt;

&lt;h1&gt;Theoretical Rate Variance (take 2)&lt;/h1&gt;

&lt;p&gt;Realized that my last attempt at this had two bugs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I used &lt;code&gt;rand()&lt;/code&gt; instead of &lt;code&gt;randn()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;I was normalizing by the &lt;em&gt;squared&lt;/em&gt; vector magnitude.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Fixing this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir = randn(3,10000000);
dir = bsxfun(@times, dir, 1./sqrt(sum(dir.^2)));
var(dir(:))

    ans =
        0.3332
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare this to earlier theoretical results of ~0.23.&lt;/p&gt;

&lt;p&gt;This new result is interesting, because it is 25% higher than the emperical results we've been getting.  I'm guessing that the fact all of the curves point upward reduces the variance. To prove, we'll force all points to be in the top hemishphere:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; dir = randn(3,10000000);
 dir(3,:) = abs(dir(3,:));
 dir = bsxfun(@times, dir, 1./sum(dir.^2));

 var(dir(:))

    ans =
        0.3149
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep.  And in practive, our values take on an even smaller range of directions.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Repeating for the 2D case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir = randn(2,10000000);
dir = bsxfun(@times, dir, 1./sqrt(sum(dir.^2)));
var(dir(:))

    ans =
        0.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This strongly suggests a pattern of variance being 1/D.&lt;/p&gt;

&lt;h1&gt;Connection test&lt;/h1&gt;

&lt;p&gt;does connecting each of the curves result in better ML?  Do we need to marginalize?&lt;/p&gt;

&lt;h1&gt;training background model&lt;/h1&gt;

&lt;p&gt;construct training ML for background ML
construct mats for bg curve models.&lt;/p&gt;

&lt;p&gt;Result: &lt;code&gt;train/tr_train_bg.m&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 1.7837e+04
     rate_variance_2d: 0.5000
    noise_variance_2d: 9.4597e-04
smoothing_variance_2d: 0.0157
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that noise_variance_2d is so low.  We expected it to be on the order of 1 pixel.  More discussion on this later.&lt;/p&gt;

&lt;p&gt;I retrained the BG model for &lt;em&gt;only&lt;/em&gt; the foreground curves, and evaluated the ML under it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 5.8116e+03
     rate_variance_2d: 0.5000
    noise_variance_2d: 4.8105e-04
smoothing_variance_2d: 0.0053
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Smaller noise variance, smaller smoothing variance.  This shrinking of variance with smaller training set is typical overfitting behavior.  Not of much concern.&lt;/p&gt;

&lt;p&gt;Here's the comparison against the ML for the trained foreground model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bg model = 4611.886746
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not good.  The FG model on true foreground curves should have a better marginal likelihood than the same curves under the BG model.&lt;/p&gt;

&lt;p&gt;Some questions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Why is bg noise variance so low??

&lt;ul&gt;
&lt;li&gt;did we smooth the detected curves before storing them?&lt;/li&gt;
&lt;li&gt;If so, why isn't the foreground model lower?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why is the background model so much better than the foreground model?

&lt;ul&gt;
&lt;li&gt;We expect foreground curves to have a higher marginal likelihood under the foreground model than the background.&lt;/li&gt;
&lt;li&gt;could it be an indexing issue?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This warrants further investigation.&lt;/p&gt;

&lt;h2&gt;Other observations&lt;/h2&gt;

&lt;p&gt;If I force the noise variance to be equal to that of the fg model (0.72), the ML drops significantly (fg results reprinted for convenience):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; bg model = -9413.1e+03
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we're back in business.  This is a good sanity check, but it doesn't explain why we can't get similar noise variances for both models when training.&lt;/p&gt;

&lt;p&gt;Possibly the smoothing variance would need to change in this case.  Retraining, with noise_variance forced to 0.72:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 5.8116e+03
     rate_variance_2d: 0.5000
    noise_variance_2d: 0.7204
smoothing_variance_2d: 2.7394e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Smoothing variance dropped dramatically.  ML comparison (fg results reprinted for convenience):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bg model = -9214.632874
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that bg ML didn't significantly change (-2%) after optimizing noise variance (which did change a lot).&lt;/p&gt;

&lt;p&gt;Might be worthwhile visualizing the optimal fits with these parameters.  Are we oversmoothing?  undersmoothing?  These would suggest a bug.&lt;/p&gt;

&lt;h2&gt;ML Validity Testing&lt;/h2&gt;

&lt;p&gt;Running reference ml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data_2 = init_data_curves_ml(data_, bg_train_params_done_force)
sum([data_2.curves_ml{:}])

    ans =
      -5.1552e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Very different from the training implementation.  Need to dig deeper to determine the cause.&lt;/p&gt;

&lt;h2&gt;Misc Thoughts&lt;/h2&gt;

&lt;p&gt;Do we need to re-estimate the index set during training of the FG model?&lt;/p&gt;

&lt;p&gt;Iterate: train, re-index, repeat.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;investigate disparity between training ML and reference ML for background curves.&lt;/li&gt;
&lt;li&gt;Further investigate the FG vs. BG marginal-likelihood issue.&lt;/li&gt;
&lt;li&gt;test the re-indexing approach to FG model training.&lt;/li&gt;
&lt;li&gt;re-build end-to-end sampler.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Tue, 13 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Re-run training, Re-reconstruction, Curve-Flipping</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Re-training&lt;/h1&gt;

&lt;p&gt;Re-ran training after several bug-fixes.&lt;/p&gt;

&lt;h2&gt;New Files:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;train/tr_train_all.m&lt;/code&gt;  - Utility method for training all four models.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;experiments/exp_2013_08_11_train_all.m&lt;/code&gt;  - end-to-end training example; recreates results here.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;All results generated by &lt;code&gt;exp_2013_08_11_train_all.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0024
            noise_variance: 1.2308
         position_variance: 1.6072e+04
             rate_variance: 0.2743
perturb_smoothing_variance: 1
     perturb_rate_variance: 1
 perturb_position_variance: 1
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7192
         position_variance: 1.6132e+04
             rate_variance: 0.2451
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1292e-06
 perturb_position_variance: 0.4849
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;OU-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0018
            noise_variance: 0.7207
         position_variance: 1.6117e+04
             rate_variance: 0.2480
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1355e-06
 perturb_position_variance: 0.5172
             perturb_scale: 0.9202
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised to see that noise-variance only changed by a factor of 10, not 100.  However, the resulting noise_variance is right in the range that you'd expect arising from pixel-grid rasterization error.&lt;/p&gt;

&lt;p&gt;OU perturb-scale is lower than in the last case, and perturb position and rate variance is lower, too.&lt;/p&gt;

&lt;p&gt;SqExp perturb-scale is higher than in the last case, and perturb rate variance is lower.&lt;/p&gt;

&lt;p&gt;Lower position and rate variance makes sense after correcting curve-reversals.&lt;/p&gt;

&lt;p&gt;However, since we trimmed the pre-tails, a higher global and perturb position variance should result.  The result we're seeing is a combination of these competing effects.&lt;/p&gt;

&lt;h1&gt;Reconstructions&lt;/h1&gt;

&lt;p&gt;Some curves are flipped; need to an approach that will detect and correct flipped curves.&lt;/p&gt;

&lt;p&gt;Images and javascript generated by &lt;code&gt;../experiments/exp_2013_08_11_reconstruct_for_web.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-ind-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-9.png&quot;
        ]

    construct_animation($(&quot;#ind-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ind-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;OO-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-ou-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-9.png&quot;
        ]

    construct_animation($(&quot;#sqexp-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;sqexp-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;h1&gt;Detecting and Flipping Curves&lt;/h1&gt;

&lt;p&gt;Experiment: &lt;code&gt;../experiments/exp_2013_08_11_flip_curves.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Result: doesn't really work.  Lots of false negatives.&lt;/p&gt;

&lt;p&gt;Algorithm output: 2     4     5     6     8    10    12
Ground Truth: 1 2 4 5 6 8 9 10 11 12 14 15&lt;/p&gt;

&lt;p&gt;Not really sure why this is failing.  After flipping, most of these curves are closer to the origin, which is promoted by position_variance.  And any tip-perturbations should be better modelled after flipping.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Think more on curve-reversing&lt;/li&gt;
&lt;li&gt;Central curve extraction&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;li&gt;end-to-end correspondence sampling&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 11 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log</guid>
			</item>
		
	</channel>
</rss>
