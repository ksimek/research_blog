<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Ancestral Sampling from poserior (Markov-aware sampling)</title>
				<description>&lt;p&gt;Realization: nystrom may be tricky, because the same thinning factor probably won't be appropriate for all scenarios (e.g. short curves).&lt;/p&gt;

&lt;p&gt;Possible approach: always take first and last inidices of a curve; thin longer curves more aggressively.&lt;/p&gt;

&lt;p&gt;Better idea: use markov structure.&lt;/p&gt;

&lt;h2&gt;Exploiting Markov structure for Sampling plants &lt;/h2&gt;

&lt;p&gt;Main ideas: Use inheritence sampling to sample fully tree.  Two rules (1) Parent curve posterior should only depend on it's observed points and a small number of points on its children.  (2) Child curves depend also on the sampled parent points.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parent points&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The main question here is how many and which observed points of  child-curves should be included to compute posterior?  Heuristic approach: start with child points nearest to branch-point and work outward until information gain is negligible.&lt;/p&gt;

&lt;p&gt;Information gain is the change in entropy of the branch-point posterior, conditioned on the active set.  Entropy is proportional to the determinant of the covariance matrix.&lt;/p&gt;

&lt;p&gt;Let bp be the branch point, and I be the active set.&lt;/p&gt;

&lt;div&gt;
\[
Entropy(bp | I) \propto K(bp, bp) - K(bp, np) [K(np, np)  + \Sigma_y(np)]^{-1} K(np, bp)
\]
&lt;/div&gt;


&lt;p&gt;Where Sigma_y is the observation covariance.  We may choose to use the prior conditional entropy, which is a simpler stand-in for the posterior conditional entropy.&lt;/p&gt;

&lt;div&gt;
\[
Entropy(bp | I) \propto K(bp, bp) - K(bp, np) [K(np, np)]^{-1} K(np, bp)
\]
&lt;/div&gt;


&lt;p&gt;Note that the entropy is now only a function of the indices of the observations, not the observations themselves.  Thus, each observation only contribues one dimension to the matrix inversion, instead of three, making the computation 3&lt;sup&gt;3&lt;/sup&gt; times faster.  This is only sensible if the observation variance is relatively uniform between observations, which isn't necessarilly true, but it may be servicable heuristic if thresholds are set appropriately high.&lt;/p&gt;

&lt;p&gt;Opportunities exist to do rank-one updates to inverse, but probably not necessary, as the matrices should be small.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Child curves&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Note that child-curves can also be parent-curves, so the previous approach for determining the active set should be followed.&lt;/p&gt;

&lt;p&gt;We need to estimate the conditional prior probability for the curve, conditioned on its sampled parent.  If the sampled parent contains the branch point, bp, the conditional prior covariance is already available in the track's pre-computed prior_K field, and the mean is bp.  In general, bp isn't sampled, so we must use the nearby points on the parent to find the conditional prior of bp first and sample it explicitly.&lt;/p&gt;

&lt;p&gt;Let bp be a branch point, cp be the child point, and nbp be the &quot;neighbors&quot; of bp on the parent curve, i.e. the markov blanket of bp on the parent.&lt;/p&gt;

&lt;p&gt;Given bp, the child points are distributed by&lt;/p&gt;

&lt;div&gt;
\[
cp \mid bp \sim \mathcal{N}(bp, K_{prior})
\]
&lt;/div&gt;


&lt;p&gt;The branch point is distributed by&lt;/p&gt;

&lt;div&gt;
\[
bp \mid pnp \sim \mathcal{N}(\mu, \Sigma)
\]
&lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt;
\[
\mu = k(bp, nbp) k(nbp, nbp)^{-1} nbp \\
\Sigma = k(bp, bp) - k(bp, pnp) K(pnp, pnp)^{-1} K(pnp, bp)
\]
&lt;/div&gt;


&lt;p&gt;The a neighborhood size of four points around bp should be sufficient, since this model is piecewise cubic, but it may be worthwhile to experiment with more, since we aren't in bottleneck territory.&lt;/p&gt;

&lt;p&gt;Now that we have conditional priors, we can construct the conditional posterior using the standard formula.&lt;/p&gt;

&lt;h2&gt;Mixing noisy and noise-free data in Posterior&lt;/h2&gt;

&lt;p&gt;Ran into a snag when trying to incorporate the noise-free conditional values that arise during ancestral sampling.  Derived an elegant solution; see the &lt;a href=&quot;/ksimek/research/2013/10/29/reference&quot;&gt;writeup here&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Implementing markov-aware pixel likelihood&lt;/h2&gt;

&lt;p&gt;For now, using a constant number of points on parent, instead of using the entropy method above.  Could add later; basic approach: precompute all covariances and add/remove one at a time.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Seems to be working, but slow as hell (slower than nystrom method). profiling&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Construct_attachment_covariance_3 is called 486 times!&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Construct non view-specific matrices outside of view loop.&lt;/li&gt;
&lt;li&gt;nystrom for large curves?&lt;/li&gt;
&lt;li&gt;do all covariances at once?  saves the computation of sibling and parent objecst?&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;implemented a first attempt at speedup; producing faulty results.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish optimized ancestral sampling&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Tue, 29 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/29/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/29/work-log</guid>
			</item>
		
			<item>
				<title>Mixing Noisy and Noise-free values in GP Posterior</title>
				<description>&lt;p&gt;When doing ancestral sampling of the posterior, each curve's conditional posterior depends on (a) it's relevant (noisy) observation and (b) the sampled (noise-free) values of the parent.  We can incorporate both in the posterior by treating the noise-free values as observations with zero variance in the likelihood.&lt;/p&gt;

&lt;p&gt;In practice, theres a minor issue implementing this.  Recall that because 3D observations are degenerate in one direction (namely the backprojection direction), we prefer to work with the precision matrix, \(\Lambda\), instead of a covariance matrix.  Under this representation, noise-free values have infinite precision, so operations on \(\Lambda\) are invalid.  Instead, we take a hybrid appraoch, using both precisions and covariances.&lt;/p&gt;

&lt;p&gt;Recall the standard formulation for the posterior mean:&lt;/p&gt;

&lt;div&gt;
\[
    \mu = K_* \left[ K + \Sigma \right]^{-1} y
\]
&lt;/div&gt;


&lt;p&gt;Without loss of generality, assume zero-noise observations appear after noisy observations last.  We can rewrite the posterior in terms of the precision matrix \(\Lambda\) of the noisy values:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \mu &amp;= K_* \left[ K + \left( 
                \begin{array}{cc}
                    \Lambda^{-1} &amp; 0 \\
                    0 &amp; 0
                \end{array}
            \right) \right]^{-1} y \\
        &amp;= K_* \left[
        
            \left( 
                \begin{array}{cc}
                    \Lambda &amp; 0 \\
                    0 &amp; I
                \end{array}
             \right) 
        K + 
            \left( 
                \begin{array}{cc}
                    I &amp; 0 \\
                    0 &amp; 0
                \end{array}
             \right) 

            \right]^{-1} 
            \left( 
                \begin{array}{cc}
                    \Lambda &amp; 0 \\
                    0 &amp; I
                \end{array}
             \right) 
            y \\
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;We can implement by slightly modifying our code for the precision-matrix formulation of posterior.  First, give all noise-free values a precision of 1.0 in \(\Lambda\), and then modifying the \(I\) inside the parentheses by zeroing-out elements corresponding to noise-free values.&lt;/p&gt;

&lt;p&gt;The expression for posterior covariance is derived in the same way, and has similar form.&lt;/p&gt;
</description>
				<pubDate>Tue, 29 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/29/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/29/reference</guid>
			</item>
		
			<item>
				<title>Implementing Nystrom method; bugs in posterior sampling code</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Re-ran with method 2.  Still getting negative eigenvalues.&lt;/p&gt;

&lt;p&gt;Possibly non-symmetry issue.&lt;/p&gt;

&lt;p&gt;Change gears -- work on fast implementation, then resume debugging.&lt;/p&gt;

&lt;h2&gt;Reduced rank approximation of data-covariance&lt;/h2&gt;

&lt;div&gt;
&lt;p&gt;
Goal: approximate \(\left(K + \Sigma_D \right)^{-1}\), using a low-rank approximation of K;  where \(\Sigma_D\) is the likelihood covariance.
&lt;/p&gt;&lt;p&gt;
In our case \(\Sigma_D\) has infinite covariance, so this inverse doesn't exist. However, if we work with precision matrix \(\Lambda = \Sigma_D^{-1}\), and use the decomposition \( S' S = \Lambda \) we can replace this expression with the equivalent

\[
    S' \left( S K S' + I \right)^{-1} S
\]

Even though S is rank-deficient, the inverse in this expression does exist, thanks to the finite positive values being added to the diagonal.  
&lt;/p&gt;&lt;p&gt;
We approximate the above expression as follows.  Let \(K\) be an \(N\) by \(N\) matrix. We can take the eigendecomposition \(K = V D V'\) and approximate it with \(\tilde K = \tilde V \tilde D \tilde V'\), where \(\tilde V\) and \(\tilde D\) consist of the first \(n\) eigenvalues and eigenvectors of \(K\) respectively.   We can use this low-rank approximation with the [Woodbury matrix identity](http://en.wikipedia.org/wiki/Woodbury_matrix_identity) to approximate the above inverse in \(O(n^3)\) time, rather than \(O(N^3)\).  The Woodbury identity is
        
\[
    (A + U C V )^{-1} = A^{-1} - A^{-1} U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}
\]

Setting \(A = I\), \(U = V' = S \tilde V \) and \(C = \tilde D \), we get:
    
\[
    \left( S K S' + I \right)^{-1} = I - S \tilde V (\tilde D^{-1} + \tilde V' S' S \tilde V)^{-1} \tilde V' S'
\]
&lt;/p&gt;&lt;p&gt;
It remains to find \(\tilde V\) and \(\tilde D\) efficiently.  Naive eigenvalue decomposition takes \(O(N^3)\) time, which isn't any better than direct inversion.  Sections 4.3.2 and 8.1 of Williams and Rasmussen show how to approximate \(n\) eigenfunctions and eigenvalues from \(n\) data points in \(O(n^3)\) time.  Eigenvectors \(\tilde V\) arise by evaluating the approximate eigenfunctions at the appropriate indices.
&lt;/p&gt;
&lt;p&gt;

Substitutuing back into the original expression by surrounding with (\(S'\) and \(S\)), we get the final expression:
    
\[
\begin{align}
    \approx &amp; S' \left ( I - S \tilde V (\tilde D^{-1} + \tilde V' S' S \tilde V)^{-1} \tilde V' S' \right) S \\
    =&amp;\Lambda - \Lambda \tilde V (\tilde D^{-1} + \tilde V' \Lambda \tilde V)^{-1} \tilde V' \Lambda
    \end{align}

\]
&lt;/p&gt;
&lt;/div&gt;


&lt;h2&gt;Work log&lt;/h2&gt;

&lt;p&gt;Implemented two different implementations of &quot;nystrom solve&quot; (&lt;code&gt;tools/nystrom_solve.m&lt;/code&gt;).  Crude testing shows big speedup, but stalling on a big eigenvalue decomposition.  Replacing with a call to &quot;chol&quot;, waiting on visual results.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&quot;chol&quot; gives much faster results. (no need to symmetrize)  But results are junk -- blank screen.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, bug in call to chol; fixed.&lt;/p&gt;

&lt;p&gt;still getting black.  Posterior covariance is not positive definite; eigenvalues are negative.&lt;/p&gt;

&lt;p&gt;When I force covariance to zero (to view the mean value), it's still in the ballpark but wonky (same as before).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Clue&lt;/strong&gt;:   When re-using (thinned) input covariance matrix and indices, result looks sensible.  Clearly, we have a problem with how K_star is computed (and likelihood K_star_star, too).&lt;/p&gt;

&lt;p&gt;Comparing different K_star's...&lt;/p&gt;

&lt;p&gt;Looks like on-diagonal elements differ, off-diagonals are okay.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Got it!  Wacv was trained using no-perturb model, but is being tested with OU-perturb.&lt;/p&gt;

&lt;p&gt;TODO: edit get_all_Wacv and/or get_wacv_result to receive a model-type... done.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;reducing test to block (1,1)&lt;/p&gt;

&lt;p&gt;possible sources of disparity: indices, function&lt;/p&gt;

&lt;p&gt;checked indices -- same.&lt;/p&gt;

&lt;p&gt;check params -- model-type doesnt match.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;First view now looks good in matlab.&lt;/p&gt;

&lt;p&gt;Running all views in likelihood_server.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Means look great!.  Now on to random perturbed.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Cholesky claims posterior covariance isn't positive definite.&lt;/p&gt;

&lt;p&gt;Possibly a bug in construct_attachment_covariance_3?&lt;/p&gt;

&lt;p&gt;Re-running with K_star_star from original covariance matrix.  Still gives &quot;not positive definite&quot; errors.&lt;/p&gt;

&lt;p&gt;Resorting to eigenvalue decomposition method.&lt;/p&gt;

&lt;p&gt;Results look good!  perturbation from mode is &lt;em&gt;TINY&lt;/em&gt;, at least in the ground truthed WACV dataset.  If this is true for hyptoheses in the full system, we can probably get away with simply taking the mean and saving significant computation of the covariacne matrix.&lt;/p&gt;

&lt;h2&gt;Tuning Nystrom factor&lt;/h2&gt;

&lt;p&gt;nystrom factor: 50&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-28-nystrom_50.png&quot; alt=&quot;nystrom factor = 50&quot; /&gt;&lt;/p&gt;

&lt;p&gt;nystrom factor: 20&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-28-nystrom_20.png&quot; alt=&quot;nystrom factor = 20&quot; /&gt;&lt;/p&gt;

&lt;p&gt;nystrom factor: 10&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-28-nystrom_10.png&quot; alt=&quot;nystrom factor = 10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nystrom factor apparently has a huge affect on reconstruction quality.  Possibly better choice of thinned index set (compared to uniform sampling) will improve sensitivity, but a brief attempt at this (grouping xyz indices ) had net negative effect.&lt;/p&gt;

&lt;p&gt;Needs more thought, possibly should resort to subset of data (SD) or subset of regressors (SR) method, or exploit markov structure.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Nystrom method is implemented and provides huge speedups without obvious in rendered results.&lt;/li&gt;
&lt;li&gt;Spent many hours debugging issues with covariance matrix.  Turned out to not be bug in theory or implementation of the math, but from an unexpected hard-coded value in wacvin wacv results.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Nystrom thinning: 10
ll2 spacing: 3
Unclear&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;what is best thinning amount for Nystrom method?&lt;/li&gt;
&lt;li&gt;what is best spacing for output index set&lt;/li&gt;
&lt;li&gt;are perturbations always virtually nil?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;refactor construct_attachment_covariance_* into a single implementation.&lt;/li&gt;
&lt;li&gt;optimize &lt;code&gt;curve_tree_ml_2.m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 28 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/28/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/28/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Testing likelihood #2 (2-day)</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Client send/receive timeout &lt;/h2&gt;

&lt;p&gt;Tried to get send/receive to timeout if the server didn't respond.  It turns out, although this feature exists in native Unix sockets, the boost library abstractions render them useless.  To get this, I'll need to use asynchronous IO, which I'm not ready to jump to, yet (also not sure if callbacks will work in mex, since the callback code might not exist after the constructor function returns).&lt;/p&gt;

&lt;p&gt;Let's just take the approach that the server must always respond quickly or disconnect.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Testing &lt;code&gt;curve_tree_ml_2&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Need a reliable way to get a testing Trackset.  Write something for wacv -- &lt;code&gt;get_wacv_trackset&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Modified run_all_wacv to save Tracks;  re-running on all datasets.  &lt;code&gt;get_wacv_results&lt;/code&gt; now returns Tracks as well as means.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tweaked semantics&lt;/strong&gt; so this now &lt;em&gt;only&lt;/em&gt; computes the pixel likelihood.  The full likelihood is the sum of this and &lt;code&gt;curve_tree_ml&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tweaked &lt;code&gt;construct_attachment_covariance_2.m&lt;/code&gt; so the self-covariance matrices are computed in the function (if needed) instead of being passed in.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Spin-off &lt;code&gt;construct_attachment_covariance_3.m&lt;/code&gt; a fully general version that receives input and output indices, and optionally a pre-computed self-covariance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TODO: &lt;/strong&gt; make this the &quot;official&quot;/dispatch version, make other versions call this (or eliminate altogether)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;data matrix is HUGE.  8913 dimensions.  Is this right?  Also, we can move inversion outside of the loop (maybe).&lt;/p&gt;

&lt;p&gt;Consider 'subset of data' method, or other dimensionality-reduction method&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;&lt;strong&gt;Issue&lt;/strong&gt;: Can't do cholesky decomposition, because points branching from the base are redundant.  Using SVD instead.&lt;/h2&gt;

&lt;p&gt;Okay, it appears to be successfully running end-to-end.  Haven't confirmed results yet, but one thing is clear... its REALLLLLY slow (3 minutes and counting)&lt;/p&gt;

&lt;p&gt;Can do ancenstral sampling to exploit tree structure to speed it up.  can further use markov property to break up curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, not quite end-to-end success.  Some indexing, reshaping issues.   Other bugs found&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;wasn't adding posterior mean to sampled results&lt;/li&gt;
&lt;li&gt;missing transpose in posterior covariance equation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Debugging sampled curves&lt;/h2&gt;

&lt;p&gt;Message looks okay under inspection, but getting -inf (due to exception).  Dumping...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found a recent bug in code that builds the Gl_curve.  When I chnaged an assert to an if/throw, I forgot to negate the conditional.&lt;/p&gt;

&lt;p&gt;Getting finite values now.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Result looks in the ballpark, but some perturbations look questionable (considering the tight constraints on the WACV dataset).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-27-perturb-comparison.png&quot; alt=&quot;perturbed vs. original&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Doind a full dump-mode run...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found a bug in a recent refactor of dump-mode, causing segfaults.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;All views drift very far from their true values.  Possibly a math bug in the magnitude of the posterior covariance?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Refactored server's &quot;dump mode&quot;  to continuously dump each message as it's received, instead of running in offline mode and dumping only the passed-in model.&lt;/p&gt;

&lt;p&gt;Trying method2.  Got similar results, qualitatively; per-view curves still have some bizzarre features.  Interestingly, the perturbations between successive samples of method 2 (and between method2 and method 1) are relatively small, suggesting this is an issue with the mean, not the variance.&lt;/p&gt;

&lt;p&gt;Recall that we only tested the no-perturb model for wacv reconstruction; not the per-view reconstruction.&lt;/p&gt;

&lt;p&gt;Re-running WACV dataset 2 with OU-perturb model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Looks sensible.  So that rules out the parameter settings causing bad mu's.  We should be getting exactly WACV results;  can we get there?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;use K_star = K&lt;/li&gt;
&lt;li&gt;use zero covariance matrix (always sample at the mean)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Running...  (slow, because matrix multiplication is so much larger)&lt;/p&gt;

&lt;p&gt;I'm guessing the bug is in the full-tree covariance .&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Getting &quot;degenerate curve&quot; error.  Thinning points fixed.&lt;/p&gt;

&lt;p&gt;Dumped results look good.  This suggests that mean math is likely correct, as long as K_star is okay.  So K_star math is probably wrong.&lt;/p&gt;

&lt;p&gt;Need to write a test to confirm and start debugging.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done. Results match on symmetric and non-symmetric index sets. Which means no progress made on this bug...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Re-ran, but using non-zero Sigma.  Resulting covariance was not positive definite; SVD was complex-valued.&lt;/p&gt;

&lt;p&gt;Possibly this is a result of the same bug?&lt;/p&gt;

&lt;h2&gt;Covariance matrix Rank-reduction&lt;/h2&gt;

&lt;p&gt;While previous test was running, read-up on reduced-rank approximations to K.  Nystrom method seems sensible to speed up matrix inversion.  Doesn't avoid the matrix multiplication  with K_star, or cholesky decomposition of the posterior covariance...  but using a smaller output index set seems like a reasonable approach to mitigating both of those.&lt;/p&gt;

&lt;p&gt;Curious how many non-negligible eigenvalues we have; how many data points we should use.  Recalling a plot of eignevalues from a few days ago, it looked like less than 0.1% of the dimensions are significant, but need to get a concrete number.&lt;/p&gt;

&lt;p&gt;Alternative approach is to use the problem's unique markov structure to sample each section piecewise.  Needs some thought for how to do in the case of the posterior; probably need to estimate a markov blanket for each, along with sampling each branch point and conditioning on it.  Upside: better chance at asymtotic running time improvement. Down-side: not general, not as re-usable within the project.&lt;/p&gt;

&lt;h2&gt;To Be Continued&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Bug:&lt;/strong&gt; Samples generated from &lt;code&gt;curve_tree_ml_2_debug.m&lt;/code&gt; are wonky.
&lt;strong&gt;Theory&lt;/strong&gt;: Bug is related to posterior covariance.  Posterior mean looks sensible.
&lt;strong&gt;File&lt;/strong&gt;: &lt;code&gt;curve_tree_ml_2_debug.m&lt;/code&gt; &amp;lt;-- alternative version, where K is used for K_star and K_star_star.  Currently giving non-positive-definite results for posterior covariance using method 1.  Currently Stumped&lt;/p&gt;

&lt;p&gt;Fix sampling sigma bug.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;get likelihood from multiple samples -- what is the variance of the MC estimator?&lt;/li&gt;
&lt;li&gt;re-run wacv with multi-view data, but no-perturb output&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sat, 26 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/26/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/26/work-log</guid>
			</item>
		
			<item>
				<title>params, CVPR 2014</title>
				<description>&lt;p&gt;&lt;a href=&quot;/ksimek/research/CVPR2014/params.html&quot;&gt;See CVPR 2014 parameters page&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 26 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/26/params-cvpr-2014</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/26/params-cvpr-2014</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;Implementing one-object-per-view in likelihood_server&lt;/p&gt;

&lt;p&gt;Overloaded bd_mv_likelihood' evaluate and dump  functions to receive a sequence of renderables.&lt;/p&gt;

&lt;p&gt;Write a &quot;wrap_all_As_silhouette&quot; to, well, wrap all renderables in silhouette renderers.&lt;/p&gt;

&lt;p&gt;re-wrote message-to-curve function to decode the message to a vector-vector-vector, and convert that to a vector-of-gl_curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Compiling work from last 24 hours.&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;Generate a multi-model message from matlab (dummy)&lt;/li&gt;
&lt;li&gt;test message in likelihood_server_2.cpp

&lt;ol&gt;
&lt;li&gt;generate random samples from wacv data.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;Doing dummy message test.&lt;/p&gt;

&lt;p&gt;At first, accidentally sent to old implementation... and it didn't barf!  This is unsettling, because the message format has changed significantly.  Did I ever recompile the mex files?&lt;/p&gt;

&lt;p&gt;Nope.  got some mex compile errors to deal with.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Friday&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;okay, sending a random model using the new one-per-view message system isn't crashing.&lt;/p&gt;

&lt;p&gt;Questions&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the likelihood looks high, considering it's a ranomd model .  is a null model better?  is the ground-truth model better?&lt;/li&gt;
&lt;li&gt;visualize the received message;  is each view different?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Trying the null model from 1., the mex file crashed matlab.  Empty curvesset isn't handled.&lt;/p&gt;

&lt;p&gt;trying all-zeros.  Server crashed -- coincident points aren't handled gracefully.  Fixed (now it returns -inf for log likelihood).&lt;/p&gt;

&lt;p&gt;issues
server: error creating gl_curves causes client disconnect. fix exception handling
client: doesn't handle server crashes gracefully.  adding timeout
client: doesn't handle empty curveset&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;random sample from multi-view posterior (instead of single max posterior)&lt;/li&gt;
&lt;li&gt;get likelihood from multiple samples -- what is the variance of the MC estimator?&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 24 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/24/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/24/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Implementing Two-term likelihood</title>
				<description>&lt;p&gt;Working on Matlab integration.&lt;/p&gt;

&lt;p&gt;Quick test shows we can get about 20 evaluations per second.  Not bad, considering each evaluation consists of 9 image likleihoods.  There's some room for improvement here, but probably not worth pursuing at the moment:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;leapfrog rendering: render (a) while evaluating on (b).&lt;/li&gt;
&lt;li&gt;cleaning up geometry and fragment shaders (fewer branches, less storage)&lt;/li&gt;
&lt;li&gt;try other GPU blurring routines.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;New Likelihood&lt;/h1&gt;

&lt;p&gt;Scenario: we have a decent likelihood that is linear-gaussian (and a gaussian prior), so we can compute the marginal likelihood in closed-form.  However, we'd like to incorporate additional sources of evidence whose likelihoods aren't gaussian.  We'll see how we can estimate the joint marginal likelihood with simple Monte-Carlo sampling (no MCMC needed, no gradient).&lt;/p&gt;

&lt;h2&gt;Derivation&lt;/h2&gt;

&lt;p&gt;The old marginal likelihood looked like this:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1) &amp;= \int p(x) p(D_1 | x) dx
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;After introducing the extra likelihood term, the joint probability is no longer linear-gaussian, so the exact marginal likelihood involves an intractible integral.  However, by  re-arranging, we see we can get a good monte-carlo approximation:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;= \int p(x) p(D_1 | x) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= \int p(x | D_1) p(D_1) p(D_2 | x) dx &amp; \left(\text{Bayes thm (see below)}\right) \\
p(D_1, D_2) &amp;= p(D_1) \int p(x | D_1) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= p(D_1) \frac{1}{N} \sum p\left(D_2 | x^{(*)}\right) &amp; \text{(Monte Carlo)}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;In the second line, I've replaced the first two terms using Bayes theorem.  In the last line, the x-stars are samples from \(p(x | D_1)\), which we have in closed-form due to linear-Gaussian prior and likelihood for \(D_1\).&lt;/p&gt;

&lt;p&gt;Thus, we see if at least one source of data yields a linear-gaussian likelihood, we can incorporate additional data with arbitrary likelihoods  in a principled way.  In many cases, \(p(x | D_1) \) has low variance, so a small number of Monte-Carlo samples are sufficient for a good estimate -- even a single sample could suffice.  Even if the estimates are bad, they are unbiased, so any MCMC involving the marginal likelihood will converge to the target distribution.&lt;/p&gt;

&lt;h2&gt;Importance Sampling version&lt;/h2&gt;

&lt;p&gt;We can alternatively derive this in terms of importance sampling, setting the proposal probability q(x) to \(p(x | D_1) \):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;\approx 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{1}{q(x)} \\
            &amp;= 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{p(D_1)}{p(x) p(D_1 | x)} \\
            &amp;= 1/N \sum p(D_2 | x) p(D_1) \\
            &amp;= p(D_1) 1/N \sum p(D_2 | x) 
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File:&lt;/strong&gt; curve_tree_ml_2.m&lt;/p&gt;

&lt;p&gt;Basic idea&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;construct a thinned output index set (optional, but smart)&lt;/li&gt;
&lt;li&gt;construct a posterior distribution over the thinned set&lt;/li&gt;
&lt;li&gt;Add perturbation variance to the posterior&lt;/li&gt;
&lt;li&gt;take average over n trials: sample curveset and evaluate pixel likelihood&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Step 2 required updating to construct_attachment_covariance, which only constructs symmetric covariance matrices.  We need the covariance between indices with observations and the desired output indices.  Fully refactored that function into &lt;code&gt;construct_attacment_covariance_2.m&lt;/code&gt;; confirmed correctenss in the case of the self-covariance by using an existing test for version 1 of that function.  If non-symmetric, the upper-triangular blocks are processed in a second pass, swapping indices so we can re-use existing code.&lt;/p&gt;

&lt;p&gt;Need to try view-specific sampling, i.e. sample 9 different curves from 9 different views.  This refactor affects likelihood server, client, and message format.  I'm worried about the performance hit, but probably not worth worrying about (or futile).  Coarse sampling of indices could mitigate.    In any case, view-specific sampling is probably necessary, because we're using such low blurring levels in the Bd_likelihood, so the reconstruction needs to fall near the data.  We've seen how plant motion and camera miscalibration cause &quot;good&quot; 3D curves to reproject up to 10 pixels away from the data in some views.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Side-note&lt;/strong&gt; - the tcp connection is working very reliably so far!  Even after the machine sleeping/resuming several times, and suspending the server job for serveral hours, the socket is still valid and communicating flawlessly!&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Implementing per-view sampling&lt;/h2&gt;

&lt;p&gt;need to refactor send_curves.m to send &lt;code&gt;num_views&lt;/code&gt; curves instead of one.&lt;/p&gt;

&lt;p&gt;Now receive as num_curves x num_views cell array.&lt;/p&gt;

&lt;p&gt;Coded vector&lt;sup&gt;3&lt;/sup&gt; to/from message.&lt;/p&gt;

&lt;p&gt;todo: rewrite likelihood server to receive vector3, somehow pass per-view models to likelihood&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;multi-view likelihood is out - it's one-model, multi-view.  We need multi-model, multi-view.

no, MV likelihood is okay, just add an extra operator() that receives a sequence of renderables 
whose size is equal to the number of views
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;p&gt;Some protocol for starting and loading the likelihood server from matlab code
Stress test likelihood server&lt;/p&gt;
</description>
				<pubDate>Wed, 23 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still searching for inf bug in libcudcvt blurred_difference_likelihood.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found it:  my GPU-based log_sum routine had a bug.  instead of subtracting the maximum value before exp-summing, it subtracted an arbitrary value.  The find-max loop looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Real pi = v[0];

#pragma unroll
for(unsigned int i = 1; i &amp;lt; N; ++i)
{
    pi = pi &amp;lt; v[0] ? v[0] : pi;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, the zeros should be i's.&lt;/p&gt;

&lt;p&gt;A great bug to have found and fixed, but really frustrating that I let it slip through in the first place.  So stupid!  But it didn't affect correctness in the common case, so my tests didn't catch it.  It makes a good case for randomized testing (as if a good argument was lacking...).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(semi-obvious) note to self: conditional gaussian mixture is not equal to mixture of conditional gaussians!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;okay, a few more cleanup tasks, then on to real goals:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cleanup and commit version 0.1 of the likelihood server -- done&lt;/li&gt;
&lt;li&gt;split training into it's own directory. -- done&lt;/li&gt;
&lt;li&gt;Get likelihood server connecting reliably with matlab on different computers.&lt;/li&gt;
&lt;li&gt;implement likelihood importance sampling into maltlab's curve_ml procedure&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Thought some on birth moves.  It seems like there are some possibilities for births larget than two by using the adjacency graph created by pair candidates.&lt;/p&gt;

&lt;p&gt;for now, start with greedy&lt;/p&gt;
</description>
				<pubDate>Tue, 22 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Linux NVidia/Cuda/X11 erorrs; Cuda server; matlab integration</title>
				<description>&lt;p&gt;Getting my (cuda-equipped) desktop system running with likelihod_server.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Trouble building makefiles. Csh issue with new makefiles.  Kobus fixed&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still can't build makefiles. Probilems in the ./lib subdirectory is apparently blocking kjb_add_makefiles from finishing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Some makefiles were missing in ./lib.  Adding them was problematic, because (like before) kjb_add_makefiles wasn't finishing, because dependency directories were missing makefiles.  Also had some issues in kjb /lib, out of date from svn and some compile bugs I introduced in my last commit.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Compiling now, but shader is giving errors at runtime (opengl can't compile it, but no error message is given.&lt;/p&gt;

&lt;p&gt;Rebooting...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;No desktop.  Missing disk issue?  Tweaked fstab, still no X11.  Must be a driver issue.  Couldn't find driver from NVidia's I downloaded from web site months ago.  Using driver &quot;Ubuntu-X&quot; PPA.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still no X.  /var/log/Xorg.0.log says driver and client have different versions.  Got a tip online:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo dpkg --get-selections | grep nvidia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lots of extra crap there (old versions, experimental drivers, etc).  'sudo apt-get purged' all the non-essentials.  Now we're getting to login screen, but login is failing (simply returns to login screen).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found solution:&lt;/p&gt;

&lt;p&gt;   sudo  rm ~/.Xauthority&lt;/p&gt;

&lt;p&gt;Booting successfully.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running likelihood_server...  shader is now compiling successfully!   Previous issues must have been driver issues as a result of the recent 'sudo apt-get upgrade' without restart.&lt;/p&gt;

&lt;p&gt;Getting segfault when dumping pixels, though.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Caused by trying to read from cuda memory as if it were main memory.  Should be dumping if we're using GPU (or use a different dump function).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Fixed;  if using gpu, copy from cuda to temp buffer, then call normal routine.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting nan's from likelihood.  Need to dump from GPU likleihood, which means digging into libcudcvt.  Got some build errors resulting from a boost bug from 8 months ago (which arose because I updated GCC).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;cudcvt updated to grab blurred frames for (some method names changed to better match the analogous CPU likelihood in KJB.).&lt;/p&gt;

&lt;p&gt;Moved &quot;dump&quot; routines from Bd_mv_likelihood_cpu to the base class Bd_mv_likelihood as pure virtual function, and implemented a version in Bd_mv_likelihood_gpu to call cudcvt's new frame-grabbing code.  So &quot;dump&quot; mode now works on both cpu and gpu version.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Dumped data from CUDA likelihood; looks fine.  So I'm still clueless why we're getting nan values.&lt;/p&gt;

&lt;p&gt;Sanity check -- check that tests in libcudcvt are still passing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;test is failing on an assert.  Looks like an overzealous assert with a low percent-error threshold.&lt;/p&gt;

&lt;p&gt;Lowered threshold, test finished; GMM model doesn't match between gpu and cpu versions...&lt;/p&gt;

&lt;p&gt;what's changed?  Rolling back to first releast to see if tests pass.  Is it possible I never validated this?  Or maybe different GGM's being used between cpu and gpu?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Old version crashed and burned hard.  GPU is returning 'inf'.  No help here...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, svn version has lots of uncommitted changes, including bugfixes.  No wonder it was no help.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;found issue: introduced a regression when troubleshooting last bug.  Grrr.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still getting 'inf'.  Checked:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GPU buffers contain valid data for data and model&lt;/li&gt;
&lt;li&gt;CPU version gives finite results.&lt;/li&gt;
&lt;li&gt;GPU can give finite results (e.g. in test application)&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Modified libcudcvt test to receive arbitrary data and model images.  It's giving finite results, so it must be something about how I'm calling it in the likelihood server program.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;issue:&lt;/strong&gt; Bd likelihood in GMM mode gives lower values for ground truth model than random model.&lt;/p&gt;

&lt;p&gt;Bug. fixed.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Digging deeper into 'inf' issue.&lt;/p&gt;

&lt;p&gt;Dug all the way into the thrust::inner_product call.  Probed both buffers -- look okay.  mixture components look okay&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Got it!&lt;/strong&gt;  Was able to reproduce the 'inf's in the likelihood test program in libcudcpp.  It was so hard to reproduce because it only occurred when&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;old GMM values were used&lt;/li&gt;
&lt;li&gt;&lt;em&gt;unblurred&lt;/em&gt; model &lt;em&gt;and&lt;/em&gt; data were used&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Regarding 2, previosuly I just used the unblurred model, since it was at hand from a cuda dump, but used the blurred data from a training dump.&lt;/p&gt;

&lt;p&gt;Blurring doesn't seem to matter; both 2.0 and 5.0 give inf.&lt;/p&gt;

&lt;p&gt;Interestingly, the perfect model doesn't oveflow, but the random (almost null) model does.&lt;/p&gt;

&lt;p&gt;Also, for all the 'inf' cases, the cpu results aren't terribly high, so float overflow seems unlikely.&lt;/p&gt;

&lt;p&gt;Most likely it's the conditioning, where the joint pdf is divided by the marginal.  Maybe we're getting some bizzare model values that are off the charts?  But using the model image with the blurred data image is no problem, so that suggests the model values aren't a problem.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Interesting:&lt;/strong&gt; Tried removing border edges and the problem disappeared.  Is it possible that the blurring routine is not robust to near-edge pixels?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;idea: dump image after scale-offset, but before reduce&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;p&gt;Probability maps look sensible when a 2-pixel margin is added (--data-margin=2).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_2.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 2.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 5.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice what happens when we disable the data margin (rendered dimmer and with blue padding to emphasize effect):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_no_margin.tiff.jpg&quot; alt=&quot;&quot; /&gt;
(2.0 blurring, 0 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice the white pixels around the border, which presumably correspond to inf values.&lt;/p&gt;

&lt;p&gt;Inspecting the blurred data image, it looks like these pixels are, indeed, the brightest in the image.  It's possible we were lucky enough to have found the maximum range of this gmm.&lt;/p&gt;

&lt;p&gt;It looks like evaluating the bivariate normal is underflowing.  It could have been brought back down to size during conditioning, but we never made it that far.  could refactor by computing conditional covariance matrix beforehand, instaed of taking a ratio of computed values&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Pre-test all incoming data images: evaluate against an empty image and a full image and check for NaN and inf.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 21 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - KJB EM GMM</title>
				<description>&lt;p&gt;Still struggling with the EM GMM algorithm.  Found/fixed one bug where responsibilities aren't initialized if NULL is passed to the function.&lt;/p&gt;

&lt;p&gt;Ran overnight and it hadn't finished at the end.  Found/fixed a bug where the entire @M element responsibility matrix is rescaled 2M times (once per point).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting rank-deficient errors.  Looks like this is a result of the huge dataset; covariance matrix is divided by 1e6 (the number of soft-members of the cluster).&lt;/p&gt;

&lt;p&gt;For now, hack by trying to thin the dataset.  Long-term, adding a minimum offset to the emperical covariance seems to be the common solution.&lt;/p&gt;

&lt;p&gt;Possibly normalizing the data would be good.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Added offset to covariance (command-line options and file-static variable already existed, just needed to add it to the full_GMM logic).&lt;/p&gt;

&lt;p&gt;Added &lt;code&gt;read_gmm.m&lt;/code&gt; matlab routine, and plotted along with scatter plot of data.  Results look pretty good:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-20-kjb-gmm-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Moving trained gmm to ~/data/arabidopsis/training/bd_likelihood_gmm/blur_2.0.gmm&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;re-train GMM on held-out dataset.&lt;/li&gt;
&lt;li&gt;implement matlab likelihood sampling&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;how to sync datasets on matlab and c++ server?&lt;/li&gt;
&lt;li&gt;Wacv multi-view reconstruction for training&lt;/li&gt;
&lt;li&gt;saving wacv to files for training&lt;/li&gt;
&lt;li&gt;fixing issues in wacv reconstructions&lt;/li&gt;
&lt;li&gt;evaluation methodology&lt;/li&gt;
&lt;li&gt;split/merge&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 20 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log</guid>
			</item>
		
	</channel>
</rss>
