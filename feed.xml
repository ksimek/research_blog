<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>2 Days: debugging WACV reconstruction</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Now that gradient is working, lets try using fminunc to optimize indices.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting nonsense results.  Looking into curve data.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Inspecting the data, it's clear that our method of linearly sampling points along the bezier curve is resulting in very jagged curves.  Example in dataset 8, curve 7:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-dataset8_curve7_view9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;view 9&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-dataset8_curve7_view4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;view 4&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It's not totally clear how best to resolve this.  Ideally, we would sample at a finer grain, but this caused big slow-downs for the longer curves.  Could use coarse-grain sampling for long curves, but some curves are long in some views and short in others, and nonuniform sampling breaks some assumptions we make in the library.  Furthermore, associations are unknown at the time of samplingz&lt;/p&gt;

&lt;p&gt;It's possible&lt;/p&gt;

&lt;p&gt;It's possible the bad reconstruction we're seeing from this curve isn't due to bad correspondence, but a bad indexing estimation (a later stage of inference).  We see that although the correspondence places c7v9 toward the end of the 3D curve, our re-indexing code places it more spread out, but unevenly: the first point has index 4, while the subsequent points have indieces [21, 23, 27, 27].  We usually prevent large amounts of index-skipping during re-indexing, but possibly the second-pass refinement is destroying this.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After double-checking, realized view #5 is the biggest problem child, not #7.&lt;/p&gt;

&lt;p&gt;Interestingly, #5 has relatively reasonable looking correspondence:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-curve7_corrs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But the reconstruction (both attached and free) is terrible:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-curve7_reconstr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;ll_means has really bad points at the beginning and end of the curve.  It looks like tails might be handled poorly in corr_to_likelihood_2.m&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Attempting to run with &quot;FIX_WORLD_T = false&quot; in corr_to_likelihood.  Early probes suggest this improves things; however, now getting a crash when calling &lt;code&gt;construct_attachment_covariance.m&lt;/code&gt;.  Getting NaN's from curve #8.  I've observed that curve #8's indices start with two zeros.  Maybe this is causing our covariance algorithm to choke?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Got it: when we don't correct endpoints (i.e. FIX_WORLD_T = false), endpoints can get repeated, which makes our &quot;initial direction&quot; computation fail.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Find out why duplicate endpoints occur (doesn't triangulation make this improbable?)&lt;/li&gt;
&lt;li&gt;Handle duplicated points gracefully when computing start point.&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Spun off &lt;code&gt;../correspondence/corr_to_likelihood_2.m&lt;/code&gt;.  Added some plotting code to see data vs. smoothed reconstruction.  Reconstruction is particularly ugly for curve #5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-curve5_reconstr_rough.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At first glance, it looks like end points are poorly localized, and then are preserved through the smoothing pass.   But the last 12 points which are poorly localized hive quite a few correspondences, according to the correspondence table.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-curve5_reconstr_rough.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With pre-smoothed reconstructrion in green:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-reconstr_w_data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tried using GP smoothing (implemented in &lt;code&gt;reconstruction/gp_smooth_curve.m&lt;/code&gt;) instead of matlab's &lt;code&gt;csaps()&lt;/code&gt;, but I'm getting weird tails.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-19-gp_reconstr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Must be a bug in how I'm computing the covariance (which I'm doing by hand, since the necessary fields aren't constructed at that stage of the pipeline).  I've been over it a few times... time to sleep and look again in the morning.&lt;/p&gt;

&lt;p&gt;Eventually, chicken/egg approach might be the solution: optimize points, optimize indices, repeat.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It's clear that smarter smoothing doesn't improve reconstruction of curve #5.&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;plot projected reconstruction vs. data

&lt;ul&gt;
&lt;li&gt;Terrible&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;plot camera centers&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;is it all bad cameras?  why do others reconstruct okay?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Test: incrementally reconstruct, adding one curve each time&lt;/p&gt;

&lt;p&gt;Reverse mode: started with views 9 &amp;amp; 8, then added 7, 6, etc..  Goes to pot at curve 4, gets progressively worse through curve 1&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The purpose of &lt;code&gt;corr_to_likelihood&lt;/code&gt; is to correct position and indexing errors made during naive triangulation.  Let's test if that's  happinging by viewing post-correction results.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Dramatic improvement over pre-correction results.  Still significant change after adding curve 2, and esp. after curve 1.&lt;/p&gt;

&lt;p&gt;Last 2 points and first 4 (?) are problematic.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;trying camera recalibration. wrote a quick 7-point calibration: &lt;code&gt;cal_cam_dlt.m&lt;/code&gt;.  Also wrote a camera visualization routine: &lt;code&gt;visualization/draw_camera.m&lt;/code&gt;.  The newly calibrated results are clearly different, but not obviously better, at least by inspection.&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;touch up cameras for views 3 and 7.&lt;/li&gt;
&lt;li&gt;back up old cameras, copy-in new cameras, re-run wacv dataset 8.  Is curve 5 improved?&lt;/li&gt;
&lt;li&gt;Try chicken/egg approach,

&lt;pre&gt;&lt;code&gt;  * note that maximizing indices is easier than quasi-newton: just arc-length parameterize it.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Troubleshoot FIX_WORLD_T issue that's causing everything to break&lt;/li&gt;
&lt;li&gt;Need to adapt gp-smooth to handle perturbations&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 18 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/18/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/18/work-log</guid>
			</item>
		
			<item>
				<title>Friday - iPlant Reading Group</title>
				<description>&lt;h2&gt;iPLant Reading Group Meeting&lt;/h2&gt;

&lt;p&gt;Decided to create a latex file for recording all of our reading.&lt;/p&gt;

&lt;p&gt;Shared readings&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Kyle:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hyptrace&lt;/li&gt;
&lt;li&gt;HyDe&lt;/li&gt;
&lt;li&gt;Miller et al&lt;/li&gt;
&lt;li&gt;Neuron tracing paper&lt;/li&gt;
&lt;li&gt;(see work log from a few days ago for full list, details)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Kate:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Geotrace -&lt;/li&gt;
&lt;li&gt;RooTrack - Pridmore lab (2012)&lt;/li&gt;
&lt;li&gt;RootNav - Uses EM + others?  pridmore lab, (2013)&lt;/li&gt;
&lt;li&gt;two survey apers on segmentation&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Did a a writeup on iPlant wiki that details my reading from last week.&lt;/p&gt;
</description>
				<pubDate>Fri, 15 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/15/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/15/work-log</guid>
			</item>
		
			<item>
				<title>Debugging ML Gradient (part 2)</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Resuming debugging of analytical gradient.&lt;/p&gt;

&lt;p&gt;I noted yesterday that the error we observed in K' was small -- on the order of 1e-5 -- implying that it wasn't sufficient to explain the end-to-end error we're seeing.  But we should put that into context.  The delta from the numerical gradient is 1e-7, so the error in K' is around 1%, i.e. in the noticible range.&lt;/p&gt;

&lt;p&gt;Also remember that running analytical grdient using the K' from the numeric algorithm showed no noticible change to resutls.  So K' probably &lt;em&gt;isn't&lt;/em&gt; the issue.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Today's strategy: implement a legitimate test-bed that we can run for several deltas and get all intermediate values for both numeric and analytical gradient.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Implemented, examining results.&lt;/p&gt;

&lt;p&gt;Okay, this is weird.  All of the partial derivatives have low error (1e-5), but the final derivative of g(x) is huge (0.1).&lt;/p&gt;

&lt;p&gt;Consider the analytical expression we're using for the gradient of g(x)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;g' = -0.5 * y^\top * S^\top * V' * S * y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plugging-in the numerical estimate for V' above should give something close to the numerical estimate for g'.  But they're nowhere close.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; -0.5 * y' * S' * dV_hat * S * y

ans =

   -0.1015

&amp;gt;&amp;gt; g_hat

ans =

    0.0522
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stated differently, if we propagate the error in dV_hat through the expression for g', this theoretical error in g' is far smaller than the true error in the analytical expression for g'.&lt;/p&gt;

&lt;p&gt;This strongly suggests there's a problem in our expression for g', that the error is not due to approximation or precision issues.  But if g' is so wrong, why are so many elements of the gradient so close to being correct?  Recall the plot of gradients from yesterday, shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-gradient_test.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Element 22 (the element under scrutiny in the tests above) has large error, but several other elements are nearly perfect.  This suggests that if there is a problem in our expression.&lt;/p&gt;

&lt;p&gt;Is it possible there's a bug in the test logic?  Maybe we're using the wrong field for y or S?  Or maybe when we perturb x, we aren't updating all the intermediate fields?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Got it!&lt;/strong&gt;  We're neglecting the change in the normalizing &quot;constant&quot;, Z. In the usual scenario, Z is constant w.r.t. the Gaussian distribution input y, but it is most certainly a function of the values of the Sigma matrix.&lt;/p&gt;
</description>
				<pubDate>Thu, 14 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/14/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/14/work-log</guid>
			</item>
		
			<item>
				<title>Debugging ML gradient</title>
				<description>&lt;p&gt;Implemented end-to-end ML gradient in &lt;code&gt;curve_ml_derivative.m&lt;/code&gt;.  Now testing.&lt;/p&gt;

&lt;p&gt;Fixed some obvious math errors, changes reflected in &lt;a href=&quot;/ksimek/research/2013/11/10/reference&quot;&gt;writeup&lt;/a&gt;.  Now getting close results, but still getting some noticible error, see below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-gradient_test.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Green is reference, blue is testing).&lt;/p&gt;

&lt;p&gt;Results are qualitatively close, but enough error to suggest a bug&lt;/p&gt;

&lt;p&gt;Debugging so far:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;two different impementations for analytical gradient&lt;/li&gt;
&lt;li&gt;two different implementations for numerical gradient&lt;/li&gt;
&lt;li&gt;one-sided and two-sided numerical gradient.&lt;/li&gt;
&lt;li&gt;several delta sizes for numerical gradient {0.0001, 0.0001, ..., 0.1, 1.0}&lt;/li&gt;
&lt;li&gt;Using emperical \(\Delta\)' (from finite differences) for analytical gradient.&lt;/li&gt;
&lt;li&gt;using both cholesky and direct method for matrix inversion (testing for numerical issues).&lt;/li&gt;
&lt;li&gt;sanity check: used intermediate values from gradient computation to compute function output.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To try:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;use numerical gradient for K' instead of from \(\Delta'\).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Is it possible we're not handling XYZ independence properly?&lt;/p&gt;

&lt;p&gt;Noticed that using a really large delta (~1.0) actually improves resuls.  Is it possible we're seeing precision errors being exacerbated somewhere in the end-to-end formula?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Strategy:  pick gradient element with mode error and run the following test.  For each derivative component (dK, dU, dV, dg), compare against reference to determine where the error is being introduced.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Index #22&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;dK/dt&lt;/strong&gt; has 1e-4 error in on-diagonal.  Off diagonals max out at 1e-10.&lt;/p&gt;

&lt;p&gt;   delta: 0.01
   on-diagonal error ~ 1e-3
   below-and-right &amp;lt; 1e-4&lt;/p&gt;

&lt;p&gt;   delta: 0.001
   on-diagonal error ~ 1e-4
   other error &amp;lt; 1e-10&lt;/p&gt;

&lt;p&gt;   delta: 0.0001
   on-diagonal error ~ 1e-5
   below and right ~ 1e-6&lt;/p&gt;

&lt;p&gt;   delta: 0.00001
   on-diagonal error ~ 1e-6
   below and right ~ 1e-4&lt;/p&gt;

&lt;p&gt;Decreasing delta improves on-diagonal, makes below-and-right worse.&lt;/p&gt;

&lt;p&gt;This is weird that we're even getting error in dK/dt, because it passed our unit test.  Well, \(\Delta'\) passed our unit test, but that's basically the same thing...&lt;/p&gt;

&lt;p&gt;However, reduced error at delta of 1e-3 &lt;em&gt;seems&lt;/em&gt; to agree with our end-to-end test.  So maybe this is the culprit.&lt;/p&gt;

&lt;p&gt;It's also surprising that there's so much fluctuation as delta changes.  The computation for K isn't that involved, and we shouldn't be hitting the precision limit yet.  However, the values do get pretty large, so maybe that's a factor.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Magnitude of the original matrix does seem to be a factor.  Look at this slice of the error matrix (dK_test - dK_ref):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-error_trace.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Compare that to the diagonal of the matrix we did finite differences on.  This is basically a plot of the cubed index values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-K_trace.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The error seems to increases in lockstep with the magnitude of the original values (note the jumps occur at similar positions).  I guess this is to be expected, but I was surprised at the magnitude.&lt;/p&gt;

&lt;p&gt;I'm still curious why the error starts to climb exactly at index #22, i.e. the index we're differentiating with respect to.&lt;/p&gt;

&lt;p&gt;This plot should drive home the relationship between index valuea end error.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-error_regress_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Definitely a linear relationship after index #22.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A back of the envelope error analysis suggests that below index 22, the analytical derivative's approximation error is not a function of X, but above index 22, it's a linear function of X.  This is a pretty reasonable explanation, although I couldn't get the exact numbers to explain the slope of the line ( the slope seems high).  But at this hour I wouldn't trust my error analysis as far as I could throw it, quantitatively speaking.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We can attempt to place an upper bound on the error estimate by propagating the error in K' through the differential formula for g'.  Assume every nonzero element of K has error of 3e-5 (the maximum we observed emperically).  Let this error matrix be \(\epsilon\), and it has the same banded structure as \(K'\).  Then we can replace \(K'\) with \(\epsilon\) in the formula for \(g'\) (formula (1) in the &lt;a href=&quot;/ksimek/research/2013/11/10/reference&quot;&gt;writeup&lt;/a&gt;) to get the upper bound error on our data-set.&lt;/p&gt;

&lt;div&gt;
\[
    \text{max error} = \frac{1}{2}z^\top \epsilon z \tag{1}\\
\]
&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; 0.5 * z' * Epsilon * z

ans =

  -1.1758e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can conclude that the error we're observing is coming from somewhere else.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To conclude for tonight, we're seeing some error in dK/ds, but probably nothing out of the ordinary, and it has low enough error that we can hopefully ignore it.&lt;/p&gt;

&lt;p&gt;Lets look a the outher sources of error tomorrow, i.e. U' and V'&lt;/p&gt;
</description>
				<pubDate>Wed, 13 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/13/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/13/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;Implementing index optimization.&lt;/p&gt;

&lt;p&gt;Main accopmlishment of the day: Derived an approach for finding the entire gradient with a single matrix multiplication.  Added this to &lt;a href=&quot;/ksimek/research/2013/11/10/reference&quot;&gt;reference post on ML gradient&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Diversion: updating Ruby&lt;/h2&gt;

&lt;p&gt;Struggling with blog engine, need to upate rdiscount to get new feature, but gem install fails because gcc-4.2 is missing since upgrading to Mavericks.  Need to recompile/reinstall ruby, so the gem system uses clang.  &lt;code&gt;rvm xxx instal&lt;/code&gt; is failing, because gcc-4.2 is missing.  Forcing clang; building causes segfault. Research says upgrade to HEAd of rvm, install ruby 2.0.0.  Tried, but still segfaulting... out of ideas.&lt;/p&gt;

&lt;p&gt;New plan: install gcc-4.2 manually.  Luckilly homebrew has it in the repo.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After some struggle, just gave up.  Annoyed at having wasted over an hour on this.&lt;/p&gt;
</description>
				<pubDate>Tue, 12 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/12/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/12/work-log</guid>
			</item>
		
			<item>
				<title>WACV Reconstruction issues; iPlant Reading; Index optimization</title>
				<description>&lt;h2&gt;WACV Reconstruction issues&lt;/h2&gt;

&lt;p&gt;I've been struggling getting good results on the Columbia-strain images from the WACV dataset.  I was hoping that manually re-tracing the ground trough would improve results, but problems still remain.  MY current theory is that these datasets exhibit significant shifting of the plants over time.  This is causing the dynamic programming algorithm for finding point-correspondences to give bad correspondences, which are never fixed later in the pipeline.&lt;/p&gt;

&lt;p&gt;This is best illustrated using the point-correspondence tables below.  These tables describe the point-corerspondences between views of a curve.
Each row represents a 2D curve from a different view; values in the table represent the index of a point along the 2D curve.
Each column represents a position along the underlying 3D curve, and the values in a column are a set of corresponding 2D points from each view.
An 'x' represents 'no match'; x's only occur at the beginning and end of a row.&lt;/p&gt;

&lt;p&gt;An ideal point-correspondence table looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;              3D curve position
           +--------------------+
view   1   | 1 2 2 3 ...  50 50 |  
index  2   | 1 1 2 2 ...  50 51 |
      ...  |         ...        |
       n-1 | 1 2 3 3 ...  48 48 |
       n   | 1 1 1 2 ...  45 46 |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the plant-stem exhibits drift over time, we get problems where each subsesquent curve is shifted left in the correspondence table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                3D curve position
           +--------------------------------------+
view   1   | 1 2 2 3 ...  50 50 x  x  x  x  x  x  |  
index  2   | x x 1 1 ...  48 49 50 51 x  x  x  x  |
       ... |         ...                          |
       n-1 | x x x x ...  45 46 47 47 48 48 x  x  |
       n   | x x x x ...  43 43 44 44 45 45 46 47 |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a result, we get 3D curves that are longer and tend to exhibit loopy curvature.&lt;/p&gt;

&lt;p&gt;My current theory assumes this occurs because the point-correspondence algorithm optimizes a local score, not taking into account smoothness or per-view perturbations that our full model allows.  I'm working on implementing a post-processing step that does local optimization on the index set w.r.t. the full marginal likelihood, which I'm hoping will fix these problems when they occur.&lt;/p&gt;

&lt;p&gt;I've derived an efficient method for computing the analytical gradient of the index set w.r.t. the marginal likelihood, which &lt;a href=&quot;/ksimek/research/2013/11/10/reference/&quot;&gt;I wrote up yesterday&lt;/a&gt;.  I've implemented some of the pieces, but finish the end-to-end code.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Finished testing the function that computes dK/dt; accurate to within 1e-8 (which is probably due to the numerical approximation used for comparison).&lt;/p&gt;

&lt;h2&gt;iPlant Reading&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Hypocotyl Tracing&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Read three papers on Hypocotyl tracing from plant biology.  Methods are mostly straighforward: threshold, extract curve using {distance transform, morphological skeleton, gaussian tracing}, terminate using hand-built criterion.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hypotrace, Want et al.&lt;/li&gt;
&lt;li&gt;HyDE, Cole et al.&lt;/li&gt;
&lt;li&gt;Miller et al.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Root Tracing&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Read several papers on root tracing&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;RootRead3D&quot; Clark  et al. 2011

&lt;ul&gt;
&lt;li&gt;Zhu 2006 - hough-transform / space carving to find voxels; skeletonize using &quot;minumum cross section&quot;,  fit with NURBS curve.&lt;/li&gt;
&lt;li&gt;&quot;Smart-root&quot; Lobet et. al 2011 - Most cited Clark descendant; Semi-auto, &lt;em&gt;2D&lt;/em&gt;; hand-build algorithm, trace bright regions with consistent radius.  Grayscale features, strong GUI.

&lt;ul&gt;
&lt;li&gt;&quot;EZRhizo&quot; Armengaud et al. 2009 - Manual tracing? (To Read)&lt;/li&gt;
&lt;li&gt;&quot;DART&quot; Le Bot et al., 2010 - Manual Tracing? (To read)&lt;/li&gt;
&lt;li&gt;Iyer-Pascuzzi et a., 2010 - Automatic.  Multiple angles, but &lt;em&gt;2D analysis&lt;/em&gt;.  preprocess: Adaptive threshold.  Medial axis: distance transform&lt;/li&gt;
&lt;li&gt;&quot;Root-trace&quot; Nadeem et al. 2009 - auto; (To Download)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fang et al. 2009  - Laser scanner, skeleton using hough transform method.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Neuron Analysis&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Reading papers of the &lt;a href=&quot;http://diademchallenge.org/algorithms.html&quot;&gt;teams that won the Diadam Challenge&lt;/a&gt; for tracing neurons.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Wang et al. A Broadly Applicable 3-D Neuron Tracing Method Based on Open-Curve Snake&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Preprocessing: eigenanalysis of image hessian, eigenvalues are used to determine &quot;vesselness&quot; of each pixel.
Presegmentation: Graph cut on &quot;vesselness image&quot;.  Actually &quot;Vessel Cut&quot;;  &quot;tubular structures are further enhanced and close-lying axons are thinned in the vesselness image&quot;.  A bit vague here.  See Freiman et al. (2009).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;To Read:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Freiman et al. (2009). vessel-cut
Narayanaswamy et al. (2011) curvelets and scalar voting&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Applications to TULIPS:  datasets, evaluation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This paper uses a handfull of branching curve datasets that we could use for our project.&lt;/p&gt;

&lt;p&gt;We also get an evaluation metric for comparing centerlines.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Possible Bisque analysis&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Run graph-cut or grab-cut to do foreground segmentation.&lt;/p&gt;
</description>
				<pubDate>Mon, 11 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/11/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/11/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;h2&gt;Correspondence Misalignment&lt;/h2&gt;

&lt;p&gt;WACV ground truth reconstruction errors are due to correpondence misalignment that occurs when different views don't line up (esp because of drift in the plant position over time).&lt;/p&gt;

&lt;p&gt;Ultimately, this causes index set estimates to be suboptimal.  To correct this, we can either focus on fixing the point-correspondence stage, or fixing the index set directly, after the correspondence stage.&lt;/p&gt;

&lt;p&gt;Two possible approaches come to mind.&lt;/p&gt;

&lt;p&gt;The first approach attempts to fix the point correspondences.  Do some keypoint matching and use this to constrain certain parts of the correspondence-matching (i.e. force the dynamic programming to pass through certain entries in the cost matrix).  Currently unclear how to deal with incorrect keypoint matches, or many-to-many correspondenes.&lt;/p&gt;

&lt;p&gt;The second approach is to do some optimization on the index points to improve the marginal likelihood.&lt;/p&gt;

&lt;h2&gt;Optimizing index points&lt;/h2&gt;

&lt;p&gt;The following assumes that the problem is the index set obtained by greedy point correspondence is sub-optimal under the marginal likelihood, and the indices at the true optimium would be immune to misalignment, because our kernel is robust to trnalsation and scaling errors.&lt;/p&gt;

&lt;p&gt;Doing 350-dimensional optimization seems like a terrible pain, but we might be able to take some shortcuts.&lt;/p&gt;

&lt;p&gt;First, we observe that the analytical derivative of the ML w.r.t. any change in the index set will require a computation on the same order as evaluating the marginal likelihood (nxn matrix inversion, multiplication of several nxn matrices).&lt;/p&gt;

&lt;p&gt;However, if we cache some values, we can avoid matrix inversion when computing subsequent elements of the gradient.  This still leaves nxn matrix multilication, but this becomes sparse, since most elements of K don't change when a single index changes.&lt;/p&gt;

&lt;p&gt;Alternatively, we can choose a few good directions and optimize in those directions only.  For example, when (a) all values are shifted together, (b) one endpoint is unchanged, and each subsequent index is changed more and (c) the complement of b.&lt;/p&gt;

&lt;p&gt;Regardless of the gradient approach, we still have to do a matrix inverse at every iteration.  We might be able to do this blockwise and save computation for indices that don't change.  Are there other approximations that we can do?  This doesn't need to be exact, but good enough to improve the indices&lt;/p&gt;

&lt;p&gt;Lets keep in mind, we're dealing with relatively small matrices, here.&lt;/p&gt;

&lt;h2&gt;Other thoughts&lt;/h2&gt;

&lt;p&gt;It's possible a long, curvy curve (like the one's we get when misalignment occurs) are actually the result best supported under the marginal likelihood.  In that case, we need to think more deeply about our overall approach.&lt;/p&gt;

&lt;h2&gt;Camera calibration&lt;/h2&gt;

&lt;p&gt;There is likely some room for improvement in the camera calibration.  Maybe this is the best approach overall?&lt;/p&gt;

&lt;h2&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Improve camera calibration&lt;/li&gt;
&lt;li&gt;Write function to get derivatives of covariance matrix.&lt;/li&gt;
&lt;li&gt;Try index optimization.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 10 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/10/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/10/work-log</guid>
			</item>
		
			<item>
				<title>Gradient w.r.t. Indices</title>
				<description>&lt;p&gt;To optimize indices, we'll need to compute the derivative of the marginal log-likelihood w.r.t. changing indices.&lt;/p&gt;

&lt;p&gt;I first tried to derive this using the generalization of the chain rule to matrix expressions (see matrix cookbook, section 2.8.1), but the computation exploded.  Since ultimately, the derivative is a simple single-input, single output function, we can use differentials to derive the solution.&lt;/p&gt;

&lt;p&gt;Let the marginal likelihood as a function of indices be \(g(x)\):&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial g(x)}{\partial x_i} = \frac{\partial}{\partial x_i} 
        \frac{1}{2}(-y^\top S^\top ( I + S K(x) S^\top)^{-1} S y)
\]

Let \(U = I + S K(x) S^\top\), and \(V = U^{-1}\).  Working inside out, lets find \(\frac{\partial U}{\partial x_i}\).

\[
\begin{align}
    U + dU  &amp;= I + S (K + dK) S ^\top \\
            &amp;= I + S K S^\top + S dK S ^\top \\
        dU  &amp;= S \, dK\, S^\top \\
        U'  &amp;= S K' S^\top
\end{align}
\]

Where \(M'\) is the derivative of the elements of \(M\) w.r.t. \(x_i\).  Next, \(\frac{\partial V}{\partial x_i}\), which comes from the matrix cookbook, equation (36).

\[
    dV = -U^{-1} \, dU \, U^{-1} \\
    V' = -U^{-1} U' U^{-1}
\]

Finally,  \(\frac{\partial g(x)}{\partial x_i}\):
    
\[
\begin{align}
    g + dg  &amp;= -\frac{1}{2}y^\top S^\top (V + dV) S y \\
    g + dg  &amp;= -\frac{1}{2}y^\top S^\top \, V \, S y + y^\top S^\top \,dV \,S y \\
        dg  &amp;= -\frac{1}{2}y^\top S^\top \,dV \,S y \\
        g'  &amp;= -\frac{1}{2}y^\top S^\top \, V' \,S y \\
\end{align}
\]

Expanding \(V\) gives the final formula:
\[
\begin{align}
        g'  &amp;= \frac{1}{2}y^\top S^\top U^{-1} S K' S^\top U^{-1} S y \\
        g'  &amp;= \frac{1}{2}y^\top M K' M y \\
        g'  &amp;= \frac{1}{2}z^\top K' z \tag{1}\\
\end{align}
\]

&lt;p&gt;
Here, \(M = S^\top U^{-1} S \), (which is symmetric), and \(z = M y\).  
&lt;/p&gt;

&lt;p&gt;
This equation gives us a single element of the gradient, namely \(d g(x)/dx_i\).  However, once \(z\) is computed, we can reuse it  when recomputing (1) for all other \(x_j\)'s.  The cost of each subsequent gradient element becomes \(O(n^2)\), making the total gradient \(O(n^3)\), which is pretty good. (This assumes the K's can be computed efficiently, which is true; see below.)  However, we also observe that \(K'\) is sparse with size \(O(n)\), so we can do sparse multiplication to reduce the running time to linear, and &lt;strong&gt;the full gradient takes \(O(n^2)\)&lt;/strong&gt;, assuming \(z\) is precomputed.  Cool! 
&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Derivatives of K(x)&lt;/h2&gt;

&lt;p&gt;Below we derive the derivative of each of the three covariance expresssions, which combine to give \(K'\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cubic covariance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Recall the cubic covariance expression:&lt;/p&gt;

&lt;div&gt;
\[
k(x_1, x_2) = (x_a - x_b) x_b^2 / 2 + x_b^3/3
\]

Where \(x_b = min(x_1, x_2)\) and \(x_a = max(x_1, x_2)\).
&lt;/div&gt;


&lt;p&gt;Taking the derivative w.r.t. (x_2) gives:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\frac{\partial k(x_1, x_2)}{\partial x_2} &amp;= 
    \begin{cases}
         x_1^2 / 2 &amp; \text{if } x_2 &gt;= x_1 \\
         x_1 x_2 - x_2^2/2 &amp; \text{if } x_2 &lt; x_1 
    \end{cases} \\
            &amp;= 
    \begin{cases}
         x_b^2 / 2 &amp; \text{if } x_2 &gt;= x_1 \\
         x_a x_b - x_b^2/2 &amp; \text{if } x_2 &lt; x_1 
    \end{cases} \\
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Or equivalently&lt;/p&gt;

&lt;div&gt;
\[
\frac{\partial k(x_1, x_2)}{\partial x_2} = 
         x_b \left ( x_1  - x_b/2 \right ) \tag{2}
\]
&lt;/div&gt;


&lt;p&gt;Note that if the goal is to find \(K' = \frac{\partial{K}}{\partial{x_i}}\), the on-diagonal element \(k'_{i i}\) needs a slightly different formula, because the kernel is a function of a single variable, \(x_i\).&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\frac{\partial k(x_i)}{\partial x_i} &amp;= 
    \frac{\partial}{\partial x_i} x_i^3/3 \\
        &amp;= x_i^2 \tag{3}
\end{align}
\]


Compare this with the general formula: if we plugged-in \(x_i'\) to both inputs of equation (2), we'd get \(x_i^2/2\), which underestimates the derivative by half.  We'll see in the next section that when computing \(g'\) the &quot;wrong&quot; expression for \(k'_{ii}\) is actually more useful than the correct one, because we'll need to scale it by 0.5 anyway.

&lt;/div&gt;




&lt;p&gt;&lt;/p&gt;


&lt;p&gt;&lt;em&gt;Linear Covariance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Recall the cubic covariance expression:&lt;/p&gt;

&lt;div&gt;
\[
k(x_1, x_2) = x_1 x_2
\]

The derivative w.r.t. \(x_2\) is simply \(x_1\).
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Offset Covariance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Recall the offset covariance expression:&lt;/p&gt;

&lt;div&gt;
\[
k(x_1, x_2) = k
\]

The derivative w.r.t. \(x_2\) is zero.
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Implementation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Implemented end-to-end version in &lt;code&gt;kernel/get_model_kernel_derivative.m&lt;/code&gt;; see also components in &lt;code&gt;kernel/get_spacial_kernel_derivative.m&lt;/code&gt; and &lt;code&gt;kernel/cubic_kernel_derivative.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;These functions return all of the partial derivatives of the matrix with respect to the first input.   The i-th row of the result make up the nonzero values in \(\frac{\partial K}{\partial x_i}\).  Below is example code that computes all of the partial derivative matrices.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;N = 100;
% construct indices
x = linspace(0, 10, N);
% construct derivative rows
d_kernel = get_model_kernel_derivative(...);
d_K = eval_kernel(d_kernel, x, x);
% construct dK/dx_i, for each i = 1..N
d_K_d_x = dcell(1,N);
for i = 1:N
    tmp = sparse(N, N);
    tmp(i,:) = d_K(i,:);
    tmp(:,i) = d_K(i,:)';
    d_K_d_x{i} = tmp;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Directional Derivatives&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I think we can get directional derivatives of \(K\) by taking the weighted sum of partial derivatives, where the weights are the component lengths of the direction vector.  I have yet to confirm this beyond a hand-wavy hunch, and in practice, this might not even be needed, since computing the full gradient is so efficient.&lt;/p&gt;

&lt;h2&gt;Sparsity of K'&lt;/h2&gt;

&lt;p&gt;The sparsity of \(\frac{\partial K}{\partial x_i}\) actually allows us to further simplify formula (1) for \(g'\), ultimately allowing us to compute the entire gradient in a single matrix multiplication.&lt;/p&gt;

&lt;p&gt;First observe that K' is only nonzero on the i-th row and column:&lt;/p&gt;

&lt;div&gt;
\[
    k'_{i,j} = 
    \begin{cases}
        \delta_{ij} &amp; \text{if } i = j \\
        0 &amp; \text{otherwise}
    \end{cases}
\]

where \(\delta_{ij} = \frac{\partial k_{ij}}{\partial x_i} \).

For convenience, we'll define the vector \(\Delta_i = [\delta_{i1}, ..., \delta_{in}]^\top\).


Let \(g_i'\) be the partial derivative of \(g\) w.r.t. \(x_i\), with the entire gradient denoted by \(\nabla g = [g'_1, ..., g'_n]^\top\).  Using sparsity, eq. (1) can be rewritten as

\[
    g_i' = \frac{1}{2} z_i (\delta_{i1} z_1 + ... + \delta_{i(i-1)} z_{i-1} + \sum_j \delta_ij z_j + \delta_{i(i+1)} z_{i+1} + ... + \delta_{in} z_n)
\]

The expression in the parentheses is almost a dot product of z and \(\Delta_i\), but with the i-th term replaced with the dot product of z and \(\Delta_i\).  We can re-write the expression in terms of dot products, minus a correction.

\[
\begin{align}
    g_i' = \frac{1}{2} z_i (2 * z \cdot \Delta_i - z_i \delta_i) \\ 
       = z_i \, z \cdot \Delta_i' \\
\end{align}
\]

Where \(\Delta_i'\)  is equal to \(\Delta_i\) in all elements except the i-th, which is equal to \(0.5 \lambda_i\).  Note that we can get \(\Delta_i'\) by using equation (2) above instead of equation (3), which allows us to avoid having a separate implementation for on-diagonal elements.

Since each \(g_i\) arises from a dot product, we can compute \(\nabla g\) using matrix multiplication.  Let \(\Delta' = [\Delta'_1, ..., \Delta'_n] \), i.e. the matrix whose i-th column is \(\Delta'_i\).  The gradient expression becomes

\[
\begin{align}
    \nabla g = z \odot (\Delta' z) \tag{4}
\end{align}
\]

where \(\odot\) denotes element-wise multiplication.
&lt;br /&gt;&lt;br /&gt;
To handle multiple dimensions, simply apply to each dimension independently and sum the results.

&lt;/div&gt;



</description>
				<pubDate>Sun, 10 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/10/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/10/reference</guid>
			</item>
		
			<item>
				<title>iPlant Literature Review Planning Meetings</title>
				<description>&lt;h2&gt;Document discussion&lt;/h2&gt;

&lt;p&gt;Some random notes about issues to bring up in the final document.&lt;/p&gt;

&lt;p&gt;Talk about representation vs. &quot;features&quot; (characteristics).  Clarify &quot;features&quot;, don't use indescriminitively (call them high-level features, topological features, as opposed to image features).&lt;/p&gt;

&lt;p&gt;Two ways to go about finding &quot;features&quot;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;look directly for charactersitics/features&quot;&lt;/li&gt;
&lt;li&gt;&quot;look for model, find the other stuff&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;i.e. looking for representations vs. characteristics&lt;/p&gt;

&lt;p&gt;Bisque vs. Nonbisque?  Unclear what the final application will be, but if Bisque is the goal, some discussion of it's feasibility is probably warranted.  For example, interactive systems like the Clark paper might not be feasible for Bisque.&lt;/p&gt;

&lt;h2&gt;Taxonomy of curve-extraction methods&lt;/h2&gt;

&lt;p&gt;Kobus mentioned the different &quot;dimensions&quot;  that the problem can be split into.  Some might be&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3D vs 2D&lt;/li&gt;
&lt;li&gt;imaging system type (microscope, MRI, camera, etc)&lt;/li&gt;
&lt;li&gt;temporal vs. nontemporal&lt;/li&gt;
&lt;li&gt;branching vs non branching&lt;/li&gt;
&lt;li&gt;biological vs. nonbiological&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Research Topics&lt;/h2&gt;

&lt;p&gt;We can split the background reading into several areas, both in image processing/CV and biological application.
Kobus recommended splitting time between these broad areas.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image Processing Topics&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;snakes&quot; - active contours&lt;/li&gt;
&lt;li&gt;Medial axis filtering&lt;/li&gt;
&lt;li&gt;curve modelling (polynomial/splines, GP, level-set methods)&lt;/li&gt;
&lt;li&gt;finding branches&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Application Areas&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pollen tube tracking&lt;/li&gt;
&lt;li&gt;root tracking&lt;/li&gt;
&lt;li&gt;Blood vessel literature / vascular segmentation&lt;/li&gt;
&lt;li&gt;Neuron tracing&lt;/li&gt;
&lt;li&gt;Alternaria / Fungus?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Other stuff&lt;/h2&gt;

&lt;p&gt;Other topics that might be worth looking into, but maybe of speculative value.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rhizotron - nondestructive underground root imaging system&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Data Inventory &lt;/h2&gt;

&lt;p&gt;We currently have data sets from a handful of different applications/domains.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pollen tubes&lt;/li&gt;
&lt;li&gt;&quot;Clark&quot; root image (2D, one image)&lt;/li&gt;
&lt;li&gt;&quot;Max Plank&quot; root images&lt;/li&gt;
&lt;li&gt;Neurons&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Any questions for Martha?&lt;/p&gt;

&lt;p&gt;Reading for next week: look into citations from Clark and Hypotrace&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kate - related work, Clark Paper&lt;/li&gt;
&lt;li&gt;Andrew - related work, Hypotrace paper&lt;/li&gt;
&lt;li&gt;Kyle image processing&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As we read, lets try to place each paper somewhere in the &quot;Taxonomy&quot; above.&lt;/p&gt;
</description>
				<pubDate>Fri, 08 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/08/meeting-notes</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/08/meeting-notes</guid>
			</item>
		
			<item>
				<title>WACV reconstruction (revisited)</title>
				<description>&lt;p&gt;Addressing issues that came up last time we ran WACV.&lt;/p&gt;

&lt;p&gt;Currently working on dataset #8, which has some real problems.&lt;/p&gt;

&lt;p&gt;Spent some time tweaking the ground truth, adding missing curves, and correcting one or two obvious mistakes.&lt;/p&gt;

&lt;p&gt;Still getting terrible results; one problem might be the &quot;rough&quot; reconstruction, which doesn't take into account anisotropic data uncertainty.  Fixed.&lt;/p&gt;

&lt;p&gt;False-positive reversal has been corrected, but still getting terrible results; it seems like the camera calibration is waaaay off.&lt;/p&gt;

&lt;p&gt;I dont have the calibration data for this dataset available, locally re-syncing full dataset.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Are these calibrated cameras ordered in counter-clockwise direction? No.  but the calibration_data.mat file in for this dataset is.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Inspected correspondence diagrams.  Possibly some of the ground truth curves are reversed?  Dataset #8, curve #7, view #9, only corresponds to very end of other points:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-07-bad_correspondence.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tweaking ground truth tracing program to show start-point of curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Ground truth program doesn't compile.  Clang++ for mountain lion is complaining.&lt;/p&gt;

&lt;p&gt;Apparently clang no longer uses gnu stdc++ library, and some library components use proprietary gnu symbols (I admit, it was my code).  Fixed.&lt;/p&gt;

&lt;p&gt;Getting some new errors regarding some forward declarations of STL pair.  Was hard to debug, because errors weren't local to the problematic code, and errors were cryptic.  Removed forward declarations; fixed.&lt;/p&gt;

&lt;p&gt;Found a template function I hand't ported from my experimental branch of KJB.&lt;/p&gt;

&lt;p&gt;Linker errors -- libXmu and libXi not found.  Tweaked init_compile's logic for OpenGL on Macs. Hopefully I didn't break anyone else's builds...&lt;/p&gt;

&lt;p&gt;More linker errors. Apparently i need to recompile &lt;strong&gt;everything&lt;/strong&gt; to use clang's c++ libraries?? rebuilt boost, next is casadi, but I can't find the source on my machine.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;recalibrate cameras&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 07 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/07/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/07/work-log</guid>
			</item>
		
	</channel>
</rss>
