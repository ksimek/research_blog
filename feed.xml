<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>FIRE - cleanup</title>
				<description>&lt;h2&gt;Skype w/ Kobus&lt;/h2&gt;

&lt;p&gt;Reflecting on &lt;a href=&quot;/ksimek/research/2014/05/15/reference&quot;&gt;immunity histograms&lt;/a&gt; I created late last night.  In discussion with Kobus, we&#39;ve decided a few things could help: (a) log-transforming, (b) introducing a scaling factor, and (c) using robust likelihood (e.g. truncated squared error).&lt;/p&gt;

&lt;p&gt;Will Skype with Warren on Monday afternoon to hand off the self-report clustering analysis.  Will spend this afternoon cleaning up the data and code to make it easier for him to pick it up.&lt;/p&gt;

&lt;p&gt;Kobus will likely need my input in the coming weeks to incorporate branching Gaussian processes into an upcoming grant proposal.&lt;/p&gt;

&lt;h2&gt;Self-report clustering cleanup&lt;/h2&gt;

&lt;p&gt;Add documentation and READMES for clustering, experiment directory, output files.&lt;/p&gt;

&lt;p&gt;Add matlab script to visualize clustering results.&lt;/p&gt;

&lt;p&gt;Created &lt;a href=&quot;/ksimek/research/2014/05/16/reference/&quot;&gt;reference page&lt;/a&gt; for clustering results.&lt;/p&gt;
</description>
				<pubDate>Fri, 16 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/16/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/16/work-log</guid>
			</item>
		
			<item>
				<title>FIRE - Self-report clustering results.</title>
				<description>&lt;p&gt;Results of initial clustering of self-report data.  (I showed these a few weeks ago, but never recorded them here).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;: Five survey values, recorded once per each of nine visits, for a total of 45 dimensions per cluster.  Missed visits are treated as missing data and ignored during inference.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model, Inference&lt;/strong&gt;: Multivariate gaussians with diagonal covariance were fit to each cluster.  Cluster memberships and cluster parameters were fit with EM, with 100 different initializations to avoid local minima.  Number of clusters was varied between 2 and 50, and the optimal choice of three clusters was determined using BIC.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Means&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-16-cluster_means.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Standard Deviations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-16-cluster_std_devs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Disucssion&lt;/h2&gt;

&lt;p&gt;PSS is in the 0.9-1.0 range for all clusters.&lt;/p&gt;

&lt;p&gt;Note that FACT data has high standard deviation, making its curves harder to interpret.&lt;/p&gt;

&lt;p&gt;Also note that unlike other variables, high FACT scores correspond to &lt;em&gt;negative&lt;/em&gt; outcomes (e.g. high pain, high nausea, etc)&lt;/p&gt;

&lt;p&gt;Third cluster is the &quot;always good&quot; cluster.&lt;/p&gt;

&lt;p&gt;Second cluster is the &quot;high emotional acceptance cluster&quot;.  Notable that DAS and SRI start low, but increase over time.  This suggests a hyptothesis that high emotional acceptance can result in improved outcomes over time.&lt;/p&gt;

&lt;p&gt;First cluster is the &quot;low emotional acceptance cluster&quot;.  SRI and DAS start low and never improve.&lt;/p&gt;
</description>
				<pubDate>Fri, 16 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/16/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/16/reference</guid>
			</item>
		
			<item>
				<title>FIRE - first clustering test</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;16806&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Run #1:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Run initial model estimation on real data for the first time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Single cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0.00000000e+00  0.00000000e+00
8.73210306e-03  6.75372609e-02  3.38471516e-02
1.88790755e+00 -1.88387427e+01 -9.88494611e+00
-8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
-5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Multiple cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;num_clusters:3
log weights: 0 0 0
cluster #1
  0.00000000e+00  0.00000000e+00
 -2.68876589e-01 -7.04390256e-02  1.45450526e-02
  6.16258430e+03  6.13067965e+03  6.12930057e+03
 -1.08771413e-02  5.76005556e-04 -1.04761899e-02 -1.69093096e-03  9.36991863e-04 -3.78348959e-04  1.27422332e-03
 -4.89152880e+01 -9.21454922e+00 -1.77681544e+01  2.47259001e+00  6.28886806e+00  8.63120843e+00 -2.43004449e+01
26.2893
cluster #2
  0.00000000e+00  0.00000000e+00
  1.20215477e-01  0.00000000e+00  2.24383835e-01
  6.11014579e+03  0.00000000e+00  5.96700128e+03
 -1.08771413e-02  5.76005556e-04 -1.04761899e-02 -1.69093096e-03  9.36991863e-04 -3.78348959e-04  1.27422332e-03
 -4.89152880e+01 -9.21454922e+00 -1.77681544e+01  2.47259001e+00  6.28886806e+00  8.63120843e+00 -2.43004449e+01
26.2893
cluster #3
  0.00000000e+00  0.00000000e+00
 -1.52115349e-01  2.77516102e+00  1.82492585e+00
 -3.84569849e+00 -7.18131761e+02 -1.63056790e+02
 -1.08771413e-02  5.76005556e-04 -1.04761899e-02 -1.69093096e-03  9.36991863e-04 -3.78348959e-04  1.27422332e-03
 -4.89152880e+01 -9.21454922e+00 -1.77681544e+01  2.47259001e+00  6.28886806e+00  8.63120843e+00 -2.43004449e+01
26.2893
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Surprisingly high epsilon (~26).  This is far beyond the dynamic range of the data, suggesting either (a) a bug, (b) a terrible model, or (c) failure of the analytical estimation method to find a good result.  Option (a) seems more likely, since a flat line give a lower error variance than this.  Perhaps our observation basis A was poorly estimated.  Lets re-run with PCA method.&lt;/p&gt;

&lt;h2&gt;Run #2: PCA method&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Re-run but using PCA instead of regression to estimate observation transformation, A.  (i.e. change constant &lt;code&gt;use_regression_method&lt;/code&gt; to false).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Single cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  0.00000000e+00  0.00000000e+00
  8.73210306e-03  6.75372609e-02  3.38471516e-02
  1.88790755e+00 -1.88387427e+01 -9.88494611e+00
 -8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
 -5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Multiple Cluster&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;num_clusters:3
log weights: 0 0 0
cluster #1
  0.00000000e+00  0.00000000e+00
  1.13041314e-02  6.75372609e-02  3.41203573e-02
  3.56421036e-01 -1.88387427e+01 -1.11922319e+01
 -8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
 -5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497
cluster #2
  0.00000000e+00  0.00000000e+00
  1.05708618e-03  0.00000000e+00  0.00000000e+00
  5.35534897e+01  0.00000000e+00  0.00000000e+00
 -8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
 -5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497
cluster #3
  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  3.06188063e-02
  1.18920520e+02  0.00000000e+00  7.71079754e+01
 -8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
 -5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;No noticable improvement.  During K-means, cluster collapse was frequent, which didn&#39;t occur in previous run.&lt;/p&gt;

&lt;h2&gt;Immunity data inspection&lt;/h2&gt;

&lt;p&gt;Below are histograms of raw immunity readings for each marker. TNF-\(\alpha\), IL-2, and IL-6 have sensible distributions.&lt;/p&gt;

&lt;p&gt;IL-1b and IL-10 have weirdly peaked distribution with heavy tails.  Perhaps outliers are isolated to specific plates (investigated next).&lt;/p&gt;

&lt;p&gt;IFN and IL-8 are borderline; peaked with heavy tails but not as bad as IL-1b and IL-10.&lt;/p&gt;

&lt;script&gt;
    $(function() {
        $( &quot;#hist_tabs&quot; ).tabs();
    });
&lt;/script&gt;




&lt;div id=&quot;hist_tabs&quot;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#IFN&quot;&gt;IFN-g&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#TNFa&quot;&gt;TNF-\(\alpha\)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL1B&quot;&gt;IL-1b&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL2&quot;&gt;IL-2&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL6&quot;&gt;IL-6&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL8&quot;&gt;IL-8&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL10&quot;&gt;IL-10&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
    &lt;div id=&quot;TNFa&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-tnfa.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL2&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL2.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL8&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL8.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL6&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL6.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL1B&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL1B.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IFN&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IFN.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL10&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL10.png&quot; /&gt;
    &lt;/div&gt;
  &lt;/div&gt;


&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;h2&gt;Per-plate distributions.&lt;/h2&gt;

&lt;script&gt;
    $(function() {
        $( &quot;#plate_hist_tabs&quot; ).tabs();
    });
&lt;/script&gt;


&lt;div id=&quot;plate_hist_tabs&quot;&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#ifng_plates&quot;&gt;IFN-g&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#tnfa_plates&quot;&gt;TNF-\(\alpha\)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#IL1B_plates&quot;&gt;IL-1b&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il2_plates&quot;&gt;IL-2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il6_plates&quot;&gt;IL-6&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il8_plates&quot;&gt;IL-8&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il10_plates&quot;&gt;IL-10&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;div id=&quot;ifng_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-ifng_plates.png&quot; /&gt;
    &lt;br /&gt;
    One of the worse plates in terms of missing/out-of-range data (331/710).  Irregular distributions, and plate 9 has no radings within range.
    &lt;/div&gt;
    &lt;div id=&quot;il10_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il10_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
        Another reasonably strong variable in terms of missing data (637/710).  Plots seem somewhat irregular, but possibly due to excessible outliers in plates 10, 7, 8, 4.  This could be good support for a clustering model if it would explain the heavy tails in these plates
    &lt;/p&gt;

    &lt;/div&gt;
    &lt;div id=&quot;IL1B_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL1B_plates.png&quot; /&gt;
    &lt;p&gt;
    In terms of missing data, this variable is borderline (550/710).  Plates seem to split between (a) seemingly exponential-distrubted data (2,3,4,7,8, 10), and (b) irregular data (plates 1,5,6,9).  I&#39;m doubtful that this will be useful for inference.
    &lt;/p&gt;
    &lt;p&gt;
    Note &lt;em&gt;massive&lt;em&gt; variation in support between plates.  e.g. plate 8 maxes out at 80, while plate 1 stops at 1.3.
    &lt;/p&gt;
    &lt;/div&gt;

    &lt;div id=&quot;il6_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il6_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
    Very little missing data in this dataset (699/710).  Seems much more consistent than most other datasets. Scale and shape vary a bit, but not glaring inconsistencies, aside from a few outliers (e.g. plate 9).
    &lt;/p&gt;
    &lt;p&gt;
    Lots of between-plate variability (plate 7 vs. 10).
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il8_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il8_plates.png&quot; /&gt;
    &lt;/div&gt;
    &lt;p&gt;
        Seems to be Gamma-distributed or log-normal distributed but with lots of big outliers.  Plates 5 and 10 have extreme outliers
    &lt;/p&gt;
    &lt;div id=&quot;tnfa_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-tnfa_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
    The best plate in terms of missing data (710/710), TNF-\(\alpha\) seems very regular within-plates.  Note the strong gaussian shapes here, compared to the full-dataset histogram earlier.  
    &lt;/p&gt;
    &lt;p&gt;
    Variation between plates is notable (see plate 5 vs. plate 8).  Supports a regularization approach.
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il2_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il2-plates.png&quot; /&gt;
    &lt;p&gt;
    This is one of the stronger plates in terms of missing data (710/710 present).  Several plates have decent gaussian distributions, albeit with heavy tails (1, 7, 9), but several are irregular (4, 5, 8, 9), and others look more exponential (2, 3, 10).
    &lt;/p&gt;
    &lt;/div&gt;
&lt;/div&gt;

</description>
				<pubDate>Thu, 15 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/15/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/15/work-log</guid>
			</item>
		
			<item>
				<title>FIRE immunity plots</title>
				<description>&lt;p&gt;Below are histograms of raw immunity readings for each marker.&lt;/p&gt;

&lt;p&gt;Aside from TNF-\(\alpha\), IL-2, and IL-6, distributions are very peaked and heavy-tailed. Specific-plate histograms (shown later) reveal more detail.&lt;/p&gt;

&lt;script&gt;
    $(function() {
        $( &quot;#hist_tabs&quot; ).tabs();
    });
&lt;/script&gt;




&lt;div id=&quot;hist_tabs&quot;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#IFN&quot;&gt;IFN-g&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#TNFa&quot;&gt;TNF-\(\alpha\)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL1B&quot;&gt;IL-1b&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL2&quot;&gt;IL-2&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL6&quot;&gt;IL-6&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL8&quot;&gt;IL-8&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL10&quot;&gt;IL-10&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
    &lt;div id=&quot;TNFa&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-tnfa.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL2&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL2.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL8&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL8.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL6&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL6.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL1B&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL1B.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IFN&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IFN.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL10&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL10.png&quot; /&gt;
    &lt;/div&gt;
  &lt;/div&gt;


&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;h2&gt;Per-plate distributions.&lt;/h2&gt;

&lt;p&gt;Below are per-plate histograms of immunity readings, broken into plates.  Compared to the full-popuation histograms above, signfiicant regularity emerges in these plots, esp. in TNF-\(\alpha\), IL-8, IL-10.&lt;/p&gt;

&lt;script&gt;
    $(function() {
        $( &quot;#plate_hist_tabs&quot; ).tabs();
    });
&lt;/script&gt;


&lt;div id=&quot;plate_hist_tabs&quot;&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#ifng_plates&quot;&gt;IFN-g&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#tnfa_plates&quot;&gt;TNF-\(\alpha\)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#IL1B_plates&quot;&gt;IL-1b&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il2_plates&quot;&gt;IL-2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il6_plates&quot;&gt;IL-6&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il8_plates&quot;&gt;IL-8&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il10_plates&quot;&gt;IL-10&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;div id=&quot;ifng_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-ifng_plates.png&quot; /&gt;
    &lt;br /&gt;
    One of the worse plates in terms of missing/unreadable data (331/710).  Irregular between and within plates.
    &lt;/div&gt;
    &lt;div id=&quot;il10_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il10_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
        Another reasonably strong variable in terms of missing data (637/710).  Plots seem somewhat irregular, but possibly due to excessible outliers in plates 10, 7, 8, 4.  This could be a good argument for a clustering/mixture model, as it might explain the heavy tails in the aforementioned plates.
    &lt;/p&gt;

    &lt;/div&gt;
    &lt;div id=&quot;IL1B_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL1B_plates.png&quot; /&gt;
    &lt;p&gt;
    In terms of missing data, this variable is borderline (550/710).  Plates seem to split between (a) seemingly exponential-distrubted data (2,3,4,7,8, 10), and (b) irregular data (plates 1,5,6,9).  This could pose problems for our inference efforts.
    &lt;/p&gt;
    &lt;p&gt;
    Note &lt;em&gt;massive&lt;/em&gt; variation between plates.  e.g. plate 8 has a maximum at 60, while plate 1 stops at 1.3.
    &lt;/p&gt;
    &lt;/div&gt;

    &lt;div id=&quot;il6_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il6_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
    Very little missing data in this variable (699/710 observed).  Seems much more consistent than most other datasets. Scale and shape vary a bit, but no glaring inconsistencies, aside from a few outliers (e.g. plate 9).
    &lt;/p&gt;
    &lt;p&gt;
    Lots of between-plate variability (e.g. plate 7 vs. 10).
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il8_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il8_plates.png&quot; /&gt;
    &lt;p&gt;
        An excellent variable in terms of missing data (710 of 710 readings within range). The results seem very regular compared to other variables.  Seems to be Gamma-distributed or log-normal distributed but some big outliers.  Plates 5 and 10 have extreme outliers
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;tnfa_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-tnfa_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
    The best plate in terms of missing data (710/710), TNF-\(\alpha\) seems very regular within-plates.  Note the strong gaussian shapes here, compared to the full-dataset histogram earlier.  
    &lt;/p&gt;
    &lt;p&gt;
    Variation between plates is notable (see plate 5 vs. plate 8).  Supports a regularization approach.
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il2_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il2-plates.png&quot; /&gt;
    &lt;p&gt;
    This is one of the strongest plates in terms of missing data (710 of 710 observed).  Several plates have decent gaussian distributions, albeit with heavy tails (1, 7, 9), but several are irregular (4, 5, 8, 9), and others look more exponential (2, 3, 10). 
    &lt;/p&gt;
    &lt;/div&gt;
&lt;/div&gt;

</description>
				<pubDate>Thu, 15 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/15/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/15/reference</guid>
			</item>
		
			<item>
				<title>FIRE - cluster w/ missing data</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Testing missing data in cluster model&lt;/p&gt;

&lt;h2&gt;Run #1 - enable missing data&lt;/h2&gt;

&lt;p&gt;Segfault resulting from empty cluster.  Writing routine to create a cluster from worst point.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Still getting weird results.  Clusters are collapsing constantly.&lt;/p&gt;

&lt;p&gt;Even a &lt;em&gt;single&lt;/em&gt; missing value screws up results.  There must be a bug in my initial estimate script&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;BUG: true/false swap when determining whether to use missing-data-enabled line fitting&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Several bugs related to computing epsilon.  Fixed after several hours :-/&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;It seems we can continue to increase the missing percentage indefinitely, without the clustering suffering (or at least until an entire observation becomes missing, which isn&#39;t handled).&lt;/p&gt;

&lt;p&gt;Likely the small amount of noise is helping us a lot here.  We&#39;ll see how it works on real data.&lt;/p&gt;

&lt;h2&gt;Real FIRE data&lt;/h2&gt;

&lt;p&gt;High-level Tasks&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;merge radation data from Laura (into demograph dataset?)&lt;/li&gt;
&lt;li&gt;for each subject,
 first chemo
 last chemo
 first rad
 last rad&lt;/li&gt;
&lt;li&gt;write results in FIRE data format&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Reading and merging radiation data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;construct out_db cols: subject_ID, had_radiation&lt;/li&gt;
&lt;li&gt;if row has start and end date,

&lt;ul&gt;
&lt;li&gt;if out_db already has start or end date, record error&lt;/li&gt;
&lt;li&gt;else record start and end date&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;assert all &quot;radiation=yes&quot; have start and end date

&lt;ul&gt;
&lt;li&gt;can already see 57533 fails this test&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Do same for chemo dates.&lt;/p&gt;

&lt;p&gt;Merge chemo and rad.&lt;/p&gt;

&lt;p&gt;compute &quot;type&quot;&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;All implemented in &lt;code&gt;in_progress/process_treatment_dates.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Data consistency issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;some subjects surgeries occur after treatment

&lt;ul&gt;
&lt;li&gt;57527, 57563, 575139, 575145&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Only 97 of 136 subjects have immune data&lt;/li&gt;
&lt;li&gt;some subjects disagree about treatment type

&lt;ul&gt;
&lt;li&gt;57517 - no histo dates, but hist = 70&lt;/li&gt;
&lt;li&gt;575146, 575156 - demo:rad = yes, but no dates in rad_supl&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Tue, 13 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/13/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/13/work-log</guid>
			</item>
		
			<item>
				<title>FIRE - streamlining; missing data</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;16784&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Refactored kjb::Matrix::resize to be re-use allocated space under more circumstances.  This is currently a bottleneck in inference, and signficantly improves runtime.&lt;/p&gt;

&lt;p&gt;Implemented Missing data handling:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Likelihood ignores missing values&lt;/li&gt;
&lt;li&gt;initial model estimation handles gracefully handles missing data&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;TODO:  Test missing data.  Randomly convert observations to missing, see if cluster parameters and memberships are still correctly estimated.  Is there a &quot;critical point&quot; above which missing data ruins results?&lt;/p&gt;
</description>
				<pubDate>Mon, 12 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/12/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/12/work-log</guid>
			</item>
		
			<item>
				<title>FIRE Debugging cluster model</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Initial cluster estimation, something like k-means.&lt;/p&gt;

&lt;p&gt;likelihood isn&#39;t monotonically increasing over time.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;we aren&#39;t updating cluster weights&lt;/li&gt;
&lt;li&gt;we&#39;re doing soft assignment&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;found bug:&lt;/p&gt;

&lt;h2&gt;Run #1: initialize by K-means (part 1)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: do k-means to initialize cluster assignments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Estimate A, B: no
estimate epsilon: no
shared observation model: yes
continuity constraints: yes
num clusters: 3
observed dimensions: 7
num observations: 150
A estimation method: regression
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Revision&lt;/strong&gt;: 16767&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: Build first cluster from all data.  Iteratively choose &quot;bad&quot; points under current model as prototypes for new cluster.  Then run k-means.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;:  Converges after 15 iterations.&lt;/p&gt;

&lt;p&gt;In the 2x150 image below, first row is ground truth clustering, second row is experimental results.  Specific coloring is irrelevant, grouping is.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-12-cluster_results_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Not great results.  Hopefully noise is just too high to find any useful structure without more observations.&lt;/p&gt;

&lt;p&gt;Interesting that the g.t. clustering is so uneven.  May want to increase Dirichlet distribtuion&#39;s alpha parameter.&lt;/p&gt;

&lt;h2&gt;Run 2:  Rerun with small noise&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;:  Re-run with noise lowered to 0.01.
&lt;strong&gt;Revision&lt;/strong&gt;:  16773
&lt;strong&gt;Results&lt;/strong&gt;:  Converges after 3 iterations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-12-cluster_results.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Much better!  We don&#39;t correctly find the &lt;em&gt;tiny&lt;/em&gt; middle cluster, which we can hopefully find during HMC.&lt;/p&gt;

&lt;p&gt;TODO: Return to this model when testing HMC&lt;/p&gt;

&lt;h2&gt;Aside: speeding up Matrix::resize(); adding Matrix::realloc()&lt;/h2&gt;

&lt;p&gt;The documentation for Matrix::resize claims it reuses allocated space, which is actually untrue.  Spent some time playing with reimplementations before realizing any good solution will require reworking the C library, which would require more precision and care than I have the time for at the moment.&lt;/p&gt;

&lt;p&gt;Instead, made two halfway measures:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;added spacial case; space is reused iff new_cols == num_cols &amp;amp;&amp;amp; new_rows &amp;lt;= num_rows.&lt;/li&gt;
&lt;li&gt;Added new function: Matrix::realloc, which is like resize, but always preserves allocated storage if possible, and doesn&#39;t guarantee data is preserved afterward.&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Run 3: Rerun with unknown observation model&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: same&lt;/p&gt;
</description>
				<pubDate>Fri, 09 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/09/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/09/work-log</guid>
			</item>
		
			<item>
				<title>FIRE = Cluster model</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Implemented cluster model in &lt;code&gt;cluster_model.{cpp,h}&lt;/code&gt;.  Implemented synthetic data generation in &lt;code&gt;synthetic.{cpp,h}&lt;/code&gt;.  Working on initial model estimation using k-means.&lt;/p&gt;

&lt;h2&gt;Run #1:  Adding estimation of epsilon.&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: After estimating &#39;A&#39;, also estimate the noise scale, &lt;code&gt;epsilon&lt;/code&gt; (assumed 1.0 until now).&lt;br/&gt;
&lt;strong&gt;Method&lt;/strong&gt;: Project data onto &#39;A&#39; in data space, take mean projection error (see kjb_c::project_rows_onto_basis).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Using known epsilon: 0.5
-------------------
Training error: -45.3955
Prediction error: -46.5

Estimating epsilon: 0.46192
----------------------
Training error: -45.7958
Prediction error: -47.1319
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: in the ballpark!&lt;/p&gt;

&lt;h2&gt;Run #2: initial cluster estimation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Iteratively estimate three clusters and assign memebership.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issues&lt;/strong&gt;: third cluster initialization is identical to second.  Guess: it&#39;s picking the same bad point over and over.&lt;/p&gt;

&lt;h2&gt;Run #3: initial cluster estimation, per-cluster obs. model&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Like Run #2, but each cluster has an individual observation model&lt;/p&gt;
</description>
				<pubDate>Thu, 08 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/08/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/08/work-log</guid>
			</item>
		
			<item>
				<title>FIRE - continuous model, clustering</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Today I finished run #10 from yesterday, in which I tried whitening the data before doing inference.  The results were inconclusive but not encouraging.&lt;/p&gt;

&lt;p&gt;I need to clean up the code which has gained some cruft from yesterday&#39;s tests.  I&#39;m removing the whitening option, since it doesn&#39;t help with synthetic data, but I&#39;ll keep the whtening function for testing on real data later.&lt;/p&gt;

&lt;h2&gt;Run 1: contiguous model&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Introduce piecewise continuity constriants.   Re-run and evaluate prediction error.&lt;br/&gt;
&lt;strong&gt;Method&lt;/strong&gt;:  add a special case to Piecewise_linear_model.  If b.size() == 1, assume contiguous model and infer the other b&#39;s on the fly.  Initial model estimate uses the same code as the non-continouous model, and simply resizes b.x.b to 1 afterward.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Ground Truth
--------------
Training error:   88.6756
Prediction error: 90.3623

Initial Model
------------
Training error:   88.6303
Prediction error: 90.4582

Best Model
----------
Training error:   88.6268
Prediction error: 90.4576
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The prediction error of the estimated model is extremely close to that of ground truth.  This is a simpler model, so that probably explains improved prediction accuracy.&lt;/p&gt;

&lt;p&gt;It&#39;s notable that the best model is now different than the initial estimate.  That&#39;s because the continuity constraints make the problem not solvable analytically.&lt;/p&gt;

&lt;p&gt;Also notable is that training error for the estimated models is better than that of the ground truth.&lt;/p&gt;

&lt;hr /&gt;
</description>
				<pubDate>Wed, 07 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/07/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/07/work-log</guid>
			</item>
		
			<item>
				<title>Fire: 10 inference experiments</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Ran 10,000 iterations, keeping observation model fixed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: The three plots below are incorrect.  corrected plot follows.
Ground truth model:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_gt_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Initial model:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_initial_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Final model:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_final_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: There was a bug in my plotting code for the plots above. Here is the corrected plot, which is more sensible:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initial estimate is bad, esp y-offset.  Should review this code.&lt;/li&gt;
&lt;li&gt;Recovered results are noticably better.  Not perfect, but this may be because we&#39;re seeing the &lt;em&gt;final&lt;/em&gt; model, not the best one.&lt;/li&gt;
&lt;li&gt;Keeping observation model fixed seems to improve speed by two orders of magnitude (why???)&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Run 2:  Keep best model&lt;/h2&gt;

&lt;p&gt;Added a &#39;best_sample_recorder&#39; to keep track of the best model we&#39;ve seen.&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Observations&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Best model is pretty good&lt;/li&gt;
&lt;li&gt;Why is initial model so different from that in the previous run?  same model, same random seed.&lt;/li&gt;
&lt;li&gt;Keep in mind, on average only 1/3 of the [0,1] domain is represented in the dataset, because it&#39;s divided into &quot;before&quot;, &quot;during&quot; and &quot;after&quot; regions.&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Discussion&lt;/h3&gt;

&lt;p&gt;Initial model uses a different observation model, so it&#39;s hard to say whether it&#39;s good or not.  I get the feeling that there&#39;s a lot of slack in this model, meaning several different combinations of observed and latent parameters can give nearly equal results.&lt;/p&gt;

&lt;p&gt;Also, initial model estimation doesn&#39;t correctly handle x-offsets for &quot;during&quot; and &quot;after&quot; regions.  TODO: fix this.&lt;/p&gt;

&lt;p&gt;Note that the noise epsilon is very large relative to the model&#39;s dynamic range.  It&#39;s hard to visualize, since the plots above are sent through an observation model before noise is added. But the observation model&#39;s scaling is around 0.5.&lt;/p&gt;

&lt;p&gt;Variability in the latent variable is around 0.3.  Observation model scales that to 0.15, then adds noise on the order of 1.0.  So signal-to-noise ratio is around 0.15 -- not great.  Luckilly we have lots of observations (150 people x 9 time points x 7 observed dimensions), but in our non-synthetic model, I hope our noise is smaller.&lt;/p&gt;

&lt;h2&gt;Run 3: Improved initial model estimate&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;SVN REVISION&lt;/em&gt;: 16742&lt;br/&gt;
&lt;em&gt;Description&lt;/em&gt;: Fixed initial model estimate by centering each region at x=0.  See &lt;code&gt;model.cpp:partition_observations()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;# iterations: 10,000 iterations&lt;br/&gt;
Running time: 0:18.53&lt;br/&gt;
Results:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Summary:&lt;br/&gt;
Initial model has changed, but initial offset is still way off.  Other models (final and best) are the same.&lt;/p&gt;

&lt;p&gt;Discussion:&lt;br/&gt;
Need to dig more into the initial estimation code.  Is boost&#39;s RNG seeded with current time?&lt;/p&gt;

&lt;h2&gt;Run 4: fixed observation parameters&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;SVN REVISION&lt;/em&gt;: 16743&lt;br/&gt;
Goal: See if fixing observation parameters (A, B) improve line-fits.  If so, issue is in observation parameters.  If not, issue is with line-fitting.&lt;br/&gt;
Running time: 0:18.26&lt;/p&gt;

&lt;p&gt;Results:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Discussion:&lt;br/&gt;
Bad results; issue is probably in line-fitting, not estimation of observation model.&lt;/p&gt;

&lt;h2&gt;Run 5: fixed observation parameters (take 2)&lt;/h2&gt;

&lt;p&gt;Description: Found a bug when using fixed offset B -- wasn&#39;t subtracting offset before doing PCA to find A.&lt;br/&gt;
Revision: 16744&lt;br/&gt;
Results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Discussion:
No change.  In retrospect, this change only applies when not using fixed observation parameters, so no change is to be expected.  But a good bug to fix for later on!&lt;/p&gt;

&lt;h2&gt;Run 6:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;:&lt;br/&gt;
Found another bug: when projecting points onto pincipal component, if the direction vector \(d\) isn&#39;t normalized to 1, the projected points are off by a factor of \(|d|^2\).   The observation equation is given by:&lt;/p&gt;

&lt;div&gt;
\[
y = Ax + B
\]

The goal is to solve for \(x\), which means we need the Moore-Penrose pseudoinverse, \(A^+ = (A^\top A)^{-1} A \).  When \(A\) is a column vector, the term in perentheses on the right-hand size is the squared magnitude of \(A\), which was ommitted in the original equation.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Revision&lt;/strong&gt;:  16745&lt;br/&gt;
&lt;strong&gt;Invocation&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# On bayes01
cd ~ksimek/src/fire/src/piecewise_linear/test
./test_inference &amp;gt; /dev/null
cat results.txt | \
    awk &#39;{row=NR%7; if(row == 3 || row == 4) print;}&#39; \
    &amp;gt; ~/tmp/lin.txt

# on local machine
rsync -avz v01:tmp/lin*.txt ~/tmp/

# in matlab on local machine
cd ~ksimek/work/src/fire/src/matlab/in_progress
figure
lin = load(&#39;~/tmp/lin7_1.txt&#39;)&#39;
lin3 = reshape(lin, [3, 2, 4])
visualize_pl_result(lin3, ...
    {&#39;ground truth&#39;, ...
    &#39;initial model&#39;, ...
    &#39;final model&#39;, ...
    &#39;best model&#39;})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion:&lt;/strong&gt;&lt;br/&gt;
Finally getting good initial estimates.  In fact, initial estimate is the &lt;strong&gt;optimal estimate&lt;/strong&gt; when A and B are known.  The best model doesn&#39;t look perfect, especially in the red curve, but there seems to be significant variance in the red curve estimator, as shorn in &quot;final model&quot;.&lt;/p&gt;

&lt;h2&gt;Run 7: Initial estimate of A&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Time to add initial estimation of A.  Sampling will still only estimate latent parameters.    I&#39;m curious how close this will be to optimal.  I&#39;m forcing the ground-truth A to have unit-length so the experimental results are comparable to the ground truth.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Invocation&lt;/strong&gt;: see previous&lt;br/&gt;
&lt;strong&gt;Revision&lt;/strong&gt;: 16746&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Baseline&lt;/strong&gt;:  The results below are estimates of the linear model when A is known.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run7_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Below are results of sampling, using an estimate of A from the noisy data&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run7_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion:&lt;/strong&gt;&lt;br/&gt;
Initial estimate is slightly worse in the green and blue curves, but still in the ballpark, as we would hope.  As observation noise decreases, this estimate should improve.&lt;/p&gt;

&lt;p&gt;It&#39;s notable that HMC still can&#39;t find a better model.  This is good news for the quality of our estimate.&lt;/p&gt;

&lt;h2&gt;Run 8: initial estimate of A and B&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;:  Adding initial estimation of B to the test.&lt;br/&gt;
&lt;strong&gt;Invocation&lt;/strong&gt;: see previous&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;br/&gt;
Much worse.  This could be caused by overestimating the magitude of (B), while understimating the magnitude of (b) (which can result in identical models due to our model being overdefined).  Since (b) currently adds positive offset to all observations, estimating (B) as the mean over observations could is likely to capture some of (b).&lt;/p&gt;

&lt;p&gt;Probably the best way to evaluate is to measure prediction error.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt; run 9 shows that this model predicts quite well, despite having different model parameters.&lt;/p&gt;

&lt;h2&gt;Discussion - Scaling and offset&lt;/h2&gt;

&lt;p&gt;I&#39;m starting to think we should consider preprocessing the immunity data bafore passing it through this model, for two reasons&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. poorly scaled data; correlated noise&lt;/strong&gt;&lt;br/&gt;
It seems too easy that we can solve for &#39;A&#39; using simple PCA.  Said differently, dynamics are the interesting part of our model, but A is determined irrespective of dynamics.&lt;/p&gt;

&lt;p&gt;The observation transformation A has an intuitive interpretation as the direction of greatest variation in the data, after taking dynamics into account.  The problem with this is that if data is badly scaled, the direction of greatest variation will come from the noise, not the dynamics.   If possible, we’d prefer to separate variation due to noise from variation due to dynamics.  That way, A can capture the dynamic relationships between variables (e.g. IL-6 and IL-5 both start low and end high), not just correlation (IL-6 and IL-5 co-vary, but no temporal information).&lt;/p&gt;

&lt;p&gt;For this reason, it might help to &quot;whiten&quot; the data by PCA, so the inferred direction of A is more likely to capture dynamics only.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Variability in immunity measurements&lt;/strong&gt;&lt;br/&gt;
Second, I’ve been thinking on the fact that individuals’ immunity values differ significantly in scale and offset.  From what I understand, much of this variation occurs within “plate”, i.e. its a byproduct of laboratory conditions, not immunity dynamics.   This could affect our clustering, as the latent slope and y-intercept parameters depend directly on the data’s scale and offset.  As a result, I fear our learned clusters will reflect different groups of “laboratory conditions”, rather than different groups of immunity dynamics.&lt;/p&gt;

&lt;p&gt;We could replace readings with z-score within each person, but this has two problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;readings that are relatively constant but legitimately high would seem more typical than they are.&lt;/li&gt;
&lt;li&gt;extremely steep changes over time would tend to flatten. similarly, relatively low slopes would tend to look higher after transforming.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;No obvious answer; may need to try several solutions.&lt;/p&gt;

&lt;h2&gt;Run 9: Held-out error &lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Since our model is overdefined, comparing model parameters is no longer a good indicator of model quality.  Instead, we should held-out data to evaluate the predictive power of the model&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: hold out 20% of data, measure log-likelihood for (a) ground truth model, (b) initial model estimate.  Two cases: 1. using global mean for B, PCA for A, 2. using ground truth A and B.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Training error:
------------

    Ground truth:              -13301.3
    Ana. Soln (Known A, B):    -13295.2
    Ana. Soln (Inferred A, B): -13318.3

Testing error:
-----------

    Ground Truth:               -1886.17
    Ana. Soln (Known A, B):     -1887.3
    Ana. Soln (Inferred A, B):  -1887.06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Training error is &lt;em&gt;better&lt;/em&gt; for analytical solution with known A and B, which what we&#39;d expect if the analytical solution was the true optimum.  When A and B are inferred, training error is worse, since PCA doesn&#39;t account for dynamics.&lt;/p&gt;

&lt;p&gt;Prediction error is worse than ground truth for all models, which is a good sanity check.  But they&#39;re all in a very tight ballpark (less than 1.0), which suggests that we aren&#39;t overfitting.  A better test would be to compare against one or more simpler models, but these results are good enough to proceed.&lt;/p&gt;

&lt;p&gt;One good conclusion is that even though inferring A and B results in vastly different latent models (see run #8), the prediction is just as good.&lt;/p&gt;

&lt;h2&gt;Run 10: Whitening before inference&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: transforming observations so they follow a standard m.v. Gaussian may help in infering.  Test prediction error and compare against run #9 errors (should be nearly identical).
&lt;strong&gt;Revision&lt;/strong&gt;: 16756&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: transform data by PCA.  Using linear regression to infer initial direction (since PCA is out).  Force B to be zero.  Fit model.  Evaluate held-out error after transforming back to data space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(&quot;Error&quot; is average negative log likelihood over 150 points)

Ground truth
------------
    Training error: 88.6756
    Prediction error: 90.3623

Estimated Model
--------------
    Raw data; A,B from PCA
    ---------
    Training error: 88.7887
    Prediction error: 90.5986

    Raw data; A,B from Regression
    ------------
    Training error: 88.6285
    Prediction error: 90.4405

    Whiteened Data; A,B from Regression
    --------------
    Training error: 93.8502
    Prediction error: 95.7252
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;br/&gt;
Training and prediction error is worse for whitened data, which is contrary to my expectation.  In retrospect, we&#39;re fitting to different data, so it&#39;s not too surprising that it predicts worse.  The story may be different if the data is truely poorly scaled.  Our example data was already well scaled, so its not a great test.&lt;/p&gt;

&lt;p&gt;But in the end, its defnitely clear that whitening isn&#39;t innocuous, and possibly harmful.&lt;/p&gt;

&lt;h2&gt;Mid-term TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Tuning HMC&lt;/li&gt;
&lt;li&gt;refactor model

&lt;ul&gt;
&lt;li&gt;Start and end times out of model&lt;/li&gt;
&lt;li&gt;epsilon per-dimension&lt;/li&gt;
&lt;li&gt;missing data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add offset constraints (continuous model)&lt;/li&gt;
&lt;li&gt;clustering&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Meta-notes on experiments&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;always test on synthetic data first&lt;/li&gt;
&lt;li&gt;always test on a simpler model first (fix some parameters)&lt;/li&gt;
&lt;li&gt;always have a visualization in mind when running a test.&lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Tue, 06 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/06/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/06/work-log</guid>
			</item>
		
	</channel>
</rss>
