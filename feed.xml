<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>iPlant Reading; Index optimization</title>
				<description>&lt;h2&gt;WACV Reconstruction issues&lt;/h2&gt;

&lt;p&gt;I've been struggling getting good results on the Columbia-strain images from the WACV dataset.  I was hoping that manually re-tracing the ground trough would improve results, but problems still remain.  MY current theory is that these datasets exhibit significant shifting of the plants over time.  This is causing the dynamic programming algorithm for finding point-correspondences to give bad correspondences, which are never fixed later in the pipeline.&lt;/p&gt;

&lt;p&gt;This is best illustrated using the point-correspondence tables below.  These tables describe the point-corerspondences between views of a curve.
Each row represents a 2D curve from a different view; values in the table represent the index of a point along the 2D curve.
Each column represents a position along the underlying 3D curve, and the values in a column are a set of corresponding 2D points from each view.
An 'x' represents 'no match'; x's only occur at the beginning and end of a row.&lt;/p&gt;

&lt;p&gt;An ideal point-correspondence table looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;              3D curve position
           +--------------------+
view   1   | 1 2 2 3 ...  50 50 |  
index  2   | 1 1 2 2 ...  50 51 |
      ...  |         ...        |
       n-1 | 1 2 3 3 ...  48 48 |
       n   | 1 1 1 2 ...  45 46 |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the plant-stem exhibits drift over time, we get problems where each subsesquent curve is shifted left in the correspondence table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                3D curve position
           +--------------------------------------+
view   1   | 1 2 2 3 ...  50 50 x  x  x  x  x  x  |  
index  2   | x x 1 1 ...  48 49 50 51 x  x  x  x  |
       ... |         ...                          |
       n-1 | x x x x ...  45 46 47 47 48 48 x  x  |
       n   | x x x x ...  43 43 44 44 45 45 46 47 |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a result, we get 3D curves that are longer and tend to exhibit loopy curvature.&lt;/p&gt;

&lt;p&gt;My current theory assumes this occurs because the point-correspondence algorithm optimizes a local score, not taking into account smoothness or per-view perturbations that our full model allows.  I'm working on implementing a post-processing step that does local optimization on the index set w.r.t. the full marginal likelihood, which I'm hoping will fix these problems when they occur.&lt;/p&gt;

&lt;p&gt;I've derived an efficient method for computing the analytical gradient of the index set w.r.t. the marginal likelihood, which &lt;a href=&quot;/ksimek/research/2013/11/10/reference/&quot;&gt;I wrote up yesterday&lt;/a&gt;.  I've implemented some of the pieces, but finish the end-to-end code or test anything yet.&lt;/p&gt;

&lt;h2&gt;iPlant Reading&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Hypocotyl Tracing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Read three papers on Hypocotyl tracing from plant biology.  Methods are mostly straighforward: threshold, extract curve using {distance transform, morphological skeleton, gaussian tracing}, terminate using hand-built criterion.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hypotrace, Want et al.&lt;/li&gt;
&lt;li&gt;HyDE, Cole et al.&lt;/li&gt;
&lt;li&gt;Miller et al.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;Root Tracing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Read several papers on root tracing&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;RootRead3D&quot; Clark  et al. 2011

&lt;ul&gt;
&lt;li&gt;Zhu 2006 - hough-transform / space carving to find voxels; skeletonize using &quot;minumum cross section&quot;,  fit with NURBS curve.&lt;/li&gt;
&lt;li&gt;&quot;Smart-root&quot; Lobet et. al 2011 - Most cited Clark descendant; Semi-auto, &lt;strong&gt;2D&lt;/strong&gt;; hand-build algorithm, trace bright regions with consistent radius.  Seems tuned, nonrobust, relies on a strong GUI.  Naive?

&lt;ul&gt;
&lt;li&gt;&quot;EZRhizo&quot; Armengaud et al. 2009 - Manual tracing? (To Read)&lt;/li&gt;
&lt;li&gt;&quot;DART&quot; Le Bot et al., 2010 - Manual Tracing? (To read)&lt;/li&gt;
&lt;li&gt;Iyer-Pascuzzi et a., 2010 - Automatic.  Multiple angles, but &lt;strong&gt;2D analysis&lt;/strong&gt;.  preprocess: Adaptive threshold.  Medial axis: distance transform&lt;/li&gt;
&lt;li&gt;&quot;Root-trace&quot; Nadeem et al. 2009 - auto; (To Download)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fang et al. 2009  - Laser scanner, skeleton using hough transform method.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;Neuron Analysis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Reading papers of the &lt;a href=&quot;http://diademchallenge.org/algorithms.html&quot;&gt;teams that won the Diadam Challenge&lt;/a&gt; for tracing neurons.  It seems that image processing used in these papers far surpasses those in the plant biology I've read.  Likely due to (a) better imaging systems and (b) much more ambiguity to deal with.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wang et al. A Broadly Applicable 3-D Neuron Tracing Method Based on Open-Curve Snake&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Preprocessing: eigenanalysis of image hessian, eigenvalues are used to determine &quot;vesselness&quot; of each pixel.
Presegmentation: Graph cut on &quot;vesselness image&quot;.  Actually &quot;Vessel Cut&quot;;  &quot;tubular structures are further enhanced and close-lying axons are thinned in the vesselness image&quot;.  A bit vague here.  See Freiman et al. (2009).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To Read:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Freiman et al. (2009).&lt;/p&gt;
</description>
				<pubDate>Mon, 11 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/11/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/11/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;h2&gt;Correspondence Misalignment&lt;/h2&gt;

&lt;p&gt;WACV ground truth reconstruction errors are due to correpondence misalignment that occurs when different views don't line up (esp because of drift in the plant position over time).&lt;/p&gt;

&lt;p&gt;Ultimately, this causes index set estimates to be suboptimal.  To correct this, we can either focus on fixing the point-correspondence stage, or fixing the index set directly, after the correspondence stage.&lt;/p&gt;

&lt;p&gt;Two possible approaches come to mind.&lt;/p&gt;

&lt;p&gt;The first approach attempts to fix the point correspondences.  Do some keypoint matching and use this to constrain certain parts of the correspondence-matching (i.e. force the dynamic programming to pass through certain entries in the cost matrix).  Currently unclear how to deal with incorrect keypoint matches, or many-to-many correspondenes.&lt;/p&gt;

&lt;p&gt;The second approach is to do some optimization on the index points to improve the marginal likelihood.&lt;/p&gt;

&lt;h2&gt;Optimizing index points&lt;/h2&gt;

&lt;p&gt;The following assumes that the problem is the index set obtained by greedy point correspondence is sub-optimal under the marginal likelihood, and the indices at the true optimium would be immune to misalignment, because our kernel is robust to trnalsation and scaling errors.&lt;/p&gt;

&lt;p&gt;Doing 350-dimensional optimization seems like a terrible pain, but we might be able to take some shortcuts.&lt;/p&gt;

&lt;p&gt;First, we observe that the analytical derivative of the ML w.r.t. any change in the index set will require a computation on the same order as evaluating the marginal likelihood (nxn matrix inversion, multiplication of several nxn matrices).&lt;/p&gt;

&lt;p&gt;However, if we cache some values, we can avoid matrix inversion when computing subsequent elements of the gradient.  This still leaves nxn matrix multilication, but this becomes sparse, since most elements of K don't change when a single index changes.&lt;/p&gt;

&lt;p&gt;Alternatively, we can choose a few good directions and optimize in those directions only.  For example, when (a) all values are shifted together, (b) one endpoint is unchanged, and each subsequent index is changed more and (c) the complement of b.&lt;/p&gt;

&lt;p&gt;Regardless of the gradient approach, we still have to do a matrix inverse at every iteration.  We might be able to do this blockwise and save computation for indices that don't change.  Are there other approximations that we can do?  This doesn't need to be exact, but good enough to improve the indices&lt;/p&gt;

&lt;p&gt;Lets keep in mind, we're dealing with relatively small matrices, here.&lt;/p&gt;

&lt;h2&gt;Other thoughts&lt;/h2&gt;

&lt;p&gt;It's possible a long, curvy curve (like the one's we get when misalignment occurs) are actually the result best supported under the marginal likelihood.  In that case, we need to think more deeply about our overall approach.&lt;/p&gt;

&lt;h2&gt;Camera calibration&lt;/h2&gt;

&lt;p&gt;There is likely some room for improvement in the camera calibration.  Maybe this is the best approach overall?&lt;/p&gt;

&lt;h2&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Improve camera calibration&lt;/li&gt;
&lt;li&gt;Write function to get derivatives of covariance matrix.&lt;/li&gt;
&lt;li&gt;Try index optimization.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 10 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/10/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/10/work-log</guid>
			</item>
		
			<item>
				<title>Gradient w.r.t. Indices</title>
				<description>&lt;p&gt;To optimize indices, we'll need to compute the derivative of the marginal log-likelihood w.r.t. changing indices.&lt;/p&gt;

&lt;p&gt;I first tried to derive this using the generalization of the chain rule to matrix expressions (see matrix cookbook, section 2.8.1), but the computation exploded.  Since ultimately, the derivative is a simple single-input, single output function, we can use differentials to derive the solution.&lt;/p&gt;

&lt;p&gt;Let the marginal likelihood as a function of indices be \(g(x)\):&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial g(x)}{\partial x_i} = \frac{\partial}{\partial x_i} y^\top S^\top ( I + S K(x) S^\top)^{-1} S y
\]

Let \(U = I + S K(x) S^\top\), and \(V = U^{-1}\).  Working inside out, lets find \(\frac{\partial U}{\partial x_i}\).

\[
\begin{align}
    U + dU  &amp;= I + S (K + dK) S ^\top \\
            &amp;= I + S K S^\top + S dK S ^\top \\
        dU  &amp;= S \, dK\, S^\top \\
        U'  &amp;= S K' S^\top
\end{align}
\]

Where \(M'\) is the derivative of the elements of \(M\) w.r.t. \(x_i\).  Next, \(\frac{\partial V}{\partial x_i}\), which comes from the matrix cookbook, equation (36).

\[
    dV = -U^{-1} \, dU \, U^{-1} \\
    V' = -U^{-1} U' U^{-1}
\]

Finally,  \(\frac{\partial g(x)}{\partial x_i}\):
    
\[
\begin{align}
    g + dg  &amp;= y^\top S^\top (V + dV) S y \\
    g + dg  &amp;= y^\top S^\top \, V \, S y + y^\top S^\top \,dV \,S y \\
        dg  &amp;= y^\top S^\top \,dV \,S y \\
        g'  &amp;= y^\top S^\top \, V' \,S y \\
\end{align}
\]

Expanding \(g'\) gives the final formula:
\[
\begin{align}
        g'  &amp;= y^\top S^\top U^{-1} S K' S^\top U^{-1} S y \\
        g'  &amp;= y^\top M K' M y \\
        g'  &amp;= z^\top K' z \tag{1}\\
\end{align}
\]

&lt;p&gt;
Here, \(M = S^\top U^{-1} S \), (which is symmetric), and \(z = M y\).  
&lt;/p&gt;

&lt;p&gt;
This equation gives us a single element of the gradient, namely \(d g(x)/dx_i\).  However, once \(z\) is computed, we can recompute (1) for all other \(x_j\)'s at a cost of \(O(n^2)\), making the total gradient \(O(n^3)\), which is pretty good. (This assumes the K's can be computed efficiently, which is true; see below.)  However, we also observe that \(K'\) is sparse with size \(O(n)\), so we can do sparse multiplication to reduce the running time to linear, and **the full gradient takes \(O(n^2)\). ** Cool! 
&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Derivatives of K(x)&lt;/h2&gt;

&lt;p&gt;The structure of \(\frac{\partial K}{\partial x_i}\) is sparse; only the i-th row and i-th column are nonzero. We can find the values of the nonzero elements by taking the derivative of the kernel function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Cubic covariance &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recall the cubic covariance expression:&lt;/p&gt;

&lt;div&gt;
\[
k(x_1, x_2) = (x_a - x_b) x_b^2 / 2 + x_b^3/3
\]

Where \(x_b = min(x_1, x_2)\) and \(x_a = max(x_1, x_2)\).
&lt;/div&gt;


&lt;p&gt;Taking the derivative w.r.t. (x_2) gives:&lt;/p&gt;

&lt;div&gt;
\[
k(x_1, x_2) = 
    \begin{cases}
         x_2^2 / 2 &amp; \text{if } x_2 &gt;= x_1 \\
         x_1 x_2 - x_2^2/2 &amp; \text{if } x_2 &lt; x_1 
    \end{cases}
\]

Or equivalently

\[
k(x_1, x_2) = 
         x_2 \left [ \max(x_1,x_2)  - x_2/2 \right ]
\]
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt; Linear Covariance &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recall the cubic covariance expression:&lt;/p&gt;

&lt;div&gt;
\[
k(x_1, x_2) = x_1 x_2
\]

The derivative w.r.t. \(x_2\) is simply \(x_1\).
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt; Offset Covariance &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recall the offset covariance expression:&lt;/p&gt;

&lt;div&gt;
\[
k(x_1, x_2) = k
\]

The derivative w.r.t. \(x_2\) is zero.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt; Implementation &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Implemented end-to-end version in &lt;code&gt;kernel/get_model_kernel_derivative.m&lt;/code&gt;; see also components in &lt;code&gt;kernel/get_spacial_kernel_derivative.m&lt;/code&gt; and &lt;code&gt;kernel/cubic_kernel_derivative.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;These functions return all of the partial derivatives of the matrix with respect to the first input.   The i-th row of the result make up the nonzero values in \(\frac{\partial K}{\partial x_i}\).  Below is example code that computes all of the partial derivative matrices.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;N = 100;
% construct indices
x = linspace(0, 10, N);
% construct derivative rows
d_kernel = get_model_kernel_derivative(...);
d_K = eval_kernel(d_kernel, x, x);
% construct dK/dx_i, for each i = 1..N
d_K_d_x = dcell(1,N);
for i = 1:N
    tmp = sparse(N, N);
    tmp(i,:) = d_K(i,:);
    tmp(:,i) = d_K(i,:)';
    d_K_d_x{i} = tmp;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt; Directional Derivatives &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I think we can get directional derivatives of \(K\) by taking the weighted sum of partial derivatives, where the weights are the component lengths of the direction vector.  I have yet to confirm this beyond a hand-wavy hunch, and in practice, this might not even be needed, since computing the full gradient is so efficient.&lt;/p&gt;
</description>
				<pubDate>Sun, 10 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/10/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/10/reference</guid>
			</item>
		
			<item>
				<title>iPlant Literature Review Planning Meetings</title>
				<description>&lt;h2&gt;Document discussion&lt;/h2&gt;

&lt;p&gt;Some random notes about issues to bring up in the final document.&lt;/p&gt;

&lt;p&gt;Talk about representation vs. &quot;features&quot; (characteristics).  Clarify &quot;features&quot;, don't use indescriminitively (call them high-level features, topological features, as opposed to image features).&lt;/p&gt;

&lt;p&gt;Two ways to go about finding &quot;features&quot;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;look directly for charactersitics/features&quot;&lt;/li&gt;
&lt;li&gt;&quot;look for model, find the other stuff&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;i.e. looking for representations vs. characteristics&lt;/p&gt;

&lt;p&gt;Bisque vs. Nonbisque?  Unclear what the final application will be, but if Bisque is the goal, some discussion of it's feasibility is probably warranted.  For example, interactive systems like the Clark paper might not be feasible for Bisque.&lt;/p&gt;

&lt;h2&gt;Taxonomy of curve-extraction methods&lt;/h2&gt;

&lt;p&gt;Kobus mentioned the different &quot;dimensions&quot;  that the problem can be split into.  Some might be&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3D vs 2D&lt;/li&gt;
&lt;li&gt;imaging system type (microscope, MRI, camera, etc)&lt;/li&gt;
&lt;li&gt;temporal vs. nontemporal&lt;/li&gt;
&lt;li&gt;branching vs non branching&lt;/li&gt;
&lt;li&gt;biological vs. nonbiological&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Research Topics&lt;/h2&gt;

&lt;p&gt;We can split the background reading into several areas, both in image processing/CV and biological application.
Kobus recommended splitting time between these broad areas.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image Processing Topics&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;snakes&quot; - active contours&lt;/li&gt;
&lt;li&gt;Medial axis filtering&lt;/li&gt;
&lt;li&gt;curve modelling (polynomial/splines, GP, level-set methods)&lt;/li&gt;
&lt;li&gt;finding branches&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Application Areas&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pollen tube tracking&lt;/li&gt;
&lt;li&gt;root tracking&lt;/li&gt;
&lt;li&gt;Blood vessel literature / vascular segmentation&lt;/li&gt;
&lt;li&gt;Neuron tracing&lt;/li&gt;
&lt;li&gt;Alternaria / Fungus?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Other stuff&lt;/h2&gt;

&lt;p&gt;Other topics that might be worth looking into, but maybe of speculative value.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rhizotron - nondestructive underground root imaging system&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Data Inventory &lt;/h2&gt;

&lt;p&gt;We currently have data sets from a handful of different applications/domains.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pollen tubes&lt;/li&gt;
&lt;li&gt;&quot;Clark&quot; root image (2D, one image)&lt;/li&gt;
&lt;li&gt;&quot;Max Plank&quot; root images&lt;/li&gt;
&lt;li&gt;Neurons&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Any questions for Martha?&lt;/p&gt;

&lt;p&gt;Reading for next week: look into citations from Clark and Hypotrace&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kate - related work, Clark Paper&lt;/li&gt;
&lt;li&gt;Andrew - related work, Hypotrace paper&lt;/li&gt;
&lt;li&gt;Kyle image processing&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As we read, lets try to place each paper somewhere in the &quot;Taxonomy&quot; above.&lt;/p&gt;
</description>
				<pubDate>Fri, 08 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/08/meeting-notes</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/08/meeting-notes</guid>
			</item>
		
			<item>
				<title>WACV reconstruction (revisited)</title>
				<description>&lt;p&gt;Addressing issues that came up last time we ran WACV.&lt;/p&gt;

&lt;p&gt;Currently working on dataset #8, which has some real problems.&lt;/p&gt;

&lt;p&gt;Spent some time tweaking the ground truth, adding missing curves, and correcting one or two obvious mistakes.&lt;/p&gt;

&lt;p&gt;Still getting terrible results; one problem might be the &quot;rough&quot; reconstruction, which doesn't take into account anisotropic data uncertainty.  Fixed.&lt;/p&gt;

&lt;p&gt;False-positive reversal has been corrected, but still getting terrible results; it seems like the camera calibration is waaaay off.&lt;/p&gt;

&lt;p&gt;I dont have the calibration data for this dataset available, locally re-syncing full dataset.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Are these calibrated cameras ordered in counter-clockwise direction? No.  but the calibration_data.mat file in for this dataset is.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Inspected correspondence diagrams.  Possibly some of the ground truth curves are reversed?  Dataset #8, curve #7, view #9, only corresponds to very end of other points:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-07-bad_correspondence.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tweaking ground truth tracing program to show start-point of curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Ground truth program doesn't compile.  Clang++ for mountain lion is complaining.&lt;/p&gt;

&lt;p&gt;Apparently clang no longer uses gnu stdc++ library, and some library components use proprietary gnu symbols (I admit, it was my code).  Fixed.&lt;/p&gt;

&lt;p&gt;Getting some new errors regarding some forward declarations of STL pair.  Was hard to debug, because errors weren't local to the problematic code, and errors were cryptic.  Removed forward declarations; fixed.&lt;/p&gt;

&lt;p&gt;Found a template function I hand't ported from my experimental branch of KJB.&lt;/p&gt;

&lt;p&gt;Linker errors -- libXmu and libXi not found.  Tweaked init_compile's logic for OpenGL on Macs. Hopefully I didn't break anyone else's builds...&lt;/p&gt;

&lt;p&gt;More linker errors. Apparently i need to recompile &lt;strong&gt;everything&lt;/strong&gt; to use clang's c++ libraries?? rebuilt boost, next is casadi, but I can't find the source on my machine.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;recalibrate cameras&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 07 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/07/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/07/work-log</guid>
			</item>
		
			<item>
				<title>CVPR cleanup, documentation</title>
				<description>&lt;p&gt;Cleaned up blog, added &lt;a href=&quot;/ksimek/research/projects&quot;&gt;project pages&lt;/a&gt;, &lt;a href=&quot;/ksimek/research/events&quot;&gt;events page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Added to &lt;a href=&quot;/ksimek/research/events/CVPR2014/&quot;&gt;CVPR 2012 page&lt;/a&gt;, created &lt;a href=&quot;/ksimek/research/events/CVPR2014/summary.html&quot;&gt;summary of &quot;done&quot; and &quot;TODO&quot;&lt;/a&gt; post-CVPR.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;WACV - extend to multi-view/tracking; fix reconstruction errors.&lt;/p&gt;

&lt;p&gt;Working, but need to remove translation and scale perturb component.&lt;/p&gt;

&lt;h2&gt;Testing branch index and start index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;negative start index causes gaps&lt;/li&gt;
&lt;li&gt;positive start index causes overshot (pre-tails)&lt;/li&gt;
&lt;li&gt;tree base is shifting between view&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO (new) &lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Testing branch index and start index, and reversal&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Wed, 06 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/06/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/06/work-log</guid>
			</item>
		
			<item>
				<title>iPlant </title>
				<description>&lt;p&gt;Doing background research inre: forwarded email from Kobus, &quot;Fwd: What we really need from your students&quot;.&lt;/p&gt;

&lt;h1&gt;Literature review&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Paper 1:&lt;/strong&gt; &lt;em&gt;Three-Dimensional root Phenotyping with a Novel Imaging and Software Platform&lt;/em&gt;, by Clark, et. al.&lt;/p&gt;

&lt;p&gt;Introduces a semi-automated method for semantic reconstruction of root structures from turntable images, using our lab's definition of semantic reconstruction, i.e. 3D curves with topology and part labels.  It looks like they're using standard voxel-carving to identify foreground and background voxels (i.e. root and non-root voxels).  This approach uses calibrated cameras  to backproject silhouette images and take the intersection--in other words, visual hull but with voxels instead of polygonal meshes.   Then the skeleton of the foreground voxels is extracted using a median filter method to extract 3D curves.  Skeleton branches are then manually labeled by domain experts as one of several root types.  There also seems to be some functionality to manually correct errors during backprojection and skeleton extraction phase.&lt;/p&gt;

&lt;p&gt;They key contribution seems to be a list of 27 features derived from the resulting 3D data.  Bushiness, centroid and volume distribution seem to be discriminative for classifying a specimen between the two different species under study.    They also  measure the amount of helical curvature and how much gravity affects growth, which has apparently not been studied in rice plants prior to this?.&lt;/p&gt;

&lt;p&gt;Extracting most of the interesting features requires full semantic reconstruction, which is very difficult to obtain using known fully-automatic methods.  Further, this approach requires a calibrated camera, which likely precludes us from using it for post-hoc analysis of existing datasets that might exist in Bisque, unless calibration data is available.&lt;/p&gt;

&lt;p&gt;The &quot;Clark Rice root&quot; image provided on the wiki is a high resolution 2D image, which appears to be different from those used in the Clark paper, so it's unclear what its relevance is in this context.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Other notes&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Its unclear how silhouettes are extracted, but likely just intensity thresholding with manually-chosen threshold value.  They're using a lightbox background, so this approach seems sensible.   There's probably some tunable parameters for the skeleton-extraction phase, but these aren't discussed.&lt;/p&gt;

&lt;!--
Bisque thoughts
===================

* 3D approaches might be unrealistic in Bisque
    1. Most datasets won't have calibrated cameras.
        * Could try auto-calibration, but I double their effectiveness in the wild.
        * Could have user click corresponding points, between views, but this leads us to...
    2. Most datasets won't have multiple views.
        * could try bayesian approaches to solving the inverse problem, but this is an open research problem.

    3. If 1. and 2. are satisfied, we still need significant user interaction to implement Clarke; unlikely to be possible in Bisque?
        * Maybe Bisque could receive the files output by Clarke's software?  But at that point, the analysis is already done, what is Bisque being used for?
* Most effective 3D systems will collect data under a process specifically tailored to 3D data.  How does Bisque fit into this scenario?




iPlant recognizes that most image-processing software developed by biologists is heavily tuned to a specific application, and doesn't generalize to other datasets.  This is a problem endemic to the entire field of computer vision; even today many approaches work well on the dataset they were trained on and fail on others.  Even the Deva/Feltzenswalb human detectors--arguably one of the most robust object detectors in the field -- performs poorly on new datasets, as we saw when we applied it to Mind's Eye.  Only after rescaling images, tuning thresholds, and retraining did we get reasonable performance.

This highlights a very CV-centric mindset, which I claim hinders the progress toward the goal of obtaining the best possible plant informatics.  The problem is a misalignment of goals.  For CV researchers, the goal is to do the best CV possible with as little human interaction possible.     In contrast, biologists' goals are to do the best biology possible, which hinges on obtaining the best possible data at the least possible cost. As a result, CV researchers judge our algorithms on &quot;percent correct&quot; assuming zero human interaction, whereas biologists are more likely to judge an approach by the number of human-hours needed to get to 100% accuracy or close to it.   The Clark paper is a good example, where humans manually make corrections to the computer vision algorithm and manually label the parts of a model; only then do they obtain the 27 features of plant roots that make their research novel.  We should recognize that our roles as iPLant &quot;consultants&quot; may conflict with our natural instincts as CV researchers and try to embrace strategies that leverage our CV expertise but ultimately serve the goals of biologists.

One such strategy is to relax the &quot;fully automatic&quot; constraint and look toward semi-automatic systems that use human input to improve robustness and achieve broad applicability across datasets.  Clark is a perfect example of such a method that uses some human interaction in exchange for drastically better results.  I've seen similar successes in the area of plant modeling, where interactive CAD interfaces are combined with CV algorithms from 20 years ago (e.g. snakes) to create a method that is efficient and provides precise models.

This not to say that advanced CV methods have not place in iPlant, but if Martha says our current charges is to make recommendations as to broadly-applicable methods that are currently &quot;doable,&quot; my answer is &quot;semi-automatic methods&quot;.  Putting this in the context of CV, even the best fully automatic methods aren't really fully automatic, because they require training on the part of the user, which requires some form of interactive system.

So the overall theme of my argument is using CV to construct better interactive GUIs.  At the moment, the greatest challenge to this strategy are the constraints posed by the current Bisque system, which provides only a minimal amount of HTML-esqe interactivity (e.g. click points, type in &quot;tags&quot;), in a painfully &quot;modal&quot; framework (e.g. submit user's response to server, wait for server do some analysis, repeat).  One solution is to push hard in improviing the Bisque framework, for example exploiting HTML5 and javascript to provide rich client-side user interfaces.  However, not only would this be an expensive undertaking, but the most effective GUIs will need to be specific to the task at hand, and building an interactive system for building interactive system would be a herculean task in both design and implementation.  

Maybe the answer lies outside of Bisque itself.  Bisque's real value is as a database of images and metadata.  Let's recognize this and push more toward promoting scientists adding Bisque-compatible &quot;export&quot; functionality to existing data-analysis systems (like Clark, et al.), so Bisque can become the world-standard repository for rich image metadata.  

Alternatively, if iPlant wants to continue to push in the direction of performing analysis inside the system, it needs to put significant effort into interfaces that are dynamic, responsive, and powerful.  This means leaning heavily on javascript, using canvas, webGL, and client-side image analysis so users can respond in real-time to the results of their interactions.  For example, imagine an interface in which users can &quot;paint&quot; some foregound and background pixels to train a classifier, quickly see the result of the classifier, and re-train by painting misclassified pixels.   Or an automatic curve-tracing algorithm where users can drag incorrect curves onto their correct path.  These are approaches that are proven and can provide excellent results, but require human-hours to get there.  Minimizing human hours will be a combination of good UI to fix CV errors quickly and good CV to minimize the UI time.  



The alternative is to focus on the features we can extract in the absence of near 100% accuracy.  

--&gt;

</description>
				<pubDate>Tue, 05 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/05/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/05/work-log</guid>
			</item>
		
			<item>
				<title>Post-CVPR-deadline; 2-part likelihood efficiency, 2-pass sampling</title>
				<description>&lt;p&gt;First day after CVPR deadline.&lt;/p&gt;

&lt;p&gt;Comp exam - committee, planning.&lt;/p&gt;

&lt;p&gt;Computer issues - upgrade, crashing, keyboard?&lt;/p&gt;

&lt;h1&gt;2-Pass sampling.&lt;/h1&gt;

&lt;p&gt;Evaluating the second likelihood term is still very slow, even after a 100x speedup.&lt;/p&gt;

&lt;p&gt;First do MH using single-term likelihood.  Then treat that as the proposal for the two-term likelihood.  Since the first step satisfies detailed balance, we have:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \hat p(\theta) q(\theta' | \theta) &amp;=
    \hat p(\theta') q(\theta | \theta') \\
    \frac{\hat p(\theta)}{\hat p(\theta')} &amp;=
    \frac{q(\theta | \theta')}{q(\theta' | \theta)} 
\end{align}

\]
&lt;/div&gt;


&lt;p&gt;Where \(\hat p(\theta)\) is the surrogate posterior, using only the single-term likelihood.  Substituting this identity into the full MCMC acceptance term, we get:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\alpha &amp;= \left \{ \frac{p(\theta') q(\theta | \theta')}{p(\theta)q(\theta' | \theta)}  \right \} \\
    &amp;= \left \{ \frac{p(\theta') \hat p(\theta)}{p(\theta)\hat p(\theta')}  \right \} \\
    &amp;= \frac{L_2(\theta')}{L_2(\theta)}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;This obviously isn't applicable for traditional gibbs sampling, but gibbs could be used for proposal, and MH used to accept/reject&lt;/p&gt;
</description>
				<pubDate>Mon, 04 Nov 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/11/04/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/11/04/work-log</guid>
			</item>
		
			<item>
				<title>(Two day) Markov Sampling (ctd).  Implementing, testing, optimizing</title>
				<description>&lt;p&gt;Note: changes to &lt;a href=&quot;/ksimek/research/events/CVPR2014/params.html&quot;&gt;params&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Resuming from yesterday's discussion about Markov sampling.  I was concerned about piecewise sampling of interpolated values with insufficient data being used.  After some thought, realized there's a better approach: construct blocks from input indices, not output indices.&lt;/p&gt;

&lt;p&gt;Secondly: only use the markov approach when there's too much data to eat at once.  Those cases are also the ones with the least probability of poor-data issues.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Re-implemented (&lt;code&gt;curve_tree_ml_5.ml&lt;/code&gt;), geting weird results.  Huge negative eigenvalues&lt;/p&gt;

&lt;p&gt;symptom: 30 output indices, 1000 output indices
symptom: K has 26k elements!&lt;/p&gt;

&lt;p&gt;This code is a mess, proving hard to debug.  I'm going to roll back to version 4, use ideas developed in v5 to implement Markov sampling.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Done.  No errors, but results aren't great.  Is it possible the markov blanket is wrong, or maybe we're misusing previous data?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;I think I found it... wasn't conditioning on previous sampled values.&lt;/p&gt;

&lt;p&gt;Seems to be fixed now.  Next on to timing and tuning&lt;/p&gt;

&lt;h2&gt;Profiling / Optimizing&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;attachment covariance&lt;/li&gt;
&lt;li&gt;markov order&lt;/li&gt;
&lt;li&gt;block size&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;strong&gt;Constructing Attachment covariance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;construct_attachment_covariance_3 is still a bottleneck.  Need to investigate some of the suggestions from a few days ago.&lt;/p&gt;

&lt;p&gt;Only computing &lt;code&gt;Cov_star_star&lt;/code&gt; once (exploiting stationarity in temporal GP) helps somewhat.&lt;/p&gt;

&lt;p&gt;Grouping &quot;sibling&quot; object construction should help a lot too.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Markov order&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Markov order as low as 10 doesn't seem to negatively affect results.&lt;/p&gt;

&lt;p&gt;Need to crop observations before the earliest sampled point we're conditioning on.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Timing
Before: 17.0s (total,  7.5s on inversion)
After: 16.5 (7.2s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiny improvement, but at no cost so who's complaining?&lt;/p&gt;

&lt;h2&gt;Block Size&lt;/h2&gt;

&lt;p&gt;Not much to say here...  100 seems to be optimal.&lt;/p&gt;

&lt;h2&gt;Markov Order&lt;/h2&gt;

&lt;p&gt;Plotted maximum posterior for various markov orders to compare reconstruction quality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-01-mls_vs_mo.png&quot; alt=&quot;maximum liklehood vs. markov order&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;range: 1.40e3
(unweighted) mean: 4.0156
std deviation: 510.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the mean should be taken with a grain of salt,
Some chaos below 500, but the scale of the fluctuations is actually relatively small, 0.035%.  We should note that in MCMC, its the absolute fluctuations that are significant, so percent error can be deceptive.  But even so, I think these results are pretty good.&lt;/p&gt;

&lt;p&gt;It's interesting that markov order of 10 is only slightly better than markov order of zero!    Also surprising that between 10 and 500, error increases.&lt;/p&gt;

&lt;p&gt;Should compare variance here vs. variance of log-likelihood w.r.t. posterior.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Result of 20 posterior samples (low markov order)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markov order = 10
block size =  100
ll std deviation = 2.33e3
ll mean: 4.0153e6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Result of 20 posterior samples (high markov order)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markov order = 500
block size =  100
ll std deviation = 2.17e3
ll mean: 4.0132e6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The log-likelihood standard deviation are high relative to the error range in the graph above.   This is weak evidence that the variance in the graph above is dues to small-scale instabilities in the pixel likelihood.  That is to say, tiny insignificant changes to the 3D model can cause noticible fluctuations to the pixel likelihod, due to it's nonlinearities.  To some extent, what we really want is the expected value of the likelihood over the entire set insignificant 3D perturbations.  So if the deviations we see due to markov-order are simply observations of this phenomenon, they can be safely ignored, because they are in any scenario.&lt;/p&gt;

&lt;p&gt;Let's test by re-running the low-markov order test with lots of samples (200 instead of 20) and see if the ll mean approaches that of the high-markov-order test.  This is also a decent stress test for the likelihood server.&lt;/p&gt;

&lt;p&gt;Result of 200 posterior samples (low markov order)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markov order = 10
block size =  100
ll std deviation = 2.5603e3
ll mean: 4.0133e6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep, we're getting closer to the high markov order results for ll mean.  I think we can drop this issue for now and be okay with a markov order of 10-20.  It is still surprising that markov order of zero seems to work so well.  I guess future data doesn't add too much if the present data is sufficient.&lt;/p&gt;

&lt;h2&gt;Bottlenecks&lt;/h2&gt;

&lt;p&gt;Inversion: 7.1s
buildling covariance:  2.5s
one_d_to_three_d: 1.5s
blkdiag: 1.2s
other: ~3.5s&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inversion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Only need to invert once for all views... Actually, not true because inversion contains view-specific sample data.  But a large block of the matrix is unchanged between views.  Implementing optimization...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Down to &lt;strong&gt;14.7 seconds&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Using sparsity and suppressing output, down to &lt;strong&gt;11.5s&lt;/strong&gt;. (8s when not in profile mode)&lt;/p&gt;

&lt;p&gt;However, getting some numerical issues (small-magnitude negative eigenvlaues); probably will do better if we use a symmetric equation and invert using cholesky.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;construct_attachment_covariance is now the bottlneck, taking a full 35% of run time.&lt;br/&gt;
Some options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;cache intermediate values here and return them as described in the previous post.&lt;/li&gt;
&lt;li&gt;group together calls to &quot;build_sibling_object&quot;&lt;/li&gt;
&lt;li&gt;avoid computing self-covariance when its available in prior_K&lt;/li&gt;
&lt;li&gt;implement one-pass version for symmetric matrices&lt;/li&gt;
&lt;li&gt;precompute self-covariance for all views in one call.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Implemented 2, 3, and 4, reduced running time to &lt;strong&gt;8.7s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Implemented 1, reduced to  &lt;strong&gt;8.4s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Implemented 5, now construct_attachment_covariance is not a bottleneck.  &lt;strong&gt;8.3s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Not many strong bottlenecks now.  Some opportunities  to avoid build_cov_ and one_d_to_three_d like we did in &lt;code&gt;curve_tree_ml_5_bak&lt;/code&gt;.  Also, using a symmetric formula for posterior might help here.&lt;/p&gt;

&lt;p&gt;Tweaked build_cov_ to skip an inner loop if it would be a no-op.  Down to &lt;strong&gt;7.8s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Set block size to 100 (had forgotten to change back yesterday, was 120)   &lt;strong&gt;7.6s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Rearranged matrix multiplication.  &lt;strong&gt;7.4s&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mex'd &lt;code&gt;one_d_to_three_d&lt;/code&gt; utility function.  &lt;strong&gt;6.9s&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Problem.  Samples look bad.  Overall shape is retained, but rough, with discontinuities.&lt;/p&gt;

&lt;p&gt;Switched off block matrix inversion, results look good, but back to &lt;strong&gt;8.3s&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found bug.  Indexing problem was causing previous samples to be ignored.  Fixed, and its better than before, but still getting some non-negligible eigenvalues and occasional discontinuities in samples.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Originally took 275s, now 8.3s in profile mode, 5.x s in regular mode.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;running time vs. number of samples to determine affect of likelihood server bottleneck (use GPU likelihood for real-world estimate)&lt;/li&gt;
&lt;li&gt;profile/optimize likelihood server directly?&lt;/li&gt;
&lt;li&gt;Symmetrize formula and use cholesky for inversion.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 31 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/31/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/31/work-log</guid>
			</item>
		
			<item>
				<title>Optimizing posterior-sampling for pixel likelihood</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting issues in &quot;optimized&quot; code in &lt;code&gt;curve_tree_ml_4&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Biggest change was in how K, K_star, K_star_star were computed.
Comparing against the reference implementation &lt;code&gt;curve_tree_m_3&lt;/code&gt; shows the &quot;parent&quot; indices aren't correct.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Solved.  When incorporating noise-free sampled values, was using indices and covariance from noisy observed values.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Timing

Old speed:    20.7 s
New runtime:  75.7 s
Speedup: 3.6x
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Still rendering black.  why?&lt;/p&gt;

&lt;p&gt;Apparently this is an issue with OSX's AMD driver -- geometry shaders fail after returning from sleep.  Restarting program solves it.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Profiling v4&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Only 19 calls to &lt;code&gt;construct_attachment_covariance_3&lt;/code&gt;, down from ~500.&lt;/p&gt;

&lt;p&gt;78% bottleneck in matrix multiplication / inversion.&lt;/p&gt;

&lt;p&gt;Main problem is the 2500x2500 matrix from the main stem.  Other curves are in the 200 to 500 dimension range, and are fairly fast.&lt;/p&gt;

&lt;p&gt;Nystrom (2x) on curves with dimension greater than 1000 brings bottlenect from 18s to 11s, but as before, results are highly sensitive to the nystrom factor and using it without careful tuning / heursitics is risky.&lt;/p&gt;

&lt;p&gt;Probably expoiting the Markov nature of the curve GP  is the answer.  Linear runtime vs. cubic.&lt;/p&gt;

&lt;h2&gt;Markov within-curve sampling&lt;/h2&gt;

&lt;p&gt;break output indices into blocks.  For each block, get observation markov blanket.  Combine observed data and ancestor data into covarance matrix and data vector, then sample.&lt;/p&gt;

&lt;p&gt;Needed three covariance matrices: data vs. data, model vs. model, and model vs. data.&lt;/p&gt;

&lt;p&gt;Opportunity for optimizing construct_attachment_covariance_3 - sample all simpling-pair covariances at once.  289 total combinations (i.e. calls to eval_kernel) per dataset, could reduce to one.&lt;/p&gt;

&lt;p&gt;Seems like we can save computation in model covariance matrices by (a) exploiting the fact that it's always between points in the same view, and (b) only view changes between calls, spacial indices stay the same.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(A) compute K_star_star only once.&lt;/li&gt;
&lt;li&gt;(B) cache prior_K between constructions of K_star.&lt;/li&gt;
&lt;li&gt;(*) Cache &lt;code&gt;obj&lt;/code&gt;s and 'status' between calls to construct_attachment_covariance.  K_star reuses the lower triangular elements of K, K_star_star simply mirrors the upper-triangular  elements of K_star.&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Implementing...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Theoretical issues with the within-curve markov assumption.  If the markov blanket is chosen crudely, your samples could drift aimlessly until they reach observations in an unexpected place.  In other words, extrapolation, or sampling from weak data is a mistake.  Need a good criterion for when to stop takin on more evidence.&lt;/p&gt;
</description>
				<pubDate>Wed, 30 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/30/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/30/work-log</guid>
			</item>
		
	</channel>
</rss>
