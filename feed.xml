<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Cleanup and speedup of curve_ml.&lt;/p&gt;

&lt;h2&gt;Task 1: Version without matrix-inversion lemma&lt;/h2&gt;

&lt;p&gt;Making a simpler version that doesn't exploit the matrix inversion lemma (MIL).&lt;/p&gt;

&lt;p&gt;The benefits of the MIL are likely mitigated by markov-decomposition (next task), and MIL
complicates the code and API design.&lt;/p&gt;

&lt;p&gt;Results (small test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (108.2 ms)
Old ML: 62.183969
Computing marginal likelihood (new way, legacy correspondence)...
Done. (120.8 ms)
New ML (Legacy corr): 63.908784
Computing marginal likelihood (new way)...
Done. (88.9 ms)
New ML (New corr): 72.304718           &amp;lt;----------  OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (97.3 ms)
New ML (New corr, no MIL): 72.304666   &amp;lt;----------  NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference is about 0.00007% -- seems good.  Slightly slower, possibly just noise.&lt;/p&gt;

&lt;p&gt;Results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6447.3 ms)
Old ML: 207.324848
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7665.1 ms)
New ML (Legacy corr): 209.199676
Computing marginal likelihood (new way)...
Done. (6205.6 ms)
New ML (New corr): 457529.406731  &amp;lt;---------- ?????
Computing marginal likelihood (new way, no MIL)...
Done. (5946.7 ms)
New ML (New corr, no MIL): 778723.327102  &amp;lt;---------- ?????
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both versions give CRAZY high results; significantly different from the &lt;a href=&quot;/ksimek/research/2013/07/05/work-log/&quot;&gt;same test on Friday&lt;/a&gt;.  Need to investigate...&lt;/p&gt;

&lt;h2&gt;Investigating new results&lt;/h2&gt;

&lt;p&gt;Observations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All results differ from friday, including old ML (slightly)&lt;/li&gt;
&lt;li&gt;New ML using legacy correspondence is still reasonable.&lt;/li&gt;
&lt;li&gt;Both crazy results are using new correspondence.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Look at pieces (log posterior, likelihood, prior)&lt;/li&gt;
&lt;li&gt;Inspect new correspondence&lt;/li&gt;
&lt;li&gt;Think about what changed since Friday?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:10:19 PM&lt;/p&gt;

&lt;p&gt;Ran old test, &lt;code&gt;test_ml_end_to_end.m&lt;/code&gt; and compared to old results in new test, &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt;.  Old test still gives old results, so I'll compare the two tests to determine what has changed.&lt;/p&gt;

&lt;p&gt;12:12:48 PM&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;mix&lt;/code&gt; parameter was set to 0.5 instead of 0.0.  I had forgotten I changed it at the end if Friday.&lt;/p&gt;

&lt;p&gt;Now getting &quot;good&quot; results again.  It raises a question that needs to be answered: why is ML sooo sensitive to evalaution position when using new correspondence?&lt;/p&gt;

&lt;h2&gt;Resuming&lt;/h2&gt;

&lt;p&gt;New results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6009.5 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7286.3 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6146.6 ms)
New ML (New corr): 793.585038             &amp;lt;---------- OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (6008.3 ms)
New ML (New corr, no MIL): 793.600835     &amp;lt;---------- NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.002% error, slightly faster.  Good.&lt;/p&gt;

&lt;h2&gt;Investigating Anomaly in new ML &lt;/h2&gt;

&lt;p&gt;So why is new ML so sensitive to where it is evaluated?&lt;/p&gt;

&lt;p&gt;Note that it's only extreme when using new correspondence.  Old correspondence is okay.&lt;/p&gt;

&lt;p&gt;Inspect difference between max likelihood and max posterior.  Maybe it's just more extreme than with old correspondence.&lt;/p&gt;

&lt;p&gt;12:20:59 PM&lt;/p&gt;

&lt;p&gt;Idea: bug in non-0.0 case?  Nope.&lt;/p&gt;

&lt;p&gt;Tried mixes: 0.0, 0.01, 0.1. Steady and dramatic increase in ML for new correspondence.  Old correspondence is nearly constant.  What is different between these correspodnences (other than the correspondences themselves).&lt;/p&gt;

&lt;p&gt;12:57:08 PM&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;: The max likelihood in the new correspondences is waaaaay into the tails of the prior.&lt;/p&gt;

&lt;p&gt;Since no real triangulation is done to ensure agreement between views, the maximum likelihood triangulation is very rough  (infinite likelihood variance allows this to happen without penalty).  This places the curve far into the tails of the prior, where evaluation is highly unstable.  Both the prior and likelihood are then extremely low (-1e-8 in log space), which should cancel, but don't due to numerical instability.  Hence, marginal likelihoods with huge magnitude.&lt;/p&gt;

&lt;p&gt;This is great news, since it means everything is working mostly as expected.  The numerical instability issue is easilly solved by always evaluating at the posterior, not the maximum likelihood.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The version of the new ML that doesn't use the matrix inversion lemma is accurate, so we can proceed to the markov-decomposed version next.&lt;/p&gt;
</description>
				<pubDate>Mon, 08 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still trying to troubleshoot the difference between old and new likelihood implementations.&lt;/p&gt;

&lt;p&gt;Reviewing what is known&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prior is same&lt;/li&gt;
&lt;li&gt;Likelihood is same&lt;/li&gt;
&lt;li&gt;Posterior evaluates same in two different ways.&lt;/li&gt;
&lt;li&gt;mean curve is different&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Try using mean curve from old alg. in new alg.&lt;/p&gt;

&lt;p&gt;10:13:35 AM&lt;/p&gt;

&lt;p&gt;SOLVED.&lt;/p&gt;

&lt;p&gt;Yesterday I noticed that setting mix to 1.0 give decent results, but 0.0 was terrible.  Now I know why: 0.0 is a special case, in which a simplified calculation is used.  There was a bug in that special case, where the normalization constant didn't take into account the redundant dimensions that we eliminated.&lt;/p&gt;

&lt;h2&gt;ML Test&lt;/h2&gt;

&lt;p&gt;11:02:29 AM&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ml_test_end_to_end_2.m&lt;/code&gt; is now running&lt;/p&gt;

&lt;p&gt;Running on subset of correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...  
Done. (96.9 ms)  
Old ML: 62.176782

Computing marginal likelihood (new way, legacy correspondence)...  
Done. (112.3 ms)  
New ML (Legacy corr): 63.859940

Computing marginal likelihood (new way)...  
Done. (83.5 ms)  
New ML (New corr): 71.964026
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised the magnitude of difference between &quot;Old ML&quot; and &quot;New ML (Legacy corr)&quot;.  I guess the posterior variance is larger in this case, which means the posterior means can differ more.&lt;/p&gt;

&lt;p&gt;Running on full correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...
Done. (6400.1 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8328.4 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6349.2 ms)
New ML (New corr): 793.585038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that &quot;New ML (New Corr)&quot; is dramatiacally higher than the old and legacy ML's.  I suppose I should have been expected this, since this entire approach &lt;a href=&quot;/ksimek/research/2013/06/28/work-summary/&quot;&gt;was motivated by the terrible correspondences arising from the legacy correspondence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It's nice to see the ML for good correspondences getting even better.  Hopefully we'll see even more improvement as we try new curve models.&lt;/p&gt;

&lt;p&gt;Taking a break for lunch...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Next steps:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;fast ml_curve3

&lt;ul&gt;
&lt;li&gt;linear eval&lt;/li&gt;
&lt;li&gt;try without matrix inversion lemma&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;try new models (check that they're nonsingular; don't crash)&lt;/li&gt;
&lt;li&gt;set up training framework&lt;/li&gt;
&lt;li&gt;consider version without inversion lemma&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Fri, 05 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Continuing debugging of new likelihood&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Test:&lt;/strong&gt; &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;07:11:52 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Trouble with NaN error in &lt;code&gt;correspondence/corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;08:22:43 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Narrowed it down to gap-handling code-branch.&lt;/p&gt;

&lt;p&gt;08:53:34 AM&lt;/p&gt;

&lt;p&gt;Fixed. Arose from a nasty indexing scheme in the original &lt;code&gt;clean_correspondence.m&lt;/code&gt;, which I didn't handle well when adapting for &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;09:29:35 AM&lt;/p&gt;

&lt;p&gt;Nonsingular prior covariance matrix causing crash.  Apparently caused by same index appearing multiple times.&lt;/p&gt;

&lt;p&gt;In the past, we always merged such points.&lt;/p&gt;

&lt;p&gt;The new idea is different views of the same point would make the posterior covariance singular.  Either this isn't true, or there's a bug...&lt;/p&gt;

&lt;p&gt;Debugging this will take some serious coding and math brainpower and I fell debugging fatigue coming on.  Taking a brief break to refresh, then will tackle...&lt;/p&gt;

&lt;p&gt;11:56:21 AM&lt;/p&gt;

&lt;p&gt;Using a simple toy example, I (partially) confirmed my intuition that a degenerate prior matrix is okay, as long as its nullspace is spanned by the likelihood matrix.&lt;/p&gt;

&lt;p&gt;In the context of this bug, it means that two points sharing the same index value is fine as long as they arose from different views (assuming non-degenerate camera configuration, which is true in this case).&lt;/p&gt;

&lt;p&gt;Possible causes&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Numerical instability -- near-singular matrix.&lt;/li&gt;
&lt;li&gt;Error in Linear Algebra logic (seems unlikely, same code worked in old likelihood)&lt;/li&gt;
&lt;li&gt;Some bug earlier in the pipeline, causing invalid matrices here.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I'm guessing its 3; if not that, then maybe 2.&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;inspect the shared indices -- same view?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:48:49 PM&lt;/p&gt;

&lt;p&gt;Reversing my previous stance:  Degenerate prior with shared index will always result in a degenerate posterior.  The prior has two variables that are 100% correlated, and the posterior will also be 100% correlated.  This implies a degenerate covariance matrix.&lt;/p&gt;

&lt;p&gt;In such a case, the posterior's implied dimensionality is lower than it's apparent dimensionality.  The prior and posterior will be degenerate in the same way; this symmetry is aesthetically appealing, because the Candidates estimator for the marginal likelihood involves their ratio.  This suggests that eliminating the redundant dimensions for both pdfs is a sensible thing to do.&lt;/p&gt;

&lt;p&gt;How to resolve this?  Possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detect redundant indices early, eliminate them in the prior and posterior.  USe bookkeeping to remember which observations refer to the same model point.&lt;/li&gt;
&lt;li&gt;Handle reduntancies later, after degenerate posterior is formed, before evaluating posterior or prior .  The mean will automatically contain redundancies, which will avoid bookkeeping when comparing the mean to the data points (when &lt;code&gt;mix&lt;/code&gt; is != 0).&lt;/li&gt;
&lt;li&gt;Try to determine how identical indices arose in the first place.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Decided on 2 for now.  In retrospect, approach 1 may have had slightly less code and faster.&lt;/p&gt;

&lt;p&gt;01:45:00 PM&lt;/p&gt;

&lt;p&gt;ML is now running on legacy correspondence.  Results are significantly higher than the legacy ML algorithm (~ 2.25 times).  Not yet sure why yet.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6665.8 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8853.1 ms)
New ML (Legacy corr): 439.273126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Noticible slow-down, probably because of extra indices arising from not merging duplicate observations.&lt;/p&gt;

&lt;p&gt;possible causes of discrepancy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In old code, re-triangulation used simple averaging, not accounting for differing precisions of the points&lt;/li&gt;
&lt;li&gt;index sets differ between old and new code.  (update: checked, they are same)&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Plotting means for both to view differences...&lt;/p&gt;

&lt;p&gt;Investigation into ML discrepancy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Posterior means are similar, but obviously differ.&lt;/li&gt;
&lt;li&gt;both means are very non-smooth (need to re-address the smoothness issue)&lt;/li&gt;
&lt;li&gt;new mean has signficantly more points (~7%, 80 pts).  Could this alone affect ML?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;almost all difference is in posterior&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Maybe posterior kernel is different?  but prior is same
It's just the data covariance thats different.
but why??  There's more of them.&lt;/p&gt;

&lt;p&gt;New ml changes significantly when evaluation point changes.  This implies that the posterior curvature differs from the curvature of the joint distribution, which shouldn't happen.  A small amount of difference can arise naturally due to the approximation of the likelihood, but this is too large to be explained in that way.  Most noticibly, the new results look very similar to the old ones &lt;strong&gt;when evaluating at the likelihood mean&lt;/strong&gt;.  The largest difference occurs when evaluating at the posterior mean.  Not clear yet what conclusions to draw from this, or if this is just a coincidence.&lt;/p&gt;

&lt;p&gt;Maybe likelihood curvatures are differing.&lt;/p&gt;

&lt;p&gt;07:35:28 PM&lt;/p&gt;

&lt;p&gt;Was able to get identical likelihood value using matrix-form as I did with the original 2D geometric error form.  Seems like the precision matrices are good.&lt;/p&gt;

&lt;p&gt;Priors appear good too, since it evaluates to the same value as the reference implementation.&lt;/p&gt;

&lt;h2&gt;Reference implementation for new ML&lt;/h2&gt;

&lt;p&gt;Idea: direct implementation for quadform, but with hacked nomralization constant&lt;/p&gt;

&lt;h2&gt;In progress&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnosing errors in new likelihood &lt;code&gt;curve_ml_3.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;getting &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt; running.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 04 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Implementing covariance functions&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood&lt;/li&gt;
&lt;li&gt;Implement fast generalized marginal likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implement generalized marginal likelihood&lt;/h2&gt;

&lt;p&gt;Rewriting &lt;code&gt;curve_ml3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Based on &lt;code&gt;curve_ml.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Kernel now takes two sets of indices, curve-index and view-index.&lt;/p&gt;

&lt;p&gt;Kernel is now passed-in as a lambda.&lt;/p&gt;

&lt;p&gt;Need to decide whether to use matrix inversion lemma for global offset and linear prior.  Currently being used;  not using would be simpler; try both and compare results.&lt;/p&gt;

&lt;h2&gt;Testing new marginal likelihood&lt;/h2&gt;

&lt;p&gt;Creating new version of &lt;code&gt;test/test_ml_end_to_end.m&lt;/code&gt;, named &lt;code&gt;test/test_ml_end_to_end2.m&lt;/code&gt;.  This version uses the new likelihood format, and compares against the old.&lt;/p&gt;

&lt;h2&gt;New curve_likelihood&lt;/h2&gt;

&lt;p&gt;Need to write new version of &lt;code&gt;curve_likelihood.m&lt;/code&gt; to handle new likelihood format.&lt;/p&gt;

&lt;p&gt;It's going to be difficult to compare to older version, since we've changed the correspondence method...&lt;/p&gt;

&lt;p&gt;I'll have to add a flag that simulates the old correspondence method, but stores it in the new likelihood format...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;01:33:52 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I decided to write a separate function for the old correspondence method, called &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.  Moved &lt;code&gt;clean_correspondence3.m&lt;/code&gt; to &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The legacy function isn't exactly identical to the old function.  In particular, the index values might be slightly different, since it depends on the old function's global mean curve, which we don't compute in the new function.  If it's important, we can call the old function, and then call the new one.  This is a debugging function, so speed doesn't matter. For the moment, we'll accept the small difference, and use this function simply to confirm that we're in the ballpark.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;03:06:24 PM&lt;/em&gt;
Test is implemented.  Troubleshooting syntax errors...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;04:11:17 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Elusive NaN error in &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;
</description>
				<pubDate>Wed, 03 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing marginal likelihood &lt;/h2&gt;

&lt;p&gt;Working on generalizing marginal likelihood for all three foreground models.&lt;/p&gt;

&lt;p&gt;Considered briefly how to use the matrix inversion lemma to improve numerical stability of all models.  Decided it could be difficult to build in a general way;  will proceed with the direct method for now, until there is evidence of numerical instability.&lt;/p&gt;

&lt;p&gt;Generalized how covariance is generated.  Wrote several new functions for generating differenct covariance matrices.  Will need to rework some due to the developments I describe later.&lt;/p&gt;

&lt;h2&gt;Theoretical Developments&lt;/h2&gt;

&lt;p&gt;Thought extensively about perturbation models I described yesterday.  I realized there is a serious problem with modeling motion using brownian motion -- the prior variance grows without bound as time approaches infinity.  Thus, the marginal prior for the curve in view 36 has much greater prior variance than the curve one in view 1.  This doesn't make sense -- ideally, they should all have the same marginal prior.&lt;/p&gt;

&lt;p&gt;This led to a reading session in Williams and Rasmussen, which led me to develop two new motion models, which I describe extensively in today's accompanying post.&lt;/p&gt;

&lt;h2&gt;Tomorrow&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish implementing new covariance functions.&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood.&lt;/li&gt;
&lt;li&gt;implement &lt;em&gt;fast&lt;/em&gt; generalized marginal likelihood (approximation)

&lt;ul&gt;
&lt;li&gt;Test against existing ML for old models.&lt;/li&gt;
&lt;li&gt;Test with new models.  Are the ML's higher? (will probably need training).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build training data set from hand-traced ground-truth.&lt;/li&gt;
&lt;li&gt;Write training code.  Train all three candidate models.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Tue, 02 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log</guid>
			</item>
		
			<item>
				<title>Rethinking covariance functions</title>
				<description>&lt;p&gt;Noticed a problem with the cubic spline perturbation model.  As the view number increases, the variance marginal prior of the curve in that view approaches infinity.  This means that the &quot;smoothness&quot; prior is ignored more and more in later views.&lt;/p&gt;

&lt;h2&gt;Squared Exponential perturbation model&lt;/h2&gt;

&lt;p&gt;A better model would have the same marginal prior for all views, but with correlation between nearby views.  This allows curves to revert to the prior as temporally correlated evidence becomes less informative.  I believe the squared exponential covariance function has this property.  Instead of adding the sq-exp covariance to the existing covariance function, we should multiply it, so self-covariance is unchanged, but pairwise correlation is non-zero.&lt;/p&gt;

&lt;p&gt;An added benefit of this is that it only has one tunable parameter.&lt;/p&gt;

&lt;p&gt;It should be easy to incorporate into our test-bed, along with the other two newly proposed foreground models.&lt;/p&gt;

&lt;h2&gt;Ornstein Uhlenbeck perturbation model&lt;/h2&gt;

&lt;p&gt;Digging deeper into Williams and Rasmussen, I found precisely the GP I was looking for a few days ago:  the Ornstein–Uhlenbeck (OU) process.  This   process describes brownian motion under the influence of a regularizing force that pulls toward the mean.&lt;/p&gt;

&lt;p&gt;In other words, I can model correlation between views without affecting the marginal prior of the curve any particular view.  This is also accomplished by the squared-exponential model, but the OU process is probably more realistic, because the plant's motion looks non-smooth.&lt;/p&gt;

&lt;h2&gt;Modelling the mean or not?&lt;/h2&gt;

&lt;p&gt;I'm struggling with whether or not to explicitly model a &quot;mean&quot; curve with the SE and the OU processes.&lt;/p&gt;

&lt;p&gt;If I did model the mean, each curve's covariance function would be the sum of a standard covariance plus a perturbation covariance.  The standard covariance models the &quot;mean&quot; curve, and it would be the same for all views (100% correlated).  The perturbation covariance would be partially correlated between the views, using the SE or the OU process.  The bayes net has the following structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        O   &amp;lt;-- mean curve
     / /\ \
   /  /  \  \
 /   |    |   \
O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The alternative is to model the per-view curves directly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Under this model, each view's curve has a cubic-spline marginal distribution, and the SE or UO process controls the correlation between them.&lt;/p&gt;

&lt;p&gt;What isn't clear is whether the perturbations in the latter model will be independent between points.  We need to model within-view perturbations as correlated, otherwise the marginal likelihood will drop too low.  There is no explicit description of how perturbations of adjacent points correlate.&lt;/p&gt;

&lt;h2&gt;What follows is me thinking out-loud...&lt;/h2&gt;

&lt;p&gt;Intuitively, points nearby in curve-space (have similar \(\tau\)'s) can never be more correlated than they are when they appear in the same view.  Separating them in time (view) will decreases their correlation, until finally, there is no correlation; the only remaining correlation is between points in the same view.  The SE kernel modulates that correlation.  The SE kernel doesn't explicitly model correlation between perturbations, but this doesn't mean the correlation doesn't exist -- it is implicit in the original kernel.&lt;/p&gt;

&lt;p&gt;There is an analogy here with the classic krigging example.  In there models, the squared exponential over a 2D landscape (i.e. joint function over \(\mathbb{R}\)&lt;sup&gt;2&lt;/sup&gt; space) is equal to the product of 1D squared exponentials (i.e. two functions over \(\mathbb{R}\)&lt;sup&gt;1&lt;/sup&gt; space).  In other words, the 2D kernel is constructed by the product of 1D kernels.  There is no worry that the delta between nearby &quot;slices&quot; of the surface are uncorrelated, because the marginal covariance within that slice will enforce that smoothness.&lt;/p&gt;

&lt;p&gt;In our case, we also have a product of 1D covariance functions constructing a 2D covariance function.  The difference is that one of the kernels (the curve-space kernel) is a cubic-spline process, while the other (the time-dimension kernel) is squared exponential (or Ornstein-Uhlenbeck).  Despite this difference, my intuition is that the conclusions are the same - the deltas will be smooth (i.e. correlated), because they will be the difference between two smooth curves.&lt;/p&gt;

&lt;p&gt;Considering the marginals in both directions further illustrates why this works.  Obviously, a slice in the curve-direction will always look curve-like, since the marginals are all the same cubic-covariance GP prior.  In the time-direction, a single point will follow a GP or OU process, with initial conditions dictated by the first curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;...BUT...&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;...in the absence of data, each individual point will drift over time to the prior mean, I.e. &lt;strong&gt;zero&lt;/strong&gt;.  In other words, the 3D curve observed in view infinity gains no information from curve observed at time zero.&lt;/p&gt;

&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; realistic.  In reality, these points shouldn't drift far from some &quot;sensible&quot; center curve, implying that a mean curve exists.&lt;/p&gt;

&lt;p&gt;The time-slice of any particular point &lt;em&gt;should&lt;/em&gt; look like either a OU or SE process, but the mean needs to be explicit.  This implies an additive model, with distinct models for the unobserved &quot;mean curve&quot; and the deviations from it, which are summed to get the 3D curve seen in each view.&lt;/p&gt;

&lt;h2&gt;Modeling perturbation&lt;/h2&gt;

&lt;p&gt;So if we must model perturbation, how should we model it?&lt;/p&gt;

&lt;p&gt;One thing is clear: the marginal prior for the curve in any view &lt;strong&gt;must&lt;/strong&gt; still be a cubic-spline process.&lt;/p&gt;

&lt;p&gt;This implies that the perturbation must be a cubic-spline process, too.&lt;/p&gt;

&lt;p&gt;However, the variances for each component (offset, linear, and cubic) are likely to be different for the perturbation model, compared to the mean-curve model.  Most importantly, the magnitude of the offset variance must be large in the mean-curve model but will be &lt;em&gt;much&lt;/em&gt; lower (relative to linear and cubic variance) in the perturbation model.&lt;/p&gt;

&lt;p&gt;I was hoping to avoid adding four extra parameters to our model (perturbation offset, linear and cubic variance, plus perturbation scale-length).  The mean-free model only adds one parameter - scale-length.  I guess this is the price we pay for a better model -- and for higher marginal likelihoods.  Ideally, the occam's razor quality of the marginal likelihood will allow us to avoid overfitting this many parameters (7 total).  Any parameters that are superfluous should become near-zero during training.&lt;/p&gt;

&lt;p&gt;...I hope.&lt;/p&gt;
</description>
				<pubDate>Tue, 02 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Background:  This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;p&gt;Also developed a new approach to evaluate marginal likelhoods thats much simpler, but need to confirm that it matches reference implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implement both new foreground models, plus old one in new &quot;kernel function&quot; way.&lt;/li&gt;
&lt;li&gt;Implement new evaluation method and test against reference.&lt;/li&gt;
&lt;li&gt;Learn parameters for all three models (needs some ground truthing).&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implementing new models&lt;/h2&gt;

&lt;p&gt;Worked on general framework for evaluating ML under general new model framework.&lt;/p&gt;

&lt;p&gt;After some struggling with covariance degeneracies inherent in backprojection, I realized that the 3D marginal likelihood is naturally degenerate, because it isn't the true likelihood function (which is in 2D).&lt;/p&gt;

&lt;p&gt;I'm kicking myself for not remembering that I struggled with this exact problem 5 months ago.  At that time, I realized the better approach is to use Candidate's estimator, which is the ratio of the unnormalized posterior to the normalized posterior.  The unnormalized posterior comes from the prior and 2D likelIhood;  the normalized posterior is obtainable from the 3D likelihood and the prior.&lt;/p&gt;

&lt;p&gt;This was already implemented in &lt;code&gt;curve_ml.m&lt;/code&gt;, but was  O(n&lt;sup&gt;3&lt;/sup&gt; ), so was all but abandoned in favor of the junction-tree method in &lt;code&gt;curve_ml2.m&lt;/code&gt;, which is O(n).&lt;/p&gt;

&lt;p&gt;However, it's recently become clear that the candidate's estimator should be evaluated in \(O(n)\)  by exploiting the Markovian nature of the covariance matrix.&lt;/p&gt;

&lt;p&gt;It should be easy to try both, by simply swapping out covariance matrices with ones arising from the new covariance functions.  Tomorrow...&lt;/p&gt;
</description>
				<pubDate>Mon, 01 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log</guid>
			</item>
		
			<item>
				<title>Foreground Curve Models as Gaussian Process Covariance Function</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Background&lt;/h2&gt;

&lt;p&gt; This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;h2&gt;Covariance functions as models&lt;/h2&gt;

&lt;p&gt;I realized this weekend that all of the models I've considered are achievable by using different covariance functions.&lt;/p&gt;

&lt;p&gt;The original model is given by&lt;/p&gt;

&lt;div&gt; \[
k_1(i,j) = \sigma_s k_{\text{cubic}}(i,j) + \sigma_o  k_{\text{offset}}(i,j) + \sigma_r k_{\text{linear}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
k_{\text{cubic}}(i,j) &amp;= \frac{|\tau_i - \tau_j| \min(\tau_i, \tau_j)^2}{2} + \frac{\min(\tau_i, \tau_j)^3}{3} \\
k_{\text{linear}}(i,j) &amp;= \tau_i \tau_j  \\
k_{\text{offset}}(i,j) &amp;= 1 \\ 
\end{align}
\] &lt;/div&gt;


&lt;p&gt;The cubic model penalizes non-zero second derivative over the length of the curve.  The offset and linear model penalize zero and first derivative initial conditions.&lt;/p&gt;

&lt;h2&gt;White noise perturbation model&lt;/h2&gt;

&lt;p&gt;Both of the two new models expand on the original by modelling how the observed curve differs between views.  That is, the model for the curves are the same as before (they are cubic spline curves), but we additionally model &lt;strong&gt;how they are perturbed between views&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The first new model I call the &quot;White Noise&quot; perturbation model, which treats each view of the curve as arising from a white noise process that perturbs a &quot;master curve&quot;, I.e. the unobserved mean curve.  Its covariance is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{white}}(i,j) = k_1(i,j) + \delta(v_i-v_j) k_{\text{w}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(v_i\) is the view that captured the \(i\)th point and \(k_w\) is&lt;/p&gt;

&lt;div&gt; \[
k_w(i,j) = \sigma_{o,w}  k_{\text{offset}}(i,j) + \sigma_{r,w} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model adds extra covariance that is independent per-view.  This treats the perterbations from the mean curve as independent.&lt;/p&gt;

&lt;p&gt;The perturbations themselves are assumed to be purely to the curve's initial conditions, i.e. its position and direction.&lt;/p&gt;

&lt;p&gt;This model is motivated by the assumption that perturbations arise due to camera mis-calibrations, which result in mostly translation and small rotational changes.&lt;/p&gt;

&lt;h2&gt;Brownian motion perturbation model&lt;/h2&gt;

&lt;p&gt;The second model treats perturbations as arising from Brownian motion, i.e. each curve is independent, conditioned on the previous view's curve.  The covariance function is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{brown}}(i,j) = k_1(i,j) + \min(\tau_i, \tau_j) k_{\text{b}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(k_b\) is&lt;/p&gt;

&lt;div&gt; \[
k_b(i,j) = \sigma_{s,b} k_{\text{cubic}}(i,j) + \sigma_{o,b}  k_{\text{offset}}(i,j) + \sigma_{r,b} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model assumes perturbations arise by motion of the plant during imaging, like moving closer to a light source, or responding to a temperature gradient in the room.  &quot;View index&quot; is a surrogate for a time variable, since time between captures is roughly constant.  The use of Brownian motion means the magnitude of perturbation increases by the square root of the distance between the views (time).  \(k_b\) models the nature of the perturbation; we use the cubic-spline kernel which says that point-wise perturbations are strongly correlated and increase in magnitude the further they get from the base of the curve.&lt;/p&gt;

&lt;p&gt;This \(k_b\) is possibly overkill;  using the simpler \(k_w\) from the white-noise model might work just as well.  This simpler model implies curves drift in position and direction only, and these perturbations are correlated over time.&lt;/p&gt;

&lt;h2&gt;End stuff&lt;/h2&gt;

&lt;p&gt;After some consideration, I think implementing these models in the current system will will very simple.  Training them will be harder; some more ground truthing will be needed.&lt;/p&gt;

&lt;p&gt;Of the two new models, I suspect that the white noise model will be sufficient to get the gains in marginal likelihood we seek.  It is a much better explanation for misaligned data-points compared to the old model models all point perturbations as independent.&lt;/p&gt;

&lt;p&gt;The brownian motion model will give us something to compare against in evaluation.&lt;/p&gt;
</description>
				<pubDate>Mon, 01 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models</guid>
			</item>
		
			<item>
				<title>Work summary</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finished editing new likelihood function.  See &lt;code&gt;correspondence/clean_correspondence3.m&lt;/code&gt; (filename likely to change soon).  Below is a summary of results and comparison to the old approach.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;Below is a plot of the maximum likelihood curves:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-likelihood_1.gif&quot; alt=&quot;Maximum likelihood reconstruction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each colored curve arises from a single 2D data curve.  The curve represents the mean of a gaussian process. Variance is not shown; it is infinite in the direction of backprojection.&lt;/p&gt;

&lt;p&gt;The dark-blue curve is an estimate of the posterior curve, which was used to backproject the 2D curves against.  It is estimated from the point-correspondences.&lt;/p&gt;

&lt;p&gt;Note that for clarity, x/y scale is significantly smaller than z-scale (up-direction).  When axis are equally-scaled, max-likelihood curves lie very close the to blue curve.&lt;/p&gt;

&lt;h2&gt;Improved correspondence&lt;/h2&gt;

&lt;p&gt;The old likelihood suffered from a small but non-negligible number of awful correspondences, which severely damaged both reconstruction and marginal likelihood values.  This was because the likelihood was derived from the point-to-point correspondences, which (a) is problematic at gaps, and (b) suffer bad correspondences which can't be fixed later.
The new approach uses the old approach as a starting point, but then recomputes all 2D curve correspondences against a rough reconstruction.  This dramatically improves correspondences as we see below.&lt;/p&gt;

&lt;p&gt;This is the old correspondence.  Blue points are points from the 2D data curve;  the teal line is the posterior 3D curve (projected to 2D); red lines show correspondences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug.png&quot; alt=&quot;Correspondence bug &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next is the fixed correspondence.  Notice how the red correspondence lines are much shorter, indicating a less costly correspondence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug-fixed.png&quot; alt=&quot;Correspondence bug, fixed &quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Per-data-curve likelihood&lt;/h2&gt;

&lt;p&gt;The old likelihood had a single GP curve that represented all of the different views.  Now we have a GP curve &lt;strong&gt;per data-curve&lt;/strong&gt;, which will be related by a GP prior.&lt;/p&gt;

&lt;p&gt;This will allow us to simultaneously track and triangulate, a key novelty to this approach.  More importantly, it will give us higher marginal likelihood numbers for true 3D curve observations, because we can make the independent noise component very small.&lt;/p&gt;

&lt;h2&gt;TODO - short-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt; - This new function adds an extra pass of DTW per data-curve, per iteration where previously, only one pass was needed.  This has introduced a significant performance bottleneck on the order of 10-50x.  I need to profile and optimize this function if we want runtimes that aren't measured in weeks.  This may motivate a full re-thinking of &lt;code&gt;merge_correspondence.m&lt;/code&gt;, to avoid a full DTW after every merge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Marginal Likelhood &lt;/strong&gt; - Need to build the new marginal likelihood for the foreground curve model.  This proposed version will take advantage of the new per-data-curve likelihood.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO - mid-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end&lt;/strong&gt; - Incorporate new likelhood and ML into gibbs sampler.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Swendsen wang cuts &lt;/strong&gt; - design SWC split/merge move&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Fri, 28 Jun 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Profiling and optimizing &lt;code&gt;clean_correspondence3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Huge bottleneck in &lt;code&gt;get_dtw_matches_()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Wrote MEX version of get_dtw_matches_(),  &lt;code&gt;get_dtw_matches_horiz.c&lt;/code&gt;.   ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New bottlenecks&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;projection_error_hessian&lt;/li&gt;
&lt;li&gt;&lt;code&gt;interp1&lt;/code&gt; - linear interpolation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;csaps&lt;/code&gt; - spline smoothin&lt;/li&gt;
&lt;li&gt;dtw&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Plan&lt;/h2&gt;

&lt;p&gt;can possibly eliminate some calls to interp1.&lt;/p&gt;

&lt;p&gt;could mex projecion_error_hessian.  should be a big win&lt;/p&gt;

&lt;p&gt;DTW can be done in two passes, or mex'd&lt;/p&gt;

&lt;h2&gt;Mexing &lt;code&gt;projection_error_hessian&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Tried using Matlab's &quot;Coder&quot; feature to generate mex code automatically.  The result was totally bloated (14 files!) and no faster than the matlab version.  Next I tried hand-coding the mex, and its ~10x faster.&lt;/p&gt;
</description>
				<pubDate>Fri, 28 Jun 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log</guid>
			</item>
		
	</channel>
</rss>
