<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting Jacobians of z and mu.&lt;/p&gt;

&lt;p&gt;Analytical and numerical versions of z's Jacobian differ significantly in some entries.  To troubleshoot, stepping backward through the derivations of J_z outlined in &lt;a href=&quot;/ksimek/research/2013/12/12/reference/&quot;&gt;the reference post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I will refer to the final equation from that post and count backward from the final line, describing the result of implementing each line.  E.g. line 0 is the final result, the one used to compute z_ana; line 1 is the one labelled &quot;(3D version)&quot; .&lt;/p&gt;

&lt;p&gt;Setup:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Track = detach(trash_Tracks_11(end), params_test);
K = get_K(Track); 
S = Track.ll_S;
U = speye(size(K)) + S * K * S';
Ui = inv(U);
kern = get_model_kernel_derivative(params_test);
ind = get_curve_indices(Track);
views = Track.ll_views_flat;
Delta = eval_kernel(kern, ind, ind, views, views);
Delta3 = one_d_to_three_d(Delta);

i = 1;   
i3 = 3*(i-1)+1;
I = i3:i3+2;

delta_i = Delta(i,:)';
delta_i3 = one_d_to_three_d(delta_i);

dK = zeros(size(K));
dK(I,:) = dK(I,:) + Delta3(I,:);
dK(:,I) = dK(:,I) + Delta3(I,:)';

A = A = S' * Ui * S;
z = z = A * y;
N = length(K);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Implementing line 1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line_1 = -A(:,I) *delta_i3' * z - A * delta_i3 * z(i3:i3+2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It actually looks pretty good.  So why is line_0 wrong?&lt;/p&gt;

&lt;p&gt;Let's isolate the first term.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line_1_a = -A(:,I) *delta_i3' * z;
line_0_a = -sum_1xN(A .* repmat((Delta3 * z)', N,1), 3);
plot(xx, line_1_a - line_0_a(:,i))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Error within 1e-12.  Great!  Error must be in the second term...&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line_1_b = A * delta_i3 * z(i3:i3+2);
line_0_b =  - A * (Delta3(1:3:end, :)' .* repmat(reshape(z,3,[]), N/3, 1));
plot(xx, line_1_b - line_0_b(:,i))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep.  it's a mess.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Found the bug.  Was incorrectly trying to implement Delta&lt;em&gt;{1x3} by taking Delta&lt;/em&gt;{3x3}(1:3:end, :), which is totally wrong.  Also, was mis-associating the matrix multiplication and elementwise multiplication.  That is, I was taking (A * Delta_1x3) .&lt;em&gt; XXX  instead of A * (Delta_1x3 .&lt;/em&gt; XXX).&lt;/p&gt;

&lt;p&gt;Those two fixes, and the dZ test now passes.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still getting bad results for dmu.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Same bug as for dZ.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Developed new constant-width energy function, and derived its gradient.  Writeup &lt;a href=&quot;/ksimek/research/2013/12/19/reference&quot;&gt;is here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Derive Hessian of energy function&lt;/li&gt;
&lt;li&gt;implement and test gradient and hessian.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 19 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/19/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/19/work-log</guid>
			</item>
		
			<item>
				<title>Constant-length energy function -- revisited</title>
				<description>&lt;p&gt;My &lt;a href=&quot;/ksimek/research/2013/12/12/reference/&quot;&gt;earlier derivation&lt;/a&gt; of the constant-length energy function was flawed, because it pooled the individual lengths before comparing them to the pooled index-spacing.  Thus, this energy function enforces the &lt;em&gt;sum&lt;/em&gt; of squared lengths but not the individual lengths.  Chalk it up to trying to be too clever in avoiding a square root.&lt;/p&gt;

&lt;p&gt;In what follows, I derive a new energy function and its gradient.  The Jacobians of \(\mu\) and \(z\) are re-used from the earlier treatment.&lt;/p&gt;

&lt;div&gt;
&lt;p&gt;
  Let \(\mu\) be the maximum posterior curve, given an index set, \(\mathbf{x}\), arranged as a single column in &quot;xyzxyz&quot; format.  Let \(\mu^{(3)}\) be the 3 by N/3 matrix obtained by rearranging the points of \(\mu\) into column vectors.  That is, the i-th column \(\mu^{(3)}_i\) is the i-th reconstructed point.  
  &lt;/p&gt;

&lt;p&gt;
  Let \(\eta\) be the vector of absolute distances between adjacent points in \(mu^{3}\). Formally,
  
\[
    \eta_i = \| \mu^{(3)}_i - \mu^{(3)}_{i-1} \|.
\]

Note that \(\eta^\top \eta = \|\eta\|^2 = \mu^\top D^\top D \mu\), where \(D\) is the adjacent differences matrix, adapted to operate on column vectors in the &quot;xyzxyz&quot; format.

&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;The constant width energy function is defined as&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    E &amp;= \frac{1}{2} \left ( Dx  - \eta \right ) ^2 \\
      &amp;= \frac{1}{2} x^\top D^\top D x + \frac 12 \eta^\top \eta  - x^\top D^\top \eta \\
      &amp;= \frac{1}{2} x^\top D^\top D x + \frac 12 \mu^\top D^\top D \mu  - x^\top D^\top \eta \\

\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Gradient is given by&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial E}{\partial x} = x^\top D^\top D + \mu^\top D^\top D J_\mu - \eta^\top D - x^\top D^\top J_\eta
\]
&lt;/div&gt;


&lt;p&gt;where \(J_z\) is the Jacobian of \(\mathbf{z}\) w.r.t. \(\mathbf{x}\).&lt;/p&gt;

&lt;p&gt;The Jacobian of \(z\) and \(\mu\) are derived in &lt;a href=&quot;/ksimek/research/2013/12/12/reference/&quot;&gt;this earlier post&lt;/a&gt;.  It remains to find the Jacobian of \(\eta\).&lt;/p&gt;

&lt;div&gt;
Note the identity \( \eta_i^2 = \| \mu^{(3)}_i - \mu^{(3)}_{i-1} \|^2 \) can be rewritten in terms of the full vector \(\eta\) as 

\[
\eta \odot \eta = \operatorname{sum_{3x1}}(D \mu \odot D \mu)
\]

where \(\operatorname{sum_{kx1}}\) implements k-way blockwise summation over columns of a matrix.  Formally, it is the function \(f : \mathbb{R}^{NxM} \rightarrow R^{(N/k)xM}\) (for any N divisible by k), such that 

\[
    (f(A))_{ij} = \sum_{s = (i-1)\times k+1}^{i \times k} A_{sj}
\]
&lt;/div&gt;


&lt;p&gt;The Jacobian of \(\mathbf{\eta}\) can then be given by&lt;/p&gt;

&lt;div&gt;
\[

\begin{align}
\frac{\partial \eta}{\partial x_i} &amp;= \frac{\partial}{\partial x_i} \left ( \eta \odot \eta \right)^{\circ\frac12} \\
                                 &amp;= \frac{\partial}{\partial x_i} \left ( \operatorname{sum_{3x1}}(D \mu \odot D \mu) \right)^{\circ\frac12} \\
                                &amp;= \frac12 \left ( \eta \odot \eta \right)^{\circ \frac{-1}{2}} \frac{\partial}{\partial x} 
                                    \left ( \operatorname{sum_{3x1}}(D \mu \odot D \mu )\right) \\
                                &amp;= \frac12 \eta^{\circ (-1)} 
                                     \left ( \operatorname{sum_{3x1}}(\frac{\partial}{\partial x} D \mu \odot D \mu )\right) \\
                                &amp;= \frac12 \eta^{\circ (-1)} 
                                     \left ( \operatorname{sum_{3x1}}( 2 D \mu \odot D J_\mu )\right) \\
                                &amp;= \eta^{\circ (-1)} \left ( \operatorname{sum_{3x1}}( D \mu \odot D J_\mu )\right) \\

\end{align}
\]

where \(x^{\circ(-1)} = \left( \frac{1}{x_{ij}} \right)\) is the Hadamard (i.e. element-wise) inverse, and \(x^{\circ \frac12}\) is the Hadamard root.
&lt;/div&gt;



</description>
				<pubDate>Thu, 19 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/19/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/19/reference</guid>
			</item>
		
			<item>
				<title>Constant-length energy function</title>
				<description>&lt;p&gt;&lt;strong&gt;Update: What follows is an early working of the constant-length energy function and much of which I learned to be invalid.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Energy function and its gradient are:&lt;/p&gt;

&lt;div&gt;
\[
E = 0.5 (0.5 \mu^\top D^\top D \mu - 0.5 t^\top D^\top D t)^2 \\
\frac{\partial E}{\partial t_i} = 2E \left [ 0.5 \mu^\top D^\top D J_\mu - t^\top D^\top D  \right ]
\]
&lt;/div&gt;


&lt;p&gt;Mu and it's jacobian are:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\mu &amp;= K_* S^\top \left( S K S^\top + I\right)^{-1} S y \\
    &amp;= K_* S^\top U^{-1} S y \\
    &amp;= K_* z \\
\frac{\partial \mu}{\partial t_i} 
    &amp;= K'_* z + K_* z' \\
J_\mu &amp;= \operatorname{diag_{3x1}}(\Delta_{3x3}) z + \left( \Delta_{1x3} \right)^\top \operatorname{repmat}(z_3 N/3, 1) + K_* J_z 
\end{align}
\]

where \(J_z\) is the Jacobian of \(z\), and \(z_3\) is the re-arrangement of \(z\) into columns of xyz vectors.  \(\Delta_{3x3}\) is the conversion of \(\Delta\) to 3D by block-diagonalizing three copies of \(\Delta\) and permuting rows and columns so each (x,y,z) is grouped together.  \(\Delta_{1x3}\) repeats \(\Delta\) over three columns and permuting columns.  \(\operatorname{diag_{3x1}}\) is a modified diagonalization operator where x is split into 3x1 matrices, which are arranged into block-diagonal form.
&lt;/div&gt;




&lt;div&gt;
Let \(A = S^\top U^{-1} S \), so \(z = Ay \).
\[
\begin{align}
\frac{\partial z}{\partial t_i} &amp;= \frac{\partial }{\partial t_i} D^\top U^{-1} S y \\
                              &amp;= - S^\top U^{-1} \frac{\partial U}{\partial t_i} U^{-1} S y \\
                              &amp;= - S^\top U^{-1} S \frac{\partial K}{\partial t_i} S^\top U^{-1} S y \\
                              &amp;= - A \frac{\partial K}{\partial t_i} z \\
                              &amp;= - A \left \{
                                      \left (
                                      \begin{array}{c} 
                                          \mathbf{0}             \\
                                          \vdots        \\
                                          \delta_i^\top \\
                                          \vdots        \\
                                          \mathbf{0}             \\
                                      \end{array} \right ) + 
                                      \left (
                                          \begin{array}{c}
                                          \mathbf{0} &amp; \cdots &amp; \delta_i &amp; \cdots &amp; \mathbf{0}
                                          \end{array} 
                                      \right) 
                                    \right \} z \\
         &amp;= - A_i (\delta_i^\top z) - A \delta_i z_i \\
         &amp;= - A_{3i:3i+2} (\delta_{i,3x3}^\top z) - A \delta_{i,3x3} z_{3i:3i+1} &amp; \text{(3D version)} \\
J_z &amp;= - \operatorname{sum_{1x3}}\left(A \odot \left( \Delta_{3x3} z \right)^\top \right) - A \left [ \left( \Delta_{1x3}  \right )^\top \odot \operatorname{repmat}(z_3, N/3, 1)^\top \right ]
\end{align}
\]
&lt;/div&gt;



</description>
				<pubDate>Thu, 12 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/12/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/12/reference</guid>
			</item>
		
			<item>
				<title>Optimizing indices: Length constraints</title>
				<description>&lt;p&gt;Continuing along the inspiration from the yesterday's Pascal Fua paper investigating length constraint terms for index optimization.&lt;/p&gt;

&lt;p&gt;It's difficult, because length implies known structure, which in turn implies known indices.  So constraining indices based on length feels like a circular argument.  This has been a sticking point in my thinking for a long time.  But we can formulate this in terms of expectations, namely, given an index set, the &lt;em&gt;expected segment length&lt;/em&gt; should be equal to difference of their indices.&lt;/p&gt;

&lt;p&gt;Let \( L_i = \|x_i - x_{i-1}\| \) be the distance between points with adjacent indices.&lt;/p&gt;

&lt;p&gt;Expected length given and index set, \(t\) is:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\mathrm{E}[L_i^2] &amp;= \mathrm{E}[\| x_i - x_{i-1}\|^2] \\
                &amp;= \mathrm{E}[x_i^2 - 2 x_i x_{i-1} + x_{i-1}^2 ] \\
                &amp;= \mathrm{E}[x_i^2] - 2 \mathrm{E}[x_i x_{i-1}] + \mathrm{E}[x_{i-1}^2 ] \\
                &amp;= \left( \mathrm{Cov}[x_i]  + \left( \mathrm{E}[x_i] \right) \right) - 2 \left( \mathrm{Cov}[x_i, x_{i-1}] + \mathrm{E}[x_i]\mathrm{E}[x_{i-1}] \right) + \left( \mathrm{Cov}[x_{i-1}]  + \left( \mathrm{E}[x_{i-1}] \right) \right)\\
                &amp;= \left(k(x_i,x_i) + k(x_{i-1}, x_{i-1} - 2 k(x_i, x_{i-1}) \right) + \left( \mu_i^2 + \mu_{i-1}^2 - 2 \mu_i \mu_{i-1} \right)\\
                &amp;= D K_i D^\top + (D \mu_{i,i-1})^\top D \mu_{i,i-1}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Where D is the differencing matrix, \(D = (1, -1)\).&lt;/p&gt;

&lt;p&gt;Observe that each dimension is independent in the squared distance equation, so we can treat each dimension separately in the equation above.&lt;/p&gt;

&lt;p&gt;For now, we'll aproximate by ignoring the first term, since nearby indices should be nearly 100% corellated.  So all that remains is the squared difference of the reconstructed points.&lt;/p&gt;

&lt;p&gt;We'll consider two different energy functions.  The first is exactly what we'd like to minimize, but it needs an approximation.&lt;/p&gt;

&lt;div&gt;
\[

E_i = \left( \mathrm{E}[L_i] - \Delta_{t_i} \right)^2
    \approx \left( \sqrt{\mathrm{E}[L_i^2]} - \Delta_{t_i} \right)^2

\]
&lt;/div&gt;


&lt;p&gt;We can't compute \(\mathrm{E}[L_i] \) directly, so we approximate it by taking the square root of the expected square length.  The square root function is concave, so Jensen's inequality tells us that this approximation will never under-estimate the expected length, and since our main objective is to prevent index shrinkage, overestimating is preferred to underestimating.    The result is \(D \mu \), the adjacent differences of the posterior mean.&lt;/p&gt;

&lt;p&gt;TODO: writeup derivation for gradient for \(E\) and \(\mu\)&lt;/p&gt;

&lt;p&gt;Implemented analytical Jacobian for \(\mu\) in &lt;code&gt;posterior_mu_gradient.m&lt;/code&gt;.  Some error in results, according to &lt;code&gt;test/test_mu_deriv.m&lt;/code&gt;, but passes the inspection test.  Overall, diagonal term looks okay, so error is probably in derivation of dZ.  Particularly damning is that the quality metric isn't sensitive to the delta step size.&lt;/p&gt;

&lt;p&gt;Should probably test Jacobian dZ.  It's undergone some changes today.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Yep, subtle but significant error.  Inspection suggests &lt;code&gt;term2&lt;/code&gt; is the culprit.  Can we focus on the individual faulty elements and check the partial derivatives?&lt;/p&gt;
</description>
				<pubDate>Tue, 10 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/10/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/10/work-log</guid>
			</item>
		
			<item>
				<title>Hyperprior</title>
				<description>&lt;p&gt;Training hyperprior over indices vs. observation ordinal.&lt;/p&gt;

&lt;p&gt;Struggling with modeling offset and rate.  Mean is usually zero, but not in this case.  Don't really care about initial conditions for now (and struggling with learning it anyway), so just learn smoothing sigma and set offset and rate covariance to large.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Reading more on GP Latend Variable Models.&lt;/p&gt;

&lt;p&gt;[1] N. D. Lawrence and A. J. Moore, “Hierarchical Gaussian process latent variable models,” presented at the ICML '07: Proceedings of the 24th international conference on Machine learning, 2007.&lt;/p&gt;

&lt;p&gt;At once similar and different from what we're doing.  In some sense, the index is a latent variable, but we aren't using latent variables as a dimension reduction technique, as is the crux of the GPLVM approach.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Reading up on existing literature dealing with reconstructing inextensible structures.&lt;/p&gt;

&lt;p&gt;[1] M. Salzmann and P. Fua, “Linear Local Models for Monocular Reconstruction of Deformable Surfaces,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 5, pp. 931–944.
[2] M. Salzmann, R. Urtasun, and P. Fua, “Local deformation models for monocular 3D shape recovery,” presented at the Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 2008, pp. 1–8.
[3] M. Perriollat, R. Hartley, and A. Bartoli, “Monocular Template-based Reconstruction of Inextensible Surfaces,” Int J Comput Vision, vol. 95, no. 2, Nov. 2011.
[4] J. Taylor, A. D. Jepson, and K. N. Kutulakos, “Non-rigid structure from locally-rigid motion,” presented at the Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, 2010, pp. 2761–2768.&lt;/p&gt;

&lt;p&gt;All these papers assume we can match keypoints of some patches, which isn't the case in our problem since our curves don't have distinctive texture.  However, I can draw inspiration from [1], in which several interesting energy functions are constructed as homogeneous linear equations and with linear constraints.  In one formulation, the normal posterior energy is supplemented with extra terms that prevent edge-lengths from changing.  In another formulation, the rigidity constraint is replaced with an inequality that prevents extension but disallows contraction, which permits sharp folds.&lt;/p&gt;

&lt;p&gt;There are significant differences in our case.  First, we don't have a reference structure to compare the model against, so we don't know the reference length of each segment.  Second, this assumes known correspondences, which we don't have.  But we can use the idea of adding an energy term based on length.&lt;/p&gt;

&lt;p&gt;Reference [2] uses GPLVM (Gaussian process latent variable model) to model local deformations.  The latent variable formulation allows for nonlinear deformations (e.g. kinks, creases).  On the downside, it's nonconvex, so is best suited for tracking only.&lt;/p&gt;
</description>
				<pubDate>Mon, 09 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/09/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/09/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;running with 5mm in-plane variance.  Dataset 4 and 5's issues seem to be resolved.  Interesting, since we still have isotropoic offset veriance in-place, too.&lt;/p&gt;

&lt;p&gt;Can we get old issue to return by setting in-plane variance to zero?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Yes.  And setting in-plane to nonzero and isotropic to zero?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Yes.  But shifting is very significant, and motion is large and random.&lt;/p&gt;

&lt;h2&gt;The offset mystery&lt;/h2&gt;

&lt;p&gt;There's obviously some part of the model thats resulting in these offsets, its just a matter of zeroing out parameters until the phenomenon goes away.&lt;/p&gt;

&lt;p&gt;Position perturbation variance is the root of the problem; both iso and nonisotropic version fail in this way.&lt;/p&gt;

&lt;p&gt;Some modification I made is causing indices to collapse...  probably insufficient flexibility since I disabled rate and smoothness perturbations.&lt;/p&gt;

&lt;p&gt;weird, rate and position variance seem tied to disappearing curves (index collape?).  Seems that increasing rate variance a lot is causing it.&lt;/p&gt;

&lt;p&gt;Increasing rate variance seems to facilitate perturb_rate_variance having more influcence.  Possibly because we're getting index shrinkage, so small fluctuations in rate result in large fluctuations in structure.&lt;/p&gt;

&lt;h2&gt;Prevending index shrinkage&lt;/h2&gt;

&lt;p&gt;My current hypothesis is that index shrinkage is resulting in the highly non-intuitive results we're seeing.  Intuition assumes index corresponds to distance, but when shrinkage occurs, this is no longer the case, so intuition becomes cloudy or fails entirely.&lt;/p&gt;

&lt;p&gt;We really need to constrain indices somehow, without preventing corrections.&lt;/p&gt;

&lt;p&gt;First attempt: force length to remain constant.  added a squared penalty over total change in index length.  It's tricky because of our view-grouped transformation; only one group contributes to the size of the maximum end-point, and only one index contributes to the minimum start-point.  Implemented, but didn't get desired result.  Instead, indices shifted wildly to comply with the length constraint.  In some cases, overall shrinkage occurred, combined with occasional spikes to keep the length up.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Observation: plot of index vs. observation number is &lt;em&gt;smooth&lt;/em&gt;.  Add a GP prior over index using observation ordinal as index.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Implemented in &lt;code&gt;optimize_ml_wrt_indices_2.m&lt;/code&gt;.  Initial tests seem promising, but need to train on parameters.  Zeroth and first derivitives w.r.t. parameters:&lt;/p&gt;

&lt;div&gt;
\[

g(x) = \log p(x | t) = -0.5 x^\top \left( \sigma_1^2 K_1 + \sigma_2^2 K_2 + \sigma_3^2 K_3 \right)^{-1} x - 0.5 log \left | \sigma_1^2 K_1 + \sigma_2^2 K_2 + \sigma_3^2 K_3 \right | - C \\
                &amp;= -0.5 x^\top U^{-1} x - 0.5 log | U | - C \\

\frac{\partial g(x)}{\partial \sigma_i} = \sigma_i \left(z' K_i z - \text{Tr}\left[U^{-1} K_i\right] \right)
\]
&lt;/div&gt;


&lt;p&gt;where \(U = \sigma_1 K_1 + ... \) and \(z = U&lt;sup&gt;{-1}&lt;/sup&gt; x \) .
TODO:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;get curve indices from reconstruction&lt;/li&gt;
&lt;li&gt;group by view; get corresponding ordinal; add to set&lt;/li&gt;
&lt;li&gt;construct energy function over set of curves using functions above&lt;/li&gt;
&lt;li&gt;initial fit of rate and offset using direct means;&lt;/li&gt;
&lt;li&gt;refine all using N-R&lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Sat, 07 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/07/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/07/work-log</guid>
			</item>
		
			<item>
				<title>Troubleshooting excessive index drift in endpoints; fixing Hessian under variable transformation.</title>
				<description>&lt;p&gt;Getting bizarre spikes in indices during optimization.  Confirmed that removing the spikes will improve ML, but there's a steep well between the two local minima as we reduce the index value:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-ml_vs_index.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The index starts at a fairly reasonable initial value, so my only guess is that the Hessian is suggesting a large step which happens to step over the well.&lt;/p&gt;

&lt;p&gt;I'm wondering if the hessian is screwy; or maybe it's just the transformation we're using.  Optimizing raw indices doesn't exhibit this problem, but it is a problem in our our current approach of working with log differences to prevent re-ordering.&lt;/p&gt;

&lt;p&gt;A prior over index spacing should probably prevent this; I'm hesitatnt to add the extra complexity at this point, considering the additional training and troubleshooting it would entail.&lt;/p&gt;

&lt;p&gt;Should unit test the transformation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Gradient looks okay.&lt;/p&gt;

&lt;p&gt;On-diagonal elements have significant error!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-hessian_error.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Actually, significant off-diagonal error, but on-diagonal dominates.&lt;/p&gt;

&lt;p&gt;Since gradient is fine, and it uses the same jacobian, I'm guessing the problem isn't the jacobian transformation, but the hessian itself.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Confirmed.  diagonal is (mostly) too high, error on order of 1e-3.  Gradient error is around 1e-9.&lt;/p&gt;

&lt;p&gt;Detective work.  Look at diagonal of each Hessian term, compare to residual of diagonal, look for patterns.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Ugh, worst afternoon ever.  Spent hours trying every trick in the book to track down the source of the error, including lots of time looking at the raw Hessian (it wasn't the raw Hessian).  Finally found the bug: I formula I used for the chain rule for Hessians was wrong.  In particular, it was missing a second term that (in my problem) corresponded to adding the transformed gradient to the diagonal.   See &lt;a href=&quot;http://en.wikipedia.org/wiki/Chain_rule#Higher_derivatives_of_multivariable_functions&quot;&gt;Faa di Bruno's formula&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Total error is much reduced now, but not zero.  around 0.1, instead of 20 before, new results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-hessian_error_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The norm-check is around 1e-4; very nice.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Re-running dataset 11 with hopes that we don't lose the global optimum.  Interesting observation: optimization requires more iterations than before to converge.  It seems a more conservative hessian results in smaller steps and is less likely   Looks better:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-reconst_hess_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice we're still getting offset, but at least the reconstruction is qualitatively better that before. However, now we're getting a small loop at the top:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-weird_loop.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It seems to be caused by changing of index order between views. Needs some thought to best address.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Re-running on all datasets.  Hopefully excessive index drift won't be too big an issue.  Possibly an extra term to prevent drift far from initial points would be sensible.&lt;/p&gt;

&lt;p&gt;Datasets 4,5  still drifts:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-drift_ds4.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/ksimek/research/img/2013-12-05-drift_ds5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Datasets 7,9  have detached curves&lt;/p&gt;

&lt;p&gt;Dataset 10, curve 2 (?) appears to have failed to prune&lt;/p&gt;

&lt;h2&gt;endpoint drift&lt;/h2&gt;

&lt;p&gt;It looks like interior points are confined by their neighbor points from drifting too far, but end points have no such constraint.  After a small amount of drift, they're able to loop back on themselves and land directly on the backprojection line.  It's surprising that the extra flexibility afforded by large spacing between indices doesn't cause marginal likelihood to suffer, since most of the new configurations are bad ones.&lt;/p&gt;

&lt;p&gt;Considerations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;in-plone offset perturbation&lt;/li&gt;
&lt;li&gt;penalize excessive drift&lt;/li&gt;
&lt;li&gt;penalize excessive gaps (a-la latent GP model).&lt;/li&gt;
&lt;li&gt;penalize shrinkage.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Constraining offset perturbation&lt;/h2&gt;

&lt;p&gt;Constructing the offset perturbation variance so perturbations can only occur parallel to the image plane.  Code is pretty straightforward:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cam_variance = blkdiag(repmat(sigma,N,N), repmat(sigma,N,N), zero(N,N));
camvariance(I,I) = cam_variance;
world_variance = R' * cam_variance * R
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where R is the rotation matrix from world to camera coordinates (i.e. from the extrinsic matrix).&lt;/p&gt;

&lt;p&gt;The main difficulty here is logistical: throughout the codebase we store the prior variance in 1D format, and assume it will be expanded to 3D isotropically.  Now we have a non-isotropic part of the prior, and we need to make sure it's included wherever the prior is needed.&lt;/p&gt;

&lt;p&gt;Time for a real refactor.  Need a 'get_K' function, instead of just taking the prior_K, adding the branch variance, and then adding the nonisotropic offset variance each time.  Throw error if prior_K or branch_variance isn't ready.  refactor strategy: search files for prior_K; those that call one_d_to_three_d shortly thereafter are refactor candidates, others will need further recursion to find the regactor point.  But it all starts with prior_K.  There shouldn't be that many leaf-nodes that use prior_K -- ML, gradient, reconstruction, WACV-spectific stuff...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Saturday&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Refactoring to use nonisotropic perturb position covariance.&lt;/p&gt;

&lt;p&gt;Refactoring construct_attachment_covariance will be difficult, because it works in 1D matrix format, but the new covariance must operate in 3D format.  Will involve refactoring everything to 3D, re-index using block indexing, and double-check that noting relies on an independence assumption.&lt;/p&gt;

&lt;p&gt;Better idea: apply perturb covariance after the fact.  All connected curves will get the same covariance offset, so none of the special logic in &lt;code&gt;construct_attachment_covariance&lt;/code&gt; is needed.&lt;/p&gt;

&lt;p&gt;IDEA: use image position as an alternative index, and define a within-plane scaling covariance over it to account for poor calibration.  Don't need to know any tree structure; shouldn't distort reconstruction positions since it's in-plane.&lt;/p&gt;

&lt;p&gt;in-plane covariance adds to branch_index &lt;em&gt;only&lt;/em&gt;.  Thus, the only change needs to be in att_set_branch_index and/or detach.  ...  But what if we want index-specific in-plane covariance (e.g. scaling).  Best to drop it into get_K, which will trickle into att_set_branch_index&lt;/p&gt;

&lt;p&gt;Question: should each curve be allowed to shift in the image, or should the entire image be shifted as one?  The former gives more flexibility, but possibly undeserved.  One concern is that attaching two curves will eliminate that freedom, unfairly penalizing attachment.  On the other hand, of all curves tend to shift in the same direction, the ML will increase after attaching them, promoting attachment.  Will use per-curve shift for now.&lt;/p&gt;

&lt;p&gt;Question: should shift be correlated between views?  The theory is shift arises due to camera miscalibration.&lt;/p&gt;

&lt;p&gt;Issue: sometimes the covariance matrix is gotten in other ways, e.g. when computing branch index, the parent's covariance is computed in-line.&lt;/p&gt;

&lt;p&gt;TODO: recurse on
* &lt;code&gt;construct_attachment_covariance&lt;/code&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 05 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/05/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/05/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Implemented re-ordering of indices.  Some are improved, some are worse.&lt;/p&gt;

&lt;p&gt;Found bug in re-ordering -- the gradient and Hessian's transformation Jacobian wasn't updated properly.  After a few false starts, got it right.  Results seem much better.  Still getting signficant positive index offset for three curves, all others start at zero.  Post-procssing to force them to zero seems to fix; more investigation is warranted in the future.&lt;/p&gt;

&lt;p&gt;Offset may be due to insufficient perturb variance?  Positive index offset only increases the variance of the first point; the conditional variance of future points is roughly the same for any index offset, due to conditional independence.  Possibly the perturb position variance is too low?  Carefull setting it too high, we'll get drift toward the camera.  Quick test: increase perturb position variance from 5&lt;sup&gt;2&lt;/sup&gt; to 20&lt;sup&gt;2.&lt;/sup&gt;  The offset phenomenon is exagerated.  When position perturbations are left in, the attached model moves all over the place, over 100 mm at times. Well beyond the perturb variance.&lt;/p&gt;

&lt;p&gt;Remember that increasing the perturb variance increases the marginal within-view variance, too.&lt;/p&gt;

&lt;h2&gt;Reintroducing connections.  All bad behaviors in dataset 8 appear to be fixed!&lt;/h2&gt;

&lt;p&gt;Running on dataset 9.  Crashed.  missing views of some curves.  Going into ground truth tool to fix...&lt;/p&gt;

&lt;p&gt;Still crashing.  bug in process_gt2 -- new minimum points code. fixed.&lt;/p&gt;

&lt;p&gt;Running okay, not much movement even though dataset has significant movement.&lt;/p&gt;

&lt;p&gt;We were removing perturbations coming from the rate kernel.  new getting more movement, still not the right kind.&lt;/p&gt;

&lt;p&gt;plotting triangulated data over reconstructed data.  Significant offset, no explanation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-04-ds9_reconst_and_triangulation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Intrestingly, one curve wasn't properly connected to the parent; that curve shows no offset.  Re-running with no connections to test this observation.  Offset seems gone, ecept for the middle part of the root curve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-04-ds9_reconst_and_riangulation_no_connection.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Observation: main curve shifts a lot between views; other curves have almost no shift. Possible index shrinkage problem?&lt;/p&gt;

&lt;p&gt;measure index shrinkage: index vs. distance&lt;/p&gt;

&lt;p&gt;If position variance is too low, we might be getting a pull toward the origin.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Off-center may be caused by pulling toward camera in per-view perturbation reconstructions.  The overall reconstruction is basically the mean of the per-view reconstructions, so if the camera-pull isn't symmetric between views, this can lead to significant shift in the mean.&lt;/p&gt;

&lt;p&gt;Indeed, lowering the perturb position variance reduces reconstruction offset.  Recall that camera-pulling occurs because likelihood cylinders converge as they approach the camera.  See the &lt;a href=&quot;/ksimek/research/2013/08/06/work-log/&quot;&gt;original discussion of this phenomenon back in August&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Non-isotropic perturbation offset variance&lt;/h2&gt;

&lt;p&gt;Idea: can we limit offset perturbation to only occur in the directions of the imaging plane?  It seem possible -- just have constant offset in xy, zero offset in z, and rotate to match camera.  Can even add this to S, so it's part of that noise model.  This should take care of camera-pulling, at least to the extent that it's caused by the offset model.  Can even scale offset variance based on the distance from the camera, effectively using pixel units instead of world units.&lt;/p&gt;

&lt;h2&gt;closing&lt;/h2&gt;

&lt;p&gt;Running current version on all datasets.&lt;/p&gt;

&lt;p&gt;Program crashes if curves are missing.  Regression in code that constructs Track assoc -- emtpy curves weren't being removed. Fixed.&lt;/p&gt;

&lt;p&gt;Will inspect results in the morning.&lt;/p&gt;
</description>
				<pubDate>Wed, 04 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/04/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/04/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Fixed change-of-variable transformation for Hessian.  OPtimization now finishes with fewer iterations and with higher likelihood solution.&lt;/p&gt;

&lt;p&gt;Still getting weird overshoot curves:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-new_recons_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It seems that our optimization occasionally causes large DC offset compared to the unoptimized indices.  See below, blue is original, green is optimized.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-updated_indices_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the other hand, some indices are much improved.  Here's our always problematic curve #10, see how the large gap after the first two indices is removed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-updated_indices_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The DC offset issue raises two questions: (1) why isn't offset of zero preferred, and (2) why does DC offset result in overshooting?&lt;/p&gt;

&lt;p&gt;I think I know the answer to (2): we always start reconstructions at index zero.  So if the evidence starts at index 10, the reconstrution will always draw the segment between index 0 and 10, even if there's no evidence.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Cached prior K is used in gradient computation; cache isn't updated when indices change.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;On second look, I was wrong, cached K is updated in a call to &lt;code&gt;att_set_start_index_2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I'll probably need to update children branch indices when parent indices change.  Or only do index optimization in isolation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Why is DC offset preferred?  It's not an inference problem; confirmed that zero-offset indeed has a lower ML (not by much though).&lt;/p&gt;

&lt;p&gt;It's clear that the curve is beneficting from extra freedom gained by starting at a higher index.  The question is which parameter is encouraging this (smoothing, offset, rate)?&lt;/p&gt;

&lt;p&gt;Lowering position variance below the optimal point reverses the trend, surprisingly.  Increasing perturb position variance helps, but doesn't reverse.&lt;/p&gt;

&lt;p&gt;Increasing rate variance past the optimal point reverses the trend, but the effect is not dramatic.&lt;/p&gt;

&lt;p&gt;Increasing smoothing variance far past optimal shows a dramatic trend reversal.&lt;/p&gt;

&lt;p&gt;Increasing rate variance makes position variance behave naturally -- zero offset is better for all values of position variance, with peak difference near the optimal.  Correction: at very low position variance ( &amp;lt;= 5) the positive-offset model looks better, as we'd expect, since offset makes the model more forgiving.&lt;/p&gt;

&lt;h2&gt;Correction: increasing position variance is required for some curves.&lt;/h2&gt;

&lt;p&gt;Let's re-run with larger rate variance.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;After increasing rate variance, indicies grow without bound!  Eventually they get into the range of 4e6.  Its our old friend, curve #10.  This is very surprising, because the marginal likelihood should strongly discourage such large indices, since the number of poor configufations grows with index magnitude.&lt;/p&gt;

&lt;p&gt;Could add a regulating factor, but it really seems like this shouldn't be necessary.&lt;/p&gt;

&lt;p&gt;Fixed by simply returning -inf on failure.  It seems this phenomenon is simply the result of an aggressive step in the wrong direction.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I'm starting to realize the allowing the indices to move freely can result in all kinds of subtle issues.  For example, nothing prevents the spacing between indices from shrinking simultaneously, because the rate kernel is still happy (in fact, happier), the curve just appears to be moving more quickly.  If the smoothing variance was set too high, this &quot;quickening&quot; makes the marginal likelihood happier, because points have less freedom to move in space -- the prior's slack is pulled out.&lt;/p&gt;

&lt;p&gt;Something similar might be happening to cause indices to drift upward simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Whatever the dynamics, index optimization might be a good idea to get a decent first-pass MAP reconstruction, but indices should be re-mapped using chord-length parameterization when computing ML's and final reconstructions.&lt;/p&gt;

&lt;p&gt;For now, just subtract the excess, so first index is zero.&lt;/p&gt;

&lt;p&gt;Reconstructions look better, but curve 7 still overshoots at the beginning:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-new_recons_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Possibly relaxing ordering constraints between views will fix this.&lt;/p&gt;

&lt;p&gt;Relaxing smoothness variance a small amount makes it worse.  Arg...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;OMG, plotting the 2D curve over the images shows significant offset!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-orrset_error_lo_res.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hi resolution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-offset_error_hi_res.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice curve #7 (dark green) is waaaay off on it's first point.  What's going on here?  I've re-traced that curve twice, is there some offset being added in opengl?  Why haven't we seen the evidence of this before?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;check other views for offset error&lt;/li&gt;
&lt;li&gt;check other datasets for offset error&lt;/li&gt;
&lt;li&gt;quantify offset error (1 pixel?)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Looks like curves are shifted left by 0.5 pixels and down by 1.5 pixels.  Part of the culprit is the flip_y function, which erroneously added 1 after flipping (presumably to account for the pixel grid).  This brings it to within 0.5 in both directions.  This is due to an ill-advised line I had added in the ground-truth tool, which translated everything by 0.5 pixels before rendering.  I presumably wanted (0,0) to render in the middle of the bottom-left pixel, but in retrospect, this is stupid.&lt;/p&gt;

&lt;p&gt;Added option 'halfPixelCorrection' for process_gt_2.  Left half-pixel offset in ground-truth tool for now; with intent to fix all existing ground truth files and then remove the offending line of code.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Starting to get some real stairstep patterns in the index spacing.  Almost certainly this means points from different views want to change order.  Do this next&lt;/p&gt;
</description>
				<pubDate>Tue, 03 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/03/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/03/work-log</guid>
			</item>
		
			<item>
				<title>Index optimization, end-to-end</title>
				<description>&lt;p&gt;Troubleshooting ML optimization.&lt;/p&gt;

&lt;p&gt;Found a few bugs in the optimization code.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;math bug in variable transformation formula.  Applying chain rule to account for transformation used wrong formula.&lt;/li&gt;
&lt;li&gt;math bug in new version of gradient function.  Used Uc where Uc' was called for (transpose of cholesky)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;As a result of bug #2, decided to re-implement end-to-end test (which hadn't been run for some time; was running a unit test on synthetic data).  Refactored test to use the functional style in &lt;code&gt;test/test_ml_deriv.m&lt;/code&gt;, added hessian computation.&lt;/p&gt;

&lt;p&gt;Fixed gradient looks good.  Hessian is way off.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;found some bugs in hessian&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;forgot to include terms from linear covariance function.&lt;/li&gt;
&lt;li&gt;forgot to scale second derivitives by variance parameters.&lt;/li&gt;
&lt;li&gt;used H1 + H2 instead of H1 - H2&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;All of these weren't caught in the unit test, because everything was tested in isolation, with scaling constants omitted.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Looks&lt;/em&gt; a lot better now, still some error on the order of 1e-2.  Most troubling isn't the magnitude, but the fact that the error seems to be structured, as opposed to random:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-02-hessian_error.png&quot; alt=&quot;hessian error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We're comparing against numerical approximation, so the error might be in the approximation, not in the math.  For now we'll proceed, but there's probably room for further investigation in the future.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Ran index optimization, and results for curve #10 are much improved.  Curve #7 still overshoots, but beyond that, no significant nastiness.&lt;/p&gt;

&lt;p&gt;Initially, we were getting errors from the gradient checker.  Swtiching the central differences fixed it.&lt;/p&gt;

&lt;p&gt;Interestingly, the optimization algorithm takes 10X to 30x more iterations to complete.  (update: Hessian isn't being transformed correctly)&lt;/p&gt;

&lt;p&gt;Recall that we increased the position-perturb variance significantly.  This seems to improve ground-truth reconstructions; setting it too small causes bizarre curves and over-extensions.  Below, we see the old method, followed by the new method with large position perturbation variance, and the new method with small perturbation variance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-02-old_recons.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above: old method&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-02-new_recons_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above: new method, large position perturbation variance&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-02-new_recons_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above: new method, small position perturbation variance&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tried fixing hessian, results are worse.  Probably got the transformation math wrong.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;per-view ordering constraints&lt;/li&gt;
&lt;li&gt;fix hessian transformation&lt;/li&gt;
&lt;li&gt;investigate curve #7

&lt;ul&gt;
&lt;li&gt;possibly larger position perturbation variance will help?&lt;/li&gt;
&lt;li&gt;maybe fixing hessian will help?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 02 Dec 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/12/02/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/12/02/work-log</guid>
			</item>
		
	</channel>
</rss>
