<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;Rewrote training procedure from scratch and now getting much more sensible results.  The overall goal is to isolate parameters to try to make them as independent as possible; and then use one or two-dimensional optimization to estimate them.&lt;/p&gt;

&lt;p&gt;First, I hard-coded several variables.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rate variance is set to its theoretical value of 0.333 &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log/&quot;&gt;as described here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;perturb_rate_variance is now zero -- it seems to be causing weird results probably due to overfitting.  Results are very sensitive to small changes of rate perturbatyion.  Could re-introduce later.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Second, I estimated perturb_position_variance directly, by measuring the variance of all groups of initial points.&lt;/p&gt;

&lt;p&gt;Third, estimated noise and smoothness variance under the assumption of near-infinite position and rate variance.  This follows from the theory of cubic-spline models, which places no penalty over initial position and rate.  Used gradient-free minimizer, &lt;code&gt;fminsearch&lt;/code&gt;, which works well for two-dimensional problems.&lt;/p&gt;

&lt;p&gt;Fourth, estimated perturb smoothing and perturb scale, using noise and smoothness variance.&lt;/p&gt;

&lt;p&gt;Fifth, estimated position mean and variance directly.&lt;/p&gt;

&lt;p&gt;Note: all this is done using heuristic indexing, not the marginal likelihood optimization.  This is another key change from earlier training.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;: Much more sensible perturb smoothing parameter (~1e-5 vs 1e-4) and perturb scale (2.06 vs 0.9).  End-to-end reconstruction is much more sensible; curve is centered among triangulated points.&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;train on run dataset with significant motion (wacv dataset #10; i.e. arab_6)&lt;/li&gt;
&lt;li&gt;iteratively update noise/smoothness and perturb smoothness&lt;/li&gt;
&lt;li&gt;iteratively update indexing after setting params&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;PERTURB_POSITION_VARIANCE MAY BE THE OVERFITTER!&lt;/p&gt;
</description>
				<pubDate>Tue, 08 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/08/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/08/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;I've been experimenting with alternative methods for inferring the index set and have several good options.  When equipped with a good index set, it seems most of the problems we've been seeing disappear.  As sad as it seems, its seeming like index optimization should be tossed out, despite all the time I spent deriving the equations.&lt;/p&gt;

&lt;p&gt;I've observed that too high of a perturb smoothing variance causes the mean curve to drift toward the linear-initial model.  Manually lowering perturb smoothing variance and raising smoothing variance causes better reconstruction and much higher marginal likelihood.&lt;/p&gt;

&lt;p&gt;Centering matters -- too low of a position_variance causes rate variance to take over, which causes weird results in some cases.&lt;/p&gt;

&lt;p&gt;Should refact tr_train to use new index estimation scheme; goal: getting better higher smoothness variance and lower perturb_smoothness variance&lt;/p&gt;

&lt;p&gt;Open question: is camera linearization beneficial in training?&lt;/p&gt;

&lt;p&gt;Refactored proces_tracks to recieve index estimation method.&lt;/p&gt;

&lt;p&gt;Refactored run_wacv_5 to use iterative index estimation.&lt;/p&gt;
</description>
				<pubDate>Mon, 07 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/07/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/07/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;Question: is curve stretching only present in last point?  If so, can we hack it away?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;detect index stretching (threshold on diff)&lt;/li&gt;
&lt;li&gt;for each, visualize with test_optim&lt;/li&gt;
&lt;li&gt;fun on full dataset, count (1) how many stretching occur and (2) how many are the last point?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Example&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[data, parents] = load_wacv_data(5, [397 530]', params, 1:4:36); %'
[mu, Tracks, params] = get_wacv_result(5, params.model_type, 'deleteme_new4');
out = test_for_wandering_index(Tracks,20);
[wander_is_last, wander_view_num] = exp_2014_04_04_test_wandering_index(Tracks, data, params, 20)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Answer: Usually, wandering is present in last point, but not always.  Curve 1 in the example has 15 internal wandering events.  All other curves are last point.&lt;/p&gt;

&lt;p&gt;Question: Can we correct the bad indices in curve 1 by using a numerical hessian?&lt;/p&gt;

&lt;p&gt;Question: could we avoid internal wandering by preventing endpoint wandering?  e.g. is internal wandering occurring because endpoint wandering is so extreme?&lt;/p&gt;

&lt;p&gt;Question: does stretched point have higher likelihood than hand-picked index?  If so, what adjustments to params reverses this case?&lt;/p&gt;
</description>
				<pubDate>Fri, 04 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/04/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/04/work-log</guid>
			</item>
		
			<item>
				<title>Crash recovery; Index estimation - the saga continues</title>
				<description>&lt;p&gt;Machine crashed; need to get matlab environment back to where it was yesterday&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;load save tracks files&lt;/li&gt;
&lt;li&gt;re-run training (compare against any recorded values)&lt;/li&gt;
&lt;li&gt;re-run wacv with hessian disabled&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Done.  Process saved in &lt;code&gt;setups/setup_workspace_2014_04.m&lt;/code&gt; for future reference.&lt;/p&gt;

&lt;h2&gt;Index estimation&lt;/h2&gt;

&lt;p&gt;Using &lt;code&gt;wacv-2012/test_optim.m&lt;/code&gt; to visualize the correspondence between triangulated points and reconstructed curve.  This is a rough proxy for visualizing curve indices -- bad indices result in a bad reconstruction, and the matching will look bizarre.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;After much debugging, found that test_optim.m had a regressed, and was displaying a bad reconstruction&lt;/p&gt;

&lt;p&gt;I learned an excellent approach for visually assessing indices: run &lt;code&gt;test_optim&lt;/code&gt; with centered_mode=true and split_views=true.  The resulting plot will connect triangulated points to posterior points, WITH an arrow indicating the backprojection direction.  For each curve, rotate the plot so the backprojection arrow disappears (looking in the backprojection direction) -- the connecting line for that point should be nearly perpendicular to the reconstructed curve if the reconstruction is good.&lt;/p&gt;

&lt;p&gt;A second key observation: although minimizing the marginal likelihood does result in index shrinkage, the ratio of distances between adjacent points is equal to the ratio of differences between adjacent indices.  Thus, the scale component&lt;/p&gt;

&lt;p&gt;Third:  When bad index estimates cause terrible recponstructions, the per-view reconstructions actually look not bad (i.e. not marginalizing over offset perturbations).  Position of per-view reconstructed curves is suspect.&lt;/p&gt;
</description>
				<pubDate>Thu, 03 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/03/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/03/work-log</guid>
			</item>
		
			<item>
				<title>TULIPS: index issue</title>
				<description>&lt;p&gt;Tested gradient and hessian -- is it correct with new model?  ... Yes.&lt;/p&gt;

&lt;p&gt;Is index optimization working?  Quick test:  ran this code twice, once using analytical hessian, once with hessian-free algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[mu_, Tracks_] = get_wacv_result(5, params_trained.model_type, 'deleteme_new2');
out = optimize_ml_wrt_indices(Tracks_(14), [], data, params_trained);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Analytical method halts early -- can't improve.  Final gradient is noticibly non-zero&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-04-02-grad_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hessian-free method seems very effective:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-04-02-grad_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Perhaps the optimization is failing; let's run end-to-end with hessian-free algorithm.  It will be super slow, but if it gives better results, we may have our solution.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[mu_, Tracks_] = get_wacv_result(5, params_trained.model_type, 'deleteme_new3');
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Wed, 02 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/02/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/02/work-log</guid>
			</item>
		
			<item>
				<title>FIRE: Verifying date bug, export-to-csv</title>
				<description>&lt;p&gt;Continuing the process of resolving the &quot;rotated dates&quot; issue, in which the last gold-standard date was shifted to the beginning of each visit-set.&lt;/p&gt;

&lt;p&gt;Laura has corrected the issue on her end, but we've already made several changes to our copies of the databases, so I wrote a script to make the correction
on our end.  But there was some confusion about whether the original or updated &quot;Visit&quot; values are correct.  Laura has sent me her databases so I can
confirm that we're working with the same data, and hopefully I can determine if our visit values are correct and consistent with hers.&lt;/p&gt;

&lt;p&gt;Data is in MS access, so will need to install it on my virtual machine.&lt;/p&gt;

&lt;h2&gt;Export to csv&lt;/h2&gt;

&lt;p&gt;Finished merging records, confirming, and exporting to CSV.  Formalized the process into two files under &lt;code&gt;fire/src/matlab/scripts&lt;/code&gt;, which perform end-to-end converstion.&lt;/p&gt;
</description>
				<pubDate>Tue, 01 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/01/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/01/work-log</guid>
			</item>
		
			<item>
				<title>Fire: mergin immunity data</title>
				<description>&lt;p&gt;Having written code for guessing visit-number from dates in the Immunity dataset, I ran it and found several records with significant error.  Spent a long time inspecting each one in detail, and found lots of data-entry errors (wrong year, wrong month, transposed digits, miskeyed subject id's, etc).  In one case, the visit number was off-by-one in the self-report data, which was only apparent when the immunity data was introduced and an extra &quot;halways-between&quot; visit was present.   Was able to correct about 30 of these errors confidently; in the end only about 10 out of 700 records remained error greater than 10 days.&lt;/p&gt;

&lt;p&gt;Wrote code to merge new datasets into the full database; used it to merge newly-cleaned immunity data into the full database.&lt;/p&gt;

&lt;p&gt;Next step: write export-to-CSV routine&lt;/p&gt;
</description>
				<pubDate>Mon, 31 Mar 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/03/31/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/03/31/work-log</guid>
			</item>
		
			<item>
				<title>Debugging Index optimization</title>
				<description>&lt;p&gt;Still getting terrible stretching due to extreme index drift during optimization.&lt;/p&gt;

&lt;h3&gt;Experiment 1&lt;/h3&gt;

&lt;p&gt;Theory: camera linearization is messing with our 3D point localization.  Propose solving by increasing noise covariance for noisy.&lt;/p&gt;

&lt;p&gt;Assuming an effective depth uncertainty of (h), and in-plane uncertainty of (w), the in-plane uncertainty of the adjusted covariance matrix (w') is given by&lt;/p&gt;

&lt;p&gt;[
w' = w \mathrm{cos}&lt;sup&gt;2&lt;/sup&gt; \theta + h \mathrm{sin}&lt;sup&gt;2&lt;/sup&gt; \theta
]&lt;/p&gt;

&lt;p&gt;This is illustrated roughly below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-27-covariance_adjustment.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Implemented this using hand-set depth uncertainty (h = 3)mm.  Didn't resolve the issue for extremely stretched curves, but the weak stretching seems improved.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Discussion&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Noise variance is CRAZY low.  Small deviation from true curve is causing huge contortions to resolve.  Why is it trained this low if contradictions result?  This smells like overfitting, except we're testing and training on the same date at the moment so that seems to be ruled out.&lt;/p&gt;

&lt;p&gt;Loosen and retrain?&lt;/p&gt;

&lt;p&gt;We apparently need to multiply standard deviation by 4 to recover; according to &lt;a href=&quot;/ksimek/research/2014/03/25/work-log/&quot;&gt;yesterday's experiment&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Experiment #2&lt;/h3&gt;

&lt;p&gt;Plot likelihood (reference implementation) against noise variance.&lt;/p&gt;

&lt;p&gt;Apparently it does want a higher noise variance, but only 2.5x more, not 4x. is this enough to improve reconstruction?&lt;/p&gt;

&lt;p&gt;Nope.&lt;/p&gt;

&lt;h3&gt;Experiment #3&lt;/h3&gt;

&lt;p&gt;Repeat yesterday's &quot;Overextension&quot; experiment #1.  Multiplying variance by 16x seemed to fix issues, but now I suspect that we just got lucky.  Try a few values near 16x.&lt;/p&gt;

&lt;p&gt;With linearization enabled:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;15x terrible&lt;/li&gt;
&lt;li&gt;15.5x resolved&lt;/li&gt;
&lt;li&gt;16x resolved&lt;/li&gt;
&lt;li&gt;16.5x terrible&lt;/li&gt;
&lt;li&gt;50x terrible&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Without linearization:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;15x resolved&lt;/li&gt;
&lt;li&gt;15.5x resolved&lt;/li&gt;
&lt;li&gt;16x terrible&lt;/li&gt;
&lt;li&gt;16.5x  resolved&lt;/li&gt;
&lt;li&gt;50x resolved&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Yes, tiny tweaks to variance cause us to revert to bad reconstructions.  We probably just got lucky when we observed 16x working well yesterday.&lt;/p&gt;

&lt;p&gt;Now we're back to square one.  noise variance doesn't seem to be connected to this issue of extended&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Discussion&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It seems linearization is a bane to index refinement.  We introduced it to address reconstruction issues; perhaps we should only apply it in that case.&lt;/p&gt;

&lt;h1&gt;FIRE&lt;/h1&gt;

&lt;p&gt;Re-merging and rechecking new datasets&lt;/p&gt;

&lt;p&gt;Only one error, fixed and sent changes to Rebecca.&lt;/p&gt;

&lt;p&gt;Regularizing immunity CSV; adding new column type: &quot;numeric_missing:XXX&quot;, where missing data is represented by one of a few possible strings.  Split the &quot;date(other date)&quot; column into &quot;date&quot; column and &quot;other date&quot; column.  Parsed in matlab.  comparing immunity dates to self-report dates for each subject; interpolating as needed.   Only 9 notable anomalies out of 710.&lt;/p&gt;

&lt;p&gt;Next: discretize, measure and visualize discretization errors, export to csv&lt;/p&gt;
</description>
				<pubDate>Thu, 27 Mar 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/03/27/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/03/27/work-log</guid>
			</item>
		
			<item>
				<title>Debugging reconstruction anomalies</title>
				<description>&lt;p&gt;Continuing from yesterday.  Issue: strange reconstruction results after retraining.&lt;/p&gt;

&lt;h2&gt;Issue: Reconstruction anomalies&lt;/h2&gt;

&lt;h3&gt;Experiment #1&lt;/h3&gt;

&lt;p&gt;Add a small amount to the diagonal of the covariance and re-run reconstruction.&lt;/p&gt;

&lt;p&gt;Result:  Adding a moderate amount of covariance to the diagonal seems to fix the results but probably due to additional smoothing.&lt;/p&gt;

&lt;p&gt;Adding 1.5 - makes results worse, bizarre jutting sections. This counterintuitive if the phenomenon is due to increasing noise variance.
Adding 2 - makes reconstruction look good.
Adding 10000 - reconstruction becomes straight sticks.&lt;/p&gt;

&lt;p&gt;Discussion: analytically this is equivalent to multiplying the noise variance by a constant.  We already know that increasing noise variance solves the issue.  The question remains: why did our training algorithm prefer this? Also, why does decreasing noise variance force the reconstruction &lt;em&gt;away&lt;/em&gt; from the data?  Strong contradiction between data and&lt;/p&gt;

&lt;h3&gt;Experiment #2&lt;/h3&gt;

&lt;p&gt;Add a small amount to the diagonal of the prior covariance matrix&lt;/p&gt;

&lt;p&gt;Result: no obvious improvement (inconclusive)&lt;/p&gt;

&lt;h3&gt;Experiment #4&lt;/h3&gt;

&lt;p&gt;Disable attachment&lt;/p&gt;

&lt;p&gt;Result:  &lt;em&gt;significant improvement&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Discussion:  This was totally unexpected.  Almost all issues seem to be caused by bad attachment.  The affect of bad attachment is probably exacerbated by small noise variance -- the hardness of the constraints lead to contradictions that are resolved by anomalous reconstructions.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Bad attachment guess, combined with nonrobust choice of nosie variance causes contradiction in posterior and strange reconstructions.  Need to develop a better approach to infer attachment.  Issue there is partly that reconstructed individual curves have tails that extend past the attachment point, likely due to bad index estimatization.  If we fix index estimation, we'll be closer to a good branch estimation procedure.&lt;/p&gt;

&lt;h2&gt;Misstep - index optimization &quot;bug&quot;&lt;/h2&gt;

&lt;p&gt;Noticed camera linearization was occuring &lt;em&gt;after&lt;/em&gt; index optimization, which is likely causing the looping.&lt;/p&gt;

&lt;p&gt;Added optional camera linearization to &lt;code&gt;process_tracks.m&lt;/code&gt;.  Issues seemed to get worse, but in fact, linearization wasn't running -- forget to set flag to 'true'.&lt;/p&gt;

&lt;h2&gt;Overextension&lt;/h2&gt;

&lt;h3&gt;Experiment #1&lt;/h3&gt;

&lt;p&gt;Hypothesis: overrestrictive noise variance results in contradictions requiring contortion to resolve.&lt;/p&gt;

&lt;p&gt;Approach: relax noise&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2x - much worse&lt;/li&gt;
&lt;li&gt;4x - different, still bad&lt;/li&gt;
&lt;li&gt;8x - still bad&lt;/li&gt;
&lt;li&gt;16x - mostly resolved; weak tails&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Is linearization causing it?  is model 5 causing it?  Is linearization trsutworthy?&lt;/p&gt;

&lt;p&gt;Linearization seems to make it a little worse.&lt;/p&gt;

&lt;p&gt;Should we take linearization into account when optimizing depth?&lt;/p&gt;

&lt;p&gt;scenario: curve points toward camera, high uncertainty as to index, because BP line is in direction of curve.  After linearization, BP lines change; no longer in direction of curve, index optimization can't fix bad 3d position.&lt;/p&gt;

&lt;h2&gt;Miscellaneous thoughts&lt;/h2&gt;

&lt;p&gt;It kind of makes sense that iid noise would be zero, since the data we're drawing from is so smooth to begin with, and basically noiseless. Any errors arise from mistracing and are strongly correlated due to the smoothness of the Bezier curves.&lt;/p&gt;

&lt;p&gt;If the issue is near-singular matrices, the traditional solution to this is adding a tiny amount of covariance to the diagonal of the prior matrix.&lt;/p&gt;

&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;p&gt;Index optimization resulting in weird loops at endpoints&lt;/p&gt;

&lt;p&gt;Camera linearization reuslts in extensive stretching and looping during index optimization&lt;/p&gt;

&lt;p&gt;T&lt;/p&gt;
</description>
				<pubDate>Tue, 25 Mar 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/03/25/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/03/25/work-log</guid>
			</item>
		
			<item>
				<title>TULIPS - Debugging training</title>
				<description>&lt;p&gt;Issue: after training, still getting a very large perturb_smoothing_variance:  1e-4 instead of more reasonable 1e-6.&lt;/p&gt;

&lt;p&gt;Need to update tracks to reflect new noise parameter.&lt;/p&gt;

&lt;h2&gt;Cleanup: building data pipeline&lt;/h2&gt;

&lt;p&gt;Most of the processing pipeline is currently implemented in &lt;code&gt;wacv_2014/run_wacv_4.m&lt;/code&gt;.  Moving the track-processing part into &lt;code&gt;process_tracks.m&lt;/code&gt;.  Using the '7-stage' pipeline [as documented here].&lt;/p&gt;

&lt;h2&gt;Retraining&lt;/h2&gt;

&lt;p&gt;updated tracks, retrained.  New trained parameters look reasonable, but the reconstruction is terrible:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-24-retrain_reconstruction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Maybe index optimization is failing, because the model #5 gradient has a bug?  Or hitting the iteration limit?  Running reconstruction with model #3 (whose gradient is proven).&lt;/p&gt;

&lt;p&gt;Nope, it seems to be the new parameters:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-24-retrain_reconstruction_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It's weird, because the noise variance dropped signficantly, but we're seeing greater drift away from the data.&lt;/p&gt;

&lt;p&gt;Lets try relaxing the iteration limit anyway...&lt;/p&gt;

&lt;p&gt;More iterations makes it more crazy.&lt;/p&gt;

&lt;p&gt;Sanity check time.  Roll back to old parameters and reconstruct.&lt;/p&gt;

&lt;p&gt;Check.  Old parameters give a sensible reconstruction.&lt;/p&gt;

&lt;p&gt;Differences: (1) much lower noise variance, and (2) much lower perturb smoothing variance.&lt;/p&gt;

&lt;p&gt;The noise variance seems to be the issue here -- raising it to pre-training levels returns us to sensible reconstructions.&lt;/p&gt;

&lt;p&gt;Running out of steam -- time to take a break.  Next steps -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;manual test noise variance; hypothesis: large variance gives higher likelihood.&lt;/li&gt;
&lt;li&gt;ensure we aren't suffering from near-singular matrix during reconstruction. (piece-wise reconstruction?)&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 24 Mar 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/03/24/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/03/24/work-log</guid>
			</item>
		
	</channel>
</rss>
