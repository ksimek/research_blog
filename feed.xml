<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>FIRE = Cluster model</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Implemented cluster model in &lt;code&gt;cluster_model.{cpp,h}&lt;/code&gt;.  Implemented synthetic data generation in &lt;code&gt;synthetic.{cpp,h}&lt;/code&gt;.  Working on initial model estimation using k-means.&lt;/p&gt;

&lt;h2&gt;Run #1:  Adding estimation of epsilon.&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: After estimating &#39;A&#39;, also estimate the noise scale, &lt;code&gt;epsilon&lt;/code&gt; (assumed 1.0 until now).&lt;br/&gt;
&lt;strong&gt;Method&lt;/strong&gt;: Project data onto &#39;A&#39; in data space, take mean projection error (see kjb_c::project_rows_onto_basis).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Using known epsilon: 0.5
-------------------
Training error: -45.3955
Prediction error: -46.5

Estimating epsilon: 0.46192
----------------------
Training error: -45.7958
Prediction error: -47.1319
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: in the ballpark!&lt;/p&gt;

&lt;h2&gt;Run #2: initial cluster estimation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Iteratively estimate three clusters and assign memebership.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issues&lt;/strong&gt;: third cluster initialization is identical to second.  Guess: it&#39;s picking the same bad point over and over.&lt;/p&gt;

&lt;h2&gt;Run #3: initial cluster estimation, per-cluster obs. model&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Like Run #2, but each cluster has an individual observation model&lt;/p&gt;
</description>
				<pubDate>Thu, 08 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/08/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/08/work-log</guid>
			</item>
		
			<item>
				<title>FIRE - continuous model, clustering</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Today I finished run #10 from yesterday, in which I tried whitening the data before doing inference.  The results were inconclusive but not encouraging.&lt;/p&gt;

&lt;p&gt;I need to clean up the code which has gained some cruft from yesterday&#39;s tests.  I&#39;m removing the whitening option, since it doesn&#39;t help with synthetic data, but I&#39;ll keep the whtening function for testing on real data later.&lt;/p&gt;

&lt;h2&gt;Run 1: contiguous model&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Introduce piecewise continuity constriants.   Re-run and evaluate prediction error.&lt;br/&gt;
&lt;strong&gt;Method&lt;/strong&gt;:  add a special case to Piecewise_linear_model.  If b.size() == 1, assume contiguous model and infer the other b&#39;s on the fly.  Initial model estimate uses the same code as the non-continouous model, and simply resizes b.x.b to 1 afterward.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Ground Truth
--------------
Training error:   88.6756
Prediction error: 90.3623

Initial Model
------------
Training error:   88.6303
Prediction error: 90.4582

Best Model
----------
Training error:   88.6268
Prediction error: 90.4576
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The prediction error of the estimated model is extremely close to that of ground truth.  This is a simpler model, so that probably explains improved prediction accuracy.&lt;/p&gt;

&lt;p&gt;It&#39;s notable that the best model is now different than the initial estimate.  That&#39;s because the continuity constraints make the problem not solvable analytically.&lt;/p&gt;

&lt;p&gt;Also notable is that training error for the estimated models is better than that of the ground truth.&lt;/p&gt;

&lt;hr /&gt;
</description>
				<pubDate>Wed, 07 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/07/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/07/work-log</guid>
			</item>
		
			<item>
				<title>Fire: 10 inference experiments</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Ran 10,000 iterations, keeping observation model fixed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: The three plots below are incorrect.  corrected plot follows.
Ground truth model:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_gt_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Initial model:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_initial_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Final model:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_final_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: There was a bug in my plotting code for the plots above. Here is the corrected plot, which is more sensible:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initial estimate is bad, esp y-offset.  Should review this code.&lt;/li&gt;
&lt;li&gt;Recovered results are noticably better.  Not perfect, but this may be because we&#39;re seeing the &lt;em&gt;final&lt;/em&gt; model, not the best one.&lt;/li&gt;
&lt;li&gt;Keeping observation model fixed seems to improve speed by two orders of magnitude (why???)&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Run 2:  Keep best model&lt;/h2&gt;

&lt;p&gt;Added a &#39;best_sample_recorder&#39; to keep track of the best model we&#39;ve seen.&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Observations&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Best model is pretty good&lt;/li&gt;
&lt;li&gt;Why is initial model so different from that in the previous run?  same model, same random seed.&lt;/li&gt;
&lt;li&gt;Keep in mind, on average only 1/3 of the [0,1] domain is represented in the dataset, because it&#39;s divided into &quot;before&quot;, &quot;during&quot; and &quot;after&quot; regions.&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Discussion&lt;/h3&gt;

&lt;p&gt;Initial model uses a different observation model, so it&#39;s hard to say whether it&#39;s good or not.  I get the feeling that there&#39;s a lot of slack in this model, meaning several different combinations of observed and latent parameters can give nearly equal results.&lt;/p&gt;

&lt;p&gt;Also, initial model estimation doesn&#39;t correctly handle x-offsets for &quot;during&quot; and &quot;after&quot; regions.  TODO: fix this.&lt;/p&gt;

&lt;p&gt;Note that the noise epsilon is very large relative to the model&#39;s dynamic range.  It&#39;s hard to visualize, since the plots above are sent through an observation model before noise is added. But the observation model&#39;s scaling is around 0.5.&lt;/p&gt;

&lt;p&gt;Variability in the latent variable is around 0.3.  Observation model scales that to 0.15, then adds noise on the order of 1.0.  So signal-to-noise ratio is around 0.15 -- not great.  Luckilly we have lots of observations (150 people x 9 time points x 7 observed dimensions), but in our non-synthetic model, I hope our noise is smaller.&lt;/p&gt;

&lt;h2&gt;Run 3: Improved initial model estimate&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;SVN REVISION&lt;/em&gt;: 16742&lt;br/&gt;
&lt;em&gt;Description&lt;/em&gt;: Fixed initial model estimate by centering each region at x=0.  See &lt;code&gt;model.cpp:partition_observations()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;# iterations: 10,000 iterations&lt;br/&gt;
Running time: 0:18.53&lt;br/&gt;
Results:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Summary:&lt;br/&gt;
Initial model has changed, but initial offset is still way off.  Other models (final and best) are the same.&lt;/p&gt;

&lt;p&gt;Discussion:&lt;br/&gt;
Need to dig more into the initial estimation code.  Is boost&#39;s RNG seeded with current time?&lt;/p&gt;

&lt;h2&gt;Run 4: fixed observation parameters&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;SVN REVISION&lt;/em&gt;: 16743&lt;br/&gt;
Goal: See if fixing observation parameters (A, B) improve line-fits.  If so, issue is in observation parameters.  If not, issue is with line-fitting.&lt;br/&gt;
Running time: 0:18.26&lt;/p&gt;

&lt;p&gt;Results:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Discussion:&lt;br/&gt;
Bad results; issue is probably in line-fitting, not estimation of observation model.&lt;/p&gt;

&lt;h2&gt;Run 5: fixed observation parameters (take 2)&lt;/h2&gt;

&lt;p&gt;Description: Found a bug when using fixed offset B -- wasn&#39;t subtracting offset before doing PCA to find A.&lt;br/&gt;
Revision: 16744&lt;br/&gt;
Results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Discussion:
No change.  In retrospect, this change only applies when not using fixed observation parameters, so no change is to be expected.  But a good bug to fix for later on!&lt;/p&gt;

&lt;h2&gt;Run 6:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;:&lt;br/&gt;
Found another bug: when projecting points onto pincipal component, if the direction vector \(d\) isn&#39;t normalized to 1, the projected points are off by a factor of \(|d|^2\).   The observation equation is given by:&lt;/p&gt;

&lt;div&gt;
\[
y = Ax + B
\]

The goal is to solve for \(x\), which means we need the Moore-Penrose pseudoinverse, \(A^+ = (A^\top A)^{-1} A \).  When \(A\) is a column vector, the term in perentheses on the right-hand size is the squared magnitude of \(A\), which was ommitted in the original equation.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Revision&lt;/strong&gt;:  16745&lt;br/&gt;
&lt;strong&gt;Invocation&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# On bayes01
cd ~ksimek/src/fire/src/piecewise_linear/test
./test_inference &amp;gt; /dev/null
cat results.txt | \
    awk &#39;{row=NR%7; if(row == 3 || row == 4) print;}&#39; \
    &amp;gt; ~/tmp/lin.txt

# on local machine
rsync -avz v01:tmp/lin*.txt ~/tmp/

# in matlab on local machine
cd ~ksimek/work/src/fire/src/matlab/in_progress
figure
lin = load(&#39;~/tmp/lin7_1.txt&#39;)&#39;
lin3 = reshape(lin, [3, 2, 4])
visualize_pl_result(lin3, ...
    {&#39;ground truth&#39;, ...
    &#39;initial model&#39;, ...
    &#39;final model&#39;, ...
    &#39;best model&#39;})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion:&lt;/strong&gt;&lt;br/&gt;
Finally getting good initial estimates.  In fact, initial estimate is the &lt;strong&gt;optimal estimate&lt;/strong&gt; when A and B are known.  The best model doesn&#39;t look perfect, especially in the red curve, but there seems to be significant variance in the red curve estimator, as shorn in &quot;final model&quot;.&lt;/p&gt;

&lt;h2&gt;Run 7: Initial estimate of A&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Time to add initial estimation of A.  Sampling will still only estimate latent parameters.    I&#39;m curious how close this will be to optimal.  I&#39;m forcing the ground-truth A to have unit-length so the experimental results are comparable to the ground truth.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Invocation&lt;/strong&gt;: see previous&lt;br/&gt;
&lt;strong&gt;Revision&lt;/strong&gt;: 16746&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Baseline&lt;/strong&gt;:  The results below are estimates of the linear model when A is known.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run7_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Below are results of sampling, using an estimate of A from the noisy data&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run7_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion:&lt;/strong&gt;&lt;br/&gt;
Initial estimate is slightly worse in the green and blue curves, but still in the ballpark, as we would hope.  As observation noise decreases, this estimate should improve.&lt;/p&gt;

&lt;p&gt;It&#39;s notable that HMC still can&#39;t find a better model.  This is good news for the quality of our estimate.&lt;/p&gt;

&lt;h2&gt;Run 8: initial estimate of A and B&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;:  Adding initial estimation of B to the test.&lt;br/&gt;
&lt;strong&gt;Invocation&lt;/strong&gt;: see previous&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;br/&gt;
Much worse.  This could be caused by overestimating the magitude of (B), while understimating the magnitude of (b) (which can result in identical models due to our model being overdefined).  Since (b) currently adds positive offset to all observations, estimating (B) as the mean over observations could is likely to capture some of (b).&lt;/p&gt;

&lt;p&gt;Probably the best way to evaluate is to measure prediction error.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt; run 9 shows that this model predicts quite well, despite having different model parameters.&lt;/p&gt;

&lt;h2&gt;Discussion - Scaling and offset&lt;/h2&gt;

&lt;p&gt;I&#39;m starting to think we should consider preprocessing the immunity data bafore passing it through this model, for two reasons&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. poorly scaled data; correlated noise&lt;/strong&gt;&lt;br/&gt;
It seems too easy that we can solve for &#39;A&#39; using simple PCA.  Said differently, dynamics are the interesting part of our model, but A is determined irrespective of dynamics.&lt;/p&gt;

&lt;p&gt;The observation transformation A has an intuitive interpretation as the direction of greatest variation in the data, after taking dynamics into account.  The problem with this is that if data is badly scaled, the direction of greatest variation will come from the noise, not the dynamics.   If possible, we’d prefer to separate variation due to noise from variation due to dynamics.  That way, A can capture the dynamic relationships between variables (e.g. IL-6 and IL-5 both start low and end high), not just correlation (IL-6 and IL-5 co-vary, but no temporal information).&lt;/p&gt;

&lt;p&gt;For this reason, it might help to &quot;whiten&quot; the data by PCA, so the inferred direction of A is more likely to capture dynamics only.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Variability in immunity measurements&lt;/strong&gt;&lt;br/&gt;
Second, I’ve been thinking on the fact that individuals’ immunity values differ significantly in scale and offset.  From what I understand, much of this variation occurs within “plate”, i.e. its a byproduct of laboratory conditions, not immunity dynamics.   This could affect our clustering, as the latent slope and y-intercept parameters depend directly on the data’s scale and offset.  As a result, I fear our learned clusters will reflect different groups of “laboratory conditions”, rather than different groups of immunity dynamics.&lt;/p&gt;

&lt;p&gt;We could replace readings with z-score within each person, but this has two problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;readings that are relatively constant but legitimately high would seem more typical than they are.&lt;/li&gt;
&lt;li&gt;extremely steep changes over time would tend to flatten. similarly, relatively low slopes would tend to look higher after transforming.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;No obvious answer; may need to try several solutions.&lt;/p&gt;

&lt;h2&gt;Run 9: Held-out error &lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Since our model is overdefined, comparing model parameters is no longer a good indicator of model quality.  Instead, we should held-out data to evaluate the predictive power of the model&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: hold out 20% of data, measure log-likelihood for (a) ground truth model, (b) initial model estimate.  Two cases: 1. using global mean for B, PCA for A, 2. using ground truth A and B.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Training error:
------------

    Ground truth:              -13301.3
    Ana. Soln (Known A, B):    -13295.2
    Ana. Soln (Inferred A, B): -13318.3

Testing error:
-----------

    Ground Truth:               -1886.17
    Ana. Soln (Known A, B):     -1887.3
    Ana. Soln (Inferred A, B):  -1887.06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Training error is &lt;em&gt;better&lt;/em&gt; for analytical solution with known A and B, which what we&#39;d expect if the analytical solution was the true optimum.  When A and B are inferred, training error is worse, since PCA doesn&#39;t account for dynamics.&lt;/p&gt;

&lt;p&gt;Prediction error is worse than ground truth for all models, which is a good sanity check.  But they&#39;re all in a very tight ballpark (less than 1.0), which suggests that we aren&#39;t overfitting.  A better test would be to compare against one or more simpler models, but these results are good enough to proceed.&lt;/p&gt;

&lt;p&gt;One good conclusion is that even though inferring A and B results in vastly different latent models (see run #8), the prediction is just as good.&lt;/p&gt;

&lt;h2&gt;Run 10: Whitening before inference&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: transforming observations so they follow a standard m.v. Gaussian may help in infering.  Test prediction error and compare against run #9 errors (should be nearly identical).
&lt;strong&gt;Revision&lt;/strong&gt;: 16756&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: transform data by PCA.  Using linear regression to infer initial direction (since PCA is out).  Force B to be zero.  Fit model.  Evaluate held-out error after transforming back to data space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(&quot;Error&quot; is average negative log likelihood over 150 points)

Ground truth
------------
    Training error: 88.6756
    Prediction error: 90.3623

Estimated Model
--------------
    Raw data; A,B from PCA
    ---------
    Training error: 88.7887
    Prediction error: 90.5986

    Raw data; A,B from Regression
    ------------
    Training error: 88.6285
    Prediction error: 90.4405

    Whiteened Data; A,B from Regression
    --------------
    Training error: 93.8502
    Prediction error: 95.7252
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;br/&gt;
Training and prediction error is worse for whitened data, which is contrary to my expectation.  In retrospect, we&#39;re fitting to different data, so it&#39;s not too surprising that it predicts worse.  The story may be different if the data is truely poorly scaled.  Our example data was already well scaled, so its not a great test.&lt;/p&gt;

&lt;p&gt;But in the end, its defnitely clear that whitening isn&#39;t innocuous, and possibly harmful.&lt;/p&gt;

&lt;h2&gt;Mid-term TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Tuning HMC&lt;/li&gt;
&lt;li&gt;refactor model

&lt;ul&gt;
&lt;li&gt;Start and end times out of model&lt;/li&gt;
&lt;li&gt;epsilon per-dimension&lt;/li&gt;
&lt;li&gt;missing data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add offset constraints (continuous model)&lt;/li&gt;
&lt;li&gt;clustering&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Meta-notes on experiments&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;always test on synthetic data first&lt;/li&gt;
&lt;li&gt;always test on a simpler model first (fix some parameters)&lt;/li&gt;
&lt;li&gt;always have a visualization in mind when running a test.&lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Tue, 06 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/06/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/06/work-log</guid>
			</item>
		
			<item>
				<title>FIRE - piecewise linear inference</title>
				<description>&lt;p&gt;Finished compiling inference test with synthetic data.&lt;/p&gt;

&lt;p&gt;Gradient is taking incredibly long, especially for a 7-dimensional model.  PErhaps 100 data points is too large, but I&#39;m guessing the cost of allocating vectors for each function evaluation is the botteneck (GDB seems to agree).&lt;/p&gt;

&lt;p&gt;&lt;div&gt;
&lt;strong&gt;Time to run 1 iteration&lt;/strong&gt;&lt;br/&gt;
&lt;em&gt;Single threaded, on bayes01&lt;/em&gt;
&lt;table border=&quot;1&quot;&gt;
&lt;tr&gt;
&lt;td&gt;Baseline &lt;br/&gt;
(Debugging mode, allocation checking enabled)&lt;/td&gt;
&lt;td&gt;
1:28.94
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Heap checking disabled
&lt;/td&gt;
&lt;td&gt;
0:21.06 (-1:07.88, 4.22x)
&lt;/td&gt;
&lt;tr&gt;
&lt;td&gt;Heap &amp;amp; initialization checking disabled
&lt;/td&gt;
&lt;td&gt;
0:11.19 (-9.87 1.88x)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PRODUCTION=1 (with -O2)
&lt;/td&gt;
&lt;td&gt;
0:07.24 (-3.95 1.55x)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-O3
&lt;/td&gt;
&lt;td&gt;
0:07.86 (+0.62 0.92x)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;Used gprof with grpof2dot.py to get the following diagram:
&lt;a href=&quot;/img/2014-05-05-gprof_1.pdf&quot;&gt;gprof.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;iso_mvn_lpdf is getting hit hard:&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;c&quot;&gt;    &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;iso_mvn_lpdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M_PI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;It&#39;s already pretty lean (no alloc, all c-style).  But we can move the divide-by-epsilon out of the loop for an easy 1.8x speedup.&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;c&quot;&gt;    &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;iso_mvn_lpdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M_PI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Now the bottleneck is all the allocation, copying and freeing of kjb::Vector temporaries.&lt;/p&gt;

&lt;p&gt;I tweaked the code for evaluating a piecewise linear function to avoid creating kjb::Vector temporaries, and running time dropped dramatically &lt;strong&gt;from 11.4s to 1.26s.&lt;/strong&gt;  This is in production mode, so its surprising that more temporaries aren&#39;t optimized out.&lt;/p&gt;

&lt;p&gt;GProf after eliminating temporaries: &lt;a href=&quot;/img/2014-05-05-gprof_2.pdf&quot;&gt;gprof.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since 1 iterations takes about 1.2s, bumping up to 10 iterations.&lt;/p&gt;

&lt;p&gt;Remaining speed-up opportuntiies:  exploit gradient independence, parallel gradient&lt;/p&gt;

&lt;h2&gt;Parallel gradient&lt;/h2&gt;

&lt;p&gt;Enabled 8-way parallel gradient evaluation, and got &lt;strong&gt;worse&lt;/strong&gt; performance!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;single threaded:  0:09.63
multi threaded: 0:15.48
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This despote &lt;code&gt;top&lt;/code&gt; displaying 550% CPU utilization.&lt;/p&gt;

&lt;p&gt;Maybe gnuprof is affecting performance&lt;/p&gt;

&lt;h2&gt;Tuning&lt;/h2&gt;

&lt;p&gt;step size
gradient size&lt;/p&gt;
</description>
				<pubDate>Mon, 05 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/05/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/05/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;



</description>
				<pubDate>Thu, 01 May 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/05/01/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/05/01/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;Tucson day.&lt;/p&gt;

&lt;p&gt;Attended Anh&#39;s dissertation defense.&lt;/p&gt;

&lt;p&gt;Sat in on Jinyan&#39;s practice Talk.&lt;/p&gt;

&lt;p&gt;Finished coding model, data, and inference parts of FIRE piecewise linear model.  Started driver implementation.&lt;/p&gt;
</description>
				<pubDate>Wed, 30 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/30/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/30/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;Submitted &lt;a href=&quot;https://code.google.com/p/yamlmatlab/issues/detail?id=14&amp;amp;thanks=14&amp;amp;ts=1398783433&quot;&gt;bug report and patch&lt;/a&gt; to YAMLMatlab, which fixes an issue where row vectors weren&#39;t being read correctly.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Started working on piecewise linear model proposed by Kobus.  Wrote out initial equations, started initial coding.&lt;/p&gt;
</description>
				<pubDate>Tue, 29 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/29/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/29/work-log</guid>
			</item>
		
			<item>
				<title>Ellipse and Line Segment Detector ; Experiment Log</title>
				<description>&lt;p&gt;Got &lt;a href=&quot;http://ubee.enseeiht.fr/vision/ELSD/&quot;&gt;ELSD&lt;/a&gt; compiling and running on OS X.  Details &lt;a href=&quot;http://ksimek.github.io/2014/04/28/compiling-elsd-on-osx/&quot;&gt;here&lt;/a&gt;.  &lt;a href=&quot;http://ksimek.github.io/misc/elsd_results.html&quot;&gt;Results were very nice&lt;/a&gt;; will try to use them for ECCV.&lt;/p&gt;

&lt;p&gt;Working on &lt;code&gt;publish_results&lt;/code&gt; function in matlab to store results in Experiment Log.&lt;/p&gt;
</description>
				<pubDate>Mon, 28 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/28/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/28/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Auto SVN commit and record in run script&lt;/li&gt;
&lt;li&gt;m-file to build a run after the fact with comments&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 27 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/27/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/27/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;display visualization thumbnails
    how is visualization YAML laid out?
        2D array.  How is 2D array represented in YAML?
        Running on two datasets to see
            Added dataset 10 to list
            Dataset 10 has many missing views (why?  didn&#39;t I fix this?)
            Fixing unsuppressed output in process_gt2&lt;/p&gt;

&lt;h2&gt;TODO:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;investigate missing views in dataset 10&lt;/li&gt;
&lt;li&gt;test end-to-end with multiple datasets&lt;/li&gt;
&lt;li&gt;get visualization thumnails displaying&lt;/li&gt;
&lt;li&gt;implement visualization details page&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 20 Apr 2014 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2014/04/20/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2014/04/20/work-log</guid>
			</item>
		
	</channel>
</rss>
