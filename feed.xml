<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Work Log - Re-run training</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Re-training&lt;/h1&gt;

&lt;p&gt;Re-ran training after several bug-fixes.&lt;/p&gt;

&lt;h2&gt;New Files:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;train/tr_train_all.m&lt;/code&gt;  - Utility method for training all four models.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;experiments/exp_2013_08_11_train_all.m&lt;/code&gt;  - end-to-end training example; recreates results here.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;All results generated by &lt;code&gt;exp_2013_08_11_train_all.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0024
            noise_variance: 1.2308
         position_variance: 1.6072e+04
             rate_variance: 0.2743
perturb_smoothing_variance: 1
     perturb_rate_variance: 1
 perturb_position_variance: 1
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7192
         position_variance: 1.6132e+04
             rate_variance: 0.2451
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1292e-06
 perturb_position_variance: 0.4849
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;OU-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0018
            noise_variance: 0.7207
         position_variance: 1.6117e+04
             rate_variance: 0.2480
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1355e-06
 perturb_position_variance: 0.5172
             perturb_scale: 0.9202
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised to see that noise-variance only changed by a factor of 10, not 100.  However, the resulting noise_variance is right in the range that you'd expect arising from pixel-grid rasterization error.&lt;/p&gt;

&lt;p&gt;OU perturb-scale is lower than in the last case, and perturb position and rate variance is lower, too.&lt;/p&gt;

&lt;p&gt;SqExp perturb-scale is higher than in the last case, and perturb rate variance is lower.&lt;/p&gt;

&lt;p&gt;Lower position and rate variance makes sense after correcting curve-reversals.&lt;/p&gt;

&lt;p&gt;However, since we trimmed the pre-tails, a higher global and perturb position variance should result.  The result we're seeing is a combination of these competing effects.&lt;/p&gt;

&lt;h1&gt;Reconstructions&lt;/h1&gt;

&lt;p&gt;Some curves are flipped; need to an approach that will detect and correct flipped curves.&lt;/p&gt;

&lt;p&gt;Images and javascript generated by &lt;code&gt;../experiments/exp_2013_08_11_reconstruct_for_web.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/img/2013-08-11-ind-model-1.png&quot;,
        &quot;/img/2013-08-11-ind-model-2.png&quot;,
        &quot;/img/2013-08-11-ind-model-3.png&quot;,
        &quot;/img/2013-08-11-ind-model-4.png&quot;,
        &quot;/img/2013-08-11-ind-model-5.png&quot;,
        &quot;/img/2013-08-11-ind-model-6.png&quot;,
        &quot;/img/2013-08-11-ind-model-7.png&quot;,
        &quot;/img/2013-08-11-ind-model-8.png&quot;,
        &quot;/img/2013-08-11-ind-model-9.png&quot;
        ]

    construct_animation($(&quot;#ind-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ind-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;OO-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/img/2013-08-11-ou-model-1.png&quot;,
        &quot;/img/2013-08-11-ou-model-2.png&quot;,
        &quot;/img/2013-08-11-ou-model-3.png&quot;,
        &quot;/img/2013-08-11-ou-model-4.png&quot;,
        &quot;/img/2013-08-11-ou-model-5.png&quot;,
        &quot;/img/2013-08-11-ou-model-6.png&quot;,
        &quot;/img/2013-08-11-ou-model-7.png&quot;,
        &quot;/img/2013-08-11-ou-model-8.png&quot;,
        &quot;/img/2013-08-11-ou-model-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/img/2013-08-11-sqexp-model-1.png&quot;,
        &quot;/img/2013-08-11-sqexp-model-2.png&quot;,
        &quot;/img/2013-08-11-sqexp-model-3.png&quot;,
        &quot;/img/2013-08-11-sqexp-model-4.png&quot;,
        &quot;/img/2013-08-11-sqexp-model-5.png&quot;,
        &quot;/img/2013-08-11-sqexp-model-6.png&quot;,
        &quot;/img/2013-08-11-sqexp-model-7.png&quot;,
        &quot;/img/2013-08-11-sqexp-model-8.png&quot;,
        &quot;/img/2013-08-11-sqexp-model-9.png&quot;
        ]

    construct_animation($(&quot;#sqexp-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;sqexp-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;h1&gt;Detecting and Flipping Curves&lt;/h1&gt;

&lt;p&gt;Experiment: &lt;code&gt;../experiments/exp_2013_08_11_flip_curves.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Result: doesn't really work.  Lots of false negatives.&lt;/p&gt;

&lt;p&gt;Algorithm output: 2     4     5     6     8    10    12
Ground Truth: 1 2 4 5 6 8 9 10 11 12 14 15&lt;/p&gt;

&lt;p&gt;Not really sure why this is failing.  After flipping, most of these curves are closer to the origin, which is promoted by position_variance.  And any tip-perturbations should be better modelled after flipping.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Central curve extraction&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 11 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;== Pre-tails issue ==
&lt;strong&gt;Bug&lt;/strong&gt;
Smallest index of reconstructed curves is significantly less than 1.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp_setup_workspace
min([test_Corrs_ll{1}.ll_indices{:}])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This results in long pre-tails, as seen in this image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;min([test_Corrs_ll{1}.ll_indices{:}]&quot; alt=&quot;pretails&quot; /&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recall that reconstruction occurs by doing (1) rough triangulation, (2) smoothing, then (3) re-triangulating against the smoothed curve.  The initial triangulation usually results in a very bad index set, with spacing far larger than it should be, due to poor localization by maximum likelihood.  The subsequent smoothing causes the curve to stretch out longer than it should, so when re-triangulation occurs, ends are cut off.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Solution &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After re-triangulating, re-index so the minimum index is zero.&lt;/p&gt;

&lt;p&gt;Change to &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;145  % re-index by subtracting minimum index
146  min_index = min([Corr.ll_indices{:}]);
147  Corr.ll_indices = cellfun(@(x) x - min_index, Corr.ll_indices, 'UniformOutput', false);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt; Fallout &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This likely had an effect on training results, because marginal prior variance of the initial point was over-estimated, because it's index was over-estimated.&lt;/p&gt;

&lt;h1&gt;Cleanup&lt;/h1&gt;

&lt;p&gt;I'm afraid I made a mess of things yesterday when I was addressing the noise_variance issue in training.  Need to review the end-to-end systems for training, reconstruction and marginal likelihood evaluation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is training world-variance scaled everywhere?&lt;/li&gt;
&lt;li&gt;is training world-variance projecting to 1.0?&lt;/li&gt;
&lt;li&gt;is training-ml equal to inference-ml?&lt;/li&gt;
&lt;li&gt;are the reconstructed results sensible?&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;handle reversed curves&lt;/li&gt;
&lt;li&gt;Retrain all models since the following changes

&lt;ul&gt;
&lt;li&gt;reversal fixes&lt;/li&gt;
&lt;li&gt;noise variance fix&lt;/li&gt;
&lt;li&gt;&quot;pre-tail&quot; index fix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;central curve extraction&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;li&gt;end-to-end sampling system&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sat, 10 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/10/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/10/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Refactoring, cleanup, bug fixes</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15169&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Goals&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Move &lt;code&gt;test/tmp*&lt;/code&gt; to &lt;code&gt;experiment/&lt;/code&gt; directory.&lt;/li&gt;
&lt;li&gt;Overlay reconstruction onto images.&lt;/li&gt;
&lt;li&gt;Test effect of reversing curves on ML.&lt;/li&gt;
&lt;li&gt;Train background curve model.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Git Blog Mess&lt;/h1&gt;

&lt;p&gt;Got sidetracked after screwing up a git commit of the research blog.  Not sure the cause, but several files were deleted from the &quot;source&quot; branch.  Changed the &quot;preview&quot; rake target so it builds to /tmp/research_blog_site, instead of the source directory.  Hopefully this will avoid these issues in the future.&lt;/p&gt;

&lt;h1&gt;End-to-end experiment&lt;/h1&gt;

&lt;p&gt;Created an experiment file that recreates yesterday's results from scratch: &lt;code&gt;exp_2013_08_09_animated_reconstruction.m&lt;/code&gt;.  Since it runs training, it takes about 5 minutes to run.&lt;/p&gt;

&lt;p&gt;Also broke out reconstruction code into function in &lt;code&gt;reconstruction/reconstruct_views.m&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Overlay reconstruction onto images&lt;/h1&gt;

&lt;p&gt;See &lt;code&gt;test/tmp_vis_overlay.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I'm seeing some weirdness in the reconstructions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Curves are too stiff&lt;/li&gt;
&lt;li&gt;long pre-tails on a couple of curves.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Stiff Curves Solved&lt;/h1&gt;

&lt;p&gt;Figured out what was causing stiff curves.  I forgot that during training, all precisions are stored with noise_variance fixed at 1.0, and then are scaled on the fly.  However, during visualization, that scaling doesn't occur; the precisions are assumed to be stored at the desired scale.  i.e. a dumb bug.&lt;/p&gt;

&lt;p&gt;The new reconstructions now show a moderate amount of curvature, compared to their pre-bug stiff counterparts.&lt;/p&gt;

&lt;p&gt;Found another bug:  when constructing the &quot;unscaled&quot; precisions in &lt;code&gt;tr_prep_likelihood.m&lt;/code&gt;, I called &lt;code&gt;corr_to_likelihood&lt;/code&gt; with &lt;code&gt;params.noise_variance&lt;/code&gt; instead of 1.0.  Thus, if I understand correctly, the reported training value for noise-variance is 100x lower than it should be.&lt;/p&gt;

&lt;p&gt;That means the no-perturb model has a noise standard deviation on the order of 3.4 pixels and the perturb models are around 2.7.  This is closer to the range I was expecting, but I was hoping the perturb model stddev would be closer to 0.5, because it should arise only from pixel rasterization.  However, other sources of noise could be the curve detector, and also the cubic spline model might not be expressive enough to capture the model variance.&lt;/p&gt;

&lt;p&gt;I think I need to re-run end-to-end training and reconstruction to make sure there aren't any side-effects of these fixes.&lt;/p&gt;

&lt;p&gt;Committed to revision 15169&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Determine cause of long pre-tails.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Fri, 09 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/09/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/09/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Visualizing Results; New training method</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Visualization&lt;/h1&gt;

&lt;p&gt;I reconstructed the curves using the models I trained yesterday.  I was able to recover both the overall structure and track it's motion over 9 views.  For each of these results, use the slider below to change between each of the 9 views (the changes are subtle).&lt;/p&gt;

&lt;p&gt;I'm still struggling with smoothing variance being too low, which causes curves to be too straight.  For all of these results, I manually changed &lt;code&gt;smoothing_variance&lt;/code&gt; to be 0.1.&lt;/p&gt;

&lt;script&gt;
$(function(){
    var ind_urls = [
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#ind-reconstruction&quot;), ind_urls);


    var ou_urls = [
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#ou-reconstruction&quot;), ou_urls);

    var sqexp_urls = [
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#sqexp-reconstruction&quot;), sqexp_urls);

});
&lt;/script&gt;


&lt;h2&gt;Ind-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Below is the reconstruction for the independent-perturbation model.  The curves in each view are independently perturbed versions of central mean curves (not shown).&lt;/p&gt;

&lt;div id=&quot;ind-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;p&gt;It is unclear how much of this motion is due to camera miscalibration, and how much is actual plant motion.  Nevertheless, this shows that we can use the perturbation models to simultaneously triangulate and track over time.&lt;/p&gt;

&lt;p&gt;First, although each &lt;em&gt;view's&lt;/em&gt; perturbation is independent of the others, the perturbation is correlated between nearby points within the same view.  In other words, perturbations don't violate the smoothness constraint.&lt;/p&gt;

&lt;h2&gt;OU-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Next is the Ornstein-Ulenbeck perturbation model.  As opposed to the previous model, which modeled each view's perturbations as white noise, this model assumes Brownian motion over time.  Thus, we see a more naturally evolving time-series.&lt;/p&gt;

&lt;div id=&quot;ou-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;p&gt;The OU process models brownian motion&lt;/p&gt;

&lt;h2&gt;SQEXP-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Finally we have the squared-exponential perturbation model.  Now Brownian motion has been replaced by smooth motion.  I'm doubtful that this is a natural motion model for these plants.  The scale parameter is so low, I question whether it has any significant effect.&lt;/p&gt;

&lt;div id=&quot;sqexp-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;h1&gt;New Training Method&lt;/h1&gt;

&lt;p&gt;Need to determine why learned smoothing variance is so low.&lt;/p&gt;

&lt;p&gt;Is it even valid to do max-likelihood to train the parameters of the covariance function?&lt;/p&gt;

&lt;p&gt;Should we be training the smoothness parameter using the noiseless ground-truth data?&lt;/p&gt;

&lt;h2&gt;Two-pass Learning Procedure&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;First learn using noiseless data:

&lt;ol&gt;
&lt;li&gt;Set rate-variance to fixed ~0.23 (see &lt;a href=&quot;/ksimek/research/2013/08/07/work-log/#optimal-rate-variance&quot;&gt;yesterday's results&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Estimate  position_variance by emperical distribution over all point positions in ground truth.&lt;/li&gt;
&lt;li&gt;Estimate smoothing_variance by maximum likelihood over ground truth.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Learn the noise variance for the no-perturb model.

&lt;ol&gt;
&lt;li&gt;Find point correspondence between detected curves and corresponding ground truth curve.&lt;/li&gt;
&lt;li&gt;Compute variance between projected points and observed points.  This should maximize \(p(D \mid \Theta_0) \), where \(\Theta_0\) are the ground truth curves.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Learn perturb model parameters: noise_variance, perturb_{smoothing_variance, rate_varaince, position_variance}

&lt;ol&gt;
&lt;li&gt;Use nonlinear optimization to maximize \(p(D \mid \Theta_0) \), as defined below.  Start with no-perturb parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;div&gt;Let \(\theta_0\) be the ground-truth curve, and \(\{\theta_i\}\) be the set of all (unobserved) per-view curves \(\theta_i\). Let \(S_i\) be the virtual point precision matrices, assuming noise variance \(\sigma_n^2 = 1\).  The likelihood conditioned on the ground truth data \(p(D \mid \theta_0)\) is given by: &lt;/div&gt;




&lt;div&gt; 
\begin{align}
p(D \mid \theta_0) &amp;= \int_{\{\theta_i\}}  p(D_i, \{\theta_i\} \mid \theta_0)  d\{\theta_i\} \\
                   &amp;= \int_{\{\theta_i\}} \prod_{i=1}^N p(\theta_i \mid \theta_0) p(D_i | \theta_i) d\{\theta_i\} \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  p(\theta_i \mid \theta_0) p(D_i | \theta_i) d\theta_i \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  \mathcal{N}(\theta_i; \theta_0, \Sigma_p) \mathcal{N}(D_i;  \theta_i, \sigma_n^2 S_i^{-1}) d\theta_i \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  \mathcal{N}(D_i ; \theta_0, \Sigma_p + \sigma_n^2 S_i^{-1})
\end{align}
&lt;/div&gt;




&lt;div&gt; where  \(\Sigma_p\) is the perturbation variance. &lt;/div&gt;


&lt;p&gt;This should be an improvement over the current method, where we use only ground-truth labellings (not positions) and fit all parameters simultaneously.  This method assumed too much noise variance and not enough smoothness variance.&lt;/p&gt;

&lt;p&gt;Since we actually know the noiseless curves, we should train to that, to avoid the confounding of smoothness variance and noise variance.  The ground truth curves are aso stronger sources of evidence, compared to the curves reconstructed from data.&lt;/p&gt;

&lt;h1&gt;TODO:&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;reproject per-view reconstructions and overlay with original image.&lt;/li&gt;
&lt;li&gt;obtain &amp;amp; visualize unobserved &quot;central curve&quot;&lt;/li&gt;
&lt;li&gt;investigate the low smoothness variance.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Later TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;automatic curve reversing?&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 08 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/08/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/08/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Training, Reversed Curves, and Theoretical Rate Variance</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Visualized results after capping likelihood variance.  As expected, degree of spreading stops growing as perturb_rate_variance continues to grow.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Tasks&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;manually flip some curves and see if model changes&lt;/li&gt;
&lt;li&gt;try automatically determining which to curves need flipping&lt;/li&gt;
&lt;li&gt;try to get training and visualization to agree&lt;/li&gt;
&lt;li&gt;visualize curves moving through space over time&lt;/li&gt;
&lt;li&gt;train background curve model

&lt;ul&gt;
&lt;li&gt;is background model better than foreground?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Long term goals&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sampling framework

&lt;ul&gt;
&lt;li&gt;try using background pixel modeling to prune background curves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Visualizing curve-direction revealed a bizarre artifact: most curves start somewhere in the middle of the reconstructed curve!&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Found issue - wasn't sorting by index when reconstructing.&lt;/p&gt;

&lt;h2&gt;Breaking Change&lt;/h2&gt;

&lt;p&gt;Modified &lt;code&gt;tr_curves_ml&lt;/code&gt; to &lt;em&gt;not&lt;/em&gt; include the background curve ml into the computation.  Recall that the normal ML computation code doesn't actually return ML, but a &lt;em&gt;ratio&lt;/em&gt; of the foreground curve ML and the background curve ML.  This indicates how much the model improves over the &quot;null model&quot;.&lt;/p&gt;

&lt;p&gt;Since the background curve ml is constant during training, this shouldn't affect results.  However, if you want to confirm the correctness of &lt;code&gt;tr_curves_ml&lt;/code&gt; against the reference implementation in &lt;code&gt;curve_ml5.m&lt;/code&gt;, you'll need to manually divide by a constant.  See the documentation for &lt;code&gt;tr_curves_ml&lt;/code&gt; for more details.&lt;/p&gt;

&lt;h2&gt;Reversing Curves&lt;/h2&gt;

&lt;p&gt;Investigating the effect of reversing curves.&lt;/p&gt;

&lt;p&gt;Visually determined which curves were reversed.  See modified version of &lt;code&gt;test/tmp_visualize_test&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Hacked &lt;code&gt;train/tr_construct_matrices.m&lt;/code&gt; with hard-coded list of curves to flip.  Re-ran training for &lt;em&gt;IND-Perturb&lt;/em&gt; model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;: we should see larger values for perturb_rate_variance and/or perturb_smoothing_variance, and smaller values for perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0020
            noise_variance: 0.0720
         position_variance: 1.3414e+04
             rate_variance: 0.2378
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.5332e-06
 perturb_position_variance: 0.4662

Final ML: -95.736042
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare against old results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0718
         position_variance: 1.6706e+04
             rate_variance: 0.2135
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4942e-06
 perturb_position_variance: 0.4886

Final ML: -97.463243
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary of changes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: +2.09%
            noise_variance: 0.17%
         position_variance: -19.7%
             rate_variance: +11.3%
perturb_smoothing_variance: 0 
     perturb_rate_variance: +2.61%
 perturb_position_variance: -4.59%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As expected, global position variance dropped; perturb rate grew while perturb position variance decreased.&lt;/p&gt;

&lt;p&gt;Unexpected increase in rate_variance; expected it to stay constant.  Possibly due to random fluctuations; both old and new values (0.214 and 0.238, respectively) are near the theoretical optimum (0.23, see next section).&lt;/p&gt;

&lt;p&gt;Also unexpected small increase in global smoothing variance (expected to be constant); also possibly due to random fluctuations.&lt;/p&gt;

&lt;p&gt;Literally no change to perturb smoothing variance.  I'm starting to suspect something weird is going on with this value...&lt;/p&gt;

&lt;h2 id=&quot;optimal-rate-variance&quot;&gt;Theoretical Rate Variance&lt;/h2&gt;


&lt;p&gt;Was curious what the rate variance should be, assuming the rate vectors are drawn from a uniform distribution over the unit sphere.&lt;/p&gt;

&lt;p&gt;Determined empirically that rate variance should be somewhere between 0.220 and 0.235.  Code below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% generate 10,000 3-vectors with distribution over direction
dir = rand(3,10000);
% normalize to lie on unit sphere
dir = bsxfun(@times, dir, 1./sum(dir.^2));
% Get the emperical covariance of the vectors
Sigma = cov(dir')

        ans =

            0.2105    0.0556    0.0580
            0.0556    0.2297    0.0680
            0.0580    0.0680    0.3735
% take the average of the diagonals
mean(diag(Sigma))

        ans =

            0.2290
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This strongly suggests that the global rate variances we've seen in training are consistent with the theoretical value.  Great!&lt;/p&gt;

&lt;h2&gt;Visualizing Curve Motion&lt;/h2&gt;

&lt;p&gt;Attempting to visualize perturbations between views.&lt;/p&gt;

&lt;p&gt;First attempt: tweak &lt;code&gt;test/test_visualize_test&lt;/code&gt; to only display points from a particular view.  Doesn't work great, because only part of the plant is visible in each view, and those parts differ between views.&lt;/p&gt;

&lt;p&gt;Next attempt: tweak &lt;code&gt;curve_max_posterior.m&lt;/code&gt;  to define a canonical index set for the curve, and then reconstruct for each view.&lt;/p&gt;

&lt;p&gt;More tomorrow...&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;p&gt;Training Background model&lt;/p&gt;
</description>
				<pubDate>Wed, 07 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/07/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/07/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Singular Regions Issue; Training</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Investigating the &quot;Spreading&quot; issue with increases to perturb_rate_variance.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Confirmed the same phenomenon with increase to perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Setting perturb_position_variance to 1000:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-1.png&quot; alt=&quot;perturb_position_variance = 1000&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Setting perturb_position_variance to 1000000:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-2.png&quot; alt=&quot;perturb_position_variance = 1000000&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spreading appears to increase monotonically with perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Again, this is surprising, because you'd expect them to revert to the maximum likelihood solution.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;However, recall that the per-view likelihood has infinite variance in the back-projection direction.  The spreading appears to be occuring in this direction.  The infinite variance means that any influence from the prior will overcome the likelihood.&lt;/p&gt;

&lt;p&gt;But isn't the prior centered at zero?  Why is the result drifting so far from zero?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;One thing is clear: with high perturb&lt;em&gt;* values, the correlation between nearby views becomes negligible compared to the within-view variance.  And since the likelihood variance has an infinite component, the posterior variance grows with perturb&lt;/em&gt;*.  While we can inspect the maximum posterior curve, it is relatively meaningless because the variance is so great.&lt;/p&gt;

&lt;p&gt;Even so, why doesn't it just revert to the mean?&lt;/p&gt;

&lt;p&gt;Mean rate is zero, but it can't be exactly zero, because the likelihood requires that the curve be near the data.  But the data's position is only known in two dimensions, so the posterior is free to manipulate the third dimension so that the rate is minimized.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Model is trying to use a linear-perturbation model to explain per-view deviations from the mean model.  Since the deviations don't arise from pure scaling, it has to contort into bizarre shapes to explain it.&lt;/p&gt;

&lt;p&gt;But the bizarre shapes fit the data better, so it's worth it.&lt;/p&gt;

&lt;h1&gt;&quot;Singular Regions&quot;&lt;/h1&gt;

&lt;p&gt;GOT IT!  Recall that the likelihood variance is measured in world units, even though they are really image-based values.  As the curve moves toward the camera, the likelihood variance &lt;em&gt;should&lt;/em&gt; ideally reduce, since the same size of world perturbations result in larger image perturbations.  But in our model they don't, and so curves can stray farther from the data in the image, but still look nearby the data according to the Gaussian.&lt;/p&gt;

&lt;p&gt;In the extreme case, all the points end up near the camera pinhole, and they will be in the center of the data Gaussian.  In practice, any point within \(\sigma&lt;sup&gt;2\)&lt;/sup&gt; of the camera will be well supported, where \(\sigma&lt;sup&gt;2\)&lt;/sup&gt; is the average noise variance in 3D.  I'll call this the &quot;Singular Region&quot;, where all the of likelihood's Gaussian &quot;cylinders&quot; (degenerate cones) intersect and overlap.&lt;/p&gt;

&lt;p&gt;In terms of the marginal likelihood, this can cause counterintuitive behavior.  For example, large perturb_rate_variance might be good, because it allows the curve to wander into the &quot;singular zone&quot; near the camera.  Thinking of in spherical coordinates, there is a wedge of the sphere that points toward the camera, and as the perturb_rate_variance increases, this wedge remains relatively constant in angular size, but gets longer and longer.  The longer it gets, the more of singular zone it overlaps. Even though greater variance means there are more possible model configurations, there is a period during which the proportion of these configurations that are well-supported by the likelihood doesn't necessarily decrease, so the ML doesn't necessarily decrease, either.&lt;/p&gt;

&lt;p&gt;This explains the phenomenon we saw during training, where the plot of ML vs. perturb_rate_variance (reproduced below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-training-plot.png&quot; alt=&quot;ML vs. perturb_rate_variance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The lump to the left is the result of the singular region giving false support near the camera.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Need to somehow place penalty for any point that strays too far from the mean curve.  Can this be done without radically distorting the model?&lt;/p&gt;

&lt;p&gt;What if I placed a limit on the likelihood variance, instead of letting it be infinite?  It will prevent the prior of taking credit for lousy configurations during the ML marginalization.&lt;/p&gt;

&lt;h1&gt;Experiments&lt;/h1&gt;

&lt;p&gt;Modified &lt;code&gt;train/tr_construct_matrices.m&lt;/code&gt; to clamp the likelihood's maximum variance to some multiple of the largest finite eigenvalue (see local function &quot;fix_precisions&quot;).  ML shouldn't change much when using reasonable values.&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference (no cap): 2.2675e+04  (inf)
100x cap:           2.2109e+04  (1.2 mm)
1000x cap:          2.2360e+04  (3.8 mm)
10000x cap:         2.2510e+04  (12 mm)
100000x cap:        2.2596e+04  (38 mm)
1000000x cap:       2.2654e+04  (12 cm) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Capping the variance to 10000x the triangulation variance results in a standard deviation of about 12mm in practice, which seems very reasonable.&lt;/p&gt;

&lt;p&gt;We have to raise standard deviation to 12 cm for it to be accurate to three significant digits, which seems somewhat high.  Possibly, even with reasonable model parameters, we're still seeing some influence from the &quot;singular zone,&quot; so it may be a good thing that we aren't seeing the full reference value.&lt;/p&gt;

&lt;h2 id=&quot;training-results&quot;&gt;Training Results&lt;/h2&gt;


&lt;p&gt;Running training using clamped likelihoods.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0025
            noise_variance: 0.1231
         position_variance: 1.6658e+04
             rate_variance: 0.2207

Final ML: 2.371 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Exactly the same result as the non-clamped version.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ind Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0719
         position_variance: 1.6729e+04
             rate_variance: 0.2422
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4918e-06
 perturb_position_variance: 0.4801

Final ML:  2.512 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Old non-clamped training resulted in perturb_rate_variance exploding.  The new perturb_rate_variance looks very reasonable.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;OU Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0721
         position_variance: 1.6681e+04
             rate_variance: 0.2146
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4711e-06
 perturb_position_variance: 0.7793
             perturb_scale: 3.7353

Final ML:  2.516 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of the global variance is can be pushed into the perturb_variance, since they are now correlated.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SQEXP Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0018
            noise_variance: 0.0720
         position_variance: 1.6689e+04
             rate_variance: 0.2122
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4952e-06
 perturb_position_variance: 0.5130
             perturb_scale: 0.8425

Final ML: 2.516 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;General observations&lt;/h2&gt;

&lt;p&gt;It's still somewhat weird that perturb_smoothing_variance is so low.  I'm pretty sure there are non-negligible deformations occurring during the imaging process.  Maybe it's just the Ind-perturb model?  More likely it's because the curves that deform are reversed...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Which of the perturb-components are really correlated?  Seems like position variance is probably independent, but rate variance might not be.  Definitely smoothing_variance (i.e. nonrigid deformations) should be correlated.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;try visualization with truncated likelihoods

&lt;ul&gt;
&lt;li&gt;does increasing rate variance eventually have no effect?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;remove perturb_scale from ind model (better inference)&lt;/li&gt;
&lt;li&gt;hand-pick parameters and fix them, to reduce dimensionality of search space.&lt;/li&gt;
&lt;li&gt;Handle &quot;flipped&quot; curves.  Try to infer direction&lt;/li&gt;
&lt;li&gt;Does the visualized max posterior look good for the trained values (I'm guessing not -- too strict of variances, overfitting)&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Tue, 06 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/06/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/06/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Training Bugs</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14852&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing training of OU perturb model.&lt;/p&gt;

&lt;p&gt;Yesterday, we had problems with the perturb scale exploding.  Have switched to using a logistic sigmoid instead of an exponential to map perturb_scale, which sets a lower-bound and and upper-bound during optimization.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;perturb_scale is now staying low, but stil getting weird results.&lt;/p&gt;

&lt;p&gt;perturb_position_variance wants to be 9000+ ???&lt;/p&gt;

&lt;p&gt;perturb_rate_variance wants to be ~40?&lt;/p&gt;

&lt;p&gt;perturb_smoothing_variance wants to be ~0.0.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Trying new kernel: ind_kernel.  Like OU and SQEXP kernels, but no correlation between perturbations.  It's an intermediate step between no-perturb and ou-perturb, since it allows perturbations, but doesn't need a scale-length parameter.&lt;/p&gt;

&lt;p&gt;Training results for ind_kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_3 = 

            smoothing_variance: 0.0024
                noise_variance: 0.0132
             position_variance: 1.5747e+04
                 rate_variance: 0.1709
    perturb_smoothing_variance: 0.0020
         perturb_rate_variance: 26.3458
     perturb_position_variance: 0.8770
                 perturb_scale: 1.7199
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Very weird that perturb_rate variance&lt;/p&gt;

&lt;p&gt;Consider limiting number of dimensions somehow...&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Force both smoothing_variances to be the same&lt;/li&gt;
&lt;li&gt;remove perturb_scale parameter&lt;/li&gt;
&lt;li&gt;???&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;....&lt;/p&gt;

&lt;p&gt;recall that handling view-index in kernels has never been thoroughly tested...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Let's visualize results for the ind_perturb model, and see if everything looks reasonable.&lt;/p&gt;

&lt;p&gt;Created &lt;code&gt;test/tmp_visualize_test.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Interesting...  Some curves are &quot;reversed&quot;, i.e. the base is the &quot;end&quot; of the curve and the tip is the &quot;beginning&quot;.  This has some unintended consequences when applying the perturb model, because the tips are where perturbations are greatest, but when curves are reversed, the tips aren't affected by most of the modelled perturbations.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Weird.  As perturb_rate_variance increases, the per-view curves spread out radially like flower petals.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-1.png&quot; alt=&quot;high perturb_rate_variance causes radial spreading of curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would have expected that the likelihood would take over, but this is clearly not happening.  Lets look at the likelihood...&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-2.png&quot; alt=&quot;Maximum-likelihood solution.  Posterior should revert to this as variance increases asymtotically.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We must be running up against numerical precision errors.  Let's look at the equation for maximum posterior:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;When \(\Sigma_0\) has huge eigenvalues, this equation basically reduces to&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
\mu_P = (\Sigma_0 \Sigma_l^{-1})^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l 
     &amp;= \Sigma_l \Sigma_0^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l
     &amp;= \Sigma_l \Sigma_l^{-1} \mu_l
     &amp;= \mu_l
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;However, the effective cancellation of \Sigma_0 can't occur, because the expression \((\Sigma_0 \Sigma_l&lt;sup&gt;{-1}&lt;/sup&gt; + I)\) has a huge condition number, which makes inverting it an unstable operation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Anyways, that's visualization.  Is the same issue arising in ML computations?  Higher rate variance can result in significantly larger condition numbers, and IIRC, we don't take any special steps to handle such issues in the ML  computation anymore (for example, using the matrix inversion lemma).&lt;/p&gt;

&lt;p&gt;Maybe we should force rate variance to be within a reasonable range.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Even when perturb_rate_variance is reasonable (1.0), we still get drifting:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-3.png&quot; alt=&quot;perturb_rate_variance == 1.0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The degree of this phenomenon seems to be a smooth function of perturb_rate_variance.  If it was a numerical instability issue, we'd see it arise abruptly, indicating we had entered the unstable regime.&lt;/p&gt;

&lt;p&gt;I'm now thinking this is a real issue with the model, not simply an artifact of computation (maybe it's both?).  Need to think more about it.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Determine why spreading happens when &lt;code&gt;perturb_rate_variance&lt;/code&gt; increases.&lt;/li&gt;
&lt;li&gt;Determine why huge &lt;code&gt;perturb_rate_variance&lt;/code&gt; values are promoted by the ML.&lt;/li&gt;
&lt;li&gt;Handle &quot;reversed curves&quot; issue, where perturbation is applied to the wrong end.&lt;/li&gt;
&lt;li&gt;Training

&lt;ul&gt;
&lt;li&gt;Finish &quot;ind&quot; model&lt;/li&gt;
&lt;li&gt;Traing &quot;ou&quot; and &quot;sqexp&quot; models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 05 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting training of OU perturbation model (&lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Found bug in &lt;code&gt;tr_curves_ml.m::ou_kernel&lt;/code&gt; -- &lt;code&gt;smoothing_variance&lt;/code&gt; was used where &lt;code&gt;perturb_smoothing_variance&lt;/code&gt; should have been (copy-paste bug).  Also fixed in &lt;code&gt;sqexp_kernel&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Optimizer appears to be running much more smoothly now.  Probably because there previously was a strong correlation, due to &lt;code&gt;smoothing_variance&lt;/code&gt; appearing in two different parts of the equation.  The resulting ridge could have caused slow progress.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Initial params:&lt;/p&gt;

&lt;p&gt;train_params =&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.2500
                noise_variance: 100
             position_variance: 90000
                 rate_variance: 2.2500
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 2.2500
     perturb_position_variance: 90000
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stopped after 3 iterations.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.4501
                noise_variance: 2.5982e-04
             position_variance: 6.5359e+04
                 rate_variance: 3.9645
    perturb_smoothing_variance: 3.3437e-06
         perturb_rate_variance: 0.7268
     perturb_position_variance: 3.0785e+04
                 perturb_scale: 0.5867

Final ML: 28423.438936
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final ML (2.84e4) is greater than the no-pertrub model (2.37e4).  This is expected, since this model has more parameters, so we can get a tighter fit (maybe overfitting).&lt;/p&gt;

&lt;p&gt;I like that noise variance is much smaller (close to the minimum--do I need to lower the floor?).  This is expected, because the only source of IID noise is the rasterization process.&lt;/p&gt;

&lt;p&gt;Position standard deviation is much higher than before (255 vs. 126), as is rate standard deviation (2.0 vs. 0.47).&lt;/p&gt;

&lt;p&gt;Perturb variances are harder to explain.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is low, which is nice to see, because the worst of the perturbations are arising from miscalibrations (rate and position perturbations), not deformations.  But it's still far smaller than I expected.  hOnestly, I was expecting it to be almost the same as &lt;code&gt;smoothing_variance&lt;/code&gt;, but I realize now it makes sense for it to be smaller.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perturb position variance is much, much larger than I expected.&lt;/strong&gt;  Standard deviation is 175.4, which is basically saying each plant has 17 cm of IID perturbations from the unobserved mean plant.   I have no good explanation for this, expect maybe that the initial value was ridiculously large.&lt;/p&gt;

&lt;p&gt;Pertub scale seems reasonable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Test sensitivity to initialization.  Initial values:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 0.0500
         perturb_rate_variance: 0.1000
     perturb_position_variance: 1
                 perturb_scale: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First four were initialized from the trained no-perturb model.&lt;/p&gt;

&lt;p&gt;Note the biggest change: perturb_position_variance changed from 9000 to 1.&lt;/p&gt;

&lt;p&gt;Terminated after 46 iterations; much better than the 3 iterations from last attempt.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0019
                noise_variance: 0.0721
             position_variance: 1.6267e+04
                 rate_variance: 0.2050
    perturb_smoothing_variance: 6.7471e-10
         perturb_rate_variance: 1.3201e-04
     perturb_position_variance: 403.7082
                 perturb_scale: 2.4888e+03

Final ML: 25163.231449
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Weird that this is lower than that poorly-initialized model.  Consider initializing using a combination of Attempt #1 and Attempt #2's initial values.&lt;/p&gt;

&lt;p&gt;Smoothing variance is much smaller than before, noise variance is larger.&lt;/p&gt;

&lt;p&gt;Position variance is in the same order-of-magnitude, but about 1/5 the magnitude.&lt;/p&gt;

&lt;p&gt;Rate variance is an OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is still near-zero.&lt;/p&gt;

&lt;p&gt;Perturb rate variance is 3 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb position variance is 2 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb scale is sooooooo high.  Basically flat, so the regular and perturb variances sum.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: Result is very sensitive to initialization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: When perturb scale is this high, there exists a ridge, due to perturb variances playing the same role as normal variances.  Note that when perturb scale is infinity, this model degenerates to the no-perturb model.  Notice that Final ML isn't much better than the no-perturb.  Possibly when perturb scale exploded, the model could no longer improve, so constraining perturb scale to some small set of values may be a good idea (sigmoid transform).&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Try smaller changes to initialization.&lt;/li&gt;
&lt;li&gt;Try sigmoid transform on perturb scale.&lt;/li&gt;
&lt;li&gt;Try training using standard deviations instead of variances.&lt;/li&gt;
&lt;li&gt;Try changing one or more trained values to sensible defaults and retrain.&lt;/li&gt;
&lt;li&gt;Work with standard deviations, not variances&lt;/li&gt;
&lt;li&gt;Fix some 'known' values and optimize others.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 04 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Running optimizer...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issue&lt;/strong&gt;: Likelihood variance is collapsing to zero.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Guess&lt;/strong&gt;: Training ML is different from naive ML and isn't penalizing noise correctly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test 1&lt;/strong&gt;: save params, compare training ml and naive ml&lt;/p&gt;

&lt;p&gt;Same result.&lt;/p&gt;

&lt;h2&gt;Discussion&lt;/h2&gt;

&lt;div&gt; The marginal likelihood is monotonically increasing as the noise variance \(\sigma_n^2\) approaches zero.  This shouldn't be happening, because as the likelihood variance collapses to a delta, the marginal likelihood should become equal to the prior evaluated at the (virtual) observed locations.&lt;/div&gt;




&lt;div&gt;
\begin{align}
    \lim_{\sigma_n \to 0} p(D) &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; p_l(D \mid x) dx &amp; \text{(Marginalization)}\\
         &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; f(D - x) dx &amp;   \text{(Convolution)} \\
         &amp;= \int p_0(x) \delta(D - x) dx \\
         &amp;= p_0(D) 
\end{align}
&lt;/div&gt;


&lt;p&gt;This implies there's a bug in the code causing this phenomenon; the mathematical model is not the cause.&lt;/p&gt;

&lt;h2&gt;Solved&lt;/h2&gt;

&lt;div&gt;Found the bug.  When computing the likelihood precision \(S\) from the unscaled precision \(S_0\), the noise variance \(\sigma_n^2\) was multiplied instead of divided:&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;S = S0 * sigma_n; // incorrect
S = S0 * (1/sigma_n); // corrected version
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Resuming Training&lt;/h2&gt;

&lt;p&gt;Had some trouble with noise sigmas being too low.  Solved by in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Attempts 1-3: clamping to a minimum value and then adding a penalty depending on the amount that was clamped.&lt;/li&gt;
&lt;li&gt;Attempt 4:  offset by minimum value before transforming&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = tr_train(test_Corrs_ll, train_params, data_, 400);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Handle extreme values by incuring a penalty for variances smaller than 1e-5.  Result stored in &lt;code&gt;train_params_done_1&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprisingly small smoothness sigma.  Surprisingly low rate variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the magnitude of the penalty term.  Scaled up penalty by 1000.  Results in &lt;code&gt;train_params_done_2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the threshold where the penalty term starts to be incurred.  Set MIN_NOISE_VARIANCE to 1e-3 and MIN_SMOOTHING_VARIANCE to 1e-7 (previously both 1e-5).  Results in &lt;code&gt;train_params_done_3&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_3 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Attempt #4 &lt;/strong&gt;
Handled small variances by offsetting before transforming:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% converting param to state variable
x(1) = log(smoothing_variance - MIN_SMOOTING_VARIANCE);

% converting state variable to param
x(1) = exp(smoothing_variance) + MIN_SMOOTING_VARIANCE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is more elegant than the penalty hack used in the last three attempts.   Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;h2&gt;No-Perturb model summary:&lt;/h2&gt;

&lt;p&gt;Successfully trained model.&lt;br/&gt;
Required &lt;strong&gt;115 function evaluations&lt;/strong&gt;, taking &lt;strong&gt;112 s&lt;/strong&gt;.&lt;br/&gt;
Optimal marginal likelihood:  &lt;strong&gt;23714.937760&lt;/strong&gt;.&lt;br/&gt;
Optimal parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised at how small noise_variance was, considering the calibration noise.  However, I guess the maximum-likelihood reconstruction looked pretty good, if unity-apect-ratio axis scaling is used.&lt;/p&gt;

&lt;h2&gt;Training OU-Perturb model&lt;/h2&gt;

&lt;p&gt;Getting weird results.  Halted after second iteration; second iteration took 250 function evaluations; perturb_smoothing_variance gradient is zero.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 1.5003e-05
                noise_variance: 1.0087e-04
             position_variance: 7.9393e+04
                 rate_variance: 0.7879
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 1.5183
     perturb_position_variance: 2.5159e+04
                 perturb_scale: 48.6163
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Recall that new model ML's aren't deeply tested; probably a bug in there (in the kernel implementation?  in the kernel theory? in the conversion from mats to kernel?).  Will continue tomorrow.&lt;/p&gt;
</description>
				<pubDate>Sat, 03 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Morning&lt;/h2&gt;

&lt;p&gt;Researched telecommuting strategies.&lt;/p&gt;

&lt;p&gt;Set up Google hangout: &lt;a href=&quot;https://plus.google.com/hangouts/_/89759369dd280ff225c298a7a4291745134e1d6f&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Set up IRC chat room: &lt;a href=&quot;http://webchat.freenode.net/?channels=ivilab&amp;amp;uio=d4&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Probably IRC will be best for general chat, Google Hangouts for screen sharing or group video chat.&lt;/p&gt;

&lt;h2&gt;Afternoon&lt;/h2&gt;

&lt;p&gt;Setting up training minimizer using Matlab's &lt;code&gt;fminunc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Got all params in-place, but Matlab version is too old.&lt;/p&gt;

&lt;p&gt;Trying to upgrade matlab, but Java is out of date.&lt;/p&gt;

&lt;p&gt;Taking a break to move furniture...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Downloaded and installed Java 7.&lt;/p&gt;

&lt;p&gt;Now Matlab installer isn't able to communicate with server.  Arg...&lt;/p&gt;

&lt;p&gt;Working now... downloading...  Should be done in 45 minutes.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;p&gt;Migrating settings from old Matlab...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;
</description>
				<pubDate>Fri, 02 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log</guid>
			</item>
		
	</channel>
</rss>
