<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Work Log - Implementing Two-term likelihood</title>
				<description>&lt;p&gt;Working on Matlab integration.&lt;/p&gt;

&lt;p&gt;Quick test shows we can get about 20 evaluations per second.  Not bad, considering each evaluation consists of 9 image likleihoods.  There's some room for improvement here, but probably not worth pursuing at the moment:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;leapfrog rendering: render (a) while evaluating on (b).&lt;/li&gt;
&lt;li&gt;cleaning up geometry and fragment shaders (fewer branches, less storage)&lt;/li&gt;
&lt;li&gt;try other GPU blurring routines.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;New Likelihood&lt;/h1&gt;

&lt;p&gt;Scenario: we have a decent likelihood that is linear-gaussian (and a gaussian prior), so we can compute the marginal likelihood in closed-form.  However, we'd like to incorporate additional sources of evidence whose likelihoods aren't gaussian.  We'll see how we can estimate the joint marginal likelihood with simple Monte-Carlo sampling (no MCMC needed, no gradient).&lt;/p&gt;

&lt;h2&gt;Derivation&lt;/h2&gt;

&lt;p&gt;The old marginal likelihood looked like this:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1) &amp;= \int p(x) p(D_1 | x) dx
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;After introducing the extra likelihood term, the joint probability is no longer linear-gaussian, so the exact marginal likelihood involves an intractible integral.  However, by  re-arranging, we see we can get a good monte-carlo approximation:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;= \int p(x) p(D_1 | x) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= \int p(x | D_1) p(D_1) p(D_2 | x) dx &amp; \left(\text{Bayes thm (see below)}\right) \\
p(D_1, D_2) &amp;= p(D_1) \int p(x | D_1) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= p(D_1) \frac{1}{N} \sum p\left(D_2 | x^{(*)}\right) &amp; \text{(Monte Carlo)}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;In the second line, I've replaced the first two terms using Bayes theorem.  In the last line, the x-stars are samples from \(p(x | D_1)\), which we have in closed-form due to linear-Gaussian prior and likelihood for \(D_1\).&lt;/p&gt;

&lt;p&gt;Thus, we see if at least one source of data yields a linear-gaussian likelihood, we can incorporate additional data with arbitrary likelihoods  in a principled way.  In many cases, \(p(x | D_1) \) has low variance, so a small number of Monte-Carlo samples are sufficient for a good estimate -- even a single sample could suffice.  Even if the estimates are bad, they are unbiased, so any MCMC involving the marginal likelihood will converge to the target distribution.&lt;/p&gt;

&lt;h2&gt;Importance Sampling version&lt;/h2&gt;

&lt;p&gt;We can alternatively derive this in terms of importance sampling, setting the proposal probability q(x) to \(p(x | D_1) \):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;\approx 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{1}{q(x)} \\
            &amp;= 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{p(D_1)}{p(x) p(D_1 | x)} \\
            &amp;= 1/N \sum p(D_2 | x) p(D_1) \\
            &amp;= p(D_1) 1/N \sum p(D_2 | x) 
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File:&lt;/strong&gt; curve_tree_ml_2.m&lt;/p&gt;

&lt;p&gt;Basic idea&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;construct a thinned output index set (optional, but smart)&lt;/li&gt;
&lt;li&gt;construct a posterior distribution over the thinned set&lt;/li&gt;
&lt;li&gt;Add perturbation variance to the posterior&lt;/li&gt;
&lt;li&gt;take average over n trials: sample curveset and evaluate pixel likelihood&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Step 2 required updating to construct_attachment_covariance, which only constructs symmetric covariance matrices.  We need the covariance between indices with observations and the desired output indices.  Fully refactored that function into &lt;code&gt;construct_attacment_covariance_2.m&lt;/code&gt;; confirmed correctenss in the case of the self-covariance by using an existing test for version 1 of that function.  If non-symmetric, the upper-triangular blocks are processed in a second pass, swapping indices so we can re-use existing code.&lt;/p&gt;

&lt;p&gt;Need to try view-specific sampling, i.e. sample 9 different curves from 9 different views.  This refactor affects likelihood server, client, and message format.  I'm worried about the performance hit, but probably not worth worrying about (or futile).  Coarse sampling of indices could mitigate.    In any case, view-specific sampling is probably necessary, because we're using such low blurring levels in the Bd_likelihood, so the reconstruction needs to fall near the data.  We've seen how plant motion and camera miscalibration cause &quot;good&quot; 3D curves to reproject up to 10 pixels away from the data in some views.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Side-note&lt;/strong&gt; - the tcp connection is working very reliably so far!  Even after the machine sleeping/resuming several times, and suspending the server job for serveral hours, the socket is still valid and communicating flawlessly!&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;p&gt;Some protocol for starting and loading the likelihood server from matlab code&lt;/p&gt;
</description>
				<pubDate>Wed, 23 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still searching for inf bug in libcudcvt blurred_difference_likelihood.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found it:  my GPU-based log_sum routine had a bug.  instead of subtracting the maximum value before exp-summing, it subtracted an arbitrary value.  The find-max loop looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Real pi = v[0];

#pragma unroll
for(unsigned int i = 1; i &amp;lt; N; ++i)
{
    pi = pi &amp;lt; v[0] ? v[0] : pi;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, the zeros should be i's.&lt;/p&gt;

&lt;p&gt;A great bug to have found and fixed, but really frustrating that I let it slip through in the first place.  So stupid!  But it didn't affect correctness in the common case, so my tests didn't catch it.  It makes a good case for randomized testing (as if a good argument was lacking...).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(semi-obvious) note to self: conditional gaussian mixture is not equal to mixture of conditional gaussians!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;okay, a few more cleanup tasks, then on to real goals:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cleanup and commit version 0.1 of the likelihood server -- done&lt;/li&gt;
&lt;li&gt;split training into it's own directory. -- done&lt;/li&gt;
&lt;li&gt;Get likelihood server connecting reliably with matlab on different computers.&lt;/li&gt;
&lt;li&gt;implement likelihood importance sampling into maltlab's curve_ml procedure&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Thought some on birth moves.  It seems like there are some possibilities for births larget than two by using the adjacency graph created by pair candidates.&lt;/p&gt;

&lt;p&gt;for now, start with greedy&lt;/p&gt;
</description>
				<pubDate>Tue, 22 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Linux NVidia/Cuda/X11 erorrs; Cuda server; matlab integration</title>
				<description>&lt;p&gt;Getting my (cuda-equipped) desktop system running with likelihod_server.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Trouble building makefiles. Csh issue with new makefiles.  Kobus fixed&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still can't build makefiles. Probilems in the ./lib subdirectory is apparently blocking kjb_add_makefiles from finishing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Some makefiles were missing in ./lib.  Adding them was problematic, because (like before) kjb_add_makefiles wasn't finishing, because dependency directories were missing makefiles.  Also had some issues in kjb /lib, out of date from svn and some compile bugs I introduced in my last commit.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Compiling now, but shader is giving errors at runtime (opengl can't compile it, but no error message is given.&lt;/p&gt;

&lt;p&gt;Rebooting...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;No desktop.  Missing disk issue?  Tweaked fstab, still no X11.  Must be a driver issue.  Couldn't find driver from NVidia's I downloaded from web site months ago.  Using driver &quot;Ubuntu-X&quot; PPA.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still no X.  /var/log/Xorg.0.log says driver and client have different versions.  Got a tip online:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo dpkg --get-selections | grep nvidia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lots of extra crap there (old versions, experimental drivers, etc).  'sudo apt-get purged' all the non-essentials.  Now we're getting to login screen, but login is failing (simply returns to login screen).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found solution:&lt;/p&gt;

&lt;p&gt;   sudo  rm ~/.Xauthority&lt;/p&gt;

&lt;p&gt;Booting successfully.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running likelihood_server...  shader is now compiling successfully!   Previous issues must have been driver issues as a result of the recent 'sudo apt-get upgrade' without restart.&lt;/p&gt;

&lt;p&gt;Getting segfault when dumping pixels, though.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Caused by trying to read from cuda memory as if it were main memory.  Should be dumping if we're using GPU (or use a different dump function).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Fixed;  if using gpu, copy from cuda to temp buffer, then call normal routine.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting nan's from likelihood.  Need to dump from GPU likleihood, which means digging into libcudcvt.  Got some build errors resulting from a boost bug from 8 months ago (which arose because I updated GCC).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;cudcvt updated to grab blurred frames for (some method names changed to better match the analogous CPU likelihood in KJB.).&lt;/p&gt;

&lt;p&gt;Moved &quot;dump&quot; routines from Bd_mv_likelihood_cpu to the base class Bd_mv_likelihood as pure virtual function, and implemented a version in Bd_mv_likelihood_gpu to call cudcvt's new frame-grabbing code.  So &quot;dump&quot; mode now works on both cpu and gpu version.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Dumped data from CUDA likelihood; looks fine.  So I'm still clueless why we're getting nan values.&lt;/p&gt;

&lt;p&gt;Sanity check -- check that tests in libcudcvt are still passing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;test is failing on an assert.  Looks like an overzealous assert with a low percent-error threshold.&lt;/p&gt;

&lt;p&gt;Lowered threshold, test finished; GMM model doesn't match between gpu and cpu versions...&lt;/p&gt;

&lt;p&gt;what's changed?  Rolling back to first releast to see if tests pass.  Is it possible I never validated this?  Or maybe different GGM's being used between cpu and gpu?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Old version crashed and burned hard.  GPU is returning 'inf'.  No help here...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, svn version has lots of uncommitted changes, including bugfixes.  No wonder it was no help.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;found issue: introduced a regression when troubleshooting last bug.  Grrr.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still getting 'inf'.  Checked:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GPU buffers contain valid data for data and model&lt;/li&gt;
&lt;li&gt;CPU version gives finite results.&lt;/li&gt;
&lt;li&gt;GPU can give finite results (e.g. in test application)&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Modified libcudcvt test to receive arbitrary data and model images.  It's giving finite results, so it must be something about how I'm calling it in the likelihood server program.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;issue:&lt;/strong&gt; Bd likelihood in GMM mode gives lower values for ground truth model than random model.&lt;/p&gt;

&lt;p&gt;Bug. fixed.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Digging deeper into 'inf' issue.&lt;/p&gt;

&lt;p&gt;Dug all the way into the thrust::inner_product call.  Probed both buffers -- look okay.  mixture components look okay&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Got it!&lt;/strong&gt;  Was able to reproduce the 'inf's in the likelihood test program in libcudcpp.  It was so hard to reproduce because it only occurred when&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;old GMM values were used&lt;/li&gt;
&lt;li&gt;&lt;em&gt;unblurred&lt;/em&gt; model &lt;em&gt;and&lt;/em&gt; data were used&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Regarding 2, previosuly I just used the unblurred model, since it was at hand from a cuda dump, but used the blurred data from a training dump.&lt;/p&gt;

&lt;p&gt;Blurring doesn't seem to matter; both 2.0 and 5.0 give inf.&lt;/p&gt;

&lt;p&gt;Interestingly, the perfect model doesn't oveflow, but the random (almost null) model does.&lt;/p&gt;

&lt;p&gt;Also, for all the 'inf' cases, the cpu results aren't terribly high, so float overflow seems unlikely.&lt;/p&gt;

&lt;p&gt;Most likely it's the conditioning, where the joint pdf is divided by the marginal.  Maybe we're getting some bizzare model values that are off the charts?  But using the model image with the blurred data image is no problem, so that suggests the model values aren't a problem.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Interesting:&lt;/strong&gt; Tried removing border edges and the problem disappeared.  Is it possible that the blurring routine is not robust to near-edge pixels?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;idea: dump image after scale-offset, but before reduce&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;p&gt;Probability maps look sensible when a 2-pixel margin is added (--data-margin=2).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_2.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 2.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 5.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice what happens when we disable the data margin (rendered dimmer and with blue padding to emphasize effect):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_no_margin.tiff.jpg&quot; alt=&quot;&quot; /&gt;
(2.0 blurring, 0 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice the white pixels around the border, which presumably correspond to inf values.&lt;/p&gt;

&lt;p&gt;Inspecting the blurred data image, it looks like these pixels are, indeed, the brightest in the image.  It's possible we were lucky enough to have found the maximum range of this gmm.&lt;/p&gt;

&lt;p&gt;It looks like evaluating the bivariate normal is underflowing.  It could have been brought back down to size during conditioning, but we never made it that far.  could refactor by computing conditional covariance matrix beforehand, instaed of taking a ratio of computed values&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Pre-test all incoming data images: evaluate against an empty image and a full image and check for NaN and inf.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 21 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - KJB EM GMM</title>
				<description>&lt;p&gt;Still struggling with the EM GMM algorithm.  Found/fixed one bug where responsibilities aren't initialized if NULL is passed to the function.&lt;/p&gt;

&lt;p&gt;Ran overnight and it hadn't finished at the end.  Found/fixed a bug where the entire @M element responsibility matrix is rescaled 2M times (once per point).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting rank-deficient errors.  Looks like this is a result of the huge dataset; covariance matrix is divided by 1e6 (the number of soft-members of the cluster).&lt;/p&gt;

&lt;p&gt;For now, hack by trying to thin the dataset.  Long-term, adding a minimum offset to the emperical covariance seems to be the common solution.&lt;/p&gt;

&lt;p&gt;Possibly normalizing the data would be good.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Added offset to covariance (command-line options and file-static variable already existed, just needed to add it to the full_GMM logic).&lt;/p&gt;

&lt;p&gt;Added &lt;code&gt;read_gmm.m&lt;/code&gt; matlab routine, and plotted along with scatter plot of data.  Results look pretty good:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-20-kjb-gmm-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Moving trained gmm to ~/data/arabidopsis/training/bd_likelihood_gmm/blur_2.0.gmm&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;re-train GMM on held-out dataset.&lt;/li&gt;
&lt;li&gt;implement matlab likelihood sampling&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;how to sync datasets on matlab and c++ server?&lt;/li&gt;
&lt;li&gt;Wacv multi-view reconstruction for training&lt;/li&gt;
&lt;li&gt;saving wacv to files for training&lt;/li&gt;
&lt;li&gt;fixing issues in wacv reconstructions&lt;/li&gt;
&lt;li&gt;evaluation methodology&lt;/li&gt;
&lt;li&gt;split/merge&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 20 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;h2&gt;Training likelihood&lt;/h2&gt;

&lt;p&gt;Fitting a three-component GMM resulted in a null model having a higher likelihood than the ground truth model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Spent a lot of time troubleshooting this and finally realized I had scaled all values up by 100x so Matlab wouldn't choke, but I wasn't accounting for this when evaluating.  Blame it on this blasted cold, killing my focus!&lt;/p&gt;

&lt;p&gt;Ground truth model looks much better than null model now.&lt;/p&gt;

&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;Trying different blurring levels.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.0 is to small.&lt;/li&gt;
&lt;li&gt;2.0 seems okay&lt;/li&gt;
&lt;li&gt;5.0 gives a wide variance for true-positives.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In the end, running max likleihood on this will be best, but this gives us a ballpark.&lt;/p&gt;

&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;Trying to get KJB EM algorithm working on my 2M element dataset.  Getting NaN errors at the moment.&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/19/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/19/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Uninformative likelihood?</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15701&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Likelihood sanity check (ctd)&lt;/h2&gt;

&lt;p&gt;An empty model should have a significantly worse likelihood than the ground truth model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ground truth model:  -3.38064e+08
empty model: -3.28279e+08
full model: -1.20572e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both empty and full are better than ground truth.  Why??&lt;/p&gt;

&lt;p&gt;At least possiblities here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Noise level is so high that fitting is impossible (remember the paper &quot;Fundamental limits of Bayesian Inference: Order Parameters and Phase Transitions for Road Tracking&quot;)&lt;/li&gt;
&lt;li&gt;Bug.  maybe renderings are just waaaay off?&lt;/li&gt;
&lt;li&gt;Bad calibration.  Looking at the GMM plot from yesterday (see below) it looks like the one-sigma contour is very very large.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-bd.gmm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Case 1 is what's left over once 2 and 3 are ruled out.&lt;/p&gt;

&lt;h2&gt;Rendering sanity check.&lt;/h2&gt;

&lt;p&gt;This seems to be a culprit.&lt;/p&gt;

&lt;p&gt;Here's first data view:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-data-view-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Not a good match to the actual first data image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-ler_2_36_0.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But a good match to the second data image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-ler_2_36_1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Must be an off-by-one error when reading data.&lt;/p&gt;

&lt;p&gt;The cameras are not off-by one.  Here's the rendering of the model under the first camera:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-model-view-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Good match to the first data image above.&lt;/p&gt;

&lt;p&gt;Bug is in the config file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;view-indices=1:4:36  # bug here
view-indices=0:4:35  # should be this
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Good news, bad news.&lt;/p&gt;

&lt;p&gt;Good news: after fixing config file, data and rendering look good (use slider to swap between images)&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-10-17-data.jpg&quot;,
        &quot;/ksimek/research/img/2013-10-17-model.jpg&quot;
        ]

    construct_animation($(&quot;#data-model-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;data-model-anim&quot; style=&quot;width:530px&quot;&gt; &lt;/div&gt;


&lt;p&gt;Bad news: likelihood gets &lt;strong&gt;worse&lt;/strong&gt;!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fixed likelhood: -3.38298e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like the rendered model is lighter than the data model.  This might account for the issues we're seeing.  We're in the region to the right of the diagonal &quot;true positive&quot; component in the GMM (see contour plot above).  In this region, the noise model has a stronger pull than the true positive model, which partially explains why we get better when fewer model pixels are on the screen.   It doesn't explain why the model gets better when we shift left and right -- maybe it fits better in the tails of the blurred data, but we'd expect it to eventually get worse.  Lets test it.&lt;/p&gt;

&lt;p&gt;Recall that this GMM was trained on a manipulated version of the data image, not a rendered 3D model image.  The process taken was (a) removing all background pixels from the data image and (b) perturbing the foreground pixels. We may have removed a few foreground pixels to add false positives, but the amount was small, if memory serves.  Now we have far fewer model pixels, so we expect the &quot;true positive&quot; region to be slanted more upward.&lt;/p&gt;

&lt;h2&gt;Optimality test (redux)&lt;/h2&gt;

&lt;p&gt;Code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x_values=&quot;-80 -40 -20 -10 -5 -2 -1 0 1 2 5 10 20 40 80&quot;
for x in $x_values; do
    echo -n &quot;$x &quot;;
    ./likelihood_server \
        --config=test.conf  \
        --image-bounds &quot;0 530 397 0 200 2000&quot;  \
        --cam-convention &quot;1 0 0&quot;  \
        --dbg_save_frames  \
        --dbg_load_message  \
        --dbg-ppo-x-offset=$x 2&amp;gt; /dev/null | grep ^3 | sed -e 's/3 //' -e 's/,//';
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output (delta-x vs log-likelihood):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-80 -3.36866e+08
-40 -3.36987e+08
-20 -3.37434e+08
-10 -3.37864e+08
-5 -3.38169e+08
-2 -3.38279e+08
-1 -3.38294e+08
0 -3.38298e+08
1 -3.38292e+08
2 -3.38275e+08
5 -3.38172e+08
10 -3.37912e+08
20 -3.37479e+08
40 -3.36958e+08
80 -3.36991e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-likelihood_plot_vs_x.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;&quot;Full&quot; model issue&lt;/h2&gt;

&lt;p&gt;One of the strangest things is the fact that a rendering full of &quot;1.0's&quot; performs far better than the ground truth or empty model.  This area lies along the x-axis in the GMM contour plot, where there is no support.&lt;/p&gt;

&lt;p&gt;Lets debug this.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found an insane bug.  Below is the loop that compares all the pixels of the data and rendered model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    for(size_t i = 0; i &amp;lt;= size_; ++i)
    {
        kjb::Vector2 x;
        x[0] = model[i];
        x[1] = data_[i] + 1;

        ...  // compute p(x[1] | x[0])
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have no idea why we're adding 1.0 to &lt;code&gt;data_[i]&lt;/code&gt;, but this explains everything.  In the &quot;full&quot; model, it puts us right on the diagonal of the GMM.  In the empty model, we're right on the x-axis.  Both are well-supported regions.  In the ground truth model, we're lying between these extremes, which is a no-man's land of near-zero support.&lt;/p&gt;

&lt;p&gt;Spending 5 minutes to determine how this got added...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;It was added between November 11 and December 13 last year.   No obvious reason.  Oh well.&lt;/p&gt;

&lt;p&gt;New sanity check numbers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;full model -5.2217e+08
ground truth model 4.03273e+06
empty model: 4.01252e+06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much more sensible.  Missing data looks terrible, noise looks okay but not great,  ground truth looks best.  Lets do an offset plot just to be safe.&lt;/p&gt;

&lt;h2&gt;Optimality test (redux&lt;sup&gt;2)&lt;/sup&gt;&lt;/h2&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-80 3.1039e+06
-40 3.27557e+06
-20 3.58156e+06
-10 3.77333e+06
-5 3.93159e+06
-2 4.01459e+06
-1 4.02845e+06
0 4.03273e+06
1 4.02704e+06
2 4.01209e+06
5 3.93021e+06
10 3.78782e+06
20 3.60557e+06
40 3.26585e+06
80 3.26894e+06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plot&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-likelihood_plot_vs_x_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Oh Yeah!!&lt;/p&gt;

&lt;h2&gt;BD Likelihood Data Dump Mode.&lt;/h2&gt;

&lt;p&gt;add a mode by which pixel likelihood will save pixel data to files for analysis or debugging&lt;/p&gt;

&lt;p&gt;changes will appear in:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;lib/evaluate/bd_likelihood.h - Bd_pixel_likelihood_cpu&lt;/li&gt;
&lt;li&gt;lib/evaluate/bd_likelihood.h - Bd_mv_likeliood_cpu&lt;/li&gt;
&lt;li&gt;./likelihood_server.cpp - Options&lt;/li&gt;
&lt;li&gt;./likelihood_server.cpp - main()&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;Added dump mode.  Probably should move this into a separate directory, or at least a different program, since it has almost nothing to do with the likleihood server.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;rename matlab &quot;render_client&quot; code to &quot;likelihood_client&quot;&lt;/li&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;remove edges around image border.&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;train likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;how to sync datasets on matlab and c++ server?&lt;/li&gt;
&lt;li&gt;Wacv multi-view reconstruction for training&lt;/li&gt;
&lt;li&gt;saving wacv to files for training&lt;/li&gt;
&lt;li&gt;fixing issues in wacv reconstructions&lt;/li&gt;
&lt;li&gt;evaluation methodology&lt;/li&gt;
&lt;li&gt;split/merge&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 17 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/17/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/17/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - silhouettes, training likelihood, evaluating likelihood</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Enabled glPolygonOffset and it improved results somewhat.  Still getting stippling 10% of the time.&lt;/p&gt;

&lt;p&gt;Adding edge-detection code to show stippled edges helps, but adds some internal edges for some reason.&lt;/p&gt;

&lt;p&gt;Apparently, we have point spacing too close together.  This introduces lots of little ridges, which makes silhouette edges appear on the interior of the object incorrectly.  We should pro-process curves in matlab to force spacing to be greater than or equal to to the curve radius.&lt;/p&gt;

&lt;h2&gt;Likelihood rendering bugs&lt;/h2&gt;

&lt;p&gt;Image dumps from our likelihood show all edges are being rendered (not just silhouette edges).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;seems to be an issue with Camera_render_wrapper.  Possibly the y-axis flipping?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Yep.  Reverses the handedness of the projected model.&lt;/p&gt;

&lt;p&gt;Shader assumes orientation of forward-facing triangles isn't affected by projection.  Thus, cross-product can be used to determine whether a face is forward-facing (a fundamental operation for classifying silhouette edges).&lt;/p&gt;

&lt;p&gt;Can we solve by doing visibility test in world coordinates, which avoids the projection matrix altogether?&lt;/p&gt;

&lt;p&gt;Yes, but there's a bug causing a few faulty silhouette edges to appear.  Why?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It turns out, we were assuming the normal vector of opposite faces are parallel, and used this to .  Apparently not always the case, especially when sharp corners occur (which happens routinely when point spacing is close).&lt;/p&gt;

&lt;p&gt;During testing, we somehow corrupted the GPU's memory.  System sprites (e.g. cursor) are starting to be corrupted, and program keeps crashing  from GPU errors.  Probably constructed too many shaders in a single session.  Will reboot and continue.&lt;/p&gt;

&lt;p&gt;Summary: found silhouette rendering bug caused by projection matrix that didn't preserve handedness.  Rewrote silhouette pass 2 geometry shader to handle this case better.  Now renders correctly in both cases.&lt;/p&gt;

&lt;h2&gt;Next: determine cause of likleihood NaN's; train the likelihood.&lt;/h2&gt;

&lt;h2&gt;Blurred-difference (bd) likelihood issues&lt;/h2&gt;

&lt;p&gt;Getting &quot;NaN&quot; when evaluating.  Checking the result of blurring the model shows inf's everywhere.  Weird, because I can confirm that the input data is succesfully blurring:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-blurred_data.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Same blurrer is used for data and model&lt;/li&gt;
&lt;li&gt;It isn't the input (tried substituting data in for model and still got corrupt stuff back).&lt;/li&gt;
&lt;li&gt;re-initializing blurrer doesn't seem to help&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Stepping through in GDB...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After convolution, values are on the order of 1e+158...  clearly invalid.  Converting to float causes inf.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Aside:  the padding added before convolution isn't zeros!  Fixing bug in lib/evaluate/convolve.h:fftw_convolution class.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Aside:  Need to remove edges around image border:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-border_edges.jpg&quot; alt=&quot;border edges bug&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Aside: different blurrer for each view?  unnecessary!&lt;/p&gt;

&lt;p&gt;Wrong, same blurrer, different internal buffers.  Caused by calling blurrer.init_fttw_method() every time we add a view to the multi-view likelihood.  Removed that call.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Apparently, calling blurrer_.init_fftw_method() is wreaking all kinds of havok on the blurring results.  I have idea why.  Possibly calling init_fftw_method() more than once is simply not supported.  But that call simply destroys and object and creates a new one, so why is that different from the first time we called it?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Theory: fftw wisdom is messing with us?&lt;/p&gt;

&lt;p&gt;Results: nope&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We have a working case and a non-working case.  Can we walk from one case to the other and find the exact change that causes the error?&lt;/p&gt;

&lt;p&gt;Aside: Here's a bizarre result wer'e getting during failure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-weird_results.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kind of cool!&lt;/p&gt;

&lt;p&gt;Actually, there might be a hint here... Notice the eliptical shapes that are about the same size as the turntable, but shifted and screwed up.&lt;/p&gt;

&lt;p&gt;It's possible what we're seeing is convolution with a random mask.  the two main ellipses we're seeing are two random bright pixels in the mask, and the miscellaneous roughness might be ca combinariton of small positive and negative values causing the texture we see.  Next step: inspect mask&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bingo!  We're working with a random mask.&lt;/p&gt;

&lt;p&gt;Let's step back to the original failing case and see if the mask is still random.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Confirmed.  Here's an eample mask:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-tmp.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here it is dimmer:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-bunk_maks_dimmer.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Anything look familiar?  It's derived from our plant image (notice the ellipses from the turntable).&lt;/p&gt;

&lt;p&gt;Theory: somehow the mask is getting overwritten with our blurred model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Solved.  Simple bug but was obscured by several issues.&lt;/p&gt;

&lt;p&gt;The fundamental issue was that when the blurring mask was being padded, the padding wasn't being filled with zeros.  Thus, the padded mask consisted of a small section of real mask, surrounded by huge amounts of uninitialized memory.&lt;/p&gt;

&lt;p&gt;This was a bug that I discovered and fixed 12 months ago, but it got rolled back when Andrew revamped the FFTW convolution class.  Ultimately, I was able to restore my changes to the FFTW class without much trouble, but for the six hours that followed, these changes weren't being compiled, because they were in a header file, and the build system only rebuilds object files if the &quot;cpp&quot; file is changed.&lt;/p&gt;

&lt;p&gt;Further, there was a red-herring that appeared as &quot;re-initializing the blurrer causes the errors&quot;.  While this was true, it was only because re-initializing also re-initialized the blurring mask, and it was only during re-initialization did the uninitialized mask padding have junk -- on the first initialization, the memory just happened to be blank.  Go figure.&lt;/p&gt;

&lt;p&gt;This was an absolutely essential bug to find and fix.  Glad its fixed, just wish it hadn't regressed in the first place.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Okay time to reset.  Let's confirm that the rendered models are blurring correctly.  Then confirm non-zero log-likleihood result.    Then on to training.&lt;/p&gt;

&lt;p&gt;Model blurring: CHECK
Finite likelihood: CHECK  (result: -3.38064e+08)
Likelihood is near-optimal: &lt;strong&gt;FAIL&lt;/strong&gt;. see below&lt;/p&gt;

&lt;h2&gt;Optimality test&lt;/h2&gt;

&lt;p&gt;Added debugging option to shift rendering left/right/up/down in the image: &lt;code&gt;--dbg-ppo-x-offset&lt;/code&gt; and &lt;code&gt;--dbg-ppo-y-offset&lt;/code&gt;.   Ideally, the likleihood should be optimized at zero offset&lt;/p&gt;

&lt;p&gt;Log likelihood vs x-offset&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 0  -3.38064e+08
+1  -3.38069e+08
+2  -3.38068e+08
+5  -3.38033e+08
+10 -3.37881e+08
-2  -3.38039e+08
-5  -3.37968e+08
-10 -3.37786e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-likleihood_plot_vs_x.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weird... Why is it at a local &lt;em&gt;minimum&lt;/em&gt; at the center?&lt;/p&gt;

&lt;p&gt;Log likelihood vs y-offset&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 0  -3.38064e+08
-2  -3.38062e+08
-5  -3.38033e+08
-10 -3.3789e+08
-20 -3.37321e+08
-40 -3.36139e+08
-80 -3.33693e+08
 2  -3.38052e+08
 5  -3.38025e+08
 10 -3.37983e+08
 20 -3.37955e+08
 40 -3.3785e+08
 80 -3.37523e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-likelihood_plot_vs_y.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that as y decreases, the top of the plant starts to fall off the screen. This suggests that having fewer model points might be over-preferred.  Need to sanity-check the GMM model we're using -- I trained it months ago and never really leaned on it hard, so it's the first place to look for problems.&lt;/p&gt;

&lt;p&gt;Also, might be a scaling issue.  Double-check that blurred model and blurred data have similar scale.&lt;/p&gt;

&lt;h2&gt;GMM sanity check&lt;/h2&gt;

&lt;p&gt;Plotting first and second standard deviation of the joint distribution of model-pixel-intensity and data-pixel-intensity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-bd.gmm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This looks sensible.  The first component is the diagonal one, representing true postives (in the presence of random perturbations).&lt;/p&gt;

&lt;p&gt;The second component is at the origin -- true negatives.&lt;/p&gt;

&lt;p&gt;The third component is along the y-axis, representing false positives in the data, aka noise.&lt;/p&gt;

&lt;p&gt;The first component has roughly 10x less weight than the second and third.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;No component for the false negatives (aka missing data).  As I recall, running EM on a GMM with four or more components always resulted in redundant components.  Possibly hand-initializing the components in each of the four positions might help.&lt;/p&gt;

&lt;p&gt;I also recall some sort of GPU limitation that prevented me from evaluating more than three components in hardware.  That seems unusual though, and I may be misremembering.&lt;/p&gt;

&lt;p&gt;Does this provide any insight as to why we prefer our model to drift off of the screen?  This introduces more &quot;negative&quot; model pixels, pushing toward the well-supported y-axis are of our model.  Possibly this is the result of poor calibration?  Or wrong blurring sigma?&lt;/p&gt;

&lt;p&gt;TODO: re-run the x/y offset experiment with smaller blurring sigma.&lt;/p&gt;

&lt;h2&gt;Data Sanity check&lt;/h2&gt;

&lt;p&gt;To do.  Approach: instead of evaluating likelihood pixels against data, dump model/data pixel pairs into a list and (a) plot them or (b) train on them.  Maybe just dump to a file and do it in matlab?&lt;/p&gt;

&lt;h2&gt;Blooper reel&lt;/h2&gt;

&lt;p&gt;While writing miscellaneous blocks of memory to disk as images, got some interesting but totally wrong results.&lt;/p&gt;

&lt;p&gt;The following is partially due to rendering an array of doubles as floats.  Interesting banding pattern.  A mini-Mondrian!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-fun-mess-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next is just uninitialized memory.  Iteresting patterns :-)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-fun-mess-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;remove edges around image border.&lt;/li&gt;
&lt;li&gt;share blurrer between views&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;why is it crashing on exit?&lt;/li&gt;
&lt;li&gt;Data sanity check - does it roughly match the GMM's distribution?&lt;/li&gt;
&lt;li&gt;train likelihood&lt;/li&gt;
&lt;li&gt;add &quot;dump&quot; mode to bd-likelihood which saves all image pairs to disk for analysis or debugging.&lt;/li&gt;
&lt;li&gt;test empty model likelihood&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Wed, 16 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/16/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/16/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Finishing Likelihood Server, integrating to sampling engine</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Old silhouette rendering technique is failing.  Point spacing is much tighter than before.  Our old algorithm relied on segment-ids matching between the first-pass and second pass to work.&lt;/p&gt;

&lt;p&gt;Have a replacement algorithm that works reasonably well.  Basically edge-detection on the depth image.  The down side is we're getting double-edges in some areas, and you have to adjust a threshold.  It'll have to do for now, hopefully we can train a the likelihood to be robust to these issues.&lt;/p&gt;

&lt;h2&gt;Training Bd_likelihood&lt;/h2&gt;

&lt;p&gt;added 'train_bd_likelihood()' function to 'lib/evaluate/bd_likelihood.h'.  Uses 'kjb_c::get_full_GMM()'  to fit a three-component gaussian mixture model to model the relationship between model and data pixels.&lt;/p&gt;

&lt;h2&gt;Refactoring&lt;/h2&gt;

&lt;p&gt;Shader objects from './shader.h' are now in the project library, under 'lib/ogl/shader_2.h'.  Not the greatest name, but shader.h already exists and conflicts.  Consder refactoring them later.&lt;/p&gt;

&lt;p&gt;Silhouetter rendering from './render_util.h' are now in 'lib/graphics/silhouette_renderer.h'.  Much better encapsulation now, so we can use it in other projects.&lt;/p&gt;

&lt;p&gt;In the process of updating './likelihood_server.cpp' to reflect these changes.  At the moment, getting an &quot;invalid operation&quot; when rendering.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It was a stray &lt;code&gt;glUseProgram(notzero)&lt;/code&gt; in the shader loading code.&lt;/p&gt;

&lt;hr /&gt;
</description>
				<pubDate>Tue, 15 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/15/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/15/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Troubleshooting silhouette rendering</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Silhouettes are rendering badly&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-13-dump_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looks like internal edges are rendering.  Likely an issue with shader_src/silhouette2.gs.glsl.&lt;/p&gt;

&lt;p&gt;Need a quick way to tweak shader and re-run.&lt;/p&gt;

&lt;h2&gt;Building shader debugging mode&lt;/h2&gt;

&lt;p&gt;if enabled, results will display in viewer instead of being passed to likelihood.&lt;/p&gt;
</description>
				<pubDate>Mon, 14 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/14/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/14/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Four days of OpenGL Debugging</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Status&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Server is now running, connecting, receiving and decoding messages successfully.&lt;/li&gt;
&lt;li&gt;Matlab client is constructing, sending messages, and destroying successfully.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Since we've sent our first message and saved it to a file, we no longer need matlab to debug the server, using the --dbg-load-message flag.&lt;/p&gt;

&lt;h2&gt;OpenGL crashing &lt;/h2&gt;

&lt;p&gt;In render_util.cpp -&gt; render_silhouettes(), crash is occurring somewhere in these lines.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;39      GLenum draw_buffers[2] = {GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1};
(gdb) list
40      glDrawBuffers(2, draw_buffers);
41
42      glClearColor(0.0,0.0,0.0,0.0);
43      glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
44      glEnable(GL_DEPTH_TEST);
45      DBG_GL_ETX();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The dreaded &quot;invalid operation&quot;&lt;/p&gt;

&lt;p&gt;I'm guessing opengl isn't in a valid rendering state yet; we may not have bound the fbo, or forgot to enable a rendering program.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;yep, fbo wasn't bound.&lt;/p&gt;

&lt;p&gt;now running to completion, but result is NaN.&lt;/p&gt;

&lt;p&gt;Probably because we're using random numbers for our curves.  Lets grab some real curve data and re-run.  We'll probably need to visualize the silhouette output soon.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;added function &lt;code&gt;wacv-2012/get_wacv_result.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Code to get curves and send them to server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp = get_wacv_result(2);
tmp = cellfun(@(x) [x; ones(1,size(x,2))], tmp, 'UniformOutput', false);
socket = construct_render_client('localhost', '12345')'
send_curves(socket, tmp);
result = send_curves(socket, tmp);
destroy_render_client(socket)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Got first dumps.  issues&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;all black, no content&lt;/li&gt;
&lt;li&gt;reversed wrong aspect ratio&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Possible causees for 1.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;bad camera files&lt;/li&gt;
&lt;li&gt;bad code for converting intrinsic matrix to opengl&lt;/li&gt;
&lt;li&gt;bad curves?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;2 seems most likely.  Can debug 1 and 2 by trying old camera object and see if they work;  if so, compare matrices with new matrices.&lt;/p&gt;

&lt;h2&gt;Possibly the mods I made to the multi-view likelihoods are the culprit.  I'm using raw matrices now instead of cmara objects in obj.add_view() and Camera_view_wrapper.&lt;/h2&gt;

&lt;p&gt;Added command-line option &quot;--dbg-save-frames&quot;.  Writes all rendered views to disk as &quot;dump_1.png, dump_2.png,...&quot;.  Only the most recent 10 are kept at any time.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;re-running using turntable camera; passing intrinsic and extrinsic matrices to likelihood.&lt;/p&gt;

&lt;p&gt;Renders are still black.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Forgot to set white color?&lt;/p&gt;

&lt;p&gt;No; its in the render_silhouette function.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found something: wacv results were aritficially centered on zero.  fixed.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Did matlab test on modelview and projection matrices and curve points -- z coordinate falls outside of [-1 1].  Beyond far plane.&lt;/p&gt;

&lt;p&gt;reason: not negating z-coordinate in test code.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;changing gears.  running matlab test on original input modelview and projection matrices.&lt;/p&gt;

&lt;p&gt;Modelview looks okay. Projection is way off.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;intrinsic matrix?&lt;/li&gt;
&lt;li&gt;NDC matrix?&lt;/li&gt;
&lt;li&gt;bounds?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Intrinsic looks reasonable.&lt;/p&gt;

&lt;p&gt;probably NDC/bounds issue&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bug in ndc math&lt;/p&gt;

&lt;p&gt;projecting outside from far plane&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;needed to flip axes to handle convention mismatch.&lt;/p&gt;

&lt;p&gt;Now getting something, but the positions are looking weird.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;was re-negating ppo; already flipped during convention resolution.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;now getting decent results, but off-center and smaller:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-12-dump_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;rough original to rendering offsets:  up by 56, left by 115.&lt;/p&gt;

&lt;h2&gt;non-uniform scaling; width is smaller than height.&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;Projecting plant base using
(a) intrinsic and extrinsic matrices from matlab
(b) modelview and projection matrices pulled from opengl

results:
(a) projects to the right place (tested using image and pixelstick)
(b) NDC coordinates look okay.  I manually remapped (scale and offset), and they look okay.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since NDC look okay, the remapping must be wrong.  glViewport issue?  I'm manually setting it, but maybe it needs to be set for each shader?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Or maybe it's a geometry shader issue?  I'm using different modelview and projection matrices now.&lt;/p&gt;

&lt;p&gt;=--&lt;/p&gt;

&lt;p&gt;Found it (Sunday night) -- Somehow the viewport transformation is getting reset. Possibly viewport is a shader-specific state that needs to be set each time?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;now centered, need to diagnose silhouette issues:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-13-dump_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;rename matlab &quot;render_client&quot; code to &quot;likelihood_client&quot;&lt;/li&gt;
&lt;li&gt;rename likelihood_server/sampler2.cpp to something not nonsense.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Fri, 11 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/11/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/11/work-log</guid>
			</item>
		
	</channel>
</rss>
