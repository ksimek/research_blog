<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Prior, visualized</title>
				<description>&lt;p&gt;Input tree.  (Mean of the local prior)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-input_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training tree.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-training_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sample from the locality prior only:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-local_prior_sample.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-local_sample_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-smooth_sampled_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Samples from full prior (including epipolar constraints)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-12-full_prior_samples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-full_prior_samples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how all corresponding points lie on the same epipolar line.&lt;/p&gt;

&lt;p&gt;The plot of eigenvalues of the prior covariance matrix suggest a very low-dimensional embedding:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-full_prior_eigs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Plotting log-sqrt eigenvalues compresses the dynamic range, making it easier to read:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-log_sqrt_prior_eigs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Wed, 14 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/14/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/14/reference</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;Spent morning considering covariance over loopy graphs.  Implemented and example of &quot;shortest geodesic distance&quot;, modified to be minimum sum of squared distances between junctions over all paths.  This is a generalization of our metric over tree-structured graphs.  As anticipated, with loopy graphs, this results in non-positive-definite covariance matrices.&lt;/p&gt;

&lt;p&gt;Consider this example, with two paths between nodes (1) and (4), one twice as long as the other.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-13-exmple_graph.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The shortest squared-distance matrix is&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 0     1     4     9     4    13
 1     0     1     4     5     8
 4     1     0     1     8     5
 9     4     1     0    13     4
 4     5     8    13     0     4
13     8     5     4     4     0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code for this is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d1 = 0:3; % route 1 distances
d2 = 0:2:6; % route 2 distances
D1 = pdist2(d1&#39;, d1&#39;, &#39;euclidean&#39;).^2;
D2 = pdist2(d2&#39;, d2&#39;, &#39;euclidean&#39;).^2;
G  = sparse(size(D));
% top-left block is simply a straight path
G(1:length(d1),1:length(d1)) = D1;
% for bottom-left, compute shortest path between nodes through
% junctions
for i = 1:(length(d2)-2)
  i_ = length(d1)+i;
  for j = 1:length(D)
    j_ = j;
    if i_ == j_, G(i_,j_) = 0; G(j_,i_) = 0; continue, end
    G2 = sparse(4,4);
    G2(1,2) = D(i_, 1);
    G2(1,3) = D(i_, length(d1));
    G2(2,3) = D(1,length(d1));
    G2(1,4) = D(i_, j_);
    G2(2,4) = D(1,j_);
    G2(3,4) = D(length(d1),j_);
    d = graphshortestpath(G2&#39;, 1,4, &#39;directed&#39;, false);
    G(i_, j_) = d;
    G(j_, i_) = d;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At large enough scales, the squared-exponential covariance matrix has negative eigenvalues:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scale = 10;
min(eig(exp(-full(G)/(scale^2*2))))

    ans =

       -0.0134
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, this covariance function is not positive definite.&lt;/p&gt;

&lt;h2&gt;A loopy graph strategy&lt;/h2&gt;

&lt;p&gt;We need to be able to embed the graph into a Euclidean space that preserves most of the interesting properties.&lt;/p&gt;

&lt;p&gt;Here&#39;s a simple approach: construct a graph with junctions as vertices and geodesic distance as edge weight.  Find the graph&#39;s minimum spanning tree, and transform the tree into curve-tree coordinates.  For unclaimed curve segments, linearly interpolate between the coordinates of the endpoints.&lt;/p&gt;

&lt;p&gt;This is nice, but if a
A loopy graph strategyn unclaimed curve segment&#39;s length is perticularly long, it should act differently than a short (straight) line between the points.  To handle this, we introduce a new dimension for each unclaimed curve segment, and allow the transformed segment to have longer length by dipping into this dimension to some extent (with a limit of none for straight lines).&lt;/p&gt;

&lt;p&gt;Ideally, all pairs of points on the transformed segment will have the same distance as their geodesic distance in untransformed space.  However, this is not possible, because it would contradict the fact that the endpoint distance is defined by the graph distance in the minimum spanning tree.  At least we can try to preserve local distances, and a parabolic arc is a reasonable choice for this.   How to derive a parabola equation of an appropriate length?&lt;/p&gt;

&lt;p&gt;Given the distance between the end-points and the length of the desired curve, we seek a symmetric parabola with an appropriate arc length.   Given arc length and end-points, we can compute the focus the parabola (f), and from that we can compute the parabolic equation y = x&lt;sup&gt;2&lt;/sup&gt;/(4f).&lt;/p&gt;

&lt;h2&gt;Training foreground likelihood&lt;/h2&gt;

&lt;p&gt;First, reproject curve into second image.&lt;/p&gt;

&lt;p&gt;Draw as medial axis w/ width.&lt;/p&gt;

&lt;p&gt;Compute inverse medial axis.&lt;/p&gt;

&lt;p&gt;Grab all foreground/background pixels in probability map.&lt;/p&gt;

&lt;p&gt;create histogram.  Try smoothing and/or annealing.&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;p&gt;Foreground model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-lik_fg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Background model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-lik_bg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Tue, 13 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/13/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/13/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Training epipolar prior.&lt;/p&gt;

&lt;p&gt;Fixing local prior using parameters learned yesterday.  The computing the epipolar and full prior is more computationally intensive, since it involves twice as many dimensions.  To mitigate, I eliminated every other point from the data.&lt;/p&gt;

&lt;p&gt;Without the epipolar prior, the data likelihood is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood (baseline) =  -2471.12 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fixing euclidean scale and variance to zero, I trained the epipolar variance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood =  -2395.05
training_result = 

    epipolar_variance: 4.39
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By allowing some euclidean-correlated variance, the iid variance should be able to drop a bit.  Fixing euclidean scale to 1/100&lt;sup&gt;2&lt;/sup&gt;, I trained epipolar_variance and euclidean_variance jointly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood = -2394.88
training_result =

    epipolar_variance: 4.3214
      euclidean_scale: 1.0000e-04
   euclidean_variance: 0.0335
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now training all three:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood = -2394.57
training_result = 

     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now training all 10 parameters together:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood = -1294.59
training_result = 

     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
        noise_variance: 0.0821
     geodesic_variance: 193.9141
        geodesic_scale: 0.0031
branch_linear_variance: 0.0800
 branch_const_variance: 6.1725
       linear_variance: 0.1561
        const_variance: 685.7670
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;...but we actually want noise_variance to be fixed to 1.  Repeating previous run with noise_variance=1 results in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood = -2381.29
training_result = 

     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
        noise_variance: 1
     geodesic_variance: 33.6972
        geodesic_scale: 0.0011
branch_linear_variance: 0.1028
 branch_const_variance: 6.9943
       linear_variance: 0.0737
        const_variance: 1.8641e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;running for 30 more iterations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ll = 2376.76
     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
        noise_variance: 1
     geodesic_variance: 160.7604
        geodesic_scale: 5.3712e-04
branch_linear_variance: 0.1028
 branch_const_variance: 6.9943
       linear_variance: 0.0835
        const_variance: 2.1123e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;The fullly trained model is then:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
        noise_variance: 1
     geodesic_variance: 15.9174
        geodesic_scale: 0.0019
branch_linear_variance: 0.0485
 branch_const_variance: 3.7438
       linear_variance: 0.0348
        const_variance: 685.7670
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The samples from this prior look very nice:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-12-full_prior_samples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Motion is restricted to be near the epipolar lines, and the resulting curves remain smooth.&lt;/p&gt;

&lt;p&gt;Matlab says only 314 out of 3766 are nonnegligible.&lt;/p&gt;
</description>
				<pubDate>Mon, 12 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/12/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/12/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;Finished training.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;training_result = 

         final_const_variance: 1.1669e-04
  final_branch_const_variance: 1.7636e+03
        final_linear_variance: 1.4425e+03
      final_geodesic_variance: 195.4912
         final_geodesic_scale: 0.0065
                  final_noise: 0.0776

training_result.initial
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  ans =&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;         const_variance: 831.7886
  branch_const_variance: 0.8552
        linear_variance: 0.1183
     euclidean_variance: 1.7367
        euclidean_scale: 1.0000
      geodesic_variance: 225.8247
         geodesic_scale: 1.0000
      epipolar_variance: 1.7367
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These just don&#39;t make sense.  It allows for almost zero total translation, but lots of pert-curve translation (const_variance vs. branch_const_variance).  Linear variance allows for way too much scaling.  Geodesic variance seems sensible, but the (inverse-squared) geodesic scale is way too low... Basically this becomes a stand-in for const_variance.  Perhaps this is just a local minimum, we could try fixing const_variance to some large number and forcing geodesic variance to serve its intended purpose.&lt;/p&gt;

&lt;p&gt;But the strangest thing is that&lt;/p&gt;

&lt;p&gt;I sampled some curves from this prior, and they are about what I&#39;d expect: rotated and scaled with very little nonrigid deformation.   Below is a sample, along with the mean tree barely visible in the center:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-tree_sample.png&quot; alt=&quot;tree sample&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Branches remain connected to tree, despite large per-curve offset variance.&lt;/li&gt;
&lt;li&gt;covariance matrix is singular&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Ruled-out causes of error:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;badly implemented logmvnpdf.  Tested against reference implementation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Training observations&lt;/h2&gt;

&lt;p&gt;The simulated annealing algorithm is very sensitive to our local minima.  It can sometimes find a good result in less than 500 iterations, but other times it gets stuck after less than 100 and never improves.  I can probably tweak the annealing schedule to increase temperature earlier(is &quot;reannealing&quot; the right value to tweak?&quot;).&lt;/p&gt;

&lt;p&gt;The patternsearch algorithm beats simulated annealing after only 16 iterations (vs. 5k or more).   Annealing really struggles to get out of that minimum.&lt;/p&gt;

&lt;p&gt;Surprisingly, after adding a seventh parameter (for branch-specific affine deformation), patternsearch totally fails to find a good minimum starting from default parameters.  Initializing it with the result of the six-parameter model gives much better results.  This suggests a coarse-to-fine strategy for training might work well in general.  However, this is a very specific case:  the six-parameter model was flexible enough to explain the data quite well, but was overly permissive -- it allowed too many nonsensical solutions.  Adding the seventh parameter introduced an alternative explanation of the data, one requiring less nonrigid deformation.&lt;/p&gt;

&lt;p&gt;To try: genetic algorithm;  particle swarm; simplex method; start pattern search from SA result; globalsearch; grid search with multistart&lt;/p&gt;

&lt;p&gt;Globalsearch &amp;amp; multistart w/
  simplex
  fminunc&lt;/p&gt;

&lt;h2&gt;Misc results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-tree_sample2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likleihood = -3.1787e+03
training_result = 

         final_const_variance: 6.0197e+06
  final_branch_const_variance: 1.5774e-09
        final_linear_variance: 1.6250
      final_geodesic_variance: 238.6891
         final_geodesic_scale: 0.0039
         final_noise_variance: 0.1487
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not bad.  Lots of geodesic variance to account for distortion.  Why is const variance so high?  Linear variance seems a bit high, but reasonable.&lt;/p&gt;

&lt;p&gt;It might be a good idea to allow each branch to have a linear distortion relative to its initial point.&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.5318e+03
training_result = 

       final_const_variance: 2.3156e+09
final_branch_const_variance: 2.4988
      final_linear_variance: 33.8221
    final_geodesic_variance: 1.7165e+03
       final_geodesic_scale: 0.0029
       final_noise_variance: 0.0880
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much smaller noise variance, much higher for all other variances.  const variance is astronomical... is the optimizer exploiting numerical error arising from a poorly-conditioned matrix?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-tree_sample6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.4848e+03
training_result =

       final_const_variance: 0.1383
final_branch_const_variance: 1.7466
      final_linear_variance: 5.4218e-04
    final_geodesic_variance: 378.6715
       final_geodesic_scale: 0.0048
       final_noise_variance: 0.0832
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is close to what I would hope for -- small constant and linear variance, most variance coming from deformation.  Still, the scale is fairly short (~14 pixels), allowing for quite a bit of deformation.&lt;/p&gt;

&lt;p&gt;Ran for several thousand more iterations.  Seem to be converging.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.4799e+03
training_result =

       const_variance: 0.0717
branch_const_variance: 3.2826
      linear_variance: 0.0028
    geodesic_variance: 401.4291
       geodesic_scale: 0.0047
       noise_variance: 0.0831
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Running from scratch with the &quot;pattern_search&quot; algorithm, I got excellent results very quickly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.4256e+03
training_result =

         const_variance: 261.7339
branch_const_variance: 3.0796
      linear_variance: 0.0035
    geodesic_variance: 165.2169
       geodesic_scale: 0.0063
       noise_variance: 0.0821
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are the most sensible results yet!  Moderately large constant variance , small but nonnegligible branch constant variance.  A touch of linear variance to rotations.  Smaller deformation variance in response to larger const variance.  Yes!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Adding a new parameter assigning independent affine deformation variance to each branch.  This should allow less variance to be allotted to geodesic_variance, which should improve marginal-likelihood.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-sampled_tree9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Initial training pass failed to find the old local minimum.  Restarting it using the previous optimum resulted in very nice parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.3107e+03
training_result = 

        const_variance: 291.9853
 branch_const_variance: 2.0196
       linear_variance: 0.0366
branch_linear_variance: 0.0516
     geodesic_variance: 12.7402
        geodesic_scale: 0.0136
        noise_variance: 0.0796
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the dramatic drop in geodesic variance and increase in marginal likelihood.  On the other hand, the (corrected) geodesic scale dropped from(~12 to ~8), meaning slightly higher effective dimension.  The resulting tree looks much closer to the original shape than previous models.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Previous model had a problem: allowing each curve to affine-transform independently results in parents drifting away from children, which in reality isn&#39;t possible.  We need each child to inherit the covariance of the parent, which we accomplish by simply taking the dot-product of the &quot;geodesic&quot; index.&lt;/p&gt;

&lt;p&gt;The pattern_search algorithm is having a hard time finding a better set of&lt;/p&gt;

&lt;p&gt;Starting from the same position as previous starting point, results in a worse log-likelihood:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.3131e+03
training_result = 

        const_variance: 3.1017e-25
 branch_const_variance: 2.9157
       linear_variance: 0.0326
branch_linear_variance: 0.0575
     geodesic_variance: 12.5426
        geodesic_scale: 0.0137
        noise_variance: 0.0796
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Zero const variance is troubling.  Other parameters are mostly unchanged, except slightly higher branch_const_variance.  It seems the translation component was mostly used to align curves affected by parallax.&lt;/p&gt;

&lt;p&gt;Actually, zero const variance isn&#39;t so surprising, because our now model alls branches to pivot around their starting point, and the root of the tree hardly moves between views.  What movement there is could be solved by pivoting the whole tree around it&#39;s center, using the global affine transformation.&lt;/p&gt;

&lt;p&gt;It seems we have a redundant representation here.  The branch affine transformations subsume the global affine transformation, unless there&#39;s evidence that a multi-level type of model is reasonable (which it might be).&lt;/p&gt;

&lt;p&gt;Starting from the result of the previous run gives slightly better results, but still not better than the &quot;bad&quot; model used previously.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.3122e+03
training_result = 

        const_variance: 498.3842
 branch_const_variance: 2.9623
       linear_variance: 0.0400
branch_linear_variance: 0.0523
     geodesic_variance: 12.0562
        geodesic_scale: 0.0138
        noise_variance: 0.0799
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Now optimization approach, adding one dimension at a time and reoptimizing. Results are almost identical to second-to-last run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood:  -2.3131e+03
training_result = 

        noise_variance: 0.0800
     geodesic_variance: 12.3965
        geodesic_scale: 0.0136
branch_linear_variance: 0.0576
 branch_const_variance: 2.9157
       linear_variance: 0.0327
        const_variance: 8.3205e-09
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, low const_variance, but we manually can do 1D optimization to improve it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-ml_vs_const_variance.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;training_result = 

        noise_variance: 0.0800
     geodesic_variance: 12.3965
        geodesic_scale: 0.0136
branch_linear_variance: 0.0576
 branch_const_variance: 2.9157
       linear_variance: 0.0327
        const_variance: 435.8995
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Some plots showed that at low const_variance, the terrain become rocky, making search difficult.&lt;/h2&gt;

&lt;p&gt;Forcing noise variance to be 1.0 results in much smoother priors:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-smooth_sampled_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;training_result = 

        noise_variance: 1
     geodesic_variance: 15.9174
        geodesic_scale: 0.0019
branch_linear_variance: 0.0485
 branch_const_variance: 3.7438
       linear_variance: 0.0348
        const_variance: 685.7670
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This kind of makes sense, since our point-correspondence algorithm has mean error of 0.5, even if correspondences are perfect.  In many cases, correspondences aren&#39;t perfect, because the triangulation metric used to find correspondences isn&#39;t sufficiently informative to infer the true correspondence.  So forcing i.i.d. noise variance to some minimum value prevents the prior from taking up that noise.&lt;/p&gt;

&lt;h2&gt;misc notes&lt;/h2&gt;

&lt;p&gt;Try different deformation model - cubic spline?&lt;/p&gt;

&lt;p&gt;euclidean affine transform&lt;/p&gt;

&lt;p&gt;next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;train epipolar likelihood&lt;/li&gt;
&lt;li&gt;MCMC

&lt;ul&gt;
&lt;li&gt;from sample to raster&lt;/li&gt;
&lt;li&gt;train bernoulli likelihood&lt;/li&gt;
&lt;li&gt;adaptive MH (or test with just simulated annealing)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Dangling tasks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;train on multiple different view-pairs&lt;/li&gt;
&lt;li&gt;ground-truth and train on neighboring views (rather than four-separated)&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 11 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/11/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/11/work-log</guid>
			</item>
		
			<item>
				<title>Trained prior parameters</title>
				<description>&lt;p&gt;Training only local prior w/ noise_variance fixed at 1.0:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        noise_variance: 1
     geodesic_variance: 15.9174
        geodesic_scale: 0.0019
branch_linear_variance: 0.0485
 branch_const_variance: 3.7438
       linear_variance: 0.0348
        const_variance: 685.7670
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Training full model (local prior and epipolar prior) w/ noise variance fixed at 1.0&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2368.62

     epipolar_variance: 4.5465
    euclidean_variance: 0.0422
       euclidean_scale: 4.2001e-07
        noise_variance: 1
     geodesic_variance: 233.9050
        geodesic_scale: 4.3327e-04
branch_linear_variance: 0.1079
 branch_const_variance: 7.0217
       linear_variance: 0.1016
        const_variance: 2.4123e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  Notice significantly larger geodesic variance, and much longer scale length (48 vs 22 pixels).  All other variances increased too.  This is probably because we&#39;ve treated the two priors as independent, but they aren&#39;t, so multiplying them results in too little overall variance.&lt;/p&gt;

&lt;p&gt;By comparison, below is a nearby local minimum, obtained by training all 9 parameters from scratch.&lt;/p&gt;

&lt;p&gt;  log-likelihood:  -2370.05&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       epipolar_variance: 4.6435
      euclidean_variance: 3.1113e-35
         euclidean_scale: 7.4415e+152
          noise_variance: 1
       geodesic_variance: 283.5387
          geodesic_scale: 3.9692e-04
  branch_linear_variance: 0.1236
   branch_const_variance: 6.5436
         linear_variance: 0.0746
          const_variance: 5.4990e-109
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This model moves all epipolar prior variance into the iid epipolar_variance variables. Also the local prior&#39;s offset variance has been moved into the deformation variance, geodesic_variance, while shortening the scale.&lt;/p&gt;
</description>
				<pubDate>Sun, 11 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/11/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/11/reference</guid>
			</item>
		
			<item>
				<title>Projection of point onto line using distance from two reference points</title>
				<description>&lt;p&gt;Consider two known points A, and B, an unknown point C. If we know the distances between C and the other two points, we can recover \(\pi_{AB}(C)\), the projection of C onto the line AB.&lt;/p&gt;

&lt;p&gt;The distance between \(\pi_{AB}(C)\) and A is&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
d(A, \pi_{AB}(C)) = d(A,B) (1+g) / 2 \text{, where} \\
g &amp;= \left ( d(A,C)^2 - d(B,C)^2 \right ) / d(A,B)^2
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;This can be derived using the pythagorean theorem and fact triangles AC\pi(C) and BC\pi(C) share a side.&lt;/p&gt;

&lt;p&gt;This can be used to derive an expression for \(\pi_{AB}(C)\) using a weighted sum of A and B:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\pi_{AB}(C) &amp;= (A(1-g) + B(1+g) ) / 2
\end{align}
\]
&lt;/div&gt;



</description>
				<pubDate>Fri, 09 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/09/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/09/reference</guid>
			</item>
		
			<item>
				<title>Geodesic distance kernel and BGP kernel -- simplified representation</title>
				<description>&lt;p&gt;With a nod to Mercer&#39;s theorem, we can represent gaussian processes over graph-structured points in a very simple way.  We may transform our input space over graph nodes into a new eucliean space.&lt;/p&gt;

&lt;p&gt;For trees with N branches, we denote a point a distance x along the i-th branch by the ordered pair \(i, x\).  For each branch i, its parent is denoted by p(i), and its branch point is denoted by b(i).  If branch i has not parent, we define p(i) = b(i) = 0.  We can define a transformation \(\phi : \mathbb{N}\times\mathbb{R} \rightarrow \mathbb{R}^N\), whose k-th dimension is:&lt;/p&gt;

&lt;p&gt;  &lt;div&gt;
  [
    \phi_k(i,x) = \begin{cases}
        x &amp;amp; \text{ if } i == k \
        0 &amp;amp; \text{ if } i == 0 \
        \phi_k(p(i), b(i)) &amp;amp; \text{ otherwise.}
        \end{cases}
  ]
  &lt;/div&gt;.&lt;/p&gt;

&lt;p&gt;If branch k is the root of a subtree containing (i,x), \(\phi_k\) is the branch position of sub-subtree containing (i,x).&lt;/p&gt;

&lt;p&gt;This formulation makes it easier to define covariance functions over tree structures.  O&lt;/p&gt;

&lt;p&gt;It is often useful to model each tree branch as a squared-exponential curve.  This is sometimes falsely implemented as squared exponential covariance function over geodesic distance instead of Euclidean distance.  However, this covariance function isn&#39;t positive definite (need illustration).  Instead, if we define a distance function \(d(i,x,i&#39;,x&#39;)\) as the sum of squared geodesic distances between adjacent junctions along the path between (i,x) and (i&#39;,x&#39;).  Using this distance metric with a squared-exponential covariance function results in exactly the model we seek.&lt;/p&gt;

&lt;div&gt;
  k(i,x,i&#39;,x&#39;) = \exp\{-d(i,x,i&#39;,x&#39;)\}
&lt;/div&gt;


&lt;p&gt;We can use our transformation above to represent this more succinctly:&lt;/p&gt;

&lt;div&gt;
  k(\phi, \phi&#39;) = \exp\{- \| \phi - \phi&#39; \|^2\}
&lt;/div&gt;


&lt;p&gt;Since each dimension in phi corresponds to exactly one branch, \( \phi - \phi \) is a vector of distances between branch points on the each curve corresponding to (i,x) and (i&#39;,x&#39;).  The squared l2 norm of this expression is equivalent to d(i,x,i&#39;,x&#39;).  Note that the fact that we can represent this covariance function using a standard covariance function with transformed inputs shows that it is positive definite.&lt;/p&gt;

&lt;p&gt;A second useful tree model is the one I introduced in my dissertation proposal -- the branching cubic spline model.  The covariance function for this model involved a recursive function that was complicated and wasn&#39;t previously proven to be positive definite.  Using our input transformation, this simplifies to:&lt;/p&gt;

&lt;div&gt;
\[
  k(\phi, \phi) = \sum_i k_c(\phi_i, \phi&#39;_i)
  \]
&lt;/div&gt;


&lt;p&gt;where k_c() is the cubic spline covariance function.  The recursive nature of the definition of \(\phi\) ensures that each curve inherits the covariance of its parent curve.  And because \(k_c(0,x) == 0\), points on different subtrees only receive covariance from their shared ancestors.  Again, this formulation suffices to show that our original covariance function is positive definite.&lt;/p&gt;

&lt;h2&gt;Loopy graphs&lt;/h2&gt;

&lt;p&gt;Non-tree structured graphs are more difficult, but I have a few possible approaches.&lt;/p&gt;

&lt;p&gt;One way of interpreting a loop is as two separate branches (one at each junction) that gradually blend into one another.  Implementing a transformation \(\phi\) is easy under this interpretation -- just linearly interpolate between the two independent lines in \(\phi\).  Unfortunately, the resulting embedding doesn&#39;t preserve distances between nearby points. I tried a similar approach with cubic Hermite splines instead of linear interpolation, but again, distances aren&#39;t preserved.  This could be solved if we could (a) guarantee that the bridging curve has exactly the right length and (b) the mapping between graph points and the bridging curve in \(\phi\) preserves distance.  Both of these result in equations that can&#39;t be solved analytically, but numerical techniques could work if we cared enough.&lt;/p&gt;

&lt;p&gt;We could try something similar, but use a spherical arc to connect the two branch points in \(\phi\).  This would preserve total length and adjacent distances, but it violates the property that the curve&#39;s endpoint is orthogonal to its parent.&lt;/p&gt;

&lt;p&gt;Both of these bridging techniques also violate the elegant property of the tree-based covariance, namely that the L1 distance between points is equal to thier geodesic distance.&lt;/p&gt;

&lt;p&gt;A third possibility is to simply use the distance function \(d(i,x,i&#39;,x&#39;)\) we introduced before.  The down side of this is I con&#39;t think of a proof for its positive-definiteness.  But if it is PD, it should have the properties we want, and is relatively easy to implement.&lt;/p&gt;
</description>
				<pubDate>Thu, 08 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/08/reference</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/08/reference</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;



</description>
				<pubDate>Sun, 04 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/04/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/04/work-log</guid>
			</item>
		
			<item>
				<title>Skeleton deformation fitting</title>
				<description>&lt;p&gt;Since the skeletons are very different between views, the old graph-matching approach probably won&#39;t work.  I believe the graph deformation idea is still a good one, but the likelihood involving correspondences won&#39;t work because it doesn&#39;t handle false positive or false negative parts of the graph.&lt;/p&gt;

&lt;p&gt;A better idea is to go back to our old approach: use a pixel-based likelihood to evaluate deformed graphs.  Use MCMC to explore the space and use the GP prior covariance for proposals or search directions.  After computing the cholesky decomposition, this should proceed relatively quickly, even for large point sets.&lt;/p&gt;

&lt;p&gt;The model is the medial axis transform of the original image, plus GP-distributed deformation, which includes an epipolar constraint.  To render into the second image, we perform an inverse medial axis transform.&lt;/p&gt;

&lt;p&gt;Because of the epipolar constraint, the number of effective dimensions is nearly halved.  The correlation between nearby points should reduce dimension by an even more significant factor.&lt;/p&gt;

&lt;h2&gt;Initial tasks&lt;/h2&gt;

&lt;p&gt;Train GP on ground truth data.
  (try adding runners between almost-bridged points)
Train Bernoulli distribution over foreground, background&lt;/p&gt;

&lt;p&gt;Algorithms
    binary image to graph
    All pairs shortest path.
    polyline w/ interpolated intensity
  x medial axis transform
  x Inverse medial axis transform
    specialized mcmc
        adaptive metropolis hastings
        hit and run sampling?&lt;/p&gt;

&lt;h2&gt;Possible extensions:&lt;/h2&gt;

&lt;p&gt;  use a weaker distance function for geodesic distance (huber function?)
  make geodesic distance greater at junctions&lt;/p&gt;
</description>
				<pubDate>Fri, 02 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/02/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/02/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;  Some improvements to the approach outlined at the end of previous post...&lt;/p&gt;

&lt;p&gt;  Tweak epipolar likelihood to add a small affine and nonlinear distortion component
  (e.g. due to lens distortion).  this allows epipolar variance to be smaller;
  forces deviations to be spatially correlated.&lt;/p&gt;

&lt;p&gt;  Find some good correspondences; fix camera distortion
      approach 1: distort image using GP and known correspondences.
      approach 2: optimize camera distortion parameters under GP and known correspondences&lt;/p&gt;

&lt;p&gt;  Use unambiguous correspondences first.&lt;/p&gt;

&lt;p&gt;  I fear that graph topology may be too noisy and fluxuating to be a reliable source
  of information.  Is keypoint matching with epipolar constraint enough to solve this?
  Maybe with added spatial distortion?&lt;/p&gt;

&lt;p&gt;  How can adding correspondences reduce uncertainty in remaining points?  The approach
  above can reduce uncertainty to almost lying on the epipolar line.   If we can recover
  small subgraphs, those can help too.  But will false joints break things? (this would
  be a good experiment)  Maybe just linear sections with interior keypoint nodes.
  This should at least improve fine-scale matching later.&lt;/p&gt;

&lt;p&gt;  2D gives us lateral branch candidates, but are ambiguous.  Can resolve them in 3D?&lt;/p&gt;

&lt;p&gt;  Idea: branch points have higher geodesic distance?&lt;/p&gt;

&lt;p&gt;  Simpler problem: Camera repair for volumetric reconstruction.&lt;/p&gt;

&lt;p&gt;  Sift keypoint matching
    nonlinear distortion fix
    ransac matching keypoints + distortion correction&lt;/p&gt;
</description>
				<pubDate>Thu, 01 Jan 2015 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2015/01/01/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2015/01/01/work-log</guid>
			</item>
		
	</channel>
</rss>
