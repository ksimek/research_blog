<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Work Log - Training Bugs</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14852&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing training of OU perturb model.&lt;/p&gt;

&lt;p&gt;Yesterday, we had problems with the perturb scale exploding.  Have switched to using a logistic sigmoid instead of an exponential to map perturb_scale, which sets a lower-bound and and upper-bound during optimization.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;perturb_scale is now staying low, but stil getting weird results.&lt;/p&gt;

&lt;p&gt;perturb_position_variance wants to be 9000+ ???&lt;/p&gt;

&lt;p&gt;perturb_rate_variance wants to be ~40?&lt;/p&gt;

&lt;p&gt;perturb_smoothing_variance wants to be ~0.0.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Trying new kernel: ind_kernel.  Like OU and SQEXP kernels, but no correlation between perturbations.  It's an intermediate step between no-perturb and ou-perturb, since it allows perturbations, but doesn't need a scale-length parameter.&lt;/p&gt;

&lt;p&gt;Training results for ind_kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_3 = 

            smoothing_variance: 0.0024
                noise_variance: 0.0132
             position_variance: 1.5747e+04
                 rate_variance: 0.1709
    perturb_smoothing_variance: 0.0020
         perturb_rate_variance: 26.3458
     perturb_position_variance: 0.8770
                 perturb_scale: 1.7199
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Very weird that perturb_rate variance&lt;/p&gt;

&lt;p&gt;Consider limiting number of dimensions somehow...&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Force both smoothing_variances to be the same&lt;/li&gt;
&lt;li&gt;remove perturb_scale parameter&lt;/li&gt;
&lt;li&gt;???&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;....&lt;/p&gt;

&lt;p&gt;recall that handling view-index in kernels has never been thoroughly tested...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Let's visualize results for the ind_perturb model, and see if everything looks reasonable.&lt;/p&gt;

&lt;p&gt;Created &lt;code&gt;test/tmp_visualize_test.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Interesting...  Some curves are &quot;reversed&quot;, i.e. the base is the &quot;end&quot; of the curve and the tip is the &quot;beginning&quot;.  This has some unintended consequences when applying the perturb model, because the tips are where perturbations are greatest, but when curves are reversed, the tips aren't affected by most of the modelled perturbations.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Weird.  As perturb_rate_variance increases, the per-view curves spread out radially like flower petals.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-1.png&quot; alt=&quot;high perturb_rate_variance causes radial spreading of curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would have expected that the likelihood would take over, but this is clearly not happening.  Lets look at the likelihood...&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-2.png&quot; alt=&quot;Maximum-likelihood solution.  Posterior should revert to this as variance increases asymtotically.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We must be running up against numerical precision errors.  Let's look at the equation for maximum posterior:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;When \(\Sigma_0\) has huge eigenvalues, this equation basically reduces to&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
\mu_P = (\Sigma_0 \Sigma_l^{-1})^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l 
     &amp;= \Sigma_l \Sigma_0^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l
     &amp;= \Sigma_l \Sigma_l^{-1} \mu_l
     &amp;= \mu_l
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;However, the effective cancellation of \Sigma_0 can't occur, because the expression \((\Sigma_0 \Sigma_l&lt;sup&gt;{-1}&lt;/sup&gt; + I)\) has a huge condition number, which makes inverting it an unstable operation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Anyways, that's visualization.  Is the same issue arising in ML computations?  Higher rate variance can result in significantly larger condition numbers, and IIRC, we don't take any special steps to handle such issues in the ML  computation anymore (for example, using the matrix inversion lemma).&lt;/p&gt;

&lt;p&gt;Maybe we should force rate variance to be within a reasonable range.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Even when perturb_rate_variance is reasonable (1.0), we still get drifting:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-3.png&quot; alt=&quot;perturb_rate_variance == 1.0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The degree of this phenomenon seems to be a smooth function of perturb_rate_variance.  If it was a numerical instability issue, we'd see it arise abruptly, indicating we had entered the unstable regime.&lt;/p&gt;

&lt;p&gt;I'm now thinking this is a real issue with the model, not simply an artifact of computation.  Need to think more about it.&lt;/p&gt;
</description>
				<pubDate>Mon, 05 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting training of OU perturbation model (&lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Found bug in &lt;code&gt;tr_curves_ml.m::ou_kernel&lt;/code&gt; -- &lt;code&gt;smoothing_variance&lt;/code&gt; was used where &lt;code&gt;perturb_smoothing_variance&lt;/code&gt; should have been (copy-paste bug).  Also fixed in &lt;code&gt;sqexp_kernel&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Optimizer appears to be running much more smoothly now.  Probably because there previously was a strong correlation, due to &lt;code&gt;smoothing_variance&lt;/code&gt; appearing in two different parts of the equation.  The resulting ridge could have caused slow progress.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Initial params:&lt;/p&gt;

&lt;p&gt;train_params =&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.2500
                noise_variance: 100
             position_variance: 90000
                 rate_variance: 2.2500
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 2.2500
     perturb_position_variance: 90000
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stopped after 3 iterations.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.4501
                noise_variance: 2.5982e-04
             position_variance: 6.5359e+04
                 rate_variance: 3.9645
    perturb_smoothing_variance: 3.3437e-06
         perturb_rate_variance: 0.7268
     perturb_position_variance: 3.0785e+04
                 perturb_scale: 0.5867

Final ML: 28423.438936
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final ML (2.84e4) is greater than the no-pertrub model (2.37e4).  This is expected, since this model has more parameters, so we can get a tighter fit (maybe overfitting).&lt;/p&gt;

&lt;p&gt;I like that noise variance is much smaller (close to the minimum--do I need to lower the floor?).  This is expected, because the only source of IID noise is the rasterization process.&lt;/p&gt;

&lt;p&gt;Position standard deviation is much higher than before (255 vs. 126), as is rate standard deviation (2.0 vs. 0.47).&lt;/p&gt;

&lt;p&gt;Perturb variances are harder to explain.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is low, which is nice to see, because the worst of the perturbations are arising from miscalibrations (rate and position perturbations), not deformations.  But it's still far smaller than I expected.  hOnestly, I was expecting it to be almost the same as &lt;code&gt;smoothing_variance&lt;/code&gt;, but I realize now it makes sense for it to be smaller.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perturb position variance is much, much larger than I expected.&lt;/strong&gt;  Standard deviation is 175.4, which is basically saying each plant has 17 cm of IID perturbations from the unobserved mean plant.   I have no good explanation for this, expect maybe that the initial value was ridiculously large.&lt;/p&gt;

&lt;p&gt;Pertub scale seems reasonable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Test sensitivity to initialization.  Initial values:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 0.0500
         perturb_rate_variance: 0.1000
     perturb_position_variance: 1
                 perturb_scale: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First four were initialized from the trained no-perturb model.&lt;/p&gt;

&lt;p&gt;Note the biggest change: perturb_position_variance changed from 9000 to 1.&lt;/p&gt;

&lt;p&gt;Terminated after 46 iterations; much better than the 3 iterations from last attempt.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0019
                noise_variance: 0.0721
             position_variance: 1.6267e+04
                 rate_variance: 0.2050
    perturb_smoothing_variance: 6.7471e-10
         perturb_rate_variance: 1.3201e-04
     perturb_position_variance: 403.7082
                 perturb_scale: 2.4888e+03

Final ML: 25163.231449
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Weird that this is lower than that poorly-initialized model.  Consider initializing using a combination of Attempt #1 and Attempt #2's initial values.&lt;/p&gt;

&lt;p&gt;Smoothing variance is much smaller than before, noise variance is larger.&lt;/p&gt;

&lt;p&gt;Position variance is in the same order-of-magnitude, but about 1/5 the magnitude.&lt;/p&gt;

&lt;p&gt;Rate variance is an OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is still near-zero.&lt;/p&gt;

&lt;p&gt;Perturb rate variance is 3 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb position variance is 2 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb scale is sooooooo high.  Basically flat, so the regular and perturb variances sum.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: Result is very sensitive to initialization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: When perturb scale is this high, there exists a ridge, due to perturb variances playing the same role as normal variances.  Note that when perturb scale is infinity, this model degenerates to the no-perturb model.  Notice that Final ML isn't much better than the no-perturb.  Possibly when perturb scale exploded, the model could no longer improve, so constraining perturb scale to some small set of values may be a good idea (sigmoid transform).&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Try smaller changes to initialization.&lt;/li&gt;
&lt;li&gt;Try sigmoid transform on perturb scale.&lt;/li&gt;
&lt;li&gt;Try training using standard deviations instead of variances.&lt;/li&gt;
&lt;li&gt;Try changing one or more trained values to sensible defaults and retrain.&lt;/li&gt;
&lt;li&gt;Work with standard deviations, not variances&lt;/li&gt;
&lt;li&gt;Fix some 'known' values and optimize others.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 04 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Running optimizer...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issue&lt;/strong&gt;: Likelihood variance is collapsing to zero.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Guess&lt;/strong&gt;: Training ML is different from naive ML and isn't penalizing noise correctly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test 1&lt;/strong&gt;: save params, compare training ml and naive ml&lt;/p&gt;

&lt;p&gt;Same result.&lt;/p&gt;

&lt;h2&gt;Discussion&lt;/h2&gt;

&lt;div&gt; The marginal likelihood is monotonically increasing as the noise variance \(\sigma_n^2\) approaches zero.  This shouldn't be happening, because as the likelihood variance collapses to a delta, the marginal likelihood should become equal to the prior evaluated at the (virtual) observed locations.&lt;/div&gt;




&lt;div&gt;
\begin{align}
    \lim_{\sigma_n \to 0} p(D) &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; p_l(D \mid x) dx &amp; \text{(Marginalization)}\\
         &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; f(D - x) dx &amp;   \text{(Convolution)} \\
         &amp;= \int p_0(x) \delta(D - x) dx \\
         &amp;= p_0(D) 
\end{align}
&lt;/div&gt;


&lt;p&gt;This implies there's a bug in the code causing this phenomenon; the mathematical model is not the cause.&lt;/p&gt;

&lt;h2&gt;Solved&lt;/h2&gt;

&lt;div&gt;Found the bug.  When computing the likelihood precision \(S\) from the unscaled precision \(S_0\), the noise variance \(\sigma_n^2\) was multiplied instead of divided:&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;S = S0 * sigma_n; // incorrect
S = S0 * (1/sigma_n); // corrected version
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Resuming Training&lt;/h2&gt;

&lt;p&gt;Had some trouble with noise sigmas being too low.  Solved by in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Attempts 1-3: clamping to a minimum value and then adding a penalty depending on the amount that was clamped.&lt;/li&gt;
&lt;li&gt;Attempt 4:  offset by minimum value before transforming&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = tr_train(test_Corrs_ll, train_params, data_, 400);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Handle extreme values by incuring a penalty for variances smaller than 1e-5.  Result stored in &lt;code&gt;train_params_done_1&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprisingly small smoothness sigma.  Surprisingly low rate variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the magnitude of the penalty term.  Scaled up penalty by 1000.  Results in &lt;code&gt;train_params_done_2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the threshold where the penalty term starts to be incurred.  Set MIN_NOISE_VARIANCE to 1e-3 and MIN_SMOOTHING_VARIANCE to 1e-7 (previously both 1e-5).  Results in &lt;code&gt;train_params_done_3&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_3 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Attempt #4 &lt;/strong&gt;
Handled small variances by offsetting before transforming:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% converting param to state variable
x(1) = log(smoothing_variance - MIN_SMOOTING_VARIANCE);

% converting state variable to param
x(1) = exp(smoothing_variance) + MIN_SMOOTING_VARIANCE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is more elegant than the penalty hack used in the last three attempts.   Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;h2&gt;No-Perturb model summary:&lt;/h2&gt;

&lt;p&gt;Successfully trained model.&lt;br/&gt;
Required &lt;strong&gt;115 function evaluations&lt;/strong&gt;, taking &lt;strong&gt;112 s&lt;/strong&gt;.&lt;br/&gt;
Optimal marginal likelihood:  &lt;strong&gt;23714.937760&lt;/strong&gt;.&lt;br/&gt;
Optimal parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised at how small noise_variance was, considering the calibration noise.  However, I guess the maximum-likelihood reconstruction looked pretty good, if unity-apect-ratio axis scaling is used.&lt;/p&gt;

&lt;h2&gt;Training OU-Perturb model&lt;/h2&gt;

&lt;p&gt;Getting weird results.  Halted after second iteration; second iteration took 250 function evaluations; perturb_smoothing_variance gradient is zero.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 1.5003e-05
                noise_variance: 1.0087e-04
             position_variance: 7.9393e+04
                 rate_variance: 0.7879
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 1.5183
     perturb_position_variance: 2.5159e+04
                 perturb_scale: 48.6163
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Recall that new model ML's aren't deeply tested; probably a bug in there (in the kernel implementation?  in the kernel theory? in the conversion from mats to kernel?).  Will continue tomorrow.&lt;/p&gt;
</description>
				<pubDate>Sat, 03 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Morning&lt;/h2&gt;

&lt;p&gt;Researched telecommuting strategies.&lt;/p&gt;

&lt;p&gt;Set up Google hangout: &lt;a href=&quot;https://plus.google.com/hangouts/_/89759369dd280ff225c298a7a4291745134e1d6f&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Set up IRC chat room: &lt;a href=&quot;http://webchat.freenode.net/?channels=ivilab&amp;amp;uio=d4&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Probably IRC will be best for general chat, Google Hangouts for screen sharing or group video chat.&lt;/p&gt;

&lt;h2&gt;Afternoon&lt;/h2&gt;

&lt;p&gt;Setting up training minimizer using Matlab's &lt;code&gt;fminunc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Got all params in-place, but Matlab version is too old.&lt;/p&gt;

&lt;p&gt;Trying to upgrade matlab, but Java is out of date.&lt;/p&gt;

&lt;p&gt;Taking a break to move furniture...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Downloaded and installed Java 7.&lt;/p&gt;

&lt;p&gt;Now Matlab installer isn't able to communicate with server.  Arg...&lt;/p&gt;

&lt;p&gt;Working now... downloading...  Should be done in 45 minutes.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;p&gt;Migrating settings from old Matlab...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;
</description>
				<pubDate>Fri, 02 Aug 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log</guid>
			</item>
		
			<item>
				<title>Work Log, Week summary</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;No work logs this week.  All week I've been splitting time between research and packing for the move to Phoenix on Monday.  I've made some progress on the training framework.&lt;/p&gt;

&lt;h2&gt;Miscellaneous&lt;/h2&gt;

&lt;p&gt;Due to periodic crashing of Matlab (user error!), I added a script &lt;code&gt;tmp_setup_workspace.m&lt;/code&gt;, which is intended to restore all the needed variables for whatever task I'm currently working on.&lt;/p&gt;

&lt;h2&gt;Optimized likelihood for training&lt;/h2&gt;

&lt;p&gt;Spend some time thinking about how to design a version of the marginal likelihood computation that is optimized for training.&lt;/p&gt;

&lt;p&gt;Some time was spent deriving the analytical ML gradient w.r.t. the training parameters.  I believe the result won't be too complicated, but deriving it was taking up too much time.  We'll use a numerical gradient for now; will return to analytical gradient if the numerical proves to be lacking.&lt;/p&gt;

&lt;p&gt;To save some computation during training, I precomputed the component matrices for the prior and likelihood.  Constructing the prior and likelihood covariance matrices will involve scaling and summing the component parts.  These cached matrices consume about &lt;strong&gt;740 MB&lt;/strong&gt; with the current training set.&lt;/p&gt;

&lt;p&gt;However, one unavoidable bottleneck continues to be the cholesky decomposition, which isn't improved by this precomutation.  I was hoping there would be some matrix inversion tricks involving linear combinations of matrices, but my research on this came up empty.  My last hope is to run Cholesky on the GPU (Matlab makes this trivial), but if that doesn't speed things up, I'll resign myself to waiting 10 hours for training.&lt;/p&gt;

&lt;p&gt;Nevertheless, the component caching still gives a 1.5x end-to-end speedup on the no_purturb_kernel  case, compared the to direct implementation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;div&gt;After optimizing, the bottlenecks are split evenly three ways: (1) constructing Prior kernel; (2) constructing ML covariance matrix; and (3) evaluating ML pdf.   The latter two are dominated mostly by \(O(n^3)\) matrix multiplication.  Surprisingly, Cholesky is significant, but not dominating.&lt;/div&gt;


&lt;h2&gt;Summary:&lt;/h2&gt;

&lt;p&gt;Training-optimized Marginal Likelihood is finished; in &lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;.  Results confirmed against reference implementation: &lt;code&gt;train/tr_curves_ml_ref.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;finish Training framework:

&lt;ul&gt;
&lt;li&gt;setup local minimizer with &lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;determine if numerical gradient is okay&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sanity check: compare the ML's of each trained model on training data.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Fri, 26 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/26/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/26/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;: The likelihood covariance for the &quot;virtual observations&quot; scales linearly with the 2D likelihood variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt;: see &lt;code&gt;experiments/exp_2013_07_21_likelihood_covariance.m&lt;/code&gt;.  Constructs a likelihood using noise_variance of one and then scaling precisions afterward.  Compares to directly-constructed likelihood precisions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Negligible difference&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;: Practice matches theory--scaling likelihood precisions is equivalent to constructing likelihood with the scaled precision.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: This conclusion means that we can construct the likelihood precisions exactly once during training, and simply scale them as we modify the likelihood precision.  It would make sense to always use 1.0 when computing precisions, and refactor all existing code to scale the matrix by the reciprocal of the noise variance before using it.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Build training framework

&lt;ul&gt;
&lt;li&gt;fast likelihood evalautor

&lt;ul&gt;
&lt;li&gt;custom function for evaluating ML without recomputing stuff&lt;/li&gt;
&lt;li&gt;cache the three prior component matrices (smooth, linear, and offset)&lt;/li&gt;
&lt;li&gt;cache unscaled likelihood precision.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;wrap in a lambda

&lt;ul&gt;
&lt;li&gt;scales and combines all components&lt;/li&gt;
&lt;li&gt;depending on motion model, use different expression for purturbation coefficient&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pass to quasi-newton minimizer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat for background curves&lt;/li&gt;
&lt;li&gt;Heuristic pruning using background-subtraction.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 21 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/21/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/21/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing: ground-truth-to-data-labels.  See file &lt;code&gt;train/label_from_ground_truth.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finished.&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data labels to likelihood means/covariances&lt;/li&gt;
&lt;li&gt;Likelihood means/covariances to marginal likelihood&lt;/li&gt;
&lt;li&gt;training framework&lt;/li&gt;
&lt;li&gt;training&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Visualizing &lt;code&gt;labels_from_ground_truth()&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Wrote &lt;code&gt;tmp_get_max_posterior.m&lt;/code&gt;, a temporary script that computes the posterior mean from a possibly-overconstrained prior.  In this case, the posterior covariance is singular, but the mean can still be obtained.  The math behind it &lt;a href=&quot;/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance/&quot;&gt;is available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Below is a plot using the test dataset and the ground truth labels:&lt;/p&gt;

&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/foL28SUn1JM?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;This shows that given a good labeling, a quality 3D reconstruction can be obtained using only the fragmented curves output by the curve-detector.&lt;/p&gt;

&lt;p&gt;Notice that the curves at the base have missing parts.  There isn't sufficient edge data here, but this could probably be fixed by connecting them to the base of the main stem and using the Branching Gaussian Process prior to enforce connectivity.&lt;/p&gt;
</description>
				<pubDate>Fri, 19 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log</guid>
			</item>
		
			<item>
				<title>Maximum posterior with singular prior covariance</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Consider the scenerio of Bayesian inference with a linear-Gaussian prior and likelihood.&lt;/p&gt;

&lt;p&gt;It is sometimes the case that our prior has a singular covariance matrix, indicating that our variables are embedded in a lower-dimensional hyberplane, i.e. some dimensions are redundant.  In our 3D curve triangulation application, this situation arises when  the same 3D point is observed in from multiple views.&lt;/p&gt;

&lt;p&gt;We can still find the maximum posterior arising from such a prior, as long as the likelihood is non-singular.   We can interpret this as multiple observations of the rendant dimensions, and if we are careful with our math, we can handle it the same way as the case with a non-singular prior.&lt;/p&gt;

&lt;p&gt;Given a likelihood \( \mathcal{N}(\mu_l, \Sigma_l) \) and prior \( \mathcal{N}(\mu_0, \Sigma_0) \), recall that the posterior is given by  \( \mathcal{N(\mu_P, \Sigma_P)} \), where&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
\Sigma_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} \\
\mu_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} (\Sigma_l^{-1} \mu_l + \Sigma_0^{-1} \mu_0)
\end{align}

\]
&lt;/div&gt;


&lt;p&gt;However, if \(\Sigma_0 \) is singular, we must avoid inverting it when computing \( \mu_P  \).  An equivalent equation for \(\mu_P\) that satisfies this condition is:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;




&lt;div&gt;This should be computable as long as the likelihood precision matrix \(\Sigma_l^{-1}\) has no infinite eigenvalues.  &lt;/div&gt;


&lt;p&gt;To see an example result, see &lt;a href=&quot;/ksimek/research/2013/07/19/work-log/&quot;&gt;today's Work Log&lt;/a&gt; entry.&lt;/p&gt;

&lt;h2&gt;Implementation Notes&lt;/h2&gt;

&lt;p&gt;I tried implementing a version of this that uses Cholesky decomposition and backsubstitution instead of generic matrix inversion.  I needed a symmetric matrix, so I modified the equation for \(\mu_P\):&lt;/p&gt;

&lt;div&gt; \[
\mu_P = \Sigma_0 (\Sigma_0 \Sigma_l^{-1} \Sigma_0 + \Sigma_0)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;This was not noticibly faster than general matrix inversion, because it involves two additional large matrix multiplications.&lt;/p&gt;

&lt;p&gt;Surprisingly, I &lt;em&gt;was&lt;/em&gt; able to get a significant speedup (~7x) be using backsubstitution (Matlab's '\' operator) &lt;em&gt;without&lt;/em&gt; Cholesky decomposition.  I always assumed that the lower-triangular form was what made backsubstitution so fast, but it is apparently also fast with dense matrices.  So we can avoid the extra expensive dens-matrix multiplications, and also avoid expensive matrix inversion.&lt;/p&gt;
</description>
				<pubDate>Fri, 19 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bootcamp demo session&lt;/li&gt;
&lt;li&gt;Ground truth labeling of curve fragments.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Bootcamp demo session&lt;/h2&gt;

&lt;p&gt;Ran bootcamp demo session.  Final code available at&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;svn+ssh://v01/misc/svn/src/bootcamp/kjb_demos/3d_demo
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Ground truth labeling of curve fragments&lt;/h1&gt;

&lt;p&gt;Goal: use 2D ground truth to automatically label bottom-up curve fragments.&lt;/p&gt;

&lt;h2&gt;Overview&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Draw ground truth curves using curve index&lt;/li&gt;
&lt;li&gt;dilate slightly&lt;/li&gt;
&lt;li&gt;For each rendered data curve, gather all GT indices&lt;/li&gt;
&lt;li&gt;keep most-occurring GT index.&lt;/li&gt;
&lt;li&gt;add to assoc list for that curve&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Issues:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Remember how to read and process ground truth.&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; See &lt;code&gt;../ground_truth/read_gt2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Symlinked to &lt;code&gt;train/read_gt2.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Which dataset does our example data come from?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/ler_5_36_0.jpg&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Where is ground truth data?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/resized_50%/ground_truth_2d.gt2&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Blah Blah&lt;/h2&gt;

&lt;p&gt;Refactored some code into new function &lt;code&gt;build_curve_maps.m&lt;/code&gt;, which renders each of the curves in a cell-array into a map containing their indices.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored as Bezier curves.  Symlinked the all bezier-related code into data_association_2.  Relevant function is &lt;code&gt;bezier/polybez_to_polyline.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored in OpenGL-style coordinates (bottom-left origin).  Converting to top-right using &lt;code&gt;tools/flip_y.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Summary &lt;/h2&gt;

&lt;p&gt;Got through item 2 in &quot;Overview&quot; above.  Will finish next time, and start building training framework.&lt;/p&gt;
</description>
				<pubDate>Thu, 18 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log</guid>
			</item>
		
			<item>
				<title>Work Log 2</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Experimenting with &lt;code&gt;ou_perturb_kernel.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Was getting weird results, where the ML approached infinity as &lt;code&gt;noise_variance&lt;/code&gt; approached zero.&lt;/p&gt;

&lt;p&gt;Realized that the set of precision matrices needs to be updated &lt;strong&gt;every time the noise sigma changes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is an oversight that has tripped me up before.  Need to try to avoid it in the future.&lt;/p&gt;

&lt;p&gt;It is possible (likely?) that this transformation is as simple as multiplying the precision matrices by \(\sigma_n* / \sigma_n\).  This would avoid a semi-expensive Hessian calculation for each point, which could be a bottleneck during training.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;p&gt;See previous entry&lt;/p&gt;
</description>
				<pubDate>Wed, 17 Jul 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2</guid>
			</item>
		
	</channel>
</rss>
