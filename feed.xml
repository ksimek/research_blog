<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>KLS Research Blog</title>
		<description>Nothing to see here...</description>
		<link>http://vision.sista.arizona.edu/ksimek/research</link>
		<atom:link href="http://vision.sista.arizona.edu/ksimek/research/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Re-ran with method 2.  Still getting negative eigenvalues.&lt;/p&gt;

&lt;p&gt;Possibly non-symmetry issue.&lt;/p&gt;

&lt;p&gt;Change gears -- work on fast implementation, then resume debugging.&lt;/p&gt;

&lt;h2&gt;Reduced rank approximation of data-covariance&lt;/h2&gt;

&lt;div&gt;
&lt;p&gt;
Goal: approximate \(\left(K + \Sigma_D \right)^{-1}\), using a low-rank approximation of K;  where \(\Sigma_D\) is the likelihood covariance.
&lt;/p&gt;&lt;p&gt;
In our case \(\Sigma_D\) has infinite covariance, so this inverse doesn't exist. However, if we work with precision matrix \(\Lambda = \Sigma_D^{-1}\), and use the decomposition \( S' S = \Lambda \) we can replace this expression with the equivalent

\[
    S' \left( S K S' + I \right)^{-1} S
\]

Even though S is rank-deficient, the inverse in this expression does exist, thanks to the finite positive values being added to the diagonal.  
&lt;/p&gt;&lt;p&gt;
We approximate the above expression as follows.  Let \(K\) be an \(N\) by \(N\) matrix. We can take the eigendecomposition \(K = V D V'\) and approximate it with \(\tilde K = \tilde V \tilde D \tilde V'\), where \(\tilde V\) and \(\tilde D\) consist of the first \(n\) eigenvalues and eigenvectors of \(K\) respectively.   We can use this low-rank approximation with the [Woodbury matrix identity](http://en.wikipedia.org/wiki/Woodbury_matrix_identity) to approximate the above inverse in \(O(n^3)\) time, rather than \(O(N^3)\).  The Woodbury identity is
        
\[
    (A + U C V )^{-1} = A^{-1} - A^{-1} U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}
\]

Setting \(A = I\), \(U = V' = S \tilde V \) and \(C = \tilde D \), we get:
    
\[
    \left( S K S' + I \right)^{-1} = I - S \tilde V (\tilde D^{-1} + \tilde V' S' S \tilde V)^{-1} \tilde V' S'
\]
&lt;/p&gt;&lt;p&gt;
It remains to find \(\tilde V\) and \(\tilde D\) efficiently.  Naive eigenvalue decomposition takes \(O(N^3)\) time, which isn't any better than direct inversion.  Sections 4.3.2 and 8.1 of Williams and Rasmussen show how to approximate \(n\) eigenfunctions and eigenvalues from \(n\) data points in \(O(n^3)\) time.  Eigenvectors \(\tilde V\) arise by evaluating the approximate eigenfunctions at the appropriate indices.
&lt;/p&gt;
&lt;p&gt;

Substitutuing back into the original expression by surrounding with (\(S'\) and \(S\)), we get the final expression:
    
\[
\begin{align}
    \approx &amp; S' \left ( I - S \tilde V (\tilde D^{-1} + \tilde V' S' S \tilde V)^{-1} \tilde V' S' \right) S \\
    =&amp;\Lambda - \Lambda \tilde V (\tilde D^{-1} + \tilde V' \Lambda \tilde V)^{-1} \tilde V' \Lambda
    \end{align}

\]
&lt;/p&gt;
&lt;/div&gt;


&lt;h2&gt;Work log&lt;/h2&gt;

&lt;p&gt;Implemented two different implementations of &quot;nystrom solve&quot; (&lt;code&gt;tools/nystrom_solve.m&lt;/code&gt;).  Crude testing shows big speedup, but stalling on a big eigenvalue decomposition.  Replacing with a call to &quot;chol&quot;, waiting on visual results.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&quot;chol&quot; gives much faster results. (no need to symmetrize)  But results are junk -- blank screen.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, bug in call to chol; fixed.&lt;/p&gt;

&lt;p&gt;still getting black.  Posterior covariance is not positive definite; eigenvalues are negative.&lt;/p&gt;

&lt;p&gt;When I force covariacne to zero (to view the mean value), it's still in the ballpark but wonky (same as before).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Clue&lt;/strong&gt;:   When re-using (thinned) input covariance matrix and indices, result looks sensible.  Clearly, we have a problem with how K_star is computed (and likelihood K_star_star, too).&lt;/p&gt;

&lt;p&gt;Comparing different K_star's...&lt;/p&gt;

&lt;p&gt;Looks like on-diagonal elements differ, off-diagonals are okay.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Got it!  Wacv was trained using no-perturb model, but is being tested with OU-perturb.&lt;/p&gt;

&lt;p&gt;TODO: edit get_all_Wacv and/or get_wacv_result to receive a model-type... done.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;reducing test to block (1,1)&lt;/p&gt;

&lt;p&gt;possible sources of disparity: indices, function&lt;/p&gt;

&lt;p&gt;checked indices -- same.&lt;/p&gt;

&lt;p&gt;check params -- model-type doesnt match.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;First view now looks good in matlab.&lt;/p&gt;

&lt;p&gt;Running all views in likelihood_server.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Means look great!.  Now on to random perturbed.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Cholesky claims posterior covariance isn't positive definite.&lt;/p&gt;
</description>
				<pubDate>Mon, 28 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/28/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/28/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Testing likelihood #2 (2-day)</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Client send/receive timeout &lt;/h2&gt;

&lt;p&gt;Tried to get send/receive to timeout if the server didn't respond.  It turns out, although this feature exists in native Unix sockets, the boost library abstractions render them useless.  To get this, I'll need to use asynchronous IO, which I'm not ready to jump to, yet (also not sure if callbacks will work in mex, since the callback code might not exist after the constructor function returns).&lt;/p&gt;

&lt;p&gt;Let's just take the approach that the server must always respond quickly or disconnect.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Testing &lt;code&gt;curve_tree_ml_2&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Need a reliable way to get a testing Trackset.  Write something for wacv -- &lt;code&gt;get_wacv_trackset&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Modified run_all_wacv to save Tracks;  re-running on all datasets.  &lt;code&gt;get_wacv_results&lt;/code&gt; now returns Tracks as well as means.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tweaked semantics&lt;/strong&gt; so this now &lt;em&gt;only&lt;/em&gt; computes the pixel likelihood.  The full likelihood is the sum of this and &lt;code&gt;curve_tree_ml&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tweaked &lt;code&gt;construct_attachment_covariance_2.m&lt;/code&gt; so the self-covariance matrices are computed in the function (if needed) instead of being passed in.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Spin-off &lt;code&gt;construct_attachment_covariance_3.m&lt;/code&gt; a fully general version that receives input and output indices, and optionally a pre-computed self-covariance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TODO: &lt;/strong&gt; make this the &quot;official&quot;/dispatch version, make other versions call this (or eliminate altogether)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;data matrix is HUGE.  8913 dimensions.  Is this right?  Also, we can move inversion outside of the loop (maybe).&lt;/p&gt;

&lt;p&gt;Consider 'subset of data' method, or other dimensionality-reduction method&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;&lt;strong&gt;Issue&lt;/strong&gt;: Can't do cholesky decomposition, because points branching from the base are redundant.  Using SVD instead.&lt;/h2&gt;

&lt;p&gt;Okay, it appears to be successfully running end-to-end.  Haven't confirmed results yet, but one thing is clear... its REALLLLLY slow (3 minutes and counting)&lt;/p&gt;

&lt;p&gt;Can do ancenstral sampling to exploit tree structure to speed it up.  can further use markov property to break up curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, not quite end-to-end success.  Some indexing, reshaping issues.   Other bugs found&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;wasn't adding posterior mean to sampled results&lt;/li&gt;
&lt;li&gt;missing transpose in posterior covariance equation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Debugging sampled curves&lt;/h2&gt;

&lt;p&gt;Message looks okay under inspection, but getting -inf (due to exception).  Dumping...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found a recent bug in code that builds the Gl_curve.  When I chnaged an assert to an if/throw, I forgot to negate the conditional.&lt;/p&gt;

&lt;p&gt;Getting finite values now.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Result looks in the ballpark, but some perturbations look questionable (considering the tight constraints on the WACV dataset).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-27-perturb-comparison.png&quot; alt=&quot;perturbed vs. original&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Doind a full dump-mode run...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found a bug in a recent refactor of dump-mode, causing segfaults.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;All views drift very far from their true values.  Possibly a math bug in the magnitude of the posterior covariance?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Refactored server's &quot;dump mode&quot;  to continuously dump each message as it's received, instead of running in offline mode and dumping only the passed-in model.&lt;/p&gt;

&lt;p&gt;Trying method2.  Got similar results, qualitatively; per-view curves still have some bizzarre features.  Interestingly, the perturbations between successive samples of method 2 (and between method2 and method 1) are relatively small, suggesting this is an issue with the mean, not the variance.&lt;/p&gt;

&lt;p&gt;Recall that we only tested the no-perturb model for wacv reconstruction; not the per-view reconstruction.&lt;/p&gt;

&lt;p&gt;Re-running WACV dataset 2 with OU-perturb model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Looks sensible.  So that rules out the parameter settings causing bad mu's.  We should be getting exactly WACV results;  can we get there?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;use K_star = K&lt;/li&gt;
&lt;li&gt;use zero covariance matrix (always sample at the mean)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Running...  (slow, because matrix multiplication is so much larger)&lt;/p&gt;

&lt;p&gt;I'm guessing the bug is in the full-tree covariance .&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Getting &quot;degenerate curve&quot; error.  Thinning points fixed.&lt;/p&gt;

&lt;p&gt;Dumped results look good.  This suggests that mean math is likely correct, as long as K_star is okay.  So K_star math is probably wrong.&lt;/p&gt;

&lt;p&gt;Need to write a test to confirm and start debugging.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done. Results match on symmetric and non-symmetric index sets. Which means no progress made on this bug...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Re-ran, but using non-zero Sigma.  Resulting covariance was not positive definite; SVD was complex-valued.&lt;/p&gt;

&lt;p&gt;Possibly this is a result of the same bug?&lt;/p&gt;

&lt;h2&gt;Covariance matrix Rank-reduction&lt;/h2&gt;

&lt;p&gt;While previous test was running, read-up on reduced-rank approximations to K.  Nystrom method seems sensible to speed up matrix inversion.  Doesn't avoid the matrix multiplication  with K_star, or cholesky decomposition of the posterior covariance...  but using a smaller output index set seems like a reasonable approach to mitigating both of those.&lt;/p&gt;

&lt;p&gt;Curious how many non-negligible eigenvalues we have; how many data points we should use.  Recalling a plot of eignevalues from a few days ago, it looked like less than 0.1% of the dimensions are significant, but need to get a concrete number.&lt;/p&gt;

&lt;p&gt;Alternative approach is to use the problem's unique markov structure to sample each section piecewise.  Needs some thought for how to do in the case of the posterior; probably need to estimate a markov blanket for each, along with sampling each branch point and conditioning on it.  Upside: better chance at asymtotic running time improvement. Down-side: not general, not as re-usable within the project.&lt;/p&gt;

&lt;h2&gt;To Be Continued&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Bug:&lt;/strong&gt; Samples generated from &lt;code&gt;curve_tree_ml_2_debug.m&lt;/code&gt; are wonky.
&lt;strong&gt;Theory&lt;/strong&gt;: Bug is related to posterior covariance.  Posterior mean looks sensible.
&lt;strong&gt;File&lt;/strong&gt;: &lt;code&gt;curve_tree_ml_2_debug.m&lt;/code&gt; &amp;lt;-- alternative version, where K is used for K_star and K_star_star.  Currently giving non-positive-definite results for posterior covariance using method 1.  Currently Stumped&lt;/p&gt;

&lt;p&gt;Fix sampling sigma bug.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;get likelihood from multiple samples -- what is the variance of the MC estimator?&lt;/li&gt;
&lt;li&gt;re-run wacv with multi-view data, but no-perturb output&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sat, 26 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/26/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/26/work-log</guid>
			</item>
		
			<item>
				<title>params, CVPR 2014</title>
				<description>&lt;p&gt;&lt;a href=&quot;/ksimek/research/CVPR2014/params.html&quot;&gt;See CVPR 2014 parameters page&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 26 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/26/params-cvpr-2014</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/26/params-cvpr-2014</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;p&gt;Implementing one-object-per-view in likelihood_server&lt;/p&gt;

&lt;p&gt;Overloaded bd_mv_likelihood' evaluate and dump  functions to receive a sequence of renderables.&lt;/p&gt;

&lt;p&gt;Write a &quot;wrap_all_As_silhouette&quot; to, well, wrap all renderables in silhouette renderers.&lt;/p&gt;

&lt;p&gt;re-wrote message-to-curve function to decode the message to a vector-vector-vector, and convert that to a vector-of-gl_curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Compiling work from last 24 hours.&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;Generate a multi-model message from matlab (dummy)&lt;/li&gt;
&lt;li&gt;test message in likelihood_server_2.cpp

&lt;ol&gt;
&lt;li&gt;generate random samples from wacv data.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;Doing dummy message test.&lt;/p&gt;

&lt;p&gt;At first, accidentally sent to old implementation... and it didn't barf!  This is unsettling, because the message format has changed significantly.  Did I ever recompile the mex files?&lt;/p&gt;

&lt;p&gt;Nope.  got some mex compile errors to deal with.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Friday&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;okay, sending a random model using the new one-per-view message system isn't crashing.&lt;/p&gt;

&lt;p&gt;Questions&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the likelihood looks high, considering it's a ranomd model .  is a null model better?  is the ground-truth model better?&lt;/li&gt;
&lt;li&gt;visualize the received message;  is each view different?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Trying the null model from 1., the mex file crashed matlab.  Empty curvesset isn't handled.&lt;/p&gt;

&lt;p&gt;trying all-zeros.  Server crashed -- coincident points aren't handled gracefully.  Fixed (now it returns -inf for log likelihood).&lt;/p&gt;

&lt;p&gt;issues
server: error creating gl_curves causes client disconnect. fix exception handling
client: doesn't handle server crashes gracefully.  adding timeout
client: doesn't handle empty curveset&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;random sample from multi-view posterior (instead of single max posterior)&lt;/li&gt;
&lt;li&gt;get likelihood from multiple samples -- what is the variance of the MC estimator?&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 24 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/24/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/24/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Implementing Two-term likelihood</title>
				<description>&lt;p&gt;Working on Matlab integration.&lt;/p&gt;

&lt;p&gt;Quick test shows we can get about 20 evaluations per second.  Not bad, considering each evaluation consists of 9 image likleihoods.  There's some room for improvement here, but probably not worth pursuing at the moment:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;leapfrog rendering: render (a) while evaluating on (b).&lt;/li&gt;
&lt;li&gt;cleaning up geometry and fragment shaders (fewer branches, less storage)&lt;/li&gt;
&lt;li&gt;try other GPU blurring routines.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;New Likelihood&lt;/h1&gt;

&lt;p&gt;Scenario: we have a decent likelihood that is linear-gaussian (and a gaussian prior), so we can compute the marginal likelihood in closed-form.  However, we'd like to incorporate additional sources of evidence whose likelihoods aren't gaussian.  We'll see how we can estimate the joint marginal likelihood with simple Monte-Carlo sampling (no MCMC needed, no gradient).&lt;/p&gt;

&lt;h2&gt;Derivation&lt;/h2&gt;

&lt;p&gt;The old marginal likelihood looked like this:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1) &amp;= \int p(x) p(D_1 | x) dx
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;After introducing the extra likelihood term, the joint probability is no longer linear-gaussian, so the exact marginal likelihood involves an intractible integral.  However, by  re-arranging, we see we can get a good monte-carlo approximation:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;= \int p(x) p(D_1 | x) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= \int p(x | D_1) p(D_1) p(D_2 | x) dx &amp; \left(\text{Bayes thm (see below)}\right) \\
p(D_1, D_2) &amp;= p(D_1) \int p(x | D_1) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= p(D_1) \frac{1}{N} \sum p\left(D_2 | x^{(*)}\right) &amp; \text{(Monte Carlo)}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;In the second line, I've replaced the first two terms using Bayes theorem.  In the last line, the x-stars are samples from \(p(x | D_1)\), which we have in closed-form due to linear-Gaussian prior and likelihood for \(D_1\).&lt;/p&gt;

&lt;p&gt;Thus, we see if at least one source of data yields a linear-gaussian likelihood, we can incorporate additional data with arbitrary likelihoods  in a principled way.  In many cases, \(p(x | D_1) \) has low variance, so a small number of Monte-Carlo samples are sufficient for a good estimate -- even a single sample could suffice.  Even if the estimates are bad, they are unbiased, so any MCMC involving the marginal likelihood will converge to the target distribution.&lt;/p&gt;

&lt;h2&gt;Importance Sampling version&lt;/h2&gt;

&lt;p&gt;We can alternatively derive this in terms of importance sampling, setting the proposal probability q(x) to \(p(x | D_1) \):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;\approx 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{1}{q(x)} \\
            &amp;= 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{p(D_1)}{p(x) p(D_1 | x)} \\
            &amp;= 1/N \sum p(D_2 | x) p(D_1) \\
            &amp;= p(D_1) 1/N \sum p(D_2 | x) 
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File:&lt;/strong&gt; curve_tree_ml_2.m&lt;/p&gt;

&lt;p&gt;Basic idea&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;construct a thinned output index set (optional, but smart)&lt;/li&gt;
&lt;li&gt;construct a posterior distribution over the thinned set&lt;/li&gt;
&lt;li&gt;Add perturbation variance to the posterior&lt;/li&gt;
&lt;li&gt;take average over n trials: sample curveset and evaluate pixel likelihood&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Step 2 required updating to construct_attachment_covariance, which only constructs symmetric covariance matrices.  We need the covariance between indices with observations and the desired output indices.  Fully refactored that function into &lt;code&gt;construct_attacment_covariance_2.m&lt;/code&gt;; confirmed correctenss in the case of the self-covariance by using an existing test for version 1 of that function.  If non-symmetric, the upper-triangular blocks are processed in a second pass, swapping indices so we can re-use existing code.&lt;/p&gt;

&lt;p&gt;Need to try view-specific sampling, i.e. sample 9 different curves from 9 different views.  This refactor affects likelihood server, client, and message format.  I'm worried about the performance hit, but probably not worth worrying about (or futile).  Coarse sampling of indices could mitigate.    In any case, view-specific sampling is probably necessary, because we're using such low blurring levels in the Bd_likelihood, so the reconstruction needs to fall near the data.  We've seen how plant motion and camera miscalibration cause &quot;good&quot; 3D curves to reproject up to 10 pixels away from the data in some views.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Side-note&lt;/strong&gt; - the tcp connection is working very reliably so far!  Even after the machine sleeping/resuming several times, and suspending the server job for serveral hours, the socket is still valid and communicating flawlessly!&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Implementing per-view sampling&lt;/h2&gt;

&lt;p&gt;need to refactor send_curves.m to send &lt;code&gt;num_views&lt;/code&gt; curves instead of one.&lt;/p&gt;

&lt;p&gt;Now receive as num_curves x num_views cell array.&lt;/p&gt;

&lt;p&gt;Coded vector&lt;sup&gt;3&lt;/sup&gt; to/from message.&lt;/p&gt;

&lt;p&gt;todo: rewrite likelihood server to receive vector3, somehow pass per-view models to likelihood&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;multi-view likelihood is out - it's one-model, multi-view.  We need multi-model, multi-view.

no, MV likelihood is okay, just add an extra operator() that receives a sequence of renderables 
whose size is equal to the number of views
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;p&gt;Some protocol for starting and loading the likelihood server from matlab code
Stress test likelihood server&lt;/p&gt;
</description>
				<pubDate>Wed, 23 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still searching for inf bug in libcudcvt blurred_difference_likelihood.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found it:  my GPU-based log_sum routine had a bug.  instead of subtracting the maximum value before exp-summing, it subtracted an arbitrary value.  The find-max loop looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Real pi = v[0];

#pragma unroll
for(unsigned int i = 1; i &amp;lt; N; ++i)
{
    pi = pi &amp;lt; v[0] ? v[0] : pi;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, the zeros should be i's.&lt;/p&gt;

&lt;p&gt;A great bug to have found and fixed, but really frustrating that I let it slip through in the first place.  So stupid!  But it didn't affect correctness in the common case, so my tests didn't catch it.  It makes a good case for randomized testing (as if a good argument was lacking...).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(semi-obvious) note to self: conditional gaussian mixture is not equal to mixture of conditional gaussians!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;okay, a few more cleanup tasks, then on to real goals:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cleanup and commit version 0.1 of the likelihood server -- done&lt;/li&gt;
&lt;li&gt;split training into it's own directory. -- done&lt;/li&gt;
&lt;li&gt;Get likelihood server connecting reliably with matlab on different computers.&lt;/li&gt;
&lt;li&gt;implement likelihood importance sampling into maltlab's curve_ml procedure&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Thought some on birth moves.  It seems like there are some possibilities for births larget than two by using the adjacency graph created by pair candidates.&lt;/p&gt;

&lt;p&gt;for now, start with greedy&lt;/p&gt;
</description>
				<pubDate>Tue, 22 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Linux NVidia/Cuda/X11 erorrs; Cuda server; matlab integration</title>
				<description>&lt;p&gt;Getting my (cuda-equipped) desktop system running with likelihod_server.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Trouble building makefiles. Csh issue with new makefiles.  Kobus fixed&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still can't build makefiles. Probilems in the ./lib subdirectory is apparently blocking kjb_add_makefiles from finishing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Some makefiles were missing in ./lib.  Adding them was problematic, because (like before) kjb_add_makefiles wasn't finishing, because dependency directories were missing makefiles.  Also had some issues in kjb /lib, out of date from svn and some compile bugs I introduced in my last commit.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Compiling now, but shader is giving errors at runtime (opengl can't compile it, but no error message is given.&lt;/p&gt;

&lt;p&gt;Rebooting...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;No desktop.  Missing disk issue?  Tweaked fstab, still no X11.  Must be a driver issue.  Couldn't find driver from NVidia's I downloaded from web site months ago.  Using driver &quot;Ubuntu-X&quot; PPA.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still no X.  /var/log/Xorg.0.log says driver and client have different versions.  Got a tip online:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo dpkg --get-selections | grep nvidia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lots of extra crap there (old versions, experimental drivers, etc).  'sudo apt-get purged' all the non-essentials.  Now we're getting to login screen, but login is failing (simply returns to login screen).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found solution:&lt;/p&gt;

&lt;p&gt;   sudo  rm ~/.Xauthority&lt;/p&gt;

&lt;p&gt;Booting successfully.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running likelihood_server...  shader is now compiling successfully!   Previous issues must have been driver issues as a result of the recent 'sudo apt-get upgrade' without restart.&lt;/p&gt;

&lt;p&gt;Getting segfault when dumping pixels, though.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Caused by trying to read from cuda memory as if it were main memory.  Should be dumping if we're using GPU (or use a different dump function).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Fixed;  if using gpu, copy from cuda to temp buffer, then call normal routine.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting nan's from likelihood.  Need to dump from GPU likleihood, which means digging into libcudcvt.  Got some build errors resulting from a boost bug from 8 months ago (which arose because I updated GCC).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;cudcvt updated to grab blurred frames for (some method names changed to better match the analogous CPU likelihood in KJB.).&lt;/p&gt;

&lt;p&gt;Moved &quot;dump&quot; routines from Bd_mv_likelihood_cpu to the base class Bd_mv_likelihood as pure virtual function, and implemented a version in Bd_mv_likelihood_gpu to call cudcvt's new frame-grabbing code.  So &quot;dump&quot; mode now works on both cpu and gpu version.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Dumped data from CUDA likelihood; looks fine.  So I'm still clueless why we're getting nan values.&lt;/p&gt;

&lt;p&gt;Sanity check -- check that tests in libcudcvt are still passing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;test is failing on an assert.  Looks like an overzealous assert with a low percent-error threshold.&lt;/p&gt;

&lt;p&gt;Lowered threshold, test finished; GMM model doesn't match between gpu and cpu versions...&lt;/p&gt;

&lt;p&gt;what's changed?  Rolling back to first releast to see if tests pass.  Is it possible I never validated this?  Or maybe different GGM's being used between cpu and gpu?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Old version crashed and burned hard.  GPU is returning 'inf'.  No help here...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, svn version has lots of uncommitted changes, including bugfixes.  No wonder it was no help.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;found issue: introduced a regression when troubleshooting last bug.  Grrr.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still getting 'inf'.  Checked:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GPU buffers contain valid data for data and model&lt;/li&gt;
&lt;li&gt;CPU version gives finite results.&lt;/li&gt;
&lt;li&gt;GPU can give finite results (e.g. in test application)&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Modified libcudcvt test to receive arbitrary data and model images.  It's giving finite results, so it must be something about how I'm calling it in the likelihood server program.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;issue:&lt;/strong&gt; Bd likelihood in GMM mode gives lower values for ground truth model than random model.&lt;/p&gt;

&lt;p&gt;Bug. fixed.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Digging deeper into 'inf' issue.&lt;/p&gt;

&lt;p&gt;Dug all the way into the thrust::inner_product call.  Probed both buffers -- look okay.  mixture components look okay&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Got it!&lt;/strong&gt;  Was able to reproduce the 'inf's in the likelihood test program in libcudcpp.  It was so hard to reproduce because it only occurred when&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;old GMM values were used&lt;/li&gt;
&lt;li&gt;&lt;em&gt;unblurred&lt;/em&gt; model &lt;em&gt;and&lt;/em&gt; data were used&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Regarding 2, previosuly I just used the unblurred model, since it was at hand from a cuda dump, but used the blurred data from a training dump.&lt;/p&gt;

&lt;p&gt;Blurring doesn't seem to matter; both 2.0 and 5.0 give inf.&lt;/p&gt;

&lt;p&gt;Interestingly, the perfect model doesn't oveflow, but the random (almost null) model does.&lt;/p&gt;

&lt;p&gt;Also, for all the 'inf' cases, the cpu results aren't terribly high, so float overflow seems unlikely.&lt;/p&gt;

&lt;p&gt;Most likely it's the conditioning, where the joint pdf is divided by the marginal.  Maybe we're getting some bizzare model values that are off the charts?  But using the model image with the blurred data image is no problem, so that suggests the model values aren't a problem.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Interesting:&lt;/strong&gt; Tried removing border edges and the problem disappeared.  Is it possible that the blurring routine is not robust to near-edge pixels?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;idea: dump image after scale-offset, but before reduce&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;p&gt;Probability maps look sensible when a 2-pixel margin is added (--data-margin=2).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_2.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 2.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 5.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice what happens when we disable the data margin (rendered dimmer and with blue padding to emphasize effect):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_no_margin.tiff.jpg&quot; alt=&quot;&quot; /&gt;
(2.0 blurring, 0 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice the white pixels around the border, which presumably correspond to inf values.&lt;/p&gt;

&lt;p&gt;Inspecting the blurred data image, it looks like these pixels are, indeed, the brightest in the image.  It's possible we were lucky enough to have found the maximum range of this gmm.&lt;/p&gt;

&lt;p&gt;It looks like evaluating the bivariate normal is underflowing.  It could have been brought back down to size during conditioning, but we never made it that far.  could refactor by computing conditional covariance matrix beforehand, instaed of taking a ratio of computed values&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Pre-test all incoming data images: evaluate against an empty image and a full image and check for NaN and inf.&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Mon, 21 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - KJB EM GMM</title>
				<description>&lt;p&gt;Still struggling with the EM GMM algorithm.  Found/fixed one bug where responsibilities aren't initialized if NULL is passed to the function.&lt;/p&gt;

&lt;p&gt;Ran overnight and it hadn't finished at the end.  Found/fixed a bug where the entire @M element responsibility matrix is rescaled 2M times (once per point).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting rank-deficient errors.  Looks like this is a result of the huge dataset; covariance matrix is divided by 1e6 (the number of soft-members of the cluster).&lt;/p&gt;

&lt;p&gt;For now, hack by trying to thin the dataset.  Long-term, adding a minimum offset to the emperical covariance seems to be the common solution.&lt;/p&gt;

&lt;p&gt;Possibly normalizing the data would be good.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Added offset to covariance (command-line options and file-static variable already existed, just needed to add it to the full_GMM logic).&lt;/p&gt;

&lt;p&gt;Added &lt;code&gt;read_gmm.m&lt;/code&gt; matlab routine, and plotted along with scatter plot of data.  Results look pretty good:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-20-kjb-gmm-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Moving trained gmm to ~/data/arabidopsis/training/bd_likelihood_gmm/blur_2.0.gmm&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;re-train GMM on held-out dataset.&lt;/li&gt;
&lt;li&gt;implement matlab likelihood sampling&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;how to sync datasets on matlab and c++ server?&lt;/li&gt;
&lt;li&gt;Wacv multi-view reconstruction for training&lt;/li&gt;
&lt;li&gt;saving wacv to files for training&lt;/li&gt;
&lt;li&gt;fixing issues in wacv reconstructions&lt;/li&gt;
&lt;li&gt;evaluation methodology&lt;/li&gt;
&lt;li&gt;split/merge&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Sun, 20 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log</guid>
			</item>
		
			<item>
				<title>Work Log</title>
				<description>&lt;h2&gt;Training likelihood&lt;/h2&gt;

&lt;p&gt;Fitting a three-component GMM resulted in a null model having a higher likelihood than the ground truth model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Spent a lot of time troubleshooting this and finally realized I had scaled all values up by 100x so Matlab wouldn't choke, but I wasn't accounting for this when evaluating.  Blame it on this blasted cold, killing my focus!&lt;/p&gt;

&lt;p&gt;Ground truth model looks much better than null model now.&lt;/p&gt;

&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;Trying different blurring levels.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.0 is to small.&lt;/li&gt;
&lt;li&gt;2.0 seems okay&lt;/li&gt;
&lt;li&gt;5.0 gives a wide variance for true-positives.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In the end, running max likleihood on this will be best, but this gives us a ballpark.&lt;/p&gt;

&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;Trying to get KJB EM algorithm working on my 2M element dataset.  Getting NaN errors at the moment.&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/19/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/19/work-log</guid>
			</item>
		
			<item>
				<title>Work Log - Uninformative likelihood?</title>
				<description>

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15701&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Likelihood sanity check (ctd)&lt;/h2&gt;

&lt;p&gt;An empty model should have a significantly worse likelihood than the ground truth model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ground truth model:  -3.38064e+08
empty model: -3.28279e+08
full model: -1.20572e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both empty and full are better than ground truth.  Why??&lt;/p&gt;

&lt;p&gt;At least possiblities here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Noise level is so high that fitting is impossible (remember the paper &quot;Fundamental limits of Bayesian Inference: Order Parameters and Phase Transitions for Road Tracking&quot;)&lt;/li&gt;
&lt;li&gt;Bug.  maybe renderings are just waaaay off?&lt;/li&gt;
&lt;li&gt;Bad calibration.  Looking at the GMM plot from yesterday (see below) it looks like the one-sigma contour is very very large.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-bd.gmm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Case 1 is what's left over once 2 and 3 are ruled out.&lt;/p&gt;

&lt;h2&gt;Rendering sanity check.&lt;/h2&gt;

&lt;p&gt;This seems to be a culprit.&lt;/p&gt;

&lt;p&gt;Here's first data view:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-data-view-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Not a good match to the actual first data image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-ler_2_36_0.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But a good match to the second data image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-ler_2_36_1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Must be an off-by-one error when reading data.&lt;/p&gt;

&lt;p&gt;The cameras are not off-by one.  Here's the rendering of the model under the first camera:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-model-view-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Good match to the first data image above.&lt;/p&gt;

&lt;p&gt;Bug is in the config file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;view-indices=1:4:36  # bug here
view-indices=0:4:35  # should be this
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Good news, bad news.&lt;/p&gt;

&lt;p&gt;Good news: after fixing config file, data and rendering look good (use slider to swap between images)&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-10-17-data.jpg&quot;,
        &quot;/ksimek/research/img/2013-10-17-model.jpg&quot;
        ]

    construct_animation($(&quot;#data-model-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;data-model-anim&quot; style=&quot;width:530px&quot;&gt; &lt;/div&gt;


&lt;p&gt;Bad news: likelihood gets &lt;strong&gt;worse&lt;/strong&gt;!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fixed likelhood: -3.38298e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like the rendered model is lighter than the data model.  This might account for the issues we're seeing.  We're in the region to the right of the diagonal &quot;true positive&quot; component in the GMM (see contour plot above).  In this region, the noise model has a stronger pull than the true positive model, which partially explains why we get better when fewer model pixels are on the screen.   It doesn't explain why the model gets better when we shift left and right -- maybe it fits better in the tails of the blurred data, but we'd expect it to eventually get worse.  Lets test it.&lt;/p&gt;

&lt;p&gt;Recall that this GMM was trained on a manipulated version of the data image, not a rendered 3D model image.  The process taken was (a) removing all background pixels from the data image and (b) perturbing the foreground pixels. We may have removed a few foreground pixels to add false positives, but the amount was small, if memory serves.  Now we have far fewer model pixels, so we expect the &quot;true positive&quot; region to be slanted more upward.&lt;/p&gt;

&lt;h2&gt;Optimality test (redux)&lt;/h2&gt;

&lt;p&gt;Code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x_values=&quot;-80 -40 -20 -10 -5 -2 -1 0 1 2 5 10 20 40 80&quot;
for x in $x_values; do
    echo -n &quot;$x &quot;;
    ./likelihood_server \
        --config=test.conf  \
        --image-bounds &quot;0 530 397 0 200 2000&quot;  \
        --cam-convention &quot;1 0 0&quot;  \
        --dbg_save_frames  \
        --dbg_load_message  \
        --dbg-ppo-x-offset=$x 2&amp;gt; /dev/null | grep ^3 | sed -e 's/3 //' -e 's/,//';
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output (delta-x vs log-likelihood):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-80 -3.36866e+08
-40 -3.36987e+08
-20 -3.37434e+08
-10 -3.37864e+08
-5 -3.38169e+08
-2 -3.38279e+08
-1 -3.38294e+08
0 -3.38298e+08
1 -3.38292e+08
2 -3.38275e+08
5 -3.38172e+08
10 -3.37912e+08
20 -3.37479e+08
40 -3.36958e+08
80 -3.36991e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-likelihood_plot_vs_x.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;&quot;Full&quot; model issue&lt;/h2&gt;

&lt;p&gt;One of the strangest things is the fact that a rendering full of &quot;1.0's&quot; performs far better than the ground truth or empty model.  This area lies along the x-axis in the GMM contour plot, where there is no support.&lt;/p&gt;

&lt;p&gt;Lets debug this.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found an insane bug.  Below is the loop that compares all the pixels of the data and rendered model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    for(size_t i = 0; i &amp;lt;= size_; ++i)
    {
        kjb::Vector2 x;
        x[0] = model[i];
        x[1] = data_[i] + 1;

        ...  // compute p(x[1] | x[0])
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have no idea why we're adding 1.0 to &lt;code&gt;data_[i]&lt;/code&gt;, but this explains everything.  In the &quot;full&quot; model, it puts us right on the diagonal of the GMM.  In the empty model, we're right on the x-axis.  Both are well-supported regions.  In the ground truth model, we're lying between these extremes, which is a no-man's land of near-zero support.&lt;/p&gt;

&lt;p&gt;Spending 5 minutes to determine how this got added...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;It was added between November 11 and December 13 last year.   No obvious reason.  Oh well.&lt;/p&gt;

&lt;p&gt;New sanity check numbers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;full model -5.2217e+08
ground truth model 4.03273e+06
empty model: 4.01252e+06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much more sensible.  Missing data looks terrible, noise looks okay but not great,  ground truth looks best.  Lets do an offset plot just to be safe.&lt;/p&gt;

&lt;h2&gt;Optimality test (redux&lt;sup&gt;2)&lt;/sup&gt;&lt;/h2&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-80 3.1039e+06
-40 3.27557e+06
-20 3.58156e+06
-10 3.77333e+06
-5 3.93159e+06
-2 4.01459e+06
-1 4.02845e+06
0 4.03273e+06
1 4.02704e+06
2 4.01209e+06
5 3.93021e+06
10 3.78782e+06
20 3.60557e+06
40 3.26585e+06
80 3.26894e+06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plot&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-likelihood_plot_vs_x_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Oh Yeah!!&lt;/p&gt;

&lt;h2&gt;BD Likelihood Data Dump Mode.&lt;/h2&gt;

&lt;p&gt;add a mode by which pixel likelihood will save pixel data to files for analysis or debugging&lt;/p&gt;

&lt;p&gt;changes will appear in:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;lib/evaluate/bd_likelihood.h - Bd_pixel_likelihood_cpu&lt;/li&gt;
&lt;li&gt;lib/evaluate/bd_likelihood.h - Bd_mv_likeliood_cpu&lt;/li&gt;
&lt;li&gt;./likelihood_server.cpp - Options&lt;/li&gt;
&lt;li&gt;./likelihood_server.cpp - main()&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;Added dump mode.  Probably should move this into a separate directory, or at least a different program, since it has almost nothing to do with the likleihood server.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;rename matlab &quot;render_client&quot; code to &quot;likelihood_client&quot;&lt;/li&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;remove edges around image border.&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;train likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;how to sync datasets on matlab and c++ server?&lt;/li&gt;
&lt;li&gt;Wacv multi-view reconstruction for training&lt;/li&gt;
&lt;li&gt;saving wacv to files for training&lt;/li&gt;
&lt;li&gt;fixing issues in wacv reconstructions&lt;/li&gt;
&lt;li&gt;evaluation methodology&lt;/li&gt;
&lt;li&gt;split/merge&lt;/li&gt;
&lt;/ul&gt;

</description>
				<pubDate>Thu, 17 Oct 2013 00:00:00 -0700</pubDate>
				<link>http://vision.sista.arizona.edu/ksimek/research/2013/10/17/work-log</link>
				<guid isPermaLink="true">http://vision.sista.arizona.edu/ksimek/research/2013/10/17/work-log</guid>
			</item>
		
	</channel>
</rss>
