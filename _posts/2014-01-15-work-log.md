---
layout: post
title: "Dis Prop (ctd); new HPC "
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}

Continue writing dissertation proposal.  Worked on SfM, MVS literature review.  Spent quite some time trying to understand self-calibration.  The issue is that with a general perspective camera, SfM can only perform reconstruction up to a projective ambiguity (most of the SfM literature of late is apparently reticent about this?).  It turns out, if we fix at least one intrinsic parameter (how about skew = 0? or aspect ration = 1?) then we get a metric reconstruction (which maybe explains why it's no longer covered in SfM papers).  

This shows that bundle adjustment can give a metric reconstruction, but the question remains how to initialize it.  Pollefeys et. al (1998) propose an analytical self-calibration solution.  Snavely et al. (2007) apparently just use bundle adjustment anyway, presumably using a simplified pinhole camera (no skew or ppo) with a reasonable default for *f*, or EXIF tags when possible.  Brown and Lowe (2005) initialize each new camera with the old camera's intrinsics, no word on how the first pair is initialized (maybe obvious but I missed it?).

New HPC 
--------

UA just installed their new 80-node HPC cluster and by a stroke of luck, Ive gotten early access to it!  It looks like all machines have 16 CPU cores and 256 GB RAM; 60 nodes have multiple GPU cards while the other 20 have Intel PHI general purpose compute cards.  It's amazing to query the cluster and see the entire thing at 0% utilization... that won't last long!  Having fun poking around it and reading the IBM LSF manuals [2](http://publibfp.dhe.ibm.com/epubs/pdf/c2253460.pdf) (the job queueing system).

I found the following resource to be very useful, even though it's from University of Miami's personal LSF installation:
    
* [unofficial LSF 9.1.1 Documentation site](http://www.ccs.miami.edu/hpc/lsf/9.1.1/) (University of maimi)
    * [Basics](http://www.ccs.miami.edu/hpc/lsf/9.1.1/print/lsf_foundations.pdf)
    * [Jobs](http://www.ccs.miami.edu/hpc/lsf/9.1.1/print/lsf_admin.pdf)

The hew official documents I could find were less useful:

* Command Reference [[pdf](http://publibfp.dhe.ibm.com/epubs/pdf/c2753051.pdf)]
* Administration Reference [[pdf](http://publibfp.dhe.ibm.com/epubs/pdf/c2253460.pdf)]
* Configuration Reference[[pdf](http://publibfp.dhe.ibm.com/epubs/pdf/c2753061.pdf)]

###Notes###

Okay great!  Got a general idea of how to use the system from the [user guide](http://www.ccs.miami.edu/hpc/lsf/9.1.1/print/lsf_users_guide.pdf)

* `bsub` - submit default job queue
* `bsub -q <queue-name>` - submit specific job queue
* `bsub -I` - interactive job
* `bjobs` - view submitted jobs
* `bqueues` - list info for all queues
* `bparams` - details for default queue
* `lsload` - list load on hosts
* `lshosts` - list hosts w/ configuration
* `bhosts` - view batch server hosts (what's a batch server?)
* `lsid` - cluster info (name and master host)
* `lsrun` - run a command on a free host
* `lsgrun` - run a command on a group of free hosts
* `lsltakss` - view and add local tasks (huh?)
* `lsrtasks` - view and add remote tasks (huh?)
* `bkill 1234` -  kill job 1234
* `bkill -r` - force removal of job (if hung after kill?)
* `bstop` - suspend job
* `bresume - resume job
* `btop 1234` - move job 1234 to top of queue
* `bbot 1234` - move job 1234 to bottom of queue

On gpu* machines

* `nvidia-smi` list details for all gpu cards (and running processes?)

On phi* machines

* `micinfo` lists details for all coprocessor cards


Also MPI is supported

Discovered two large gsfs disks (192 TB and 43 TB, resp.).  Apparently GSFS is a GPU-enabled encrypted disk.  No write permissions ATM

###Questions###

Some random thoughts while using the system

* can/should we get Matlab?  Octave?
* Subversion?
* write permissions on GSFS disks?
* No job report email after calling `bsub -u ksimek@email.arizona.edu`


