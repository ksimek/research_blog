---
layout: post
title: "Work Log"
description: ""
category: 
tags: []
---
{% include JB/setup %}
{% include research/tulips_da2_meta %}

Pre-tails issue
==================

**Bug**: Smallest index of reconstructed curves is significantly greater than 0.

    tmp_setup_workspace
    min([test_Corrs_ll{1}.ll_indices{:}])

This results in long pre-tails, as seen in this image:
    
![pretails]({{site.baseurl}}/img/2013-08-10-pretails.jpg)

**Hypothesis**

Recall that reconstruction occurs by doing (1) rough triangulation, (2) smoothing, then (3) re-triangulating against the smoothed curve.  The initial triangulation usually results in a very bad index set, with spacing far larger than it should be, due to poor localization by maximum likelihood.  The subsequent smoothing causes the curve to stretch out longer than it should, so when re-triangulation occurs, ends are cut off.  

** Solution **

After re-triangulating, re-index so the minimum index is zero.

Change to `correspondence/corr_to_likelihood.m`:

    145  % re-index by subtracting minimum index
    146  min_index = min([Corr.ll_indices{:}]);
    147  Corr.ll_indices = cellfun(@(x) x - min_index, Corr.ll_indices, 'UniformOutput', false);

** Fallout **

This likely had an effect on training results, because marginal prior variance of the initial point was over-estimated, because it's index was over-estimated.

Cleanup
============

I'm afraid I made a mess of things yesterday when I was addressing the noise_variance issue in training.  Need to review the end-to-end systems for training, reconstruction and marginal likelihood evaluation.

* is training world-variance scaled everywhere?
* is training world-variance projecting to 1.0?
* is training-ml equal to inference-ml?
* are the reconstructed results sensible?

TODO
=========
* handle reversed curves
* Retrain all models since the following changes
    * reversal fixes
    * noise variance fix
    * "pre-tail" index fix
* central curve extraction
* add branching
* end-to-end sampling system
