---
layout: post
title: "Work Log"
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}

Resuming from yesterday's discussion about Markov sampling.  I was concerned about piecewise sampling of interpolated values with insufficient data being used.  After some thought, realized there's a better approach: construct blocks from input indices, not output indices.


Secondly: only use the markov approach when there's too much data to eat at once.  Those cases are also the ones with the least probability of poor-data issues.

---

Re-implemented (`curve_tree_ml_5.ml`), geting weird results.  Huge negative eigenvalues


symptom: 30 output indices, 1000 output indices
symptom: K has 26k elements!

This code is a mess, proving hard to debug.  I'm going to roll back to version 4, use ideas developed in v5 to implement Markov sampling.

---

Done.  No errors, but results aren't great.  Is it possible the markov blanket is wrong, or maybe we're misusing previous data?

...

I think I found it... wasn't conditioning on previous sampled values.

Seems to be fixed now.  Next on to timing and tuning

Profiling / Optimizing
---------------------------

1. attachment covariance
2. markov order
3. block size

**Constructing Attachment covariance**

construct_attachment_covariance_3 is still a bottleneck.  Need to investigate some of the suggestions from a few days ago.

Only computing `Cov_star_star` once (exploiting stationarity in temporal GP) helps somewhat.

Grouping "sibling" object construction should help a lot too.

**Markov order**

Markov order as low as 10 doesn't seem to negatively affect results.

Need to crop observations before the earliest sampled point we're conditioning on.

    Timing
    Before: 17.0s (total,  7.5s on inversion)
    After: 16.5 (7.2s)

Tiny improvement, but at no cost so who's complaining?

Block Size
-----------

Not much to say here...  100 seems to be optimal.
