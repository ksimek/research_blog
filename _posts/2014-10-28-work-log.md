---
layout: post
title: "Work Log"
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}

Trying to address cholesky issues arising new dynamic covariance.  Eschewing clique tree implementation in favor of direct method for obtaining MAP curves.

gdb on OSX is losing stack frame at exception.  Running in vagrant...

g++ is segfaulting on my vagrant ubuntu

gdb on v11 is crashing due to missing libpython (probaby lost during upgrade to ubuntu 14.04).  

trying my ubuntu 12.04 desktop server.  Taking a realllly long time to check out (15 minutes and counting)

...

OK, finally rolling on Vll.  

...

Getting singular posterior covariance matrix.  why?  ... Singular prior matrix.  why? ...  New dynamics covariance is singular.  But this shouldn't affect joint covariance, right?

...

bug: sqexp parameters not being set when computing prior
bug: sqexp scale parameter not set when reading params
bug: conditional sqexp math error

...

AHA!  Reviewing Rasmussen and Williams, I see now that Koller and Friedman's "Canonical form" is exactly wrong for the types of tasks we want to perform.  When the prior is close to singular (or is singular), we can still do inference, we just have to be careful not to invert the prior covariance matrix.  W&R show how to do this in the simple case.  It remains to extend it to inference in varable blocks organized in markov chains.  


Curve Chain Posterior
-------------------

Below is a derivation of the conditional GP posterior recursion needed to compute the MAP estimate efficiently.  All covariance matrices \(K\) are parameterized in terms of their inputs, \(X\).

<div>
\[

\begin{align}
\mu_{* i | y_{i},y_{i-1}} &= K_{i|y_{i-1}}(X_{*}, X_i) [K_{i|y_{i-1}}(X_i,X_i) + \Sigma_{y_i}]^{-1} * y_i\\
K_{i | y_i, y_{i-1}}(X_*, X_*) &= K_{i\,|y_{i-1}}(X_{*}, X_{*}) - K_{i|y_{i-1}}(X_{*}, X_i) [K_{i|y_{i-1}}(X_i,X_i) + \Sigma_{y_i}]^{-1}K_{i|y_{i-1}} (X_i, X_{*})
\end{align}
  \]

  where 

  \[
  K_{i|y_{i-1}}(A,B) = K_{i}(A,B) - K_{i,i-1}(A, X_{i-1}) [K_{i-1}(X_{i-1}, X_{i-1}) + \Sigma_{y_{i-1}}]^{-1} K_{i-1,i}(X_{i-1}, B)
  \]

  and \(\Sigma_{y_i}\) is the noise variance of observations y_i, conditioned on latent positions z_i.  \(X_i\) are the indicies of observations at time \(i\) and \(X_{*}\) are the indices for reconstruction (same for all views).  \(K_{i,j}\) is the cross-covariance between GP's in views i and j.  \(K_{i|y_j}\) is the covariance of the GP in view i conditioned on observations in view j.  

</div>

