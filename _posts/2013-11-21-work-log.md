---
layout: post
title: "Work Log"
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}

Recall  the expression for the i-th element of the gradient of ML w.r.t. indices.

<div>
\[
\begin{align}
g'_i(x) &= \frac{1}{2} z(x)^\top K'(x) z(x) \\
        &= \frac{1}{2} z_i(x) \delta_i^\top(x) z(x) - Z'_i(x) \\
        &= f_i(x) - Z'_i(x)
\end{align}
\]
</div>

Where \\(\delta_i = k'(x_i, x_j)\\), and \\(Z'_i\\) is the derivitive of the normalization constant.

Next we'll derive the second derivitive starting with the first term, f_i(x).

First term, f_i'
------------------

First, we take the second derivitive of the kernel function, which has the conveniently simple form:

<div>
\[
\begin{align}
\frac{\partial^2 k(x_i, k_j)}{\partial x_i \partial x_j} &= \min(x_i, x_j) \\
\frac{\partial \delta_i(x)}{\partial x_j} &= (0, ..., \min(x_i, x_j), ..., 0)^\top
\end{align}
\]
</div>

We use the product rule to take the derivitive of \\(f_i = z_i \delta_i \cdot  z\\).

<div>
\[
\begin{align}
\frac{\partial f_i(x)}{\partial x_j} &=
            \left ( \frac{\partial}{\partial x_j} \, z_i(x) \right ) \delta_i^\top(x) z(x)  +
            z_i(x) \left ( \frac{\partial}{\partial x_j}\delta_i^\top(x) \right ) z(x) +
            z_i(x) \, \delta_i^\top(x) \left ( \frac{\partial}{\partial x_j} z(x) \right ) \\
&=
            z_i' (x) \, \delta_i^\top(x) \, z(x)  +
            z_i(x) \, \min(x_i, x_j) \, z_j(x) + 
            z_i(x) \, \delta_i^\top(x) \, z'(x)
\end{align}
\]
</div>

where 

<div>
\[
\begin{align}
\frac{\partial z(x)}{\partial x_j} &= \frac{\partial}{\partial x_j} S^\top U^{-1}(x) S y \\
        &= S V' S^\top y \\
        &= S^\top U^{-1} U' U^{-1} S y \\
        &= (S^\top U^{-1} S) K' (S^\top U^{-1} S) y
\end{align}
\]
</div>

All the vectors \\(f_i\\) make up the columns of the hessian matrix.

We can reformulate the entire hessian in terms of vector and matrix operations

<div>
\[
\begin{align}
H(g) &=
            
            \left ( \frac{\partial}{\partial x_j} \, z_i(x) \right ) \delta_i^\top(x) z(x)  +
            z_i(x) \left ( \frac{\partial}{\partial x_j}\delta_i^\top(x) \right ) z(x) +
            z_i(x) \, \delta_i^\top(x) \left ( \frac{\partial}{\partial x_j} z(x) \right ) \\
&=
            z_i' (x) \, \delta_i^\top(x) \, z(x)  +
            z_i(x) \, \min(x_i, x_j) \, z_j(x) + 
            z_i(x) \, \delta_i^\top(x) \, z'(x)
\end{align}
\]
</div>


Each of the \\(z_i\\)'s are are each pre-computed in quadratic time, so all precomputation is accomplished in cubic time.  Each \\(f_i(x)\\) consists of two dot products and a scalar product, which takes linear time, and there are \\(n^2\\) elements in the hessian, so the running time is cubic.

We can reformulate the hessian in terms of matrix arithmetic.  Let \\(Z'\\) be the jacobian of \\(z\\), i.e. it's columns are \\(z_i'\\).  

<div>
\[
H = Z' \odot \left[ \Delta \, z \, (1 \, 1 \, ...) \right] + A \odot \left(z z^\top \right ) + \left[ z \, (1 \, 1 \, ...) \right] \odot \left[ \Delta \, Z' \right]
\]
</div>

where \\(\Delta\\) is a matrix whose rows are composed of the \\(\delta_i\\) vectors, i.e. \\(\delta_{ij} = \frac{\partial k(x_i, x_j)}{\partial x_i}\\), 
      
the matrix \\(A\\) is the hessian of \\(k(x_i, x_j)\\), i.e. \\(a_{ij} = \frac{\partial \partial k(x_i, x_j)}{\partial x_i \partial x_j} = \min(x_i, x_j)\\),

and \\((1 \, 1 \, ...) \\) is a row-matrix of \\(N\\) ones.

Note that the definition of \\(\Delta\\) used here was denoted as \\(\Delta'\\) in the previous writeup.

Second term, \\(Z''_i(x)\\)
------------------------

Below are the expressions for the zeroth, first, and second derivitives of Z;

<div>
\[
\begin{align}
Z &= 0.5 \log(\det(S^K S^\top + I)) \\
\frac{\partial Z}{\partial x_i} &= 0.5 \text{Tr} \left[ U^{-1} U' \right] \\
\frac{\partial^2 Z}{\partial x_i \partial x_j} &= 0.5 \text{Tr} \left[ \frac{\partial U^{-1}}{\partial} U' + U^{-1} \frac{\partial U'}{\partial x_j} \right] \\
        &= 0.5 \text{Tr} \left[ V'_{(j)} U'_{(i)} + U^{-1} U''_{(ij)} \right] \\
        &= 0.5 \left \{ \text{Tr} \left[ V'_{(j)} U'_{(i)} \right] + \text{Tr} \left[ U^{-1} U''_{(ij)} \right] \right\} \\
\end{align}
\]
</div>

Observe that we can rewrite \\(U_{(i)}'\\) as sum of two outer products:

<div>
\[
\begin{align}
U'_{(i)} &= S * K' * S^\top \\
U'_{(i)} &= S * (B + B^\top) * S^\top \\
\end{align}
\]
</div>

where B is the sparse matrix,

<div>
\[
\begin{align}
B = \left( \begin{array}{ccccc}
        \mathbf{0} & \cdots & \delta_i & \cdots & \mathbf{0}
    \end{array}\right)
\end{align}.
\]
</div>

We can simplify \\(U'\\) to

<div>
\[
\begin{align}
U'_{(i)} &= \left(S \, \delta_i \right) S_i^\top  + S_i \left( S \, \delta_i \right)^\top \\
\end{align}
\]
</div>

Note two things about this expresssion: (1) the two terms are transposes of each-other, and (2) the terms are outer products.  Later this outer product will be transformed to an inner product by way of the trace operator.


