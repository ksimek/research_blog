---
layout: post
title: "Work Log"
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}
{% include research/tulips_da3_meta %}


Now that gradient is working, lets try using fminunc to optimize indices.

---

Getting nonsense results.  Looking into curve data.

...

Inspecting the data, it's clear that our method of linearly sampling points along the bezier curve is resulting in very jagged curves.  Example in dataset 8, curve 7:

![]({{site.baseurl}}/img/2013-11-18-dataset8_curve7_view9.png)

*view 9*



![]({{site.baseurl}}/img/2013-11-18-dataset8_curve7_view4.png)

*view 4*

It's not totally clear how best to resolve this.  Ideally, we would sample at a finer grain, but this caused big slow-downs for the longer curves.  Could use coarse-grain sampling for long curves, but some curves are long in some views and short in others, and nonuniform sampling breaks some assumptions we make in the library.  Furthermore, associations are unknown at the time of samplingz

It's possible 

It's possible the bad reconstruction we're seeing from this curve isn't due to bad correspondence, but a bad indexing estimation (a later stage of inference).  We see that although the correspondence places c7v9 toward the end of the 3D curve, our re-indexing code places it more spread out, but unevenly: the first point has index 4, while the subsequent points have indieces [21, 23, 27, 27].  We usually prevent large amounts of index-skipping during re-indexing, but possibly the second-pass refinement is destroying this.

--------------

After double-checking, realized view #5 is the biggest problem child, not #7.

Interestingly, #5 has relatively reasonable looking correspondence:

![]({{site.baseurl}}/img/2013-11-18-curve7_corrs.png)

But the reconstruction (both attached and free) is terrible:
    
![]({{site.baseurl}}/img/2013-11-18-curve7_reconstr.png)

---


ll_means has really bad points at the beginning and end of the curve.  It looks like tails might be handled poorly in corr_to_likelihood_2.m

---

Attempting to run with "FIX_WORLD_T = false" in corr_to_likelihood.  Early probes suggest this improves things; however, now getting a crash when calling `construct_attachment_covariance.m`.  Getting NaN's from curve #8.  I've observed that curve #8's indices start with two zeros.  Maybe this is causing our covariance algorithm to choke?

...

Got it: when we don't correct endpoints (i.e. FIX_WORLD_T = false), endpoints can get repeated, which makes our "initial direction" computation fail.

* Find out why duplicate endpoints occur (doesn't triangulation make this improbable?)
* Handle duplicated points gracefully when computing start point.
