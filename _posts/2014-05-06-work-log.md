---
layout: post
title: "Work Log"
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}
{% include research/fire_plm1_test_meta %}


Ran 10,000 iterations, keeping observation model fixed.  

**Update**: The three plots below are incorrect.  corrected plot follows.
Ground truth model:  
![]({{site.baseurl}}/img/2014-05-06-run1_gt_model.png)

Initial model:  
![]({{site.baseurl}}/img/2014-05-06-run1_initial_model.png)

Final model:  
![]({{site.baseurl}}/img/2014-05-06-run1_final_model.png)

**Update**: There was a bug in my plotting code for the plots above. Here is the corrected plot, which is more sensible:

![]({{site.baseurl}}/img/2014-05-06-run1_fixed.png)

Observations:

1. Initial estimate is bad, esp y-offset.  Should review this code.
2. Recovered results are noticably better.  Not perfect, but this may be because
3. Keeping observation model fixed seems to improve speed by two orders of magnitude (???)

Run 2:  Keep best model
-----

Added a 'best_sample_recorder' to keep track of the best model we've seen.

###Results

![]({{site.baseurl}}/img/2014-05-06-run2.png)

###Observations

1. Best model is pretty good
2. Why is initial model so different from that in the previous run?  same model, same random seed.
3. Keep in mind, on average only 1/3 of the [0,1] domain is represented in the dataset, because it's divided into "before", "during" and "after" regions.

###Discussion

Initial model uses a different observation model, so it's hard to say whether it's good or not.  I get the feeling that there's a lot of slack in this model, meaning several different combinations of observed and latent parameters can give nearly equal results.

Also, initial model estimation doesn't correctly handle x-offsets for "during" and "after" regions.  TODO: fix this.

Note that the noise epsilon is very large relative to the model's dynamic range.  It's hard to visualize, since the plots above are sent through an observation model before noise is added. But the observation model's scaling is around 0.5.  

Variability in the latent variable is around 0.3.  Observation model scales that to 0.15, then adds noise on the order of 1.0.  So signal-to-noise ratio is around 0.15 -- not great.  Luckilly we have lots of observations (150 people x 9 time points x 7 observed dimensions), but in our non-synthetic model, I hope our noise is smaller.

Run 3: Improved initial model estimate
--------------

*SVN REVISION*: 16742  
*Description*: Fixed initial model estimate by centering each region at x=0.  See `model.cpp:partition_observations()`.   

\# iterations: 10,000 iterations  
Running time: 0:18.53  
Results:  
![]({{site.baseurl}}/img/2014-05-06-run3.png)

Summary:  
Initial model has changed, but initial offset is still way off.  Other models (initia, final and best are the same).  

Discussion:  
Need to dig more into the initial estimation code.  Is boost's RNG seeded with current time?  

Run 4: fixed observation parameters
-----------------------------

*SVN REVISION*: 16743  
Goal: See if fixing observation parameters (A, B) improve line-fits.  If so, issue is in observation parameters.  If not, issue is with line-fitting.  
Running time: 0:18.26  

Results:  
![]({{site.baseurl}}/img/2014-05-06-run4.png)

Run 5: fixed observation parameters (take 2)
--------------
Descrijption: Found a bug when using fixed offset B -- wasn't subtracting offset before doing PCA to find A.  
Revision: 16744  
Results:   

![]({{site.baseurl}}/img/2014-05-06-run5.png)

Discussion: 
No change.  In retrospect, this change only applies when not using fixed observation parameters, so no change is to be expected.  But a good bug to fix for later on!

Run 6:
---------
###Description:

Found another bug: when projecting points onto pincipal component, if the direction vector \(d\) isn't normalized to 1, the projected points are off by a factor of \(|d|^2\).   The observation equation is given by:

<div>
\[
y = Ax + B
\]

The goal is to solve for \(x\), which means we need the Moore-Penrose pseudoinverse, \(A^+ = (A^\top A)^{-1} A \).  When \(A\) is a column vector, the term in perentheses on the right-hand size is the squared magnitude of \(A\), which was ommitted in the original equation.
</div>

**Revision**:  16745  
**Invocation**: 

    # On bayes01
    cd ~ksimek/src/fire/src/piecewise_linear/test
    ./test_inference > /dev/null
    cat results.txt | \
        awk '{row=NR%7; if(row == 3 || row == 4) print;}' \
        > ~/tmp/lin.txt
    
    # on local machine
    rsync -avz v01:tmp/lin*.txt ~/tmp/
    
    # in matlab on local machine
    cd ~ksimek/work/src/fire/src/matlab/in_progress
    figure
    lin = load('~/tmp/lin7_1.txt')'
    lin3 = reshape(lin, [3, 2, 4])
    visualize_pl_result(lin3, ...
        {'ground truth', ...
        'initial model', ...
        'final model', ...
        'best model'})


**Results**:  
![]({{site.baseurl}}/img/2014-05-06-run6.png)

###Discussion:

Finally getting good initial estimates.  In fact, initial estimate is the **optimal estimate** when A and B are known.  The best model doesn't look perfect, especially in the red curve, but there seems to be significant variance in the red curve estimator, as shorn in "final model".

Run 7: Initial estimate of A
----------------
**Description**: Time to add initial estimation of A.  Sampling will still only estimate latent parameters.    I'm curious how close this will be to optimal.  I'm forcing the ground-truth A to have unit-length so the experimental results are comparable to the ground truth.  
**Invocation**: see previous  
**Revision**: 16746  

**Baseline**:  The results below are estimates of the linear model when A is known.  

![]({{site.baseurl}}/img/2014-05-06-run7_1.png)

**Results**: Below are results of sampling, using an estimate of A from the noisy data

![]({{site.baseurl}}/img/2014-05-06-run7_2.png)

**Discussion:**  
Initial estimate is slightly worse in the green and blue curves, but still in the ballpark, as we would hope.  As observation noise decreases, this estimate should improve.  

It's notable that HMC still can't find a better model.  This is good news for the quality of our estimate.

Run 8: initial estimate of A and B
-------------
**Description**:  Adding initial estimation of B to the test.
**Invocation**: see previous

**Results**: 

![]({{site.baseurl}}/img/2014-05-06-run_8.png)

**Discussion**:  
Much worse.  This could be caused by overestimating the magitude of \(B\), while understimating the magnitude of \(b\) (which can result in identical models due to our model being overdefined).  Since \(b\) currently adds positive offset to all observations, estimating \(B\) as the mean over observations could is likely to capture some of \(b\).

I suspect a bug, because estimating B should be relatively easy.  On the other hand, perhaps I should have constrainted B to be centered at zero, to avoid convlation

Mid-term TODO
-------
* Tuning HMC
* refactor model 
    * Start and end times out of model
    * epsilon per-dimension
    * missing data
* Add offset constraints
* 

Meta-notes on experiments
-----------------------

1. always test on synthetic data first
2. always test on a simpler model first (fix some parameters)
3. always have a visualization in mind when running a test.
