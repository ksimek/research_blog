---
layout: post
title: "Refactoring, cleanup, bug fixes"
description: ""
category: "Work Log"
tags: []
meta: 
    "SVN Revision": 15169
---
{% include JB/setup %}
{% include research/tulips_da2_meta %}

Goals
======

* Move `test/tmp*` to `experiment/` directory.
* Overlay reconstruction onto images.
* Test effect of reversing curves on ML.
* Train background curve model.

Git Blog Mess
=====
Got sidetracked after screwing up a git commit of the research blog.  Not sure the cause, but several files were deleted from the "source" branch.  Changed the "preview" rake target so it builds to /tmp/research_blog_site, instead of the source directory.  Hopefully this will avoid these issues in the future.

End-to-end experiment
======================

Created an experiment file that recreates yesterday's results from scratch: `exp_2013_08_09_animated_reconstruction.m`.  Since it runs training, it takes about 5 minutes to run.

Also broke out reconstruction code into function in `reconstruction/reconstruct_views.m`.

Overlay reconstruction onto images
===================================

See `test/tmp_vis_overlay.m`.

I'm seeing some weirdness in the reconstructions.  

* Curves are too stiff
* long pre-tails on a couple of curves.


Stiff Curves Solved
==================

Figured out what was causing stiff curves.  I forgot that during training, all precisions are stored with noise_variance fixed at 1.0, and then are scaled on the fly.  However, during visualization, that scaling doesn't occur; the precisions are assumed to be stored at the desired scale.  i.e. a dumb bug.

The new reconstructions now show a moderate amount of curvature, compared to their pre-bug stiff counterparts.

Found another bug:  when constructing the "unscaled" precisions in `tr_prep_likelihood.m`, I called `corr_to_likelihood` with `params.noise_variance` instead of 1.0.  Thus, if I understand correctly, the reported training value for noise-variance is 100x lower than it should be.

That means the no-perturb model has a noise standard deviation on the order of 3.4 pixels and the perturb models are around 2.7.  This is closer to the range I was expecting, but I was hoping the perturb model stddev would be closer to 0.5, because it should arise only from pixel rasterization.  However, other sources of noise could be the curve detector, and also the cubic spline model might not be expressive enough to capture the model variance.

I think I need to re-run end-to-end training and reconstruction to make sure there aren't any side-effects of these fixes.

Committed to revision 15169

TODO
===

* Determine cause of long pre-tails.
