---
layout: post
title: "Troubleshooting excessive index drift in endpoints; fixing Hessian under variable transformation."
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}

Getting bizarre spikes in indices during optimization.  Confirmed that removing the spikes will improve ML, but there's a steep well between the two local minima as we reduce the index value:
    
![]({{site.baseurl}}/img/2013-12-05-ml_vs_index.png)


The index starts at a fairly reasonable initial value, so my only guess is that the Hessian is suggesting a large step which happens to step over the well.

I'm wondering if the hessian is screwy; or maybe it's just the transformation we're using.  Optimizing raw indices doesn't exhibit this problem, but it is a problem in our our current approach of working with log differences to prevent re-ordering.

A prior over index spacing should probably prevent this; I'm hesitatnt to add the extra complexity at this point, considering the additional training and troubleshooting it would entail.

Should unit test the transformation.

...

Gradient looks okay.

On-diagonal elements have significant error!

![]({{site.baseurl}}/img/2013-12-05-hessian_error.png)

Actually, significant off-diagonal error, but on-diagonal dominates.

Since gradient is fine, and it uses the same jacobian, I'm guessing the problem isn't the jacobian transformation, but the hessian itself.

...

Confirmed.  diagonal is (mostly) too high, error on order of 1e-3.  Gradient error is around 1e-9.


Detective work.  Look at diagonal of each Hessian term, compare to residual of diagonal, look for patterns.

...

Ugh, worst afternoon ever.  Spent hours trying every trick in the book to track down the source of the error, including lots of time looking at the raw Hessian (it wasn't the raw Hessian).  Finally found the bug: I formula I used for the chain rule for Hessians was wrong.  In particular, it was missing a second term that (in my problem) corresponded to adding the transformed gradient to the diagonal.   See [Faa di Bruno's formula](http://en.wikipedia.org/wiki/Chain_rule#Higher_derivatives_of_multivariable_functions).

Total error is much reduced now, but not zero.  around 0.1, instead of 20 before, new results:

![]({{site.baseurl}}/img/2013-12-05-hessian_error_2.png)

The norm-check is around 1e-4; very nice.

...

Re-running dataset 11 with hopes that we don't lose the global optimum.  Interesting observation: optimization requires more iterations than before to converge.  It seems a more conservative hessian results in smaller steps and is less likely   Looks better:

![]({{site.baseurl}}/img/2013-12-05-reconst_hess_fixed.png)

Notice we're still getting offset, but at least the reconstruction is qualitatively better that before. However, now we're getting a small loop at the top:
    
![]({{site.baseurl}}/img/2013-12-05-weird_loop.png)

It seems to be caused by changing of index order between views. Needs some thought to best address.  

...

Re-running on all datasets.  Hopefully excessive index drift won't be too big an issue.  Possibly an extra term to prevent drift far from initial points would be sensible.

Datasets 4,5  still drifts:
    
![]({{site.baseurl}}/img/2013-12-05-drift_ds4.png)
![]({{site.baseurl}}/img/2013-12-05-drift_ds5.png)

Datasets 7,9  have detached curves

Dataset 10, curve 2 (?) appears to have failed to prune

endpoint drift
------------------

It looks like interior points are confined by their neighbor points from drifting too far, but end points have no such constraint.  After a small amount of drift, they're able to loop back on themselves and land directly on the backprojection line.  It's surprising that the extra flexibility afforded by large spacing between indices doesn't cause marginal likelihood to suffer, since most of the new configurations are bad ones.

Considerations

* in-plone offset perturbation
* penalize excessive drift
* penalize excessive gaps (a-la latent GP model).
* penalize shrinkage.
