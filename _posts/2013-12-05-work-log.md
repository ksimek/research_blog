---
layout: post
title: "Work Log"
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}

Getting bizarre spikes in indices during optimization.  Confirmed that removing the spikes will improve ML, but there's a steep well between the two local minima as we reduce the index value:
    
![]({{site.baseurl}}/img/2013-12-05-ml_vs_index.png)


The index starts at a fairly reasonable initial value, so my only guess is that the Hessian is suggesting a large step which happens to step over the well.

I'm wondering if the hessian is screwy; or maybe it's just the transformation we're using.  Optimizing raw indices doesn't exhibit this problem, but it is a problem in our our current approach of working with log differences to prevent re-ordering.

A prior over index spacing should probably prevent this; I'm hesitatnt to add the extra complexity at this point, considering the additional training and troubleshooting it would entail.

Should unit test the transformation.

...

Gradient looks okay.

On-diagonal elements have significant error!

![]({{site.baseurl}}/img/2013-12-05-hessian_error.png)

Actually, significant off-diagonal error, but on-diagonal dominates.

Since gradient is fine, and it uses the same jacobian, I'm guessing the problem isn't the jacobian transformation, but the hessian itself.

...

Confirmed.  diagonal is (mostly) too high, error on order of 1e-3.  Gradient error is around 1e-9.


Detective work.  Look at diagonal of each Hessian term, compare to residual of diagonal, look for patterns
delta_Dz
