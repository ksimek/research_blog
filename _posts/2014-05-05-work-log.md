---
layout: post
title: "FIRE - piecewise linear inference"
description: ""
category: 'Work Log'
tags: []
pomodoro: true
---
{% include JB/setup %}

Finished compiling inference test with synthetic data.  

Gradient is taking incredibly long, especially for a 7-dimensional model.  PErhaps 100 data points is too large, but I'm guessing the cost of allocating vectors for each function evaluation is the botteneck (GDB seems to agree).  

<div>
<strong>Time to run 1 iteration</strong><br/>
<em>Single threaded, on bayes01</em>
<table border="1">
<tr>
<td>Baseline <br/>
(Debugging mode, allocation checking enabled)</td>
<td>
1:28.94
</td>
</tr>
<tr>
<td>Heap checking disabled
</td>
<td>
0:21.06 (-1:07.88, 4.22x)
</td>
<tr>
<td>Heap & initialization checking disabled
</td>
<td>
0:11.19 (-9.87 1.88x)
</td>
</tr>
<tr>
<td>PRODUCTION=1 (with -O2)
</td>
<td>
0:07.24 (-3.95 1.55x)
</td>
</tr>
<tr>
<td>-O3
</td>
<td>
0:07.86 (+0.62 0.92x)
</td>
</tr>
</table>


Used gprof with grpof2dot.py to get the following diagram: 
[gprof.pdf]({{site.basurl}}/img/2014-05-05-gprof_1.pdf).

iso_mvn_lpdf is getting hit hard:

{% highlight c %}
    double iso_mvn_lpdf(const double* mu, const double* y, double epsilon, size_t D)
    {
        double accum = 0;
        double d;
        for(size_t i = 0; i < D; ++i)
        {
            d = (*mu++ - *y++) / epsilon;
            accum += d*d;
        }
        return -0.5 * (accum + D*log(2*M_PI*epsilon*epsilon));
    }
{% endhighlight %}
             
It's already pretty lean (no alloc, all c-style).  But we can move the divide-by-epsilon out of the loop for an easy 1.8x speedup.
    
{% highlight c %}
    double iso_mvn_lpdf(const double* mu, const double* y, double epsilon, size_t D)
    {
        double accum = 0;
        double d;
        for(size_t i = 0; i < D; ++i)
        {
            d = *mu++ - *y++;
            accum += d*d;
        }
        accum /= epsilon*epsilon;
        return -0.5 * (accum + D*log(2*M_PI*epsilon*epsilon));
    }
{% endhighlight %}

Now the bottleneck
