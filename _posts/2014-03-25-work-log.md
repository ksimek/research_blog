---
layout: post
title: "Debugging reconstruction anomalies"
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}

Continuing from yesterday.  Issue: strange reconstruction results after retraining.

Experiments
---------------

###Experiment #1

Add a small amount to the diagonal of the covariance and re-run reconstruction.  

Result:  Adding a moderate amount of covariance to the diagonal seems to fix the results but probably due to additional smoothing.

Adding 1.5 - makes results worse, bizarre jutting sections. This counterintuitive if the phenomenon is due to increasing noise variance.
Adding 2 - makes reconstruction look good.
Adding 10000 - reconstruction becomes straight sticks.

Discussion: analytically this is equivalent to multiplying the noise variance by a constant.  We already know that increasing noise variance solves the issue.  The question remains: why did our training algorithm prefer this? Also, why does decreasing noise variance force the reconstruction *away* from the data?  Strong contradiction between data and 

###Experiment #2

Add a small amount to the diagonal of the prior covariance matrix

Result: no obvious improvement (inconclusive)

###Experiment #4

Disable attachment

Result:  *significant improvement*. 

Discussion:  This was totally unexpected.  Almost all issues seem to be caused by bad attachment.  The affect of bad attachment is probably exacerbated by small noise variance -- the hardness of the constraints lead to contradictions that are resolved by anomalous reconstructions.





###Experiment #3

Plot likelihood (reference implementation) against noise variance.


Miscellaneous thoughts
----------------------

It kind of makes sense that iid noise would be zero, since the data we're drawing from is so smooth to begin with, and basically noiseless. Any errors arise from mistracing and are strongly correlated due to the smoothness of the Bezier curves.

If the issue is near-singular matrices, the traditional solution to this is adding a tiny amount of covariance to the diagonal of the prior matrix.  

Open issues
------------

Index optimization resulting in weird loops at endpoints
