---
layout: post
title: "Work Log"
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}

Goal:
----------
* infer 
* Tree tracking
* 3D reconstruction from pairwise match
  * bayesian 3D
  * project 2D
  * optimize (3D alone)
  * optimize (w/ 2D information too)

* optimize width

---

Ran perturbation fitting to track curves between views.  Results are quite bad.  A few problems:
  
* several curves are on the background, so don't follow the epipolar constraint.
* All curves are independent, increasing the dimensionality of the search space and making local minima worse
* Several curves are duplicates of existing curves, so don't match well in the second image.

Started implementing attachment inference.

Inferring curve attachment
------------------------

Goal: infer attachment between curves.  Since the model dimension doesn't change after adding branching,
and since the likelihood doesn't change either, we'll just use the prior to evaluate branching, rather than the marginal likelihood.  Although the decision to attach two curves isn't strictly independent of the nearby attachments, we'll approximate it as such.  That way, when deciding whether a new attachment is valid, we only need the measure the change in prior of the two curves, rather than the entire tree.

First we need to estimate the optimal branch parameters, which are the position of the branch point on the parent, and the distance between the branch point and the first observed child point.  We'll call these `branch` and `offset`, respectively.

  The branch point is guessed by finding the nearest sampled point on the parent to the initial point on the child.  The offset is guessed be measuring the distance between the branch point and initial point on the child.  Starting with these guesses, we run gradient-based optimization to find the pair of values that minimizes the negative log liikelihood.  The gradient w.r.t. these parameters is given in eq (5.9) in Williams and Rasmussen, we only need to derive the expression for the partial derivative of the covariance matrix.  For this, we use automatic differentialtion, using matlab's support for complex numbers.  The general form to estimate the derivative is
  <div>
  \[
    df/dx = \mathcal{IM}\{ f(x + h i) \} / h + O(h^2)
    \]
</div>
where \\(i\\) is the imaginary number and \\(\mathcal{IM}\\) is the imaginary part of the result.  This is superior to finite differences, because it doesn't suffer loss of precision as the step size \(h\) decreases, and error decreases quadratically rather than linearly.

This seems to work relatively well, except that our curve-detection finds multiple curves for the same true curve.  We need some nonmaximal suppression.


Open issues
--------------
* handling curves that start behind the parent curve.  Currently negative offsets aren't supported, because the cubic spline process is only define over the postive reals.  
* partial overap isn't explicitly penalized in the likelihood.  An edge-based likelihood could resolve this.
