---
layout: post
title: "Work Log"
description: ""
category: 
tags: []
---
{% include JB/setup %}
{% include research/tulips_da2_meta %}

**Hypothesis**: The likelihood covariance for the "virtual observations" scales linearly with the 2D likelihood variance.

**Experiment**: see `experiments/exp_2013_07_21_likelihood_covariance.m`.  Constructs a likelihood using noise_variance of one and then scaling precisions afterward.  Compares to directly-constructed likelihood precisions.  

**Results**: Negligible difference

**Conclusion**: Practice matches theory--scaling likelihood precisions is equivalent to constructing likelihood with the scaled precision.

**Discussion**: This conclusion means that we can construct the likelihood precisions exactly once during training, and simply scale them as we modify the likelihood precision.  It would make sense to always use 1.0 when computing precisions, and refactor all existing code to scale the matrix by the reciprocal of the noise variance before using it.  

TODO
----------
* Build training framework
    * fast likelihood evalautor 
        * custom function for evaluating ML without recomputing stuff
        * cache the three prior component matrices (smooth, linear, and offset)
        * cache unscaled likelihood precision.
    * wrap in a lambda
        * scales and combines all components
        * depending on motion model, use different expression for purturbation coefficient
    * pass to quasi-newton minimizer
* Repeat for background curves
* Heuristic pruning using background-subtraction.

