---
layout: post
title: "iPlant "
description: ""
category: 'Work Log'
tags: []
---
{% include JB/setup %}

Doing background research inre: forwarded email from Kobus, "Fwd: What we really need from your students".  

Literature review
=====================

**Paper 1:** *Three-Dimensional root Phenotyping with a Novel Imaging and Software Platform*, by Clark, et. al.

Introduces a semi-automated method for semantic reconstruction of root structures from turntable images, using our lab's definition of semantic reconstruction, i.e. 3D curves with topology and part labels.  It looks like they're using standard voxel-carving to identify foreground and background voxels (i.e. root and non-root voxels).  This approach uses calibrated cameras  to backproject silhouette images and take the intersection--in other words, visual hull but with voxels instead of polygonal meshes.   Then the skeleton of the foreground voxels is extracted using a median filter method to extract 3D curves.  Skeleton branches are then manually labeled by domain experts as one of several root types.  There also seems to be some functionality to manually correct errors during backprojection and skeleton extraction phase.  

They key contribution seems to be a list of 27 features derived from the resulting 3D data.  Bushiness, centroid and volume distribution seem to be discriminative for classifying a specimen between the two different species under study.    They also  measure the amount of helical curvature and how much gravity affects growth, which has apparently not been studied in rice plants prior to this?.

Extracting most of the interesting features requires full semantic reconstruction, which is very difficult to obtain using known fully-automatic methods.  Further, this approach requires a calibrated camera, which likely precludes us from using it for post-hoc analysis of existing datasets that might exist in Bisque, unless calibration data is available.  



The "Clark Rice root" image provided on the wiki is a high resolution 2D image, which appears to be different from those used in the Clark paper, so it's unclear what its relevance is in this context.

*Other notes*

Its unclear how silhouettes are extracted, but likely just intensity thresholding with manually-chosen threshold value.  They're using a lightbox background, so this approach seems sensible.   There's probably some tunable parameters for the skeleton-extraction phase, but these aren't discussed.


Bisque thoughts
===================

* 3D approaches might be unrealistic in Bisque
    1. Most datasets won't have calibrated cameras.
        * Could try auto-calibration, but I double their effectiveness in the wild.
        * Could have user click corresponding points, between views, but this leads us to...
    2. Most datasets won't have multiple views.
        * could try bayesian approaches to solving the inverse problem, but this is an open research problem.

    3. If 1. and 2. are satisfied, we still need significant user interaction to implement Clarke; unlikely to be possible in Bisque?
        * Maybe Bisque could receive the files output by Clarke's software?  But at that point, the analysis is already done, what is Bisque being used for?
* Most effective 3D systems will collect data under a process specifically tailored to 3D data.  How does Bisque fit into this scenario?




iPlant recognizes that most IP software developed by biologists is heavilly tuned to a specific application, and doesn't generalize to other datasets.  This is a problem endemic to the entire field of computer vision; most approaches work well on the dataset they were trained on and fail on others.  Even the Deva/Feltzenswalb human detectors--arguably one of the most robust object detectors in the field -- performs poorly on new datasets, as we saw when we applied it to Mind's Eye.  Significant rescaling of image sizes, tuning of thresholds, and retraining was needed before it performed reasonably well.  

The natural response for us as CV researchers is to try to overcome this truth by looking to cutting edge research that aims for greater robustness, but I argue that this is a mistake in this scenario.  Instead, we should look at existing solutions that relax the "fully automatic" constraint and as a result provide high robustness.  Clark is a perfect example of semi-automatic methods that use some human interaction in exchange for drastically better results.  I've seen similar successes in the area of plant modelling, where interactive CAD interfaces are combined with CV/IP algorithms from 5-10 years ago to create a method that is efficient and provides precise models.



iPLant needs software that is proven, robust
Rather than trying to ignore this fundamental truth 

The most effective approaches for iPLant will be algorithms that users can train on a subset their specific dataset.  


Features biologists care about vs. algorithms CV researchers care about.  Semisupervised methods.
