<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>KLS Research Blog</title>
 <link href="http://vision.sista.arizona.edu/ksimek/research/atom.xml" rel="self"/>
 <link href="http://vision.sista.arizona.edu/ksimek/research"/>
 <updated>2013-07-21T13:07:57-07:00</updated>
 <id>http://vision.sista.arizona.edu/ksimek/research</id>
 <author>
   <name>Kyle Simek</name>
   <email>ksimek@email.arizona.edu</email>
 </author>

 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log"/>
   <updated>2013-07-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing: ground-truth-to-data-labels.  See file &lt;code&gt;train/label_from_ground_truth.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finished.&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data labels to likelihood means/covariances&lt;/li&gt;
&lt;li&gt;Likelihood means/covariances to marginal likelihood&lt;/li&gt;
&lt;li&gt;training framework&lt;/li&gt;
&lt;li&gt;training&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Visualizing &lt;code&gt;labels_from_ground_truth()&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Wrote &lt;code&gt;tmp_get_max_posterior.m&lt;/code&gt;, a temporary script that computes the posterior mean from a possibly-overconstrained prior.  In this case, the posterior covariance is singular, but the mean can still be obtained.  The math behind it &lt;a href=&quot;/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance/&quot;&gt;is available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Below is a plot using the test dataset and the ground truth labels:&lt;/p&gt;

&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/foL28SUn1JM?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;This shows that given a good labeling, a quality 3D reconstruction can be obtained using only the fragmented curves output by the curve-detector.&lt;/p&gt;

&lt;p&gt;Notice that the curves at the base have missing parts.  There isn't sufficient edge data here, but this could probably be fixed by connecting them to the base of the main stem and using the Branching Gaussian Process prior to enforce connectivity.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Maximum posterior with singular prior covariance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance"/>
   <updated>2013-07-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Consider the scenerio of Bayesian inference with a linear-Gaussian prior and likelihood.&lt;/p&gt;

&lt;p&gt;It is sometimes the case that our prior has a singular covariance matrix, indicating that our variables are embedded in a lower-dimensional hyberplane, i.e. some dimensions are redundant.  In our 3D curve triangulation application, this situation arises when  the same 3D point is observed in from multiple views.&lt;/p&gt;

&lt;p&gt;We can still find the maximum posterior arising from such a prior, as long as the likelihood is non-singular.   We can interpret this as multiple observations of the rendant dimensions, and if we are careful with our math, we can handle it the same way as the case with a non-singular prior.&lt;/p&gt;

&lt;p&gt;Given a likelihood \( \mathcal{N}(\mu_l, \Sigma_l) \) and prior \( \mathcal{N}(\mu_0, \Sigma_0) \), recall that the posterior is given by  \( \mathcal{N(\mu_P, \Sigma_P)} \), where&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
\Sigma_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} \\
\mu_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} (\Sigma_l^{-1} \mu_l + \Sigma_0^{-1} \mu_0)
\end{align}

\]
&lt;/div&gt;


&lt;p&gt;However, if \(\Sigma_0 \) is singular, we must avoid inverting it when computing \( \mu_P  \).  An equivalent equation for \(\mu_P\) that satisfies this condition is:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;




&lt;div&gt;This should be computable as long as the likelihood precision matrix \(\Sigma_l^{-1}\) has no infinite eigenvalues.  &lt;/div&gt;


&lt;p&gt;To see an example result, see &lt;a href=&quot;/ksimek/research/2013/07/19/work-log/&quot;&gt;today's Work Log&lt;/a&gt; entry.&lt;/p&gt;

&lt;h2&gt;Implementation Notes&lt;/h2&gt;

&lt;p&gt;I tried implementing a version of this that uses Cholesky decomposition and backsubstitution instead of generic matrix inversion.  I needed a symmetric matrix, so I modified the equation for \(\mu_P\):&lt;/p&gt;

&lt;div&gt; \[
\mu_P = \Sigma_0 (\Sigma_0 \Sigma_l^{-1} \Sigma_0 + \Sigma_0)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;This was not noticibly faster than general matrix inversion, because it involves two additional large matrix multiplications.&lt;/p&gt;

&lt;p&gt;Surprisingly, I &lt;em&gt;was&lt;/em&gt; able to get a significant speedup (~7x) be using backsubstitution (Matlab's '\' operator) &lt;em&gt;without&lt;/em&gt; Cholesky decomposition.  I always assumed that the lower-triangular form was what made backsubstitution so fast, but it is apparently also fast with dense matrices.  So we can avoid the extra expensive dens-matrix multiplications, and also avoid expensive matrix inversion.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log"/>
   <updated>2013-07-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bootcamp demo session&lt;/li&gt;
&lt;li&gt;Ground truth labeling of curve fragments.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Bootcamp demo session&lt;/h2&gt;

&lt;p&gt;Ran bootcamp demo session.  Final code available at&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;svn+ssh://v01/misc/svn/src/bootcamp/kjb_demos/3d_demo
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Ground truth labeling of curve fragments&lt;/h1&gt;

&lt;p&gt;Goal: use 2D ground truth to automatically label bottom-up curve fragments.&lt;/p&gt;

&lt;h2&gt;Overview&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Draw ground truth curves using curve index&lt;/li&gt;
&lt;li&gt;dilate slightly&lt;/li&gt;
&lt;li&gt;For each rendered data curve, gather all GT indices&lt;/li&gt;
&lt;li&gt;keep most-occurring GT index.&lt;/li&gt;
&lt;li&gt;add to assoc list for that curve&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Issues:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Remember how to read and process ground truth.&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; See &lt;code&gt;../ground_truth/read_gt2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Symlinked to &lt;code&gt;train/read_gt2.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Which dataset does our example data come from?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/ler_5_36_0.jpg&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Where is ground truth data?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/resized_50%/ground_truth_2d.gt2&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Blah Blah&lt;/h2&gt;

&lt;p&gt;Refactored some code into new function &lt;code&gt;build_curve_maps.m&lt;/code&gt;, which renders each of the curves in a cell-array into a map containing their indices.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored as Bezier curves.  Symlinked the all bezier-related code into data_association_2.  Relevant function is &lt;code&gt;bezier/polybez_to_polyline.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored in OpenGL-style coordinates (bottom-left origin).  Converting to top-right using &lt;code&gt;tools/flip_y.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Summary &lt;/h2&gt;

&lt;p&gt;Got through item 2 in &quot;Overview&quot; above.  Will finish next time, and start building training framework.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log 2</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2"/>
   <updated>2013-07-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Experimenting with &lt;code&gt;ou_perturb_kernel.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Was getting weird results, where the ML approached infinity as &lt;code&gt;noise_variance&lt;/code&gt; approached zero.&lt;/p&gt;

&lt;p&gt;Realized that the set of precision matrices needs to be updated &lt;strong&gt;every time the noise sigma changes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is an oversight that has tripped me up before.  Need to try to avoid it in the future.&lt;/p&gt;

&lt;p&gt;It is possible (likely?) that this transformation is as simple as multiplying the precision matrices by \(\sigma_n* / \sigma_n\).  This would avoid a semi-expensive Hessian calculation for each point, which could be a bottleneck during training.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;p&gt;See previous entry&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log"/>
   <updated>2013-07-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14866&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Note: This entry marks then end of significant work on the new likelihood function.  The SVN revision is noted in the meta-box to the right.&lt;/p&gt;

&lt;h2&gt;Optimizing &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Attempting to use sparsity to speed up &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First attempt&lt;/strong&gt;: try building only the relevant elements of the prior matrix, K.&lt;br/&gt;
&lt;strong&gt;Resut&lt;/strong&gt;: Gains in multiplication are lost in construction of K.  Insignificant speedup.  Rolling back.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second attempt&lt;/strong&gt;: convert block-diagonal matrix &lt;em&gt;S&lt;/em&gt; to a sparse matrix.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Significant gains in multiplication; tolerable losses when constructing &lt;em&gt;S&lt;/em&gt;.  ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third Attempt&lt;/strong&gt;: Convert the individual blocks &lt;em&gt;S_i&lt;/em&gt; to sparse, &lt;em&gt;then&lt;/em&gt; construct &lt;em&gt;S&lt;/em&gt; from them.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Further speedup of ~2x&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fourth Attempt&lt;/strong&gt;:  Change &lt;code&gt;(eye(size(K)) + S * K * S')&lt;/code&gt; to &lt;code&gt;(speye(size(K)) + S * K * S')&lt;/code&gt; ; i.e. changing &lt;code&gt;eye&lt;/code&gt; to &lt;code&gt;speye&lt;/code&gt;.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: moderate speedup, ~1.5x.&lt;/p&gt;

&lt;p&gt;I think we've squeezed all we can from this function.  It's now 10x faster than the naive method on a problem with 4000-dimensions.&lt;/p&gt;

&lt;h2&gt;Results: &lt;/h2&gt;

&lt;p&gt;Running &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6362.9 ms)
Old ML: 207.041742

Computing marginal likelihood (new way, legacy correspondence)...
Done. (7655.2 ms)
New ML (Legacy corr): 207.700616

Computing marginal likelihood (new way)...
Done. (6537.7 ms)
New ML (New corr): 793.585038

Computing marginal likelihood (new way, no MIL)...
Done. (6330.3 ms)
New ML (New corr, no MIL): 793.600835

Computing marginal likelihood (direct method)...
Done. (667.9 ms)  &amp;lt;-------------------  10x speedup over previous
New ML (direct method): 793.300407 &amp;lt;--  0.04% error!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note speedup and negligible error compared the previous method.&lt;/p&gt;

&lt;h2&gt;General Observations Regarding &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;I've noticed that &lt;code&gt;markov_order&lt;/code&gt; must be much larger than I expected to avoid approximation error.&lt;/p&gt;

&lt;p&gt;Recall in &lt;code&gt;curve_ml2.m&lt;/code&gt;, we use the Markov assumption to break-down the prior covariance, and then combine them with the likelihood cliques.  In that case, we could use a Markov order between 2 and 5 without significant approximation error.&lt;/p&gt;

&lt;p&gt;In &lt;code&gt;curve_ml5.m&lt;/code&gt;, using a Markov order less than 100 results in unacceptable error.&lt;/p&gt;

&lt;p&gt;There are two reasons for this.  First, we're decomposing the marginal likelihood Gaussian, not the prior.  The prior is explicitly Markovian, whereas the marginal likelihood is not.  The ML Guassian adds extra uncertainty to every point, which means we need to consider more nearby points to avoid erroneous conclusions.&lt;/p&gt;

&lt;p&gt;Second, because the new approach creates a distinct index set for each view, indices are often repeated (multiple views of the same point) and considering them doesn't tell you anything about what direction the curve is heading.&lt;/p&gt;

&lt;p&gt;The first point will likely be mitigated when we start using the new curve model, which will allow the likelihood variance to decrease dramatically.  However, the markov assumption in the prior is less likely to hold under these models, so some experimentation will be needed.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The code for the new likelihood, &lt;code&gt;curve_ml5.m&lt;/code&gt; is now stable.  It is fast and accurate in its current implementation, assuming a reasonable value for &lt;code&gt;markov_order&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Medium-term goal: calibration/training for all curve models.&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ground-truth labelling of bottom-curve curve data&lt;/li&gt;
&lt;li&gt;set-up method for evaluating ground-truth using a given parameter-set, without re-computing indices each time.&lt;/li&gt;
&lt;li&gt;Run multi-dimensional optimization to fit for all models&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log"/>
   <updated>2013-07-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing new direct method for marginal likelihood.&lt;/h2&gt;

&lt;p&gt;Implemented in &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After initial attempt, the new method produced inconsistent results.&lt;/p&gt;

&lt;p&gt;Re-derived the result two more ways and the math comes out the same.  The theory looks right.&lt;/p&gt;

&lt;p&gt;Finally found the bug -- was computing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;instead of&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = 2 * log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the Cholesky decomposition is the square-root of the matrix Sigma, I was forgetting to double it's determinant to get the determinant of Sigma.&lt;/p&gt;

&lt;p&gt;Now getting the correct results; now time to make it fast.&lt;/p&gt;

&lt;h2&gt;Optimizing direct method for ML&lt;/h2&gt;

&lt;p&gt;Need to get O(n) runtime instead of O(n&lt;sup&gt;3).&lt;/sup&gt;  Will use existing code for markov-decomposed PDF evaluation.&lt;/p&gt;

&lt;p&gt;Bottleneck is 100% the Cholesky decomposition.  The direct method doesn't remove redundant dimensions, so it's slower than &lt;code&gt;curve_ml4.m&lt;/code&gt;, which does.  Recall that this fact makes &lt;code&gt;curvE_ml4.m&lt;/code&gt; not general enough to use the new foreground models with.&lt;/p&gt;

&lt;p&gt;...........&lt;/p&gt;

&lt;p&gt;After some investagation, the previous statement appears to be untrue.  The bottlenect is dense matrix multiplication, which is fixed by using sparse matrices.&lt;/p&gt;

&lt;p&gt;Now, cholesky &lt;strong&gt;is&lt;/strong&gt; the bottlenect, but not as huge as before.  The markov decompose method only gives a ~30% speedup at best, while introducing ~2.5% error and significant extra complexity. Is it worth it?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Direct Evaluation of the Marginal Likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood"/>
   <updated>2013-07-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Previously, we've always used hacks to evaluate the marginal likelihood, because evaluating it directly was apparently impossible because of an infinite normalization constant.  I've discovered that this isn't actually true; the infinite normalization constant arose due to our approximation of the likelihood.  The trick in correctly computing the ML is to replace the usual normalization constant with a corrected one.&lt;/p&gt;

&lt;p&gt;Our old method required knowledge of the maximum posterior solution, which was costly to compute.  The new approach is straightforward to describe, more accurate than our previous method, and can be evaluated in linear time by exploiting its Markovian structure.&lt;/p&gt;

&lt;p&gt;Below is a rough-draft writeup of my derivation of the marginal likelihood function.&lt;/p&gt;

&lt;h2&gt;Direct Evaluation of the Marginal likelihood&lt;/h2&gt;

&lt;p&gt;The 3D marginal likelihood function arises as the sum of a 3D curve process and a 3D perturbation process.  Both are gaussian (the second is approximate), this the result is a Gaussian function, the convolution of two Gaussians.  However, the perturbation process has infinite variance in the direction of backprojection, which arises from the fact that perturbation actually occurs in 2D, we are just backprojecting it to 3D for tractibility.  In other words, the ML isn't a distribution in 3D, it's a distribution in 2D, and we need to determine the appropriate normalization constant for the 3D function to it generates 2D ML densities.&lt;/p&gt;

&lt;p&gt;The likelihood function is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y | x) &amp;= \prod p(y_i | x_i) \\
         &amp;= \prod \mathcal{N}(y_i; \rho_I(x_i), \sigma_n^2 I) \\
         &amp;\approx \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \mathcal{G}(Y_i; x_i, S_i^{-1}) \\
         &amp;= \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \exp\{(Y_i - x_i)^\top S_i (Y_i - x_i)\} \\
         &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}^{2n}} \exp\{(Y - x)^\top S (Y - x)\} \\
         &amp;= \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\}
\end{align}
\]&lt;/div&gt;


&lt;p&gt;Where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\rho_i(x)\) is the projection of x into the I-th view,&lt;/li&gt;
&lt;li&gt;\(Y_i\) is the estimated backprojected position of observation \(y_i\),&lt;/li&gt;
&lt;li&gt;\(S_i\) is the curvature of the 3d likelihood function w.r.t. \(x_i\) evaluated at $Y_i$,&lt;/li&gt;
&lt;li&gt;\(Y\), \(S\) and \(x\) are the concatenation of \(Y_i\), and \(x\)&lt;/li&gt;
&lt;li&gt;\(S\) is the block-diagonal matrix of \(S_i\)'s&lt;/li&gt;
&lt;li&gt;\(\mathcal{G}\) is a Gaussian function (an unnormalized normal distribution).&lt;/li&gt;
&lt;li&gt;\(Z_l\) is a normalization constant.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We have transformed the likelihood into a log-linear function of x, by rewriting the PDF in terms of only 3D entities.  Note that \(S_i\) has zero curvature in the backprojection direction, resulting in infinite variance in the Gaussian function.   Also note that the normalization constant isn't the standard one for a 3D gaussian distribution, because this isn't a distribution over x.  The normalization constant is chosen so the approximate likelihood in 3D agrees with the exact 2D likelihood when perturbation is zero (i.e. when \(Y_i\) lies anywhere on the backprojection line).&lt;/p&gt;

&lt;p&gt;This approximation ignores the nonlinearity of projection; the likelihood function as a function of \(x_i\) would actually look like a cone whose axis-perpendicular slices are Gaussians, whereas our function is cylinder-shaped.  In practice, the approximation error has minimal effect on the marginal likelihood computation, assuming $Y_i$ is a good estimate of the posterior depth, and the posterior is reasonably peaked.  (A graphic to illustrate would be good here).&lt;/p&gt;

&lt;p&gt;The prior is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(x) &amp;= \mathcal{N}(x; \mathbf{0}, K) \\
     &amp;= \frac{1}{Z_p} \exp\{x^\top K^{-1} x\}
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;where \(K\) is the covariance matrix arising from the Gaussian process kernel, and Z_p is the standard Gaussian normalization constant.&lt;/p&gt;

&lt;p&gt;The marginal likelihood is defined as&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \int p(y|x) p(x) dx \\
     &amp;= \int \left ( \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\} \right ) \left ( \frac{1}{Z} \exp\{x^\top K^{-1} x\} \right ) \\
     &amp;= \frac{1}{Z_l Z_p}\int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;The expression within the integral is the convolution of two unnormalized zero-mean Gaussians, so the integral is a zero-mean Gaussian whose covariance is the sum of the inputs' covariances.  If the input Gaussians &lt;strong&gt;were&lt;/strong&gt; properly normalized, the normalization constant would be \(1/Z\);  since they are not, the normalization constant is \((Z_1 Z_2 / Z)\), where \(1/Z_1\) and \(1/Z_2\) are the would-be normalization constants for the first and second Gaussian, respectively.  Note that the second Gaussian's normalization constant is \(1/Z_p\), so this results in the cancellation we see below&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \frac{1}{Z_l Z_p} \int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     &amp;= \frac{1}{Z_l Z_p} \left ( \frac{Z_{l,3d} Z_p}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \} \right ) \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \}
     \end{align}
\]&lt;/div&gt;




&lt;div&gt;
    Where \( Z_{l,3d} \) is the would-be 3D normalization constant for our likelihood Gaussian.  Since \(S\) is rank-deficient, the normalization constants based on \(S^{-1}\) will be infinite (i.e. \(Z_{l,3d}\) and \(Z\)).  However, the terms involving determinants of \(S^{-1}\) should cancel in the ratio, resulting in a gaussian with infinite variance, but non-infinite normalization constant. **(Need to show this mathematically next time)** 
    &lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log"/>
   <updated>2013-07-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Markov-decomposed ML&lt;/h2&gt;

&lt;p&gt;The original plan was to markov-decompose the prior and posterior, but soon remembered that I also need to compute the mean of the posterior, which isn't as straighforward to compute.&lt;/p&gt;

&lt;p&gt;After some deliberation and whiteboarding, I decided to implement Bishop's forward/backward algorithm to simultaneously marginalize and compute the maximum.  Will test this against the clique-tree implementation to ensure correctness.&lt;/p&gt;

&lt;p&gt;........&lt;/p&gt;

&lt;p&gt;During testing I realized a fundamental problem with the current likelihood: there is no obvious way to consistently handle the &quot;redundant&quot; dimensions that arise from duplicated indices.  In some kernels, these are handled naturally, namely the kernels that treat different views as different indices.  The old kernel, however, does not, and the duplicated indices result in degenerate posterior distributions unless handled using hacks.&lt;/p&gt;

&lt;p&gt;We could handle this on a kernel-by-kernel basis, but it would be unmaintainable, and could hinder further research into new kernels.  It also isn't clear that the hacks I have in mind would actually give correct results.&lt;/p&gt;

&lt;p&gt;This problem is inherent to the &quot;candidates estimator&quot; of the marginal likelihood, because it involves a ratio of the posterior and prior, both of which are degenerate in these cases.&lt;/p&gt;

&lt;p&gt;The actual marginal likelihood function has no such degeneracies, because each observation is independent, given the underlying curve.  However, until recently it was unclear how to evaluate the marginal likelihood using the approximated likelihood function.  The approximate likelihood has a rank-deficient precision matrix, and its normalization constant is non-standard due to the transformation from 2D to 3D.  However, I think I've developed a way to evaluate it, which I describe in the next article.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log"/>
   <updated>2013-07-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Cleanup and speedup of curve_ml.&lt;/p&gt;

&lt;h2&gt;Task 1: Version without matrix-inversion lemma&lt;/h2&gt;

&lt;p&gt;Making a simpler version that doesn't exploit the matrix inversion lemma (MIL).&lt;/p&gt;

&lt;p&gt;The benefits of the MIL are likely mitigated by markov-decomposition (next task), and MIL
complicates the code and API design.&lt;/p&gt;

&lt;p&gt;Results (small test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (108.2 ms)
Old ML: 62.183969
Computing marginal likelihood (new way, legacy correspondence)...
Done. (120.8 ms)
New ML (Legacy corr): 63.908784
Computing marginal likelihood (new way)...
Done. (88.9 ms)
New ML (New corr): 72.304718           &amp;lt;----------  OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (97.3 ms)
New ML (New corr, no MIL): 72.304666   &amp;lt;----------  NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference is about 0.00007% -- seems good.  Slightly slower, possibly just noise.&lt;/p&gt;

&lt;p&gt;Results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6447.3 ms)
Old ML: 207.324848
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7665.1 ms)
New ML (Legacy corr): 209.199676
Computing marginal likelihood (new way)...
Done. (6205.6 ms)
New ML (New corr): 457529.406731  &amp;lt;---------- ?????
Computing marginal likelihood (new way, no MIL)...
Done. (5946.7 ms)
New ML (New corr, no MIL): 778723.327102  &amp;lt;---------- ?????
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both versions give CRAZY high results; significantly different from the &lt;a href=&quot;/ksimek/research/2013/07/05/work-log/&quot;&gt;same test on Friday&lt;/a&gt;.  Need to investigate...&lt;/p&gt;

&lt;h2&gt;Investigating new results&lt;/h2&gt;

&lt;p&gt;Observations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All results differ from friday, including old ML (slightly)&lt;/li&gt;
&lt;li&gt;New ML using legacy correspondence is still reasonable.&lt;/li&gt;
&lt;li&gt;Both crazy results are using new correspondence.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Look at pieces (log posterior, likelihood, prior)&lt;/li&gt;
&lt;li&gt;Inspect new correspondence&lt;/li&gt;
&lt;li&gt;Think about what changed since Friday?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:10:19 PM&lt;/p&gt;

&lt;p&gt;Ran old test, &lt;code&gt;test_ml_end_to_end.m&lt;/code&gt; and compared to old results in new test, &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt;.  Old test still gives old results, so I'll compare the two tests to determine what has changed.&lt;/p&gt;

&lt;p&gt;12:12:48 PM&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;mix&lt;/code&gt; parameter was set to 0.5 instead of 0.0.  I had forgotten I changed it at the end if Friday.&lt;/p&gt;

&lt;p&gt;Now getting &quot;good&quot; results again.  It raises a question that needs to be answered: why is ML sooo sensitive to evalaution position when using new correspondence?&lt;/p&gt;

&lt;h2&gt;Resuming&lt;/h2&gt;

&lt;p&gt;New results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6009.5 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7286.3 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6146.6 ms)
New ML (New corr): 793.585038             &amp;lt;---------- OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (6008.3 ms)
New ML (New corr, no MIL): 793.600835     &amp;lt;---------- NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.002% error, slightly faster.  Good.&lt;/p&gt;

&lt;h2&gt;Investigating Anomaly in new ML &lt;/h2&gt;

&lt;p&gt;So why is new ML so sensitive to where it is evaluated?&lt;/p&gt;

&lt;p&gt;Note that it's only extreme when using new correspondence.  Old correspondence is okay.&lt;/p&gt;

&lt;p&gt;Inspect difference between max likelihood and max posterior.  Maybe it's just more extreme than with old correspondence.&lt;/p&gt;

&lt;p&gt;12:20:59 PM&lt;/p&gt;

&lt;p&gt;Idea: bug in non-0.0 case?  Nope.&lt;/p&gt;

&lt;p&gt;Tried mixes: 0.0, 0.01, 0.1. Steady and dramatic increase in ML for new correspondence.  Old correspondence is nearly constant.  What is different between these correspodnences (other than the correspondences themselves).&lt;/p&gt;

&lt;p&gt;12:57:08 PM&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;: The max likelihood in the new correspondences is waaaaay into the tails of the prior.&lt;/p&gt;

&lt;p&gt;Since no real triangulation is done to ensure agreement between views, the maximum likelihood triangulation is very rough  (infinite likelihood variance allows this to happen without penalty).  This places the curve far into the tails of the prior, where evaluation is highly unstable.  Both the prior and likelihood are then extremely low (-1e-8 in log space), which should cancel, but don't due to numerical instability.  Hence, marginal likelihoods with huge magnitude.&lt;/p&gt;

&lt;p&gt;This is great news, since it means everything is working mostly as expected.  The numerical instability issue is easilly solved by always evaluating at the posterior, not the maximum likelihood.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The version of the new ML that doesn't use the matrix inversion lemma is accurate, so we can proceed to the markov-decomposed version next.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log"/>
   <updated>2013-07-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still trying to troubleshoot the difference between old and new likelihood implementations.&lt;/p&gt;

&lt;p&gt;Reviewing what is known&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prior is same&lt;/li&gt;
&lt;li&gt;Likelihood is same&lt;/li&gt;
&lt;li&gt;Posterior evaluates same in two different ways.&lt;/li&gt;
&lt;li&gt;mean curve is different&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Try using mean curve from old alg. in new alg.&lt;/p&gt;

&lt;p&gt;10:13:35 AM&lt;/p&gt;

&lt;p&gt;SOLVED.&lt;/p&gt;

&lt;p&gt;Yesterday I noticed that setting mix to 1.0 give decent results, but 0.0 was terrible.  Now I know why: 0.0 is a special case, in which a simplified calculation is used.  There was a bug in that special case, where the normalization constant didn't take into account the redundant dimensions that we eliminated.&lt;/p&gt;

&lt;h2&gt;ML Test&lt;/h2&gt;

&lt;p&gt;11:02:29 AM&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ml_test_end_to_end_2.m&lt;/code&gt; is now running&lt;/p&gt;

&lt;p&gt;Running on subset of correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...  
Done. (96.9 ms)  
Old ML: 62.176782

Computing marginal likelihood (new way, legacy correspondence)...  
Done. (112.3 ms)  
New ML (Legacy corr): 63.859940

Computing marginal likelihood (new way)...  
Done. (83.5 ms)  
New ML (New corr): 71.964026
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised the magnitude of difference between &quot;Old ML&quot; and &quot;New ML (Legacy corr)&quot;.  I guess the posterior variance is larger in this case, which means the posterior means can differ more.&lt;/p&gt;

&lt;p&gt;Running on full correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...
Done. (6400.1 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8328.4 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6349.2 ms)
New ML (New corr): 793.585038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that &quot;New ML (New Corr)&quot; is dramatiacally higher than the old and legacy ML's.  I suppose I should have been expected this, since this entire approach &lt;a href=&quot;/ksimek/research/2013/06/28/work-summary/&quot;&gt;was motivated by the terrible correspondences arising from the legacy correspondence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It's nice to see the ML for good correspondences getting even better.  Hopefully we'll see even more improvement as we try new curve models.&lt;/p&gt;

&lt;p&gt;Taking a break for lunch...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Next steps:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;fast ml_curve3

&lt;ul&gt;
&lt;li&gt;linear eval&lt;/li&gt;
&lt;li&gt;try without matrix inversion lemma&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;try new models (check that they're nonsingular; don't crash)&lt;/li&gt;
&lt;li&gt;set up training framework&lt;/li&gt;
&lt;li&gt;consider version without inversion lemma&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log"/>
   <updated>2013-07-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Continuing debugging of new likelihood&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Test:&lt;/strong&gt; &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;07:11:52 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Trouble with NaN error in &lt;code&gt;correspondence/corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;08:22:43 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Narrowed it down to gap-handling code-branch.&lt;/p&gt;

&lt;p&gt;08:53:34 AM&lt;/p&gt;

&lt;p&gt;Fixed. Arose from a nasty indexing scheme in the original &lt;code&gt;clean_correspondence.m&lt;/code&gt;, which I didn't handle well when adapting for &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;09:29:35 AM&lt;/p&gt;

&lt;p&gt;Nonsingular prior covariance matrix causing crash.  Apparently caused by same index appearing multiple times.&lt;/p&gt;

&lt;p&gt;In the past, we always merged such points.&lt;/p&gt;

&lt;p&gt;The new idea is different views of the same point would make the posterior covariance singular.  Either this isn't true, or there's a bug...&lt;/p&gt;

&lt;p&gt;Debugging this will take some serious coding and math brainpower and I fell debugging fatigue coming on.  Taking a brief break to refresh, then will tackle...&lt;/p&gt;

&lt;p&gt;11:56:21 AM&lt;/p&gt;

&lt;p&gt;Using a simple toy example, I (partially) confirmed my intuition that a degenerate prior matrix is okay, as long as its nullspace is spanned by the likelihood matrix.&lt;/p&gt;

&lt;p&gt;In the context of this bug, it means that two points sharing the same index value is fine as long as they arose from different views (assuming non-degenerate camera configuration, which is true in this case).&lt;/p&gt;

&lt;p&gt;Possible causes&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Numerical instability -- near-singular matrix.&lt;/li&gt;
&lt;li&gt;Error in Linear Algebra logic (seems unlikely, same code worked in old likelihood)&lt;/li&gt;
&lt;li&gt;Some bug earlier in the pipeline, causing invalid matrices here.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I'm guessing its 3; if not that, then maybe 2.&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;inspect the shared indices -- same view?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:48:49 PM&lt;/p&gt;

&lt;p&gt;Reversing my previous stance:  Degenerate prior with shared index will always result in a degenerate posterior.  The prior has two variables that are 100% correlated, and the posterior will also be 100% correlated.  This implies a degenerate covariance matrix.&lt;/p&gt;

&lt;p&gt;In such a case, the posterior's implied dimensionality is lower than it's apparent dimensionality.  The prior and posterior will be degenerate in the same way; this symmetry is aesthetically appealing, because the Candidates estimator for the marginal likelihood involves their ratio.  This suggests that eliminating the redundant dimensions for both pdfs is a sensible thing to do.&lt;/p&gt;

&lt;p&gt;How to resolve this?  Possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detect redundant indices early, eliminate them in the prior and posterior.  USe bookkeeping to remember which observations refer to the same model point.&lt;/li&gt;
&lt;li&gt;Handle reduntancies later, after degenerate posterior is formed, before evaluating posterior or prior .  The mean will automatically contain redundancies, which will avoid bookkeeping when comparing the mean to the data points (when &lt;code&gt;mix&lt;/code&gt; is != 0).&lt;/li&gt;
&lt;li&gt;Try to determine how identical indices arose in the first place.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Decided on 2 for now.  In retrospect, approach 1 may have had slightly less code and faster.&lt;/p&gt;

&lt;p&gt;01:45:00 PM&lt;/p&gt;

&lt;p&gt;ML is now running on legacy correspondence.  Results are significantly higher than the legacy ML algorithm (~ 2.25 times).  Not yet sure why yet.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6665.8 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8853.1 ms)
New ML (Legacy corr): 439.273126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Noticible slow-down, probably because of extra indices arising from not merging duplicate observations.&lt;/p&gt;

&lt;p&gt;possible causes of discrepancy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In old code, re-triangulation used simple averaging, not accounting for differing precisions of the points&lt;/li&gt;
&lt;li&gt;index sets differ between old and new code.  (update: checked, they are same)&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Plotting means for both to view differences...&lt;/p&gt;

&lt;p&gt;Investigation into ML discrepancy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Posterior means are similar, but obviously differ.&lt;/li&gt;
&lt;li&gt;both means are very non-smooth (need to re-address the smoothness issue)&lt;/li&gt;
&lt;li&gt;new mean has signficantly more points (~7%, 80 pts).  Could this alone affect ML?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;almost all difference is in posterior&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Maybe posterior kernel is different?  but prior is same
It's just the data covariance thats different.
but why??  There's more of them.&lt;/p&gt;

&lt;p&gt;New ml changes significantly when evaluation point changes.  This implies that the posterior curvature differs from the curvature of the joint distribution, which shouldn't happen.  A small amount of difference can arise naturally due to the approximation of the likelihood, but this is too large to be explained in that way.  Most noticibly, the new results look very similar to the old ones &lt;strong&gt;when evaluating at the likelihood mean&lt;/strong&gt;.  The largest difference occurs when evaluating at the posterior mean.  Not clear yet what conclusions to draw from this, or if this is just a coincidence.&lt;/p&gt;

&lt;p&gt;Maybe likelihood curvatures are differing.&lt;/p&gt;

&lt;p&gt;07:35:28 PM&lt;/p&gt;

&lt;p&gt;Was able to get identical likelihood value using matrix-form as I did with the original 2D geometric error form.  Seems like the precision matrices are good.&lt;/p&gt;

&lt;p&gt;Priors appear good too, since it evaluates to the same value as the reference implementation.&lt;/p&gt;

&lt;h2&gt;Reference implementation for new ML&lt;/h2&gt;

&lt;p&gt;Idea: direct implementation for quadform, but with hacked nomralization constant&lt;/p&gt;

&lt;h2&gt;In progress&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnosing errors in new likelihood &lt;code&gt;curve_ml_3.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;getting &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt; running.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log"/>
   <updated>2013-07-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Implementing covariance functions&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood&lt;/li&gt;
&lt;li&gt;Implement fast generalized marginal likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implement generalized marginal likelihood&lt;/h2&gt;

&lt;p&gt;Rewriting &lt;code&gt;curve_ml3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Based on &lt;code&gt;curve_ml.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Kernel now takes two sets of indices, curve-index and view-index.&lt;/p&gt;

&lt;p&gt;Kernel is now passed-in as a lambda.&lt;/p&gt;

&lt;p&gt;Need to decide whether to use matrix inversion lemma for global offset and linear prior.  Currently being used;  not using would be simpler; try both and compare results.&lt;/p&gt;

&lt;h2&gt;Testing new marginal likelihood&lt;/h2&gt;

&lt;p&gt;Creating new version of &lt;code&gt;test/test_ml_end_to_end.m&lt;/code&gt;, named &lt;code&gt;test/test_ml_end_to_end2.m&lt;/code&gt;.  This version uses the new likelihood format, and compares against the old.&lt;/p&gt;

&lt;h2&gt;New curve_likelihood&lt;/h2&gt;

&lt;p&gt;Need to write new version of &lt;code&gt;curve_likelihood.m&lt;/code&gt; to handle new likelihood format.&lt;/p&gt;

&lt;p&gt;It's going to be difficult to compare to older version, since we've changed the correspondence method...&lt;/p&gt;

&lt;p&gt;I'll have to add a flag that simulates the old correspondence method, but stores it in the new likelihood format...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;01:33:52 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I decided to write a separate function for the old correspondence method, called &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.  Moved &lt;code&gt;clean_correspondence3.m&lt;/code&gt; to &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The legacy function isn't exactly identical to the old function.  In particular, the index values might be slightly different, since it depends on the old function's global mean curve, which we don't compute in the new function.  If it's important, we can call the old function, and then call the new one.  This is a debugging function, so speed doesn't matter. For the moment, we'll accept the small difference, and use this function simply to confirm that we're in the ballpark.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;03:06:24 PM&lt;/em&gt;
Test is implemented.  Troubleshooting syntax errors...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;04:11:17 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Elusive NaN error in &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log"/>
   <updated>2013-07-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing marginal likelihood &lt;/h2&gt;

&lt;p&gt;Working on generalizing marginal likelihood for all three foreground models.&lt;/p&gt;

&lt;p&gt;Considered briefly how to use the matrix inversion lemma to improve numerical stability of all models.  Decided it could be difficult to build in a general way;  will proceed with the direct method for now, until there is evidence of numerical instability.&lt;/p&gt;

&lt;p&gt;Generalized how covariance is generated.  Wrote several new functions for generating differenct covariance matrices.  Will need to rework some due to the developments I describe later.&lt;/p&gt;

&lt;h2&gt;Theoretical Developments&lt;/h2&gt;

&lt;p&gt;Thought extensively about perturbation models I described yesterday.  I realized there is a serious problem with modeling motion using brownian motion -- the prior variance grows without bound as time approaches infinity.  Thus, the marginal prior for the curve in view 36 has much greater prior variance than the curve one in view 1.  This doesn't make sense -- ideally, they should all have the same marginal prior.&lt;/p&gt;

&lt;p&gt;This led to a reading session in Williams and Rasmussen, which led me to develop two new motion models, which I describe extensively in today's accompanying post.&lt;/p&gt;

&lt;h2&gt;Tomorrow&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish implementing new covariance functions.&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood.&lt;/li&gt;
&lt;li&gt;implement &lt;em&gt;fast&lt;/em&gt; generalized marginal likelihood (approximation)

&lt;ul&gt;
&lt;li&gt;Test against existing ML for old models.&lt;/li&gt;
&lt;li&gt;Test with new models.  Are the ML's higher? (will probably need training).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build training data set from hand-traced ground-truth.&lt;/li&gt;
&lt;li&gt;Write training code.  Train all three candidate models.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Rethinking covariance functions</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions"/>
   <updated>2013-07-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions</id>
   <content type="html">&lt;p&gt;Noticed a problem with the cubic spline perturbation model.  As the view number increases, the variance marginal prior of the curve in that view approaches infinity.  This means that the &quot;smoothness&quot; prior is ignored more and more in later views.&lt;/p&gt;

&lt;h2&gt;Squared Exponential perturbation model&lt;/h2&gt;

&lt;p&gt;A better model would have the same marginal prior for all views, but with correlation between nearby views.  This allows curves to revert to the prior as temporally correlated evidence becomes less informative.  I believe the squared exponential covariance function has this property.  Instead of adding the sq-exp covariance to the existing covariance function, we should multiply it, so self-covariance is unchanged, but pairwise correlation is non-zero.&lt;/p&gt;

&lt;p&gt;An added benefit of this is that it only has one tunable parameter.&lt;/p&gt;

&lt;p&gt;It should be easy to incorporate into our test-bed, along with the other two newly proposed foreground models.&lt;/p&gt;

&lt;h2&gt;Ornstein Uhlenbeck perturbation model&lt;/h2&gt;

&lt;p&gt;Digging deeper into Williams and Rasmussen, I found precisely the GP I was looking for a few days ago:  the Ornstein–Uhlenbeck (OU) process.  This   process describes brownian motion under the influence of a regularizing force that pulls toward the mean.&lt;/p&gt;

&lt;p&gt;In other words, I can model correlation between views without affecting the marginal prior of the curve any particular view.  This is also accomplished by the squared-exponential model, but the OU process is probably more realistic, because the plant's motion looks non-smooth.&lt;/p&gt;

&lt;h2&gt;Modelling the mean or not?&lt;/h2&gt;

&lt;p&gt;I'm struggling with whether or not to explicitly model a &quot;mean&quot; curve with the SE and the OU processes.&lt;/p&gt;

&lt;p&gt;If I did model the mean, each curve's covariance function would be the sum of a standard covariance plus a perturbation covariance.  The standard covariance models the &quot;mean&quot; curve, and it would be the same for all views (100% correlated).  The perturbation covariance would be partially correlated between the views, using the SE or the OU process.  The bayes net has the following structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        O   &amp;lt;-- mean curve
     / /\ \
   /  /  \  \
 /   |    |   \
O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The alternative is to model the per-view curves directly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Under this model, each view's curve has a cubic-spline marginal distribution, and the SE or UO process controls the correlation between them.&lt;/p&gt;

&lt;p&gt;What isn't clear is whether the perturbations in the latter model will be independent between points.  We need to model within-view perturbations as correlated, otherwise the marginal likelihood will drop too low.  There is no explicit description of how perturbations of adjacent points correlate.&lt;/p&gt;

&lt;h2&gt;What follows is me thinking out-loud...&lt;/h2&gt;

&lt;p&gt;Intuitively, points nearby in curve-space (have similar \(\tau\)'s) can never be more correlated than they are when they appear in the same view.  Separating them in time (view) will decreases their correlation, until finally, there is no correlation; the only remaining correlation is between points in the same view.  The SE kernel modulates that correlation.  The SE kernel doesn't explicitly model correlation between perturbations, but this doesn't mean the correlation doesn't exist -- it is implicit in the original kernel.&lt;/p&gt;

&lt;p&gt;There is an analogy here with the classic krigging example.  In there models, the squared exponential over a 2D landscape (i.e. joint function over \(\mathbb{R}\)&lt;sup&gt;2&lt;/sup&gt; space) is equal to the product of 1D squared exponentials (i.e. two functions over \(\mathbb{R}\)&lt;sup&gt;1&lt;/sup&gt; space).  In other words, the 2D kernel is constructed by the product of 1D kernels.  There is no worry that the delta between nearby &quot;slices&quot; of the surface are uncorrelated, because the marginal covariance within that slice will enforce that smoothness.&lt;/p&gt;

&lt;p&gt;In our case, we also have a product of 1D covariance functions constructing a 2D covariance function.  The difference is that one of the kernels (the curve-space kernel) is a cubic-spline process, while the other (the time-dimension kernel) is squared exponential (or Ornstein-Uhlenbeck).  Despite this difference, my intuition is that the conclusions are the same - the deltas will be smooth (i.e. correlated), because they will be the difference between two smooth curves.&lt;/p&gt;

&lt;p&gt;Considering the marginals in both directions further illustrates why this works.  Obviously, a slice in the curve-direction will always look curve-like, since the marginals are all the same cubic-covariance GP prior.  In the time-direction, a single point will follow a GP or OU process, with initial conditions dictated by the first curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;...BUT...&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;...in the absence of data, each individual point will drift over time to the prior mean, I.e. &lt;strong&gt;zero&lt;/strong&gt;.  In other words, the 3D curve observed in view infinity gains no information from curve observed at time zero.&lt;/p&gt;

&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; realistic.  In reality, these points shouldn't drift far from some &quot;sensible&quot; center curve, implying that a mean curve exists.&lt;/p&gt;

&lt;p&gt;The time-slice of any particular point &lt;em&gt;should&lt;/em&gt; look like either a OU or SE process, but the mean needs to be explicit.  This implies an additive model, with distinct models for the unobserved &quot;mean curve&quot; and the deviations from it, which are summed to get the 3D curve seen in each view.&lt;/p&gt;

&lt;h2&gt;Modeling perturbation&lt;/h2&gt;

&lt;p&gt;So if we must model perturbation, how should we model it?&lt;/p&gt;

&lt;p&gt;One thing is clear: the marginal prior for the curve in any view &lt;strong&gt;must&lt;/strong&gt; still be a cubic-spline process.&lt;/p&gt;

&lt;p&gt;This implies that the perturbation must be a cubic-spline process, too.&lt;/p&gt;

&lt;p&gt;However, the variances for each component (offset, linear, and cubic) are likely to be different for the perturbation model, compared to the mean-curve model.  Most importantly, the magnitude of the offset variance must be large in the mean-curve model but will be &lt;em&gt;much&lt;/em&gt; lower (relative to linear and cubic variance) in the perturbation model.&lt;/p&gt;

&lt;p&gt;I was hoping to avoid adding four extra parameters to our model (perturbation offset, linear and cubic variance, plus perturbation scale-length).  The mean-free model only adds one parameter - scale-length.  I guess this is the price we pay for a better model -- and for higher marginal likelihoods.  Ideally, the occam's razor quality of the marginal likelihood will allow us to avoid overfitting this many parameters (7 total).  Any parameters that are superfluous should become near-zero during training.&lt;/p&gt;

&lt;p&gt;...I hope.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log"/>
   <updated>2013-07-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Background:  This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;p&gt;Also developed a new approach to evaluate marginal likelhoods thats much simpler, but need to confirm that it matches reference implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implement both new foreground models, plus old one in new &quot;kernel function&quot; way.&lt;/li&gt;
&lt;li&gt;Implement new evaluation method and test against reference.&lt;/li&gt;
&lt;li&gt;Learn parameters for all three models (needs some ground truthing).&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implementing new models&lt;/h2&gt;

&lt;p&gt;Worked on general framework for evaluating ML under general new model framework.&lt;/p&gt;

&lt;p&gt;After some struggling with covariance degeneracies inherent in backprojection, I realized that the 3D marginal likelihood is naturally degenerate, because it isn't the true likelihood function (which is in 2D).&lt;/p&gt;

&lt;p&gt;I'm kicking myself for not remembering that I struggled with this exact problem 5 months ago.  At that time, I realized the better approach is to use Candidate's estimator, which is the ratio of the unnormalized posterior to the normalized posterior.  The unnormalized posterior comes from the prior and 2D likelIhood;  the normalized posterior is obtainable from the 3D likelihood and the prior.&lt;/p&gt;

&lt;p&gt;This was already implemented in &lt;code&gt;curve_ml.m&lt;/code&gt;, but was  O(n&lt;sup&gt;3&lt;/sup&gt; ), so was all but abandoned in favor of the junction-tree method in &lt;code&gt;curve_ml2.m&lt;/code&gt;, which is O(n).&lt;/p&gt;

&lt;p&gt;However, it's recently become clear that the candidate's estimator should be evaluated in \(O(n)\)  by exploiting the Markovian nature of the covariance matrix.&lt;/p&gt;

&lt;p&gt;It should be easy to try both, by simply swapping out covariance matrices with ones arising from the new covariance functions.  Tomorrow...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Foreground Curve Models as Gaussian Process Covariance Function</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models"/>
   <updated>2013-07-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Background&lt;/h2&gt;

&lt;p&gt; This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;h2&gt;Covariance functions as models&lt;/h2&gt;

&lt;p&gt;I realized this weekend that all of the models I've considered are achievable by using different covariance functions.&lt;/p&gt;

&lt;p&gt;The original model is given by&lt;/p&gt;

&lt;div&gt; \[
k_1(i,j) = \sigma_s k_{\text{cubic}}(i,j) + \sigma_o  k_{\text{offset}}(i,j) + \sigma_r k_{\text{linear}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
k_{\text{cubic}}(i,j) &amp;= \frac{|\tau_i - \tau_j| \min(\tau_i, \tau_j)^2}{2} + \frac{\min(\tau_i, \tau_j)^3}{3} \\
k_{\text{linear}}(i,j) &amp;= \tau_i \tau_j  \\
k_{\text{offset}}(i,j) &amp;= 1 \\ 
\end{align}
\] &lt;/div&gt;


&lt;p&gt;The cubic model penalizes non-zero second derivative over the length of the curve.  The offset and linear model penalize zero and first derivative initial conditions.&lt;/p&gt;

&lt;h2&gt;White noise perturbation model&lt;/h2&gt;

&lt;p&gt;Both of the two new models expand on the original by modelling how the observed curve differs between views.  That is, the model for the curves are the same as before (they are cubic spline curves), but we additionally model &lt;strong&gt;how they are perturbed between views&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The first new model I call the &quot;White Noise&quot; perturbation model, which treats each view of the curve as arising from a white noise process that perturbs a &quot;master curve&quot;, I.e. the unobserved mean curve.  Its covariance is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{white}}(i,j) = k_1(i,j) + \delta(v_i-v_j) k_{\text{w}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(v_i\) is the view that captured the \(i\)th point and \(k_w\) is&lt;/p&gt;

&lt;div&gt; \[
k_w(i,j) = \sigma_{o,w}  k_{\text{offset}}(i,j) + \sigma_{r,w} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model adds extra covariance that is independent per-view.  This treats the perterbations from the mean curve as independent.&lt;/p&gt;

&lt;p&gt;The perturbations themselves are assumed to be purely to the curve's initial conditions, i.e. its position and direction.&lt;/p&gt;

&lt;p&gt;This model is motivated by the assumption that perturbations arise due to camera mis-calibrations, which result in mostly translation and small rotational changes.&lt;/p&gt;

&lt;h2&gt;Brownian motion perturbation model&lt;/h2&gt;

&lt;p&gt;The second model treats perturbations as arising from Brownian motion, i.e. each curve is independent, conditioned on the previous view's curve.  The covariance function is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{brown}}(i,j) = k_1(i,j) + \min(\tau_i, \tau_j) k_{\text{b}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(k_b\) is&lt;/p&gt;

&lt;div&gt; \[
k_b(i,j) = \sigma_{s,b} k_{\text{cubic}}(i,j) + \sigma_{o,b}  k_{\text{offset}}(i,j) + \sigma_{r,b} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model assumes perturbations arise by motion of the plant during imaging, like moving closer to a light source, or responding to a temperature gradient in the room.  &quot;View index&quot; is a surrogate for a time variable, since time between captures is roughly constant.  The use of Brownian motion means the magnitude of perturbation increases by the square root of the distance between the views (time).  \(k_b\) models the nature of the perturbation; we use the cubic-spline kernel which says that point-wise perturbations are strongly correlated and increase in magnitude the further they get from the base of the curve.&lt;/p&gt;

&lt;p&gt;This \(k_b\) is possibly overkill;  using the simpler \(k_w\) from the white-noise model might work just as well.  This simpler model implies curves drift in position and direction only, and these perturbations are correlated over time.&lt;/p&gt;

&lt;h2&gt;End stuff&lt;/h2&gt;

&lt;p&gt;After some consideration, I think implementing these models in the current system will will very simple.  Training them will be harder; some more ground truthing will be needed.&lt;/p&gt;

&lt;p&gt;Of the two new models, I suspect that the white noise model will be sufficient to get the gains in marginal likelihood we seek.  It is a much better explanation for misaligned data-points compared to the old model models all point perturbations as independent.&lt;/p&gt;

&lt;p&gt;The brownian motion model will give us something to compare against in evaluation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work summary</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary"/>
   <updated>2013-06-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finished editing new likelihood function.  See &lt;code&gt;correspondence/clean_correspondence3.m&lt;/code&gt; (filename likely to change soon).  Below is a summary of results and comparison to the old approach.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;Below is a plot of the maximum likelihood curves:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-likelihood_1.gif&quot; alt=&quot;Maximum likelihood reconstruction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each colored curve arises from a single 2D data curve.  The curve represents the mean of a gaussian process. Variance is not shown; it is infinite in the direction of backprojection.&lt;/p&gt;

&lt;p&gt;The dark-blue curve is an estimate of the posterior curve, which was used to backproject the 2D curves against.  It is estimated from the point-correspondences.&lt;/p&gt;

&lt;p&gt;Note that for clarity, x/y scale is significantly smaller than z-scale (up-direction).  When axis are equally-scaled, max-likelihood curves lie very close the to blue curve.&lt;/p&gt;

&lt;h2&gt;Improved correspondence&lt;/h2&gt;

&lt;p&gt;The old likelihood suffered from a small but non-negligible number of awful correspondences, which severely damaged both reconstruction and marginal likelihood values.  This was because the likelihood was derived from the point-to-point correspondences, which (a) is problematic at gaps, and (b) suffer bad correspondences which can't be fixed later.
The new approach uses the old approach as a starting point, but then recomputes all 2D curve correspondences against a rough reconstruction.  This dramatically improves correspondences as we see below.&lt;/p&gt;

&lt;p&gt;This is the old correspondence.  Blue points are points from the 2D data curve;  the teal line is the posterior 3D curve (projected to 2D); red lines show correspondences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug.png&quot; alt=&quot;Correspondence bug &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next is the fixed correspondence.  Notice how the red correspondence lines are much shorter, indicating a less costly correspondence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug-fixed.png&quot; alt=&quot;Correspondence bug, fixed &quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Per-data-curve likelihood&lt;/h2&gt;

&lt;p&gt;The old likelihood had a single GP curve that represented all of the different views.  Now we have a GP curve &lt;strong&gt;per data-curve&lt;/strong&gt;, which will be related by a GP prior.&lt;/p&gt;

&lt;p&gt;This will allow us to simultaneously track and triangulate, a key novelty to this approach.  More importantly, it will give us higher marginal likelihood numbers for true 3D curve observations, because we can make the independent noise component very small.&lt;/p&gt;

&lt;h2&gt;TODO - short-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt; - This new function adds an extra pass of DTW per data-curve, per iteration where previously, only one pass was needed.  This has introduced a significant performance bottleneck on the order of 10-50x.  I need to profile and optimize this function if we want runtimes that aren't measured in weeks.  This may motivate a full re-thinking of &lt;code&gt;merge_correspondence.m&lt;/code&gt;, to avoid a full DTW after every merge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Marginal Likelhood &lt;/strong&gt; - Need to build the new marginal likelihood for the foreground curve model.  This proposed version will take advantage of the new per-data-curve likelihood.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO - mid-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end&lt;/strong&gt; - Incorporate new likelhood and ML into gibbs sampler.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Swendsen wang cuts &lt;/strong&gt; - design SWC split/merge move&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log"/>
   <updated>2013-06-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Profiling and optimizing &lt;code&gt;clean_correspondence3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Huge bottleneck in &lt;code&gt;get_dtw_matches_()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Wrote MEX version of get_dtw_matches_(),  &lt;code&gt;get_dtw_matches_horiz.c&lt;/code&gt;.   ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New bottlenecks&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;projection_error_hessian&lt;/li&gt;
&lt;li&gt;&lt;code&gt;interp1&lt;/code&gt; - linear interpolation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;csaps&lt;/code&gt; - spline smoothin&lt;/li&gt;
&lt;li&gt;dtw&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Plan&lt;/h2&gt;

&lt;p&gt;can possibly eliminate some calls to interp1.&lt;/p&gt;

&lt;p&gt;could mex projecion_error_hessian.  should be a big win&lt;/p&gt;

&lt;p&gt;DTW can be done in two passes, or mex'd&lt;/p&gt;

&lt;h2&gt;Mexing &lt;code&gt;projection_error_hessian&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Tried using Matlab's &quot;Coder&quot; feature to generate mex code automatically.  The result was totally bloated (14 files!) and no faster than the matlab version.  Next I tried hand-coding the mex, and its ~10x faster.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/27/work-log"/>
   <updated>2013-06-27T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/27/work-log</id>
   <content type="html">&lt;p&gt;&lt;strong&gt; Thursday Overview &lt;/strong&gt;
* Continued refactoring of likelihood
* C++ Bootcamp
* VR lab tour and new user set-up&lt;/p&gt;

&lt;h2&gt;Continued refactoring of likelihood&lt;/h2&gt;

&lt;p&gt;Implemented &quot;alternative approach&quot; to handling tail points mentioned in last entry.  The old way didn't benefit from re-indexing; this way does.  This way handles negative index points correctly, too.&lt;/p&gt;

&lt;p&gt;In process of testing and debugging.  Possibly more outcomes later tonight.&lt;/p&gt;

&lt;h2&gt;C++ Bootcamp&lt;/h2&gt;

&lt;p&gt;today's session: inline and const-correctness&lt;/p&gt;

&lt;h2&gt;VR lab tour and new user set-up&lt;/h2&gt;

&lt;p&gt;By Angus's request, I showed showed the new postdoc Javier around the lab and set him up with an admin account.&lt;/p&gt;

&lt;p&gt;Lots of things still broken on VR01; biggest problem is video card #2 not displaying anything.  Game controller not set up yet.    Showed osgviewer demo, and by Angus's request, got his Processing demo running too.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/26/work-log"/>
   <updated>2013-06-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Likelihood construction, &lt;code&gt;clean_correspondence3.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;new tracking GP model&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Likelihood construction&lt;/h2&gt;

&lt;p&gt;Started and finished implementation today.  Need to design a test and then debug.&lt;/p&gt;

&lt;p&gt;~ 250 lines of Matlab code.  Logic overview:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;smooth triangulated points&lt;/li&gt;
&lt;li&gt;resample smoothed 3D curve uniformly&lt;/li&gt;
&lt;li&gt;handle tails (see below)&lt;/li&gt;
&lt;li&gt;project curve into each view and resample uniformly&lt;/li&gt;
&lt;li&gt;DTW to correspond 2d data curve to projected smooth curve  (see below)&lt;/li&gt;
&lt;li&gt;map corresponding projected curve points back to 3D points and indices&lt;/li&gt;
&lt;li&gt;triangulate 2d data points against corresponding 3d point&lt;/li&gt;
&lt;li&gt;compute likelihood hessian around that point&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;New DTW&lt;/h2&gt;

&lt;p&gt;Re-implemented a specialized version of DTW with following changes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;only horizontal steps accrue cost&lt;/li&gt;
&lt;li&gt;Hard-constraint on the number of vertical steps per horizontal step.&lt;/li&gt;
&lt;li&gt;keeps track of &quot;best&quot; match along vertical runs. no need for second pass

&lt;ul&gt;
&lt;li&gt;I think this is only possible because only horizontal steps accrue cost.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Tail points&lt;/h2&gt;

&lt;p&gt;Still iffy on the &quot;tail points&quot; case. Implemented late at night and likely needs review in the morning.  Still need to handle negative index values.&lt;/p&gt;

&lt;p&gt;Alternative implementation: only inspect the tails to determine the length of the 3D curve.  Then proceeed as usual. no special cases&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;review tail points code

&lt;ul&gt;
&lt;li&gt;consider alternative implementation (see above)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;handle negative index values&lt;/li&gt;
&lt;li&gt;think about hessian and transformation Jacobian&lt;/li&gt;
&lt;li&gt;testing, debugging, profiling&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Rethinking Likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/20/rethinking-likelihood"/>
   <updated>2013-06-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/20/rethinking-likelihood</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Testing and debugging &lt;code&gt;clean_correspodnence2.m&lt;/code&gt; has revealed some significant problems with the current approach to constructing the likelihood function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 5%-10% of fragements have correspondences that are nonsensical. &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I hypothesize that this is due to bad correspondences early on, when there is little evidence to drive a good correspodnence.  These bad correspodnences are propagated as new curves are added that could suggest a better correspodnence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Large lateral gaps in triangulation result in large axial gaps in posterior curve.  &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The problem is that the index set is computed from the triangulation.  The current fix for this -- smooth, re-index, repeat -- is very limited in the severity it can overcome.  In practice, most gaps are only partially reduced.&lt;/p&gt;

&lt;h2&gt;Rethinking &quot;Correspondence to Likelihood&quot;&lt;/h2&gt;

&lt;p&gt;Corresondence is good for constructing a decent-quality 3d curve, but isn't good for computing fine-grained pointwise likelihood, due to sporradic terrible correspondences and gaps.&lt;/p&gt;

&lt;p&gt;Instead of continuously Band-Aiding these issues that keep arising, its time to re-think how the likelhood is constructed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 1. the mean of 3d gaussians should project to the 2D position of the corresponding data point &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 2. the depth should be based on the corresponding position in the unobserved curve &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Two issues here.  First, how to localize the unobserved curve without having the likelihood already (chicken and egg).  Second, how to identify the corresponding point of the unobserved curve?&lt;/p&gt;

&lt;p&gt;In the old method, the answer to both was &quot;use the correspondence matrix.&quot;&lt;/p&gt;

&lt;p&gt;In the new method, we still use the correspondence matrix to triangulate, but we smooth the result using the prior and then throw away correspondence information.&lt;/p&gt;

&lt;h2&gt;Killing the correspondence grid&lt;/h2&gt;

&lt;p&gt;The corresponence matrix artificially forces points from different views to correspond to the same point.  This is out of necessity -- we need correspondence to achieve triangulation.  But we don't need to adhere to this to compute the likelhood.  Indeed, observed points may fall anywhere in the continuous index set, not into a discrete set of predefined cells.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Observations can correspond to any index in \([0 t_{end}]\).&lt;/li&gt;
&lt;li&gt;Unobserved curve is modelled at a uniform grid.&lt;/li&gt;
&lt;li&gt;Previously, the dimensionality of the unobserved curve grew with the number of observations. Now it grows with the range of the index set (i.e. the length of the curve).`&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Random thoughts&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;evaluate marginal likelihood directly?  then add extra normalization from triangulation (jacobian?)

&lt;ul&gt;
&lt;li&gt;evaluate linearly using markov conditional probabilities&lt;/li&gt;
&lt;li&gt;extend to poly model

&lt;ul&gt;
&lt;li&gt;grid-structured Bayes net&lt;/li&gt;
&lt;li&gt;topological sort for evaluation&lt;/li&gt;
&lt;li&gt;use scope variables to manage dependencies generally&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/17/work-log"/>
   <updated>2013-06-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing work on foreground curve model. Editing &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Logic overview&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;smooth triangulated points &amp;amp; estimate their indices&lt;/li&gt;
&lt;li&gt;fill-in untriangulatable points/indices&lt;/li&gt;
&lt;li&gt;resolve many-to-one data-to-3d-curve correspondences by picking best match&lt;/li&gt;
&lt;li&gt;triangulate each data point against 3d curve to get mean and covariance for point&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Logic detail&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;1. estimate indices by chord-length parameterization
2. repeat N times:
    2.1 smooth triangulated points using current index estimate (posterior mean approx.)
    2.2 re-estimate indices using chord-length parameterization
3. for each untriangulatable point
    3.1 triangulate against cubic interpolation of neighboring points (Newton's method)
    3.2 store resulting point and index with smoothed points from 2.1
4. for each 2D data curve
    4.1 triangulate against corresponding 3d point in smoothed curve to get likelihood mean
    4.2 compute curvature at this point to get likelihood precision
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;03:13:30 PM&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First run, addressing syntax errors.&lt;/li&gt;
&lt;li&gt;Plotting first test: triangulated curve w/ and wo/ gap-filling

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: gap-fill at beginning maps exactly to first non-gap point&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: interior gap point falls exactly on non-gap point&lt;/li&gt;
&lt;li&gt;need to merge points?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;04:06:03 PM&lt;/strong&gt;
wrapping up...&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnose coincident-point issue -- bug or not?&lt;/li&gt;
&lt;li&gt;Finish testing - plot per-view likelihood points&lt;/li&gt;
&lt;li&gt;Add point-merging&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/14/log-entry"/>
   <updated>2013-06-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/14/log-entry</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14528&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Working on foreground curve model.&lt;/p&gt;

&lt;p&gt;Begin of significant rework; SVN revision noted in infobox.&lt;/p&gt;

&lt;p&gt;Editing  &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt; which will replace &lt;code&gt;correspondence/clean_corespondence.m&lt;/code&gt;.  Drawing partially from &lt;code&gt;corr_to_bg_likelihood.m&lt;/code&gt;, esp. for &quot;best match&quot; logic.&lt;/p&gt;

&lt;p&gt;Probably will rename &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt; to &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt; before I finish.&lt;/p&gt;

&lt;p&gt;Will use &lt;code&gt;tests/test_ml_end_to_end&lt;/code&gt; to compare old and new implementations.&lt;/p&gt;

&lt;p&gt;Work still in progress...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/13/log"/>
   <updated>2013-06-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/13/log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;After investigating the false positives from the last entry, it seems clear that bad matches look good because missing data are not penalized.  For example&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-13-bad-match.png&quot; alt=&quot;Bad match&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the overlapping region of the blue and green curves, the distance between them is relatively low (less than 3 pixels, or 1.5px radius).  But the size of their overlap is so low that it would be hard to claim that they come from the same underlying curve with any confidence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Params &lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smoothing_variance_2d: 0.2500
    noise_variance_2d: 10
     position_mean_2d: [2x1 double]
 position_variance_2d: 1.3629e+04
     rate_variance_2d: 0.4962
   smoothing_variance: 1.0000e-04
       noise_variance: 10
        position_mean: [3x1 double]
    position_variance: 62500
        rate_variance: 2.2500
      smoothing_sigma: 0.2000
    noise_variance_bg: 0.1038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I would have expected that noise_variance_bg was low enough to discount this candidate, but the log ML ratio is 71.0.  The noise model must just look really bad...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Visualizing/Debugging BG ML</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/12/visualizingdebugging-bg-ml"/>
   <updated>2013-06-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/12/visualizingdebugging-bg-ml</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Experiment&lt;/h2&gt;

&lt;p&gt;We now have a new algorithm for computing background curve Marginal Likelihood.  Lowering noise sigma \(\sigma_n\) should rule out bad matches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: Re-run background candidate matching with new algorithm and roughly-trained \( \sigma_n \).&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;If the threshold is set right, the results are improved, but we still have some false-positives and false negatives.&lt;/p&gt;

&lt;p&gt;It's still unclear whether we can get good results without thresholding, since we haven't computed the noise ML using the new algorithm, so absolute numbers are meaningless.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Params&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smoothing_variance_2d: 0.2500
    noise_variance_2d: 10
     position_mean_2d: [2x1 double]
 position_variance_2d: 1.3629e+04
     rate_variance_2d: 0.4962
   smoothing_variance: 1.0000e-04
       noise_variance: 10
        position_mean: [3x1 double]
    position_variance: 62500
        rate_variance: 2.2500
      smoothing_sigma: 0.2000
       nlise_variance: 10
    noise_variance_bg: 0.1038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Calls&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data = offline_pair_candidates(data, params, 0, 1, 1, 'bg');
cands = tmp_get_cands(data);
visualize_bg_cands(data, cands, 250)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt; Plots &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Matched curves appear in white, unmatched appear in gray&lt;/p&gt;

&lt;p&gt;False negatives&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-12-bad-matches.png&quot; alt=&quot;bad matches&quot; /&gt;&lt;/p&gt;

&lt;p&gt;False positives:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-12-false-positive.png&quot; alt=&quot;bad matches&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;apply new indexing and cleanup algorithm to 3d and noise curves&lt;/li&gt;
&lt;li&gt;better training of foreground/background

&lt;ul&gt;
&lt;li&gt;ground truth curve fragments&lt;/li&gt;
&lt;li&gt;automatic training of background&lt;/li&gt;
&lt;li&gt;automatic training of noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;better foreground model

&lt;ol&gt;
&lt;li&gt;compute likelihood separately&lt;/li&gt;
&lt;li&gt;add smooth GP to likelihood&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;re-run experiment with trained noise model parameters&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Index set bug fixed, marginal likelihood curves improved</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/10/index-set-bug"/>
   <updated>2013-06-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/10/index-set-bug</id>
   <content type="html">&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Index Set Bug fixed &lt;/h2&gt;

&lt;p&gt;After fixing a problem with how index sets were estimated, the marginal likelihood (ML) curves are now much more sensible.&lt;/p&gt;

&lt;p&gt;Here are two marginal likelihood curves using the old approach with two different smoothing sigmas.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2012-06-10_ML_old.png&quot; alt=&quot;pre-bug ML curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how lowering the smoothing sigma \(\sigma_s\) causes the maximum ML to continuously improve, and results in a &lt;em&gt;huge&lt;/em&gt; improvement when noise sigma \(\sigma_n\) is low. There are two implications of this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ML continuously improves as \(\sigma_s\) approaches zero&lt;/li&gt;
&lt;li&gt;As \(\sigma_s\) approaches zeros, the optimal \(\sigma_n \) approaches zero&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;We &lt;em&gt;need&lt;/em&gt; to be able to use \(\sigma_n \) to control the cutoff for background curves, but this is not possible if ML is a monotonic function of \(\sigma_n\).&lt;/p&gt;

&lt;p&gt;The problem arose from the fact that index-set spacing grew in proportion to curve-noise, whereas it should have stayed roughly constant.  As a result, more noise made the curves look &lt;em&gt;smoother&lt;/em&gt;, because the unobserved points seemed to be farther apart.&lt;/p&gt;

&lt;p&gt;Obviously, this is the opposite of what we would want.  I rewrote the &quot;cleanup&quot; algorithm so index sets are now computed from an estimate of the posterior curve, not from the maximum likelihood curve.  This causes noise to be smoothed out of the curve before measuring the distance between points, so increasing noise will not significantly change the inter-point distance.&lt;/p&gt;

&lt;p&gt;Here are the ML curves after the change.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2012-06-10_ML_new.png&quot; alt=&quot;pre-bug ML curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Some observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The affect of changing \(\sigma_s\) is &lt;em&gt;far&lt;/em&gt; less dramatic&lt;/li&gt;
&lt;li&gt;As \(\sigma_n\) approaches zero, the ML &lt;em&gt;always&lt;/em&gt; drops below 0&lt;/li&gt;
&lt;li&gt;The position and value of the maximum is mostly unchanged, suggesting a &quot;natural&quot; noise-value that is independent of the smoothing value.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Point 2 is particularly important.  These ML values are actually log ratios between the ML under this model and the ML under the &quot;default&quot; noise model.  Values below zero indicate that the naive noise model is a better fit.  The fact that we can adjust \(sigma_n\) to control the trade-off between the two models is promising, and suggests that this new model can, indeed, be discriminitive.  Prior to these bugs, it was not clear that this was the case, because the background curve model &lt;em&gt;always&lt;/em&gt; looked better than noise.&lt;/p&gt;

&lt;h2&gt;Point merging&lt;/h2&gt;

&lt;p&gt;One additional complication that arose was that after smoothing, many curve points at appeared at nearly the same position.  As a result, the changes in the index set were very small and the resulting Gaussian Process covariance matrix became degenerate.  I added some code that merges points that are too close to each-other, and updates the likelihood function and index set accordingly.  My tests show that this causes a negligible decrease in the ML compared to the non-merged case, and eliminates the degeneracy problem in all the cases I encountered.&lt;/p&gt;

&lt;h2&gt;Smart Cleanup&lt;/h2&gt;

&lt;p&gt;During my investigations, I also rewrote the &quot;cleanup&quot; logic, which ensures that each point corresponds to the model curve exactly once.  It originally did this by naively taking the first correpsondence, and I thought that the problematic results from above were caused by this.  I wrote new logic that now chooses the &lt;em&gt;best&lt;/em&gt; correspondence, i.e. the correspondence that results in the lowest error.&lt;/p&gt;

&lt;h2&gt;Summary, Next steps&lt;/h2&gt;

&lt;p&gt;The new code is in &lt;code&gt;correspondence/corr_to_bg_likelihood.m&lt;/code&gt;, which now replaces the deprecated &lt;code&gt;clean_bg_correspondence.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Next test: can we distinguish BG curves from non-bg/foreground curves?&lt;/li&gt;
&lt;li&gt;Migrate this new logic to 3D curve model (clean_correspondence.m and maybe merge_correspondence.m)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 
</feed>
