<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>KLS Research Blog</title>
 <link href="http://vision.sista.arizona.edu/ksimek/research/atom.xml" rel="self"/>
 <link href="http://vision.sista.arizona.edu/ksimek/research"/>
 <updated>2013-08-14T18:21:46-07:00</updated>
 <id>http://vision.sista.arizona.edu/ksimek/research</id>
 <author>
   <name>Kyle Simek</name>
   <email>ksimek@email.arizona.edu</email>
 </author>

 
 <entry>
   <title>Work Log - Background ML bugs; Why is Foreground Noise Variance so large?</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/14/work-log"/>
   <updated>2013-08-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/14/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;ML Validity Testing&lt;/h1&gt;

&lt;p&gt;In yesterday's test, I hadn't realized that the training ML didn't include all of the curves, only the foreground curves were included.  Rerunning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference ML: -5.1552e+04
Training ML: -5.0786e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Okay, we're back in the ballpark -- Within 1.5% of the reference.&lt;/p&gt;

&lt;p&gt;Found the other issue: the roundabout way I was using to generate the training covariance matrices was ignoring the user-specified position_variance_2d.  Results now match:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference ML: -5.1552e+04
Training ML: -5.1552e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, none of this explains why training is causing noise variance to collapse so low.  All discovered problems were merely bugs in the validation logic.  At least we know the training ML logic is valid.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Quick inspection shows that 2D curves are, indeed, pre-smoothed.  This means that noise variance can collapse to near-zero when fitting 2D curve.&lt;/p&gt;

&lt;p&gt;The new question becomes: why doesn't it occur in the foreground (3D) model?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Try re-indexing...&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% update params related to smoothing variance
params_2 = tbc_.params;
params_2.smoothing_variance = training_results{1}.smoothing_variance;
params_2.noise_variance = training_results{1}.noise_variance;
% re-construct likelihood (including indicies)
test_Corrs_ll_2 = tr_prep_likelihood(test_Corrs, data_, params_2);
% re-run training
train_params_done = tr_train(test_Corrs_ll_2, training_results{2}, 400, 3);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0011
            noise_variance: 0.6846
         position_variance: 1.3597e+04
             rate_variance: 0.3042
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.3013e-06
 perturb_position_variance: 0.6621
             perturb_scale: 2.9795

Final ML: -7840.140322
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare against results prior to re-indexing: version:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654

Final ML: -8049.9e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we've improved, but nowhere near the background model's ML of 4611.9.  Note that curves got smoother and less noisy.  More correlation, more variance pushed into the perturbations.  (why the f*** perturb_smoothing_variance is just sitting there like an idiot is still beyond me).&lt;/p&gt;

&lt;h2&gt;Miscellaneous thoughts&lt;/h2&gt;

&lt;p&gt;Need to visualize ll_means against smoothed curve.  The perturbations should be correlated.  Maybe even plot them?  Note the perturbations need to be considered only in the directions parallel to the image plane.  &lt;strong&gt; This has never been done, and is necessary to validate the index-estimation in &lt;code&gt;correspondence/corr_to_likelihood&lt;/code&gt;.&lt;/strong&gt; Can we visualize after removing rate and offset components?  Yes: difference between ll_means and per-view reconstructed curve.&lt;/p&gt;

&lt;p&gt;Consider smarter smoothing in &lt;code&gt;corr_to_likelihood&lt;/code&gt; -- using posterior max instead of &lt;code&gt;csaps&lt;/code&gt;.    Could give better index estimation if the visualization test shows problems.&lt;/p&gt;

&lt;p&gt;What if we were using a 3D likelhood for background curves too? Could we still expect the BG ML to be insanely high, and the noise variance to be insanely peaked?  Backproject, estimate 3d noise variance, estimate index set.  The farther away it gets, the more variance in the correlated points.  Which means lower ML, right?  But training will push variance lower.&lt;/p&gt;

&lt;p&gt;Note that larger noise variance in FG model will explain away bad triangulations.  Perturb variances also explain to some extent, but maybe they aren't sufficient to explain enough of it.&lt;/p&gt;

&lt;p&gt;Why is perturb_smoothing_variance basically zero?  If it was higher, it could explain more of the traingulation error, and allow noise variance to drop.  Should we be using a different pertubation model?  Maybe Brownian motion instead of integrated brownian motion?  Visualizing perturbations would be informative here.&lt;/p&gt;

&lt;p&gt;Do smooth perturbations follow a different dynamics model than linear and offset perturbations?  Can we force it to be larger?  what if it was the only option for modelling perturbations?   It's true that a small amount of smoothing variance can result in a huge amount of marginal point variance, and large variances kill ML's.  Probably a mean-reverting model is more sensible -- Ornstein Ulenbeck process, perhaps?  Or SqExp?  I avoided these in the past, because it changes the form of the marginal curve covariances -- they're no longer purely cubic-spline processes.  But I never considered the fact that we need to model triangulation error.&lt;/p&gt;

&lt;p&gt;Observations: setting perturb_smoothing_variance to exactly zero has no change in ML.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Consider tying foreground and background noise variance during training.&lt;/strong&gt;  &amp;lt;---  This is the most pragmatic solution.  Avoids getting mired in details, and acknowledges what we know to be true: image noise arises from the same process in foreground and background models.&lt;/p&gt;

&lt;p&gt;Possibly the fact that we allow a nonzero position_mean in 2D but not in 3D is the issue?&lt;/p&gt;

&lt;h1&gt;Finer-grained index estimation&lt;/h1&gt;

&lt;p&gt;Got it!  In &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;, we have two parameters that determine how fine-grained the sampling is along the smoothed curve.  Each observed curve is then matched against the sampled curve.  The sampling period is 2 pixels, which means there's an average error of about 1 pixel in each index estimate.&lt;/p&gt;

&lt;p&gt;Reducing the sampling period to 1 pixel and re-training gives:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0014
            noise_variance: 0.3986
         position_variance: 1.3555e+04
             rate_variance: 0.3100
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 3.3992e-04
 perturb_position_variance: 0.6874
             perturb_scale: 2.3823

Final ML: -6357.585946
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Although we only slightly changed the sampling period, the final ML improved significantly.  The noise variance dropped from 0.7 to 0.4, too.  Perturb rate variance changed by 2 orders of magnitude!&lt;/p&gt;

&lt;p&gt;Reducing sampling period to 0.5:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0018
            noise_variance: 0.3060
         position_variance: 1.3499e+04
             rate_variance: 0.3098
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 4.3411e-04
 perturb_position_variance: 0.8152
             perturb_scale: 2.4095

Final ML: -5664.35
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The upward ML trend continues, but the noise variance appears to be flattening out.  The perturb_position_variance jumped upward unexpectedly.&lt;/p&gt;

&lt;p&gt;This might explain all of the disparity between the 2D and 3D noise variances.  Unfortunately, we can't reduce the sampling period to 0.0004, because the runtime complexity of the matching  is O(N&lt;sup&gt;2),&lt;/sup&gt; where N is the number of sampled points.&lt;/p&gt;

&lt;p&gt;Better idea: after finding the optimal region in the matching algorithm, improve it by projecting the point onto the line segments neighboring the matched point.  Constant-time, and significantly better!&lt;/p&gt;

&lt;p&gt;Another thought: if the rasterization error was approx. 1 pixel, the sampling error could be reduced to the point where the rasterization error dominated (possibly a sampling period of  0.5 or 0.01 would achieve this).  That way, both 2D and 3D noise sigma would be dominated by rasterization error, and would train to similar values.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;implement post-match index improvement.&lt;/li&gt;
&lt;li&gt;Plot reconstruction residuals, look for correlation model.

&lt;ul&gt;
&lt;li&gt;Goal: determine if residuals are truely independent, and belong in the noise bucket.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try training BG and FG together, with the same noise variance.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - Theoretical Rate variance bug; Training background curve model</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log"/>
   <updated>2013-08-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Curve reversing thoughts&lt;/h1&gt;

&lt;p&gt;The reversed curve issue only really matters during training.  Our tests show that curve-flip moves will mix well, even if the maximum isn't always correct.  Adding connections between parent and child curves should resolve these issues.&lt;/p&gt;

&lt;p&gt;During training, derive curve direction from ground-truth.&lt;/p&gt;

&lt;h1&gt;Theoretical Rate Variance (take 2)&lt;/h1&gt;

&lt;p&gt;Realized that my last attempt at this had two bugs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I used &lt;code&gt;rand()&lt;/code&gt; instead of &lt;code&gt;randn()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;I was normalizing by the &lt;em&gt;squared&lt;/em&gt; vector magnitude.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Fixing this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir = randn(3,10000000);
dir = bsxfun(@times, dir, 1./sqrt(sum(dir.^2)));
var(dir(:))

    ans =
        0.3332
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare this to earlier theoretical results of ~0.23.&lt;/p&gt;

&lt;p&gt;This new result is interesting, because it is 25% higher than the emperical results we've been getting.  I'm guessing that the fact all of the curves point upward reduces the variance. To prove, we'll force all points to be in the top hemishphere:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; dir = randn(3,10000000);
 dir(3,:) = abs(dir(3,:));
 dir = bsxfun(@times, dir, 1./sum(dir.^2));

 var(dir(:))

    ans =
        0.3149
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep.  And in practive, our values take on an even smaller range of directions.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Repeating for the 2D case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir = randn(2,10000000);
dir = bsxfun(@times, dir, 1./sqrt(sum(dir.^2)));
var(dir(:))

    ans =
        0.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This strongly suggests a pattern of variance being 1/D.&lt;/p&gt;

&lt;h1&gt;Connection test&lt;/h1&gt;

&lt;p&gt;does connecting each of the curves result in better ML?  Do we need to marginalize?&lt;/p&gt;

&lt;h1&gt;training background model&lt;/h1&gt;

&lt;p&gt;construct training ML for background ML
construct mats for bg curve models.&lt;/p&gt;

&lt;p&gt;Result: &lt;code&gt;train/tr_train_bg.m&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 1.7837e+04
     rate_variance_2d: 0.5000
    noise_variance_2d: 9.4597e-04
smoothing_variance_2d: 0.0157
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that noise_variance_2d is so low.  We expected it to be on the order of 1 pixel.  More discussion on this later.&lt;/p&gt;

&lt;p&gt;I retrained the BG model for &lt;em&gt;only&lt;/em&gt; the foreground curves, and evaluated the ML under it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 5.8116e+03
     rate_variance_2d: 0.5000
    noise_variance_2d: 4.8105e-04
smoothing_variance_2d: 0.0053
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Smaller noise variance, smaller smoothing variance.  This shrinking of variance with smaller training set is typical overfitting behavior.  Not of much concern.&lt;/p&gt;

&lt;p&gt;Here's the comparison against the ML for the trained foreground model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bg model = 4611.886746
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not good.  The FG model on true foreground curves should have a better marginal likelihood than the same curves under the BG model.&lt;/p&gt;

&lt;p&gt;Some questions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Why is bg noise variance so low??

&lt;ul&gt;
&lt;li&gt;did we smooth the detected curves before storing them?&lt;/li&gt;
&lt;li&gt;If so, why isn't the foreground model lower?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why is the background model so much better than the foreground model?

&lt;ul&gt;
&lt;li&gt;We expect foreground curves to have a higher marginal likelihood under the foreground model than the background.&lt;/li&gt;
&lt;li&gt;could it be an indexing issue?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This warrants further investigation.&lt;/p&gt;

&lt;h2&gt;Other observations&lt;/h2&gt;

&lt;p&gt;If I force the noise variance to be equal to that of the fg model (0.72), the ML drops significantly (fg results reprinted for convenience):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; bg model = -9413.1e+03
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we're back in business.  This is a good sanity check, but it doesn't explain why we can't get similar noise variances for both models when training.&lt;/p&gt;

&lt;p&gt;Possibly the smoothing variance would need to change in this case.  Retraining, with noise_variance forced to 0.72:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 5.8116e+03
     rate_variance_2d: 0.5000
    noise_variance_2d: 0.7204
smoothing_variance_2d: 2.7394e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Smoothing variance dropped dramatically.  ML comparison (fg results reprinted for convenience):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bg model = -9214.632874
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that bg ML didn't significantly change (-2%) after optimizing noise variance (which did change a lot).&lt;/p&gt;

&lt;p&gt;Might be worthwhile visualizing the optimal fits with these parameters.  Are we oversmoothing?  undersmoothing?  These would suggest a bug.&lt;/p&gt;

&lt;h2&gt;ML Validity Testing&lt;/h2&gt;

&lt;p&gt;Running reference ml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data_2 = init_data_curves_ml(data_, bg_train_params_done_force)
sum([data_2.curves_ml{:}])

    ans =
      -5.1552e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Very different from the training implementation.  Need to dig deeper to determine the cause.&lt;/p&gt;

&lt;h2&gt;Misc Thoughts&lt;/h2&gt;

&lt;p&gt;Do we need to re-estimate the index set during training of the FG model?&lt;/p&gt;

&lt;p&gt;Iterate: train, re-index, repeat.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;investigate disparity between training ML and reference ML for background curves.&lt;/li&gt;
&lt;li&gt;Further investigate the FG vs. BG marginal-likelihood issue.&lt;/li&gt;
&lt;li&gt;test the re-indexing approach to FG model training.&lt;/li&gt;
&lt;li&gt;re-build end-to-end sampler.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Dissecting the Camera Matrix, Part 3: The Intrinsic Matrix</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/13/intrinsic"/>
   <updated>2013-08-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/13/intrinsic</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Earlier we studied the extrinsic camera matrix, and examined three different interpretations of its parameters.  Today I'll demonstrate two equivalent interpretations of the camera matrix.  First, we'll see how the camera parameters correspond the geometry of real physical camera.  Then I'll show how each of the intrinsic parameters corresponds to a simple 2D transform.  Finally,  give a 3D demo illustrating both interpretations&lt;/p&gt;

&lt;p&gt;This is an entry in the series &quot;Camera matrix, an interactive tour.&quot;  To read the other entries in the series, see the table of contents, here. (TODO)&lt;/p&gt;

&lt;p&gt;The intrinsic matrix is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;** EQUATION HERE **
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We usually interpret the intrinsic matrix as a transformation from 3D non-homogeneous camera coordinates to 2D homogeneous image coordinates by projecting onto the image plane.  I call this the &quot;camera-centric&quot; interpretation of the intrinsic matrix, where each of the parameters represents a physical aspect of our pinhole camera's geometry:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Focal length&lt;/strong&gt;: \(f_x\), \(f_y\) - The distance between the pinhole and the image plane (in pixels; all intrinsic values are measured in pixels -- more on this later).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Principal Point Offset&lt;/strong&gt;: \(x_0\), \(y_0\) - The position of pinhole's projection onto the camera's film, relative to the film's origin.  This projection is called the &quot;principal point&quot;, and the line from the principal point through the pinhole is called the &quot;principal axis&quot;.  Increasing \(x_0\) by 10 shifts the pinhole right by 10 pixels, or equivalently, shifts the film left by 10 pixels.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Axis skew&lt;/strong&gt;: \(s\) - As far as I know, there isn't any analogue for this in a traditional camera, but &lt;a href=&quot;http://www.epixea.com/research/multi-view-coding-thesisse8.html#x13-320002.2.1&quot;&gt;apparently some digitization processes can cause nonzero skew&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;At first glance, it seems like these 5 parameters couldn't completely describe the pinhole camera.  The focal length and principal point offset amount to simple translations of the film in space.  There must be other ways to transform the film relative to the pinhole, for example, rotation or scaling, right?  Lets address these one-by-one.&lt;/p&gt;

&lt;p&gt;First we'll consider rotations of the film around the principal axis.  This is traightforward:  this is handled in the extrinsic matrix's rotation parameters (as the &quot;roll&quot; parameter if you're using a pitch-yaw-roll formulation).&lt;/p&gt;

&lt;p&gt;What about other rotations that cause the film to be non-perpendicular to the pinhole?  This question is a bit ill-posed, because it assumes the pinhole has an orientation.  In truth, the pinhole in our camera model doesn't care what &quot;direction&quot; the camera is pointing -- light passes through it in all directions.  The orientation of the camera is defined by the film's orientation, which (again) is handled in the extrinsic camera matrix.  If the film is rotated around a point not lying on the prinpical axis, it will also result in changes to the focal length and principal point, too.&lt;/p&gt;

&lt;p&gt;What if the film doubles in size, but stays in the same position?  This scenario changes the clipping bounds of the image, which isn't actually modelled by the intrinsic matrix.  In graphics systems like OpenGL, this is modelled by adding an extra orthographic projection after the intrinsic matrix is applied, which maps image coordinates to Normalized Device Coordinates.  This is described in more detail in my post on &lt;a href=&quot;/ksimek/research/2013/06/03/calibrated_cameras_in_opengl/&quot;&gt;calibrated cameras in OpenGL&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Why are values measured in pixels?&lt;/p&gt;

&lt;p&gt;Imagine doubling all the physical dimensions of your pinhole camera: the film size and the focal length.  The resulting image would be unchanged (assuming the film's resolution remains constant).  The intrinsic matrix is only concerned with how points in 3d space are projected into pixel coordinates, and thus, it only represents relative dimensions.  The transformation is invariant to scaling of the camera dimensions; by representing dimensions in pixel units, we naturally capture this invariance.&lt;/p&gt;

&lt;p&gt;If you know the size of your film (or digital sensor) in world units, you can convert focal length from pixels to world units.  If \(w\) is the width of the image in pixels, \(W\) is the width in world units, and the focal length in pixels is \(f_x\), you can use similar triangles to find the focal length in world units \(F_x\):&lt;/p&gt;

&lt;div&gt; F_x = f_x * \frac{W_x}{w_x} &lt;/div&gt;


&lt;p&gt;Other parameters \(f_x\), \(x_0\), and \(y_0\) can be found in a similar way.&lt;/p&gt;

&lt;p&gt;Intrinsic parameters as 2D transformations&lt;/p&gt;

&lt;p&gt;If we instead interpret our 3-vectors not as 3D image coordinates but as 2D homogeneous coordinates, we can come to an alternative interpretation of the intrinsic matrix: as a sequence of 2D affine transformations.  We can decompose the intrinsic matrix into the product of a scaling, translation, and shear amtrix, corresponding to focal length, principal point offset, and axis skew, respectively:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TODO: equation here
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This interpretation implies that the intrinsic camera transformation occurs &lt;em&gt;post-projection&lt;/em&gt;.  One notable result of this is that intrinsic parameters cannot affect visiblity -- occluded objects cannot be revealed by simple 2D transformations in image space.&lt;/p&gt;

&lt;p&gt;Full decomposition&lt;/p&gt;

&lt;p&gt;We can now combine our decomposed intrinsic matrix with the extrinsic matrix decomposition we studied last time.  The result shows the sequence of five affine transformations that make up the complete camera matrix: (1) 3D rotation, (2) 3D translation, (3) 2D scaling, (4) 2D translation, and finally (5) 2D shear.&lt;/p&gt;

&lt;p&gt;TODO EQUATION HERE&lt;/p&gt;

&lt;p&gt;Demo&lt;/p&gt;

&lt;p&gt;The demo below illustrates both interpretations of the intrinsic matrix.  The left pane represents the &quot;camera-geometry&quot; interpretation.  The yellow plane indicates the &quot;virtual&quot; image plane, but for all intents and purposes, represents the &quot;film&quot; of the pinhole camera.  Notice how the pinhole move relative to the image plane as \(x_0\) and \(y_0\) are adjusted.&lt;/p&gt;

&lt;p&gt;The right pane represents the &quot;2D transformation&quot; interpretation.  Notice how changing focal length results in scaling of the projected image, and principal point results in pure translation.  Also note that since translation occurs &lt;em&gt;after&lt;/em&gt; scaling, larger focal lengths will result in larger translations as \(x_0\) and \(y_0\) are adjusted.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TODO: demo here
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - Re-run training, Re-reconstruction, Curve-Flipping</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log"/>
   <updated>2013-08-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Re-training&lt;/h1&gt;

&lt;p&gt;Re-ran training after several bug-fixes.&lt;/p&gt;

&lt;h2&gt;New Files:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;train/tr_train_all.m&lt;/code&gt;  - Utility method for training all four models.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;experiments/exp_2013_08_11_train_all.m&lt;/code&gt;  - end-to-end training example; recreates results here.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;All results generated by &lt;code&gt;exp_2013_08_11_train_all.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0024
            noise_variance: 1.2308
         position_variance: 1.6072e+04
             rate_variance: 0.2743
perturb_smoothing_variance: 1
     perturb_rate_variance: 1
 perturb_position_variance: 1
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7192
         position_variance: 1.6132e+04
             rate_variance: 0.2451
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1292e-06
 perturb_position_variance: 0.4849
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;OU-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0018
            noise_variance: 0.7207
         position_variance: 1.6117e+04
             rate_variance: 0.2480
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1355e-06
 perturb_position_variance: 0.5172
             perturb_scale: 0.9202
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised to see that noise-variance only changed by a factor of 10, not 100.  However, the resulting noise_variance is right in the range that you'd expect arising from pixel-grid rasterization error.&lt;/p&gt;

&lt;p&gt;OU perturb-scale is lower than in the last case, and perturb position and rate variance is lower, too.&lt;/p&gt;

&lt;p&gt;SqExp perturb-scale is higher than in the last case, and perturb rate variance is lower.&lt;/p&gt;

&lt;p&gt;Lower position and rate variance makes sense after correcting curve-reversals.&lt;/p&gt;

&lt;p&gt;However, since we trimmed the pre-tails, a higher global and perturb position variance should result.  The result we're seeing is a combination of these competing effects.&lt;/p&gt;

&lt;h1&gt;Reconstructions&lt;/h1&gt;

&lt;p&gt;Some curves are flipped; need to an approach that will detect and correct flipped curves.&lt;/p&gt;

&lt;p&gt;Images and javascript generated by &lt;code&gt;../experiments/exp_2013_08_11_reconstruct_for_web.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-ind-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-9.png&quot;
        ]

    construct_animation($(&quot;#ind-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ind-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;OO-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-ou-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-9.png&quot;
        ]

    construct_animation($(&quot;#sqexp-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;sqexp-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;h1&gt;Detecting and Flipping Curves&lt;/h1&gt;

&lt;p&gt;Experiment: &lt;code&gt;../experiments/exp_2013_08_11_flip_curves.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Result: doesn't really work.  Lots of false negatives.&lt;/p&gt;

&lt;p&gt;Algorithm output: 2     4     5     6     8    10    12
Ground Truth: 1 2 4 5 6 8 9 10 11 12 14 15&lt;/p&gt;

&lt;p&gt;Not really sure why this is failing.  After flipping, most of these curves are closer to the origin, which is promoted by position_variance.  And any tip-perturbations should be better modelled after flipping.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Think more on curve-reversing&lt;/li&gt;
&lt;li&gt;Central curve extraction&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;li&gt;end-to-end correspondence sampling&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/10/work-log"/>
   <updated>2013-08-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/10/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Pre-tails issue&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Bug&lt;/strong&gt;: Smallest index of reconstructed curves is significantly greater than 0.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp_setup_workspace
min([test_Corrs_ll{1}.ll_indices{:}])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This results in long pre-tails, as seen in this image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-10-pretails.jpg&quot; alt=&quot;pretails&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recall that reconstruction occurs by doing (1) rough triangulation, (2) smoothing, then (3) re-triangulating against the smoothed curve.  The initial triangulation usually results in a very bad index set, with spacing far larger than it should be, due to poor localization by maximum likelihood.  The subsequent smoothing causes the curve to stretch out longer than it should, so when re-triangulation occurs, ends are cut off.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Solution &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After re-triangulating, re-index so the minimum index is zero.&lt;/p&gt;

&lt;p&gt;Change to &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;145  % re-index by subtracting minimum index
146  min_index = min([Corr.ll_indices{:}]);
147  Corr.ll_indices = cellfun(@(x) x - min_index, Corr.ll_indices, 'UniformOutput', false);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt; Fallout &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This likely had an effect on training results, because marginal prior variance of the initial point was over-estimated, because it's index was over-estimated.&lt;/p&gt;

&lt;h1&gt;Cleanup&lt;/h1&gt;

&lt;p&gt;I'm afraid I made a mess of things yesterday when I was addressing the noise_variance issue in training.  Need to review the end-to-end systems for training, reconstruction and marginal likelihood evaluation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is training world-variance scaled everywhere?&lt;/li&gt;
&lt;li&gt;is training world-variance projecting to 1.0?&lt;/li&gt;
&lt;li&gt;is training-ml equal to inference-ml?&lt;/li&gt;
&lt;li&gt;are the reconstructed results sensible?&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;handle reversed curves&lt;/li&gt;
&lt;li&gt;Retrain all models since the following changes

&lt;ul&gt;
&lt;li&gt;reversal fixes&lt;/li&gt;
&lt;li&gt;noise variance fix&lt;/li&gt;
&lt;li&gt;&quot;pre-tail&quot; index fix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;central curve extraction&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;li&gt;end-to-end sampling system&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - Refactoring, cleanup, bug fixes</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/09/work-log"/>
   <updated>2013-08-09T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/09/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15169&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Goals&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Move &lt;code&gt;test/tmp*&lt;/code&gt; to &lt;code&gt;experiment/&lt;/code&gt; directory.&lt;/li&gt;
&lt;li&gt;Overlay reconstruction onto images.&lt;/li&gt;
&lt;li&gt;Test effect of reversing curves on ML.&lt;/li&gt;
&lt;li&gt;Train background curve model.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Git Blog Mess&lt;/h1&gt;

&lt;p&gt;Got sidetracked after screwing up a git commit of the research blog.  Not sure the cause, but several files were deleted from the &quot;source&quot; branch.  Changed the &quot;preview&quot; rake target so it builds to /tmp/research_blog_site, instead of the source directory.  Hopefully this will avoid these issues in the future.&lt;/p&gt;

&lt;h1&gt;End-to-end experiment&lt;/h1&gt;

&lt;p&gt;Created an experiment file that recreates yesterday's results from scratch: &lt;code&gt;exp_2013_08_09_animated_reconstruction.m&lt;/code&gt;.  Since it runs training, it takes about 5 minutes to run.&lt;/p&gt;

&lt;p&gt;Also broke out reconstruction code into function in &lt;code&gt;reconstruction/reconstruct_views.m&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Overlay reconstruction onto images&lt;/h1&gt;

&lt;p&gt;See &lt;code&gt;test/tmp_vis_overlay.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I'm seeing some weirdness in the reconstructions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Curves are too stiff&lt;/li&gt;
&lt;li&gt;long pre-tails on a couple of curves.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Stiff Curves Solved&lt;/h1&gt;

&lt;p&gt;Figured out what was causing stiff curves.  I forgot that during training, all precisions are stored with noise_variance fixed at 1.0, and then are scaled on the fly.  However, during visualization, that scaling doesn't occur; the precisions are assumed to be stored at the desired scale.  i.e. a dumb bug.&lt;/p&gt;

&lt;p&gt;The new reconstructions now show a moderate amount of curvature, compared to their pre-bug stiff counterparts.&lt;/p&gt;

&lt;p&gt;Found another bug:  when constructing the &quot;unscaled&quot; precisions in &lt;code&gt;tr_prep_likelihood.m&lt;/code&gt;, I called &lt;code&gt;corr_to_likelihood&lt;/code&gt; with &lt;code&gt;params.noise_variance&lt;/code&gt; instead of 1.0.  Thus, if I understand correctly, the reported training value for noise-variance is 100x lower than it should be.&lt;/p&gt;

&lt;p&gt;That means the no-perturb model has a noise standard deviation on the order of 3.4 pixels and the perturb models are around 2.7.  This is closer to the range I was expecting, but I was hoping the perturb model stddev would be closer to 0.5, because it should arise only from pixel rasterization.  However, other sources of noise could be the curve detector, and also the cubic spline model might not be expressive enough to capture the model variance.&lt;/p&gt;

&lt;p&gt;I think I need to re-run end-to-end training and reconstruction to make sure there aren't any side-effects of these fixes.&lt;/p&gt;

&lt;p&gt;Committed to revision 15169&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Determine cause of long pre-tails.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - Visualizing Results; New training method</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/08/work-log"/>
   <updated>2013-08-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Visualization&lt;/h1&gt;

&lt;p&gt;I reconstructed the curves using the models I trained yesterday.  I was able to recover both the overall structure and track it's motion over 9 views.  For each of these results, use the slider below to change between each of the 9 views (the changes are subtle).&lt;/p&gt;

&lt;p&gt;I'm still struggling with smoothing variance being too low, which causes curves to be too straight.  For all of these results, I manually changed &lt;code&gt;smoothing_variance&lt;/code&gt; to be 0.1.&lt;/p&gt;

&lt;script&gt;
$(function(){
    var ind_urls = [
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#ind-reconstruction&quot;), ind_urls);


    var ou_urls = [
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#ou-reconstruction&quot;), ou_urls);

    var sqexp_urls = [
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#sqexp-reconstruction&quot;), sqexp_urls);

});
&lt;/script&gt;


&lt;h2&gt;Ind-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Below is the reconstruction for the independent-perturbation model.  The curves in each view are independently perturbed versions of central mean curves (not shown).&lt;/p&gt;

&lt;div id=&quot;ind-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;p&gt;It is unclear how much of this motion is due to camera miscalibration, and how much is actual plant motion.  Nevertheless, this shows that we can use the perturbation models to simultaneously triangulate and track over time.&lt;/p&gt;

&lt;p&gt;First, although each &lt;em&gt;view's&lt;/em&gt; perturbation is independent of the others, the perturbation is correlated between nearby points within the same view.  In other words, perturbations don't violate the smoothness constraint.&lt;/p&gt;

&lt;h2&gt;OU-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Next is the Ornstein-Ulenbeck perturbation model.  As opposed to the previous model, which modeled each view's perturbations as white noise, this model assumes Brownian motion over time.  Thus, we see a more naturally evolving time-series.&lt;/p&gt;

&lt;div id=&quot;ou-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;p&gt;The OU process models brownian motion&lt;/p&gt;

&lt;h2&gt;SQEXP-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Finally we have the squared-exponential perturbation model.  Now Brownian motion has been replaced by smooth motion.  I'm doubtful that this is a natural motion model for these plants.  The scale parameter is so low, I question whether it has any significant effect.&lt;/p&gt;

&lt;div id=&quot;sqexp-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;h1&gt;New Training Method&lt;/h1&gt;

&lt;p&gt;Need to determine why learned smoothing variance is so low.&lt;/p&gt;

&lt;p&gt;Is it even valid to do max-likelihood to train the parameters of the covariance function?&lt;/p&gt;

&lt;p&gt;Should we be training the smoothness parameter using the noiseless ground-truth data?&lt;/p&gt;

&lt;h2&gt;Two-pass Learning Procedure&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;First learn using noiseless data:

&lt;ol&gt;
&lt;li&gt;Set rate-variance to fixed ~0.23 (see &lt;a href=&quot;/ksimek/research/2013/08/07/work-log/#optimal-rate-variance&quot;&gt;yesterday's results&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Estimate  position_variance by emperical distribution over all point positions in ground truth.&lt;/li&gt;
&lt;li&gt;Estimate smoothing_variance by maximum likelihood over ground truth.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Learn the noise variance for the no-perturb model.

&lt;ol&gt;
&lt;li&gt;Find point correspondence between detected curves and corresponding ground truth curve.&lt;/li&gt;
&lt;li&gt;Compute variance between projected points and observed points.  This should maximize \(p(D \mid \Theta_0) \), where \(\Theta_0\) are the ground truth curves.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Learn perturb model parameters: noise_variance, perturb_{smoothing_variance, rate_varaince, position_variance}

&lt;ol&gt;
&lt;li&gt;Use nonlinear optimization to maximize \(p(D \mid \Theta_0) \), as defined below.  Start with no-perturb parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;div&gt;Let \(\theta_0\) be the ground-truth curve, and \(\{\theta_i\}\) be the set of all (unobserved) per-view curves \(\theta_i\). Let \(S_i\) be the virtual point precision matrices, assuming noise variance \(\sigma_n^2 = 1\).  The likelihood conditioned on the ground truth data \(p(D \mid \theta_0)\) is given by: &lt;/div&gt;




&lt;div&gt; 
\begin{align}
p(D \mid \theta_0) &amp;= \int_{\{\theta_i\}}  p(D_i, \{\theta_i\} \mid \theta_0)  d\{\theta_i\} \\
                   &amp;= \int_{\{\theta_i\}} \prod_{i=1}^N p(\theta_i \mid \theta_0) p(D_i | \theta_i) d\{\theta_i\} \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  p(\theta_i \mid \theta_0) p(D_i | \theta_i) d\theta_i \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  \mathcal{N}(\theta_i; \theta_0, \Sigma_p) \mathcal{N}(D_i;  \theta_i, \sigma_n^2 S_i^{-1}) d\theta_i \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  \mathcal{N}(D_i ; \theta_0, \Sigma_p + \sigma_n^2 S_i^{-1})
\end{align}
&lt;/div&gt;




&lt;div&gt; where  \(\Sigma_p\) is the perturbation variance. &lt;/div&gt;


&lt;p&gt;This should be an improvement over the current method, where we use only ground-truth labellings (not positions) and fit all parameters simultaneously.  This method assumed too much noise variance and not enough smoothness variance.&lt;/p&gt;

&lt;p&gt;Since we actually know the noiseless curves, we should train to that, to avoid the confounding of smoothness variance and noise variance.  The ground truth curves are aso stronger sources of evidence, compared to the curves reconstructed from data.&lt;/p&gt;

&lt;h1&gt;TODO:&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;reproject per-view reconstructions and overlay with original image.&lt;/li&gt;
&lt;li&gt;obtain &amp;amp; visualize unobserved &quot;central curve&quot;&lt;/li&gt;
&lt;li&gt;investigate the low smoothness variance.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Later TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;automatic curve reversing?&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - Training, Reversed Curves, and Theoretical Rate Variance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/07/work-log"/>
   <updated>2013-08-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/07/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Visualized results after capping likelihood variance.  As expected, degree of spreading stops growing as perturb_rate_variance continues to grow.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Tasks&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;manually flip some curves and see if model changes&lt;/li&gt;
&lt;li&gt;try automatically determining which to curves need flipping&lt;/li&gt;
&lt;li&gt;try to get training and visualization to agree&lt;/li&gt;
&lt;li&gt;visualize curves moving through space over time&lt;/li&gt;
&lt;li&gt;train background curve model

&lt;ul&gt;
&lt;li&gt;is background model better than foreground?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Long term goals&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sampling framework

&lt;ul&gt;
&lt;li&gt;try using background pixel modeling to prune background curves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Visualizing curve-direction revealed a bizarre artifact: most curves start somewhere in the middle of the reconstructed curve!&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Found issue - wasn't sorting by index when reconstructing.&lt;/p&gt;

&lt;h2&gt;Breaking Change&lt;/h2&gt;

&lt;p&gt;Modified &lt;code&gt;tr_curves_ml&lt;/code&gt; to &lt;em&gt;not&lt;/em&gt; include the background curve ml into the computation.  Recall that the normal ML computation code doesn't actually return ML, but a &lt;em&gt;ratio&lt;/em&gt; of the foreground curve ML and the background curve ML.  This indicates how much the model improves over the &quot;null model&quot;.&lt;/p&gt;

&lt;p&gt;Since the background curve ml is constant during training, this shouldn't affect results.  However, if you want to confirm the correctness of &lt;code&gt;tr_curves_ml&lt;/code&gt; against the reference implementation in &lt;code&gt;curve_ml5.m&lt;/code&gt;, you'll need to manually divide by a constant.  See the documentation for &lt;code&gt;tr_curves_ml&lt;/code&gt; for more details.&lt;/p&gt;

&lt;h2&gt;Reversing Curves&lt;/h2&gt;

&lt;p&gt;Investigating the effect of reversing curves.&lt;/p&gt;

&lt;p&gt;Visually determined which curves were reversed.  See modified version of &lt;code&gt;test/tmp_visualize_test&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Hacked &lt;code&gt;train/tr_construct_matrices.m&lt;/code&gt; with hard-coded list of curves to flip.  Re-ran training for &lt;em&gt;IND-Perturb&lt;/em&gt; model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;: we should see larger values for perturb_rate_variance and/or perturb_smoothing_variance, and smaller values for perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0020
            noise_variance: 0.0720
         position_variance: 1.3414e+04
             rate_variance: 0.2378
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.5332e-06
 perturb_position_variance: 0.4662

Final ML: -95.736042
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare against old results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0718
         position_variance: 1.6706e+04
             rate_variance: 0.2135
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4942e-06
 perturb_position_variance: 0.4886

Final ML: -97.463243
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary of changes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: +2.09%
            noise_variance: 0.17%
         position_variance: -19.7%
             rate_variance: +11.3%
perturb_smoothing_variance: 0 
     perturb_rate_variance: +2.61%
 perturb_position_variance: -4.59%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As expected, global position variance dropped; perturb rate grew while perturb position variance decreased.&lt;/p&gt;

&lt;p&gt;Unexpected increase in rate_variance; expected it to stay constant.  Possibly due to random fluctuations; both old and new values (0.214 and 0.238, respectively) are near the theoretical optimum (0.23, see next section).&lt;/p&gt;

&lt;p&gt;Also unexpected small increase in global smoothing variance (expected to be constant); also possibly due to random fluctuations.&lt;/p&gt;

&lt;p&gt;Literally no change to perturb smoothing variance.  I'm starting to suspect something weird is going on with this value...&lt;/p&gt;

&lt;h2 id=&quot;optimal-rate-variance&quot;&gt;Theoretical Rate Variance&lt;/h2&gt;


&lt;p&gt;Was curious what the rate variance should be, assuming the rate vectors are drawn from a uniform distribution over the unit sphere.&lt;/p&gt;

&lt;p&gt;Determined empirically that rate variance should be somewhere between 0.220 and 0.235.  Code below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% generate 10,000 3-vectors with distribution over direction
dir = rand(3,10000);
% normalize to lie on unit sphere
dir = bsxfun(@times, dir, 1./sum(dir.^2));
% Get the emperical covariance of the vectors
Sigma = cov(dir')

        ans =

            0.2105    0.0556    0.0580
            0.0556    0.2297    0.0680
            0.0580    0.0680    0.3735
% take the average of the diagonals
mean(diag(Sigma))

        ans =

            0.2290
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This strongly suggests that the global rate variances we've seen in training are consistent with the theoretical value.  Great!&lt;/p&gt;

&lt;h2&gt;Visualizing Curve Motion&lt;/h2&gt;

&lt;p&gt;Attempting to visualize perturbations between views.&lt;/p&gt;

&lt;p&gt;First attempt: tweak &lt;code&gt;test/test_visualize_test&lt;/code&gt; to only display points from a particular view.  Doesn't work great, because only part of the plant is visible in each view, and those parts differ between views.&lt;/p&gt;

&lt;p&gt;Next attempt: tweak &lt;code&gt;curve_max_posterior.m&lt;/code&gt;  to define a canonical index set for the curve, and then reconstruct for each view.&lt;/p&gt;

&lt;p&gt;More tomorrow...&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;p&gt;Training Background model&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - Singular Regions Issue; Training</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/06/work-log"/>
   <updated>2013-08-06T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/06/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Investigating the &quot;Spreading&quot; issue with increases to perturb_rate_variance.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Confirmed the same phenomenon with increase to perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Setting perturb_position_variance to 1000:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-1.png&quot; alt=&quot;perturb_position_variance = 1000&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Setting perturb_position_variance to 1000000:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-2.png&quot; alt=&quot;perturb_position_variance = 1000000&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spreading appears to increase monotonically with perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Again, this is surprising, because you'd expect them to revert to the maximum likelihood solution.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;However, recall that the per-view likelihood has infinite variance in the back-projection direction.  The spreading appears to be occuring in this direction.  The infinite variance means that any influence from the prior will overcome the likelihood.&lt;/p&gt;

&lt;p&gt;But isn't the prior centered at zero?  Why is the result drifting so far from zero?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;One thing is clear: with high perturb&lt;em&gt;* values, the correlation between nearby views becomes negligible compared to the within-view variance.  And since the likelihood variance has an infinite component, the posterior variance grows with perturb&lt;/em&gt;*.  While we can inspect the maximum posterior curve, it is relatively meaningless because the variance is so great.&lt;/p&gt;

&lt;p&gt;Even so, why doesn't it just revert to the mean?&lt;/p&gt;

&lt;p&gt;Mean rate is zero, but it can't be exactly zero, because the likelihood requires that the curve be near the data.  But the data's position is only known in two dimensions, so the posterior is free to manipulate the third dimension so that the rate is minimized.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Model is trying to use a linear-perturbation model to explain per-view deviations from the mean model.  Since the deviations don't arise from pure scaling, it has to contort into bizarre shapes to explain it.&lt;/p&gt;

&lt;p&gt;But the bizarre shapes fit the data better, so it's worth it.&lt;/p&gt;

&lt;h1&gt;&quot;Singular Regions&quot;&lt;/h1&gt;

&lt;p&gt;GOT IT!  Recall that the likelihood variance is measured in world units, even though they are really image-based values.  As the curve moves toward the camera, the likelihood variance &lt;em&gt;should&lt;/em&gt; ideally reduce, since the same size of world perturbations result in larger image perturbations.  But in our model they don't, and so curves can stray farther from the data in the image, but still look nearby the data according to the Gaussian.&lt;/p&gt;

&lt;p&gt;In the extreme case, all the points end up near the camera pinhole, and they will be in the center of the data Gaussian.  In practice, any point within \(\sigma&lt;sup&gt;2\)&lt;/sup&gt; of the camera will be well supported, where \(\sigma&lt;sup&gt;2\)&lt;/sup&gt; is the average noise variance in 3D.  I'll call this the &quot;Singular Region&quot;, where all the of likelihood's Gaussian &quot;cylinders&quot; (degenerate cones) intersect and overlap.&lt;/p&gt;

&lt;p&gt;In terms of the marginal likelihood, this can cause counterintuitive behavior.  For example, large perturb_rate_variance might be good, because it allows the curve to wander into the &quot;singular zone&quot; near the camera.  Thinking of in spherical coordinates, there is a wedge of the sphere that points toward the camera, and as the perturb_rate_variance increases, this wedge remains relatively constant in angular size, but gets longer and longer.  The longer it gets, the more of singular zone it overlaps. Even though greater variance means there are more possible model configurations, there is a period during which the proportion of these configurations that are well-supported by the likelihood doesn't necessarily decrease, so the ML doesn't necessarily decrease, either.&lt;/p&gt;

&lt;p&gt;This explains the phenomenon we saw during training, where the plot of ML vs. perturb_rate_variance (reproduced below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-training-plot.png&quot; alt=&quot;ML vs. perturb_rate_variance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The lump to the left is the result of the singular region giving false support near the camera.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Need to somehow place penalty for any point that strays too far from the mean curve.  Can this be done without radically distorting the model?&lt;/p&gt;

&lt;p&gt;What if I placed a limit on the likelihood variance, instead of letting it be infinite?  It will prevent the prior of taking credit for lousy configurations during the ML marginalization.&lt;/p&gt;

&lt;h1&gt;Experiments&lt;/h1&gt;

&lt;p&gt;Modified &lt;code&gt;train/tr_construct_matrices.m&lt;/code&gt; to clamp the likelihood's maximum variance to some multiple of the largest finite eigenvalue (see local function &quot;fix_precisions&quot;).  ML shouldn't change much when using reasonable values.&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference (no cap): 2.2675e+04  (inf)
100x cap:           2.2109e+04  (1.2 mm)
1000x cap:          2.2360e+04  (3.8 mm)
10000x cap:         2.2510e+04  (12 mm)
100000x cap:        2.2596e+04  (38 mm)
1000000x cap:       2.2654e+04  (12 cm) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Capping the variance to 10000x the triangulation variance results in a standard deviation of about 12mm in practice, which seems very reasonable.&lt;/p&gt;

&lt;p&gt;We have to raise standard deviation to 12 cm for it to be accurate to three significant digits, which seems somewhat high.  Possibly, even with reasonable model parameters, we're still seeing some influence from the &quot;singular zone,&quot; so it may be a good thing that we aren't seeing the full reference value.&lt;/p&gt;

&lt;h2 id=&quot;training-results&quot;&gt;Training Results&lt;/h2&gt;


&lt;p&gt;Running training using clamped likelihoods.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0025
            noise_variance: 0.1231
         position_variance: 1.6658e+04
             rate_variance: 0.2207

Final ML: 2.371 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Exactly the same result as the non-clamped version.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ind Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0719
         position_variance: 1.6729e+04
             rate_variance: 0.2422
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4918e-06
 perturb_position_variance: 0.4801

Final ML:  2.512 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Old non-clamped training resulted in perturb_rate_variance exploding.  The new perturb_rate_variance looks very reasonable.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;OU Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0721
         position_variance: 1.6681e+04
             rate_variance: 0.2146
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4711e-06
 perturb_position_variance: 0.7793
             perturb_scale: 3.7353

Final ML:  2.516 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of the global variance is can be pushed into the perturb_variance, since they are now correlated.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SQEXP Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0018
            noise_variance: 0.0720
         position_variance: 1.6689e+04
             rate_variance: 0.2122
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4952e-06
 perturb_position_variance: 0.5130
             perturb_scale: 0.8425

Final ML: 2.516 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;General observations&lt;/h2&gt;

&lt;p&gt;It's still somewhat weird that perturb_smoothing_variance is so low.  I'm pretty sure there are non-negligible deformations occurring during the imaging process.  Maybe it's just the Ind-perturb model?  More likely it's because the curves that deform are reversed...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Which of the perturb-components are really correlated?  Seems like position variance is probably independent, but rate variance might not be.  Definitely smoothing_variance (i.e. nonrigid deformations) should be correlated.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;try visualization with truncated likelihoods

&lt;ul&gt;
&lt;li&gt;does increasing rate variance eventually have no effect?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;remove perturb_scale from ind model (better inference)&lt;/li&gt;
&lt;li&gt;hand-pick parameters and fix them, to reduce dimensionality of search space.&lt;/li&gt;
&lt;li&gt;Handle &quot;flipped&quot; curves.  Try to infer direction&lt;/li&gt;
&lt;li&gt;Does the visualized max posterior look good for the trained values (I'm guessing not -- too strict of variances, overfitting)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - Training Bugs</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log"/>
   <updated>2013-08-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14852&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing training of OU perturb model.&lt;/p&gt;

&lt;p&gt;Yesterday, we had problems with the perturb scale exploding.  Have switched to using a logistic sigmoid instead of an exponential to map perturb_scale, which sets a lower-bound and and upper-bound during optimization.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;perturb_scale is now staying low, but stil getting weird results.&lt;/p&gt;

&lt;p&gt;perturb_position_variance wants to be 9000+ ???&lt;/p&gt;

&lt;p&gt;perturb_rate_variance wants to be ~40?&lt;/p&gt;

&lt;p&gt;perturb_smoothing_variance wants to be ~0.0.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Trying new kernel: ind_kernel.  Like OU and SQEXP kernels, but no correlation between perturbations.  It's an intermediate step between no-perturb and ou-perturb, since it allows perturbations, but doesn't need a scale-length parameter.&lt;/p&gt;

&lt;p&gt;Training results for ind_kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_3 = 

            smoothing_variance: 0.0024
                noise_variance: 0.0132
             position_variance: 1.5747e+04
                 rate_variance: 0.1709
    perturb_smoothing_variance: 0.0020
         perturb_rate_variance: 26.3458
     perturb_position_variance: 0.8770
                 perturb_scale: 1.7199
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Very weird that perturb_rate variance&lt;/p&gt;

&lt;p&gt;Consider limiting number of dimensions somehow...&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Force both smoothing_variances to be the same&lt;/li&gt;
&lt;li&gt;remove perturb_scale parameter&lt;/li&gt;
&lt;li&gt;???&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;....&lt;/p&gt;

&lt;p&gt;recall that handling view-index in kernels has never been thoroughly tested...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Let's visualize results for the ind_perturb model, and see if everything looks reasonable.&lt;/p&gt;

&lt;p&gt;Created &lt;code&gt;test/tmp_visualize_test.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Interesting...  Some curves are &quot;reversed&quot;, i.e. the base is the &quot;end&quot; of the curve and the tip is the &quot;beginning&quot;.  This has some unintended consequences when applying the perturb model, because the tips are where perturbations are greatest, but when curves are reversed, the tips aren't affected by most of the modelled perturbations.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Weird.  As perturb_rate_variance increases, the per-view curves spread out radially like flower petals.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-1.png&quot; alt=&quot;high perturb_rate_variance causes radial spreading of curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would have expected that the likelihood would take over, but this is clearly not happening.  Lets look at the likelihood...&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-2.png&quot; alt=&quot;Maximum-likelihood solution.  Posterior should revert to this as variance increases asymtotically.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We must be running up against numerical precision errors.  Let's look at the equation for maximum posterior:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;When \(\Sigma_0\) has huge eigenvalues, this equation basically reduces to&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
\mu_P = (\Sigma_0 \Sigma_l^{-1})^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l 
     &amp;= \Sigma_l \Sigma_0^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l
     &amp;= \Sigma_l \Sigma_l^{-1} \mu_l
     &amp;= \mu_l
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;However, the effective cancellation of \Sigma_0 can't occur, because the expression \((\Sigma_0 \Sigma_l&lt;sup&gt;{-1}&lt;/sup&gt; + I)\) has a huge condition number, which makes inverting it an unstable operation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Anyways, that's visualization.  Is the same issue arising in ML computations?  Higher rate variance can result in significantly larger condition numbers, and IIRC, we don't take any special steps to handle such issues in the ML  computation anymore (for example, using the matrix inversion lemma).&lt;/p&gt;

&lt;p&gt;Maybe we should force rate variance to be within a reasonable range.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Even when perturb_rate_variance is reasonable (1.0), we still get drifting:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-3.png&quot; alt=&quot;perturb_rate_variance == 1.0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The degree of this phenomenon seems to be a smooth function of perturb_rate_variance.  If it was a numerical instability issue, we'd see it arise abruptly, indicating we had entered the unstable regime.&lt;/p&gt;

&lt;p&gt;I'm now thinking this is a real issue with the model, not simply an artifact of computation (maybe it's both?).  Need to think more about it.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Determine why spreading happens when &lt;code&gt;perturb_rate_variance&lt;/code&gt; increases.&lt;/li&gt;
&lt;li&gt;Determine why huge &lt;code&gt;perturb_rate_variance&lt;/code&gt; values are promoted by the ML.&lt;/li&gt;
&lt;li&gt;Handle &quot;reversed curves&quot; issue, where perturbation is applied to the wrong end.&lt;/li&gt;
&lt;li&gt;Training

&lt;ul&gt;
&lt;li&gt;Finish &quot;ind&quot; model&lt;/li&gt;
&lt;li&gt;Traing &quot;ou&quot; and &quot;sqexp&quot; models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log"/>
   <updated>2013-08-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting training of OU perturbation model (&lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Found bug in &lt;code&gt;tr_curves_ml.m::ou_kernel&lt;/code&gt; -- &lt;code&gt;smoothing_variance&lt;/code&gt; was used where &lt;code&gt;perturb_smoothing_variance&lt;/code&gt; should have been (copy-paste bug).  Also fixed in &lt;code&gt;sqexp_kernel&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Optimizer appears to be running much more smoothly now.  Probably because there previously was a strong correlation, due to &lt;code&gt;smoothing_variance&lt;/code&gt; appearing in two different parts of the equation.  The resulting ridge could have caused slow progress.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Initial params:&lt;/p&gt;

&lt;p&gt;train_params =&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.2500
                noise_variance: 100
             position_variance: 90000
                 rate_variance: 2.2500
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 2.2500
     perturb_position_variance: 90000
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stopped after 3 iterations.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.4501
                noise_variance: 2.5982e-04
             position_variance: 6.5359e+04
                 rate_variance: 3.9645
    perturb_smoothing_variance: 3.3437e-06
         perturb_rate_variance: 0.7268
     perturb_position_variance: 3.0785e+04
                 perturb_scale: 0.5867

Final ML: 28423.438936
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final ML (2.84e4) is greater than the no-pertrub model (2.37e4).  This is expected, since this model has more parameters, so we can get a tighter fit (maybe overfitting).&lt;/p&gt;

&lt;p&gt;I like that noise variance is much smaller (close to the minimum--do I need to lower the floor?).  This is expected, because the only source of IID noise is the rasterization process.&lt;/p&gt;

&lt;p&gt;Position standard deviation is much higher than before (255 vs. 126), as is rate standard deviation (2.0 vs. 0.47).&lt;/p&gt;

&lt;p&gt;Perturb variances are harder to explain.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is low, which is nice to see, because the worst of the perturbations are arising from miscalibrations (rate and position perturbations), not deformations.  But it's still far smaller than I expected.  hOnestly, I was expecting it to be almost the same as &lt;code&gt;smoothing_variance&lt;/code&gt;, but I realize now it makes sense for it to be smaller.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perturb position variance is much, much larger than I expected.&lt;/strong&gt;  Standard deviation is 175.4, which is basically saying each plant has 17 cm of IID perturbations from the unobserved mean plant.   I have no good explanation for this, expect maybe that the initial value was ridiculously large.&lt;/p&gt;

&lt;p&gt;Pertub scale seems reasonable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Test sensitivity to initialization.  Initial values:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 0.0500
         perturb_rate_variance: 0.1000
     perturb_position_variance: 1
                 perturb_scale: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First four were initialized from the trained no-perturb model.&lt;/p&gt;

&lt;p&gt;Note the biggest change: perturb_position_variance changed from 9000 to 1.&lt;/p&gt;

&lt;p&gt;Terminated after 46 iterations; much better than the 3 iterations from last attempt.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0019
                noise_variance: 0.0721
             position_variance: 1.6267e+04
                 rate_variance: 0.2050
    perturb_smoothing_variance: 6.7471e-10
         perturb_rate_variance: 1.3201e-04
     perturb_position_variance: 403.7082
                 perturb_scale: 2.4888e+03

Final ML: 25163.231449
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Weird that this is lower than that poorly-initialized model.  Consider initializing using a combination of Attempt #1 and Attempt #2's initial values.&lt;/p&gt;

&lt;p&gt;Smoothing variance is much smaller than before, noise variance is larger.&lt;/p&gt;

&lt;p&gt;Position variance is in the same order-of-magnitude, but about 1/5 the magnitude.&lt;/p&gt;

&lt;p&gt;Rate variance is an OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is still near-zero.&lt;/p&gt;

&lt;p&gt;Perturb rate variance is 3 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb position variance is 2 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb scale is sooooooo high.  Basically flat, so the regular and perturb variances sum.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: Result is very sensitive to initialization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: When perturb scale is this high, there exists a ridge, due to perturb variances playing the same role as normal variances.  Note that when perturb scale is infinity, this model degenerates to the no-perturb model.  Notice that Final ML isn't much better than the no-perturb.  Possibly when perturb scale exploded, the model could no longer improve, so constraining perturb scale to some small set of values may be a good idea (sigmoid transform).&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Try smaller changes to initialization.&lt;/li&gt;
&lt;li&gt;Try sigmoid transform on perturb scale.&lt;/li&gt;
&lt;li&gt;Try training using standard deviations instead of variances.&lt;/li&gt;
&lt;li&gt;Try changing one or more trained values to sensible defaults and retrain.&lt;/li&gt;
&lt;li&gt;Work with standard deviations, not variances&lt;/li&gt;
&lt;li&gt;Fix some 'known' values and optimize others.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log"/>
   <updated>2013-08-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Running optimizer...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issue&lt;/strong&gt;: Likelihood variance is collapsing to zero.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Guess&lt;/strong&gt;: Training ML is different from naive ML and isn't penalizing noise correctly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test 1&lt;/strong&gt;: save params, compare training ml and naive ml&lt;/p&gt;

&lt;p&gt;Same result.&lt;/p&gt;

&lt;h2&gt;Discussion&lt;/h2&gt;

&lt;div&gt; The marginal likelihood is monotonically increasing as the noise variance \(\sigma_n^2\) approaches zero.  This shouldn't be happening, because as the likelihood variance collapses to a delta, the marginal likelihood should become equal to the prior evaluated at the (virtual) observed locations.&lt;/div&gt;




&lt;div&gt;
\begin{align}
    \lim_{\sigma_n \to 0} p(D) &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; p_l(D \mid x) dx &amp; \text{(Marginalization)}\\
         &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; f(D - x) dx &amp;   \text{(Convolution)} \\
         &amp;= \int p_0(x) \delta(D - x) dx \\
         &amp;= p_0(D) 
\end{align}
&lt;/div&gt;


&lt;p&gt;This implies there's a bug in the code causing this phenomenon; the mathematical model is not the cause.&lt;/p&gt;

&lt;h2&gt;Solved&lt;/h2&gt;

&lt;div&gt;Found the bug.  When computing the likelihood precision \(S\) from the unscaled precision \(S_0\), the noise variance \(\sigma_n^2\) was multiplied instead of divided:&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;S = S0 * sigma_n; // incorrect
S = S0 * (1/sigma_n); // corrected version
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Resuming Training&lt;/h2&gt;

&lt;p&gt;Had some trouble with noise sigmas being too low.  Solved by in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Attempts 1-3: clamping to a minimum value and then adding a penalty depending on the amount that was clamped.&lt;/li&gt;
&lt;li&gt;Attempt 4:  offset by minimum value before transforming&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = tr_train(test_Corrs_ll, train_params, data_, 400);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Handle extreme values by incuring a penalty for variances smaller than 1e-5.  Result stored in &lt;code&gt;train_params_done_1&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprisingly small smoothness sigma.  Surprisingly low rate variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the magnitude of the penalty term.  Scaled up penalty by 1000.  Results in &lt;code&gt;train_params_done_2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the threshold where the penalty term starts to be incurred.  Set MIN_NOISE_VARIANCE to 1e-3 and MIN_SMOOTHING_VARIANCE to 1e-7 (previously both 1e-5).  Results in &lt;code&gt;train_params_done_3&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_3 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Attempt #4 &lt;/strong&gt;
Handled small variances by offsetting before transforming:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% converting param to state variable
x(1) = log(smoothing_variance - MIN_SMOOTING_VARIANCE);

% converting state variable to param
x(1) = exp(smoothing_variance) + MIN_SMOOTING_VARIANCE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is more elegant than the penalty hack used in the last three attempts.   Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;h2&gt;No-Perturb model summary:&lt;/h2&gt;

&lt;p&gt;Successfully trained model.&lt;br/&gt;
Required &lt;strong&gt;115 function evaluations&lt;/strong&gt;, taking &lt;strong&gt;112 s&lt;/strong&gt;.&lt;br/&gt;
Optimal marginal likelihood:  &lt;strong&gt;23714.937760&lt;/strong&gt;.&lt;br/&gt;
Optimal parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised at how small noise_variance was, considering the calibration noise.  However, I guess the maximum-likelihood reconstruction looked pretty good, if unity-apect-ratio axis scaling is used.&lt;/p&gt;

&lt;h2&gt;Training OU-Perturb model&lt;/h2&gt;

&lt;p&gt;Getting weird results.  Halted after second iteration; second iteration took 250 function evaluations; perturb_smoothing_variance gradient is zero.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 1.5003e-05
                noise_variance: 1.0087e-04
             position_variance: 7.9393e+04
                 rate_variance: 0.7879
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 1.5183
     perturb_position_variance: 2.5159e+04
                 perturb_scale: 48.6163
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Recall that new model ML's aren't deeply tested; probably a bug in there (in the kernel implementation?  in the kernel theory? in the conversion from mats to kernel?).  Will continue tomorrow.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log"/>
   <updated>2013-08-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Morning&lt;/h2&gt;

&lt;p&gt;Researched telecommuting strategies.&lt;/p&gt;

&lt;p&gt;Set up Google hangout: &lt;a href=&quot;https://plus.google.com/hangouts/_/89759369dd280ff225c298a7a4291745134e1d6f&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Set up IRC chat room: &lt;a href=&quot;http://webchat.freenode.net/?channels=ivilab&amp;amp;uio=d4&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Probably IRC will be best for general chat, Google Hangouts for screen sharing or group video chat.&lt;/p&gt;

&lt;h2&gt;Afternoon&lt;/h2&gt;

&lt;p&gt;Setting up training minimizer using Matlab's &lt;code&gt;fminunc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Got all params in-place, but Matlab version is too old.&lt;/p&gt;

&lt;p&gt;Trying to upgrade matlab, but Java is out of date.&lt;/p&gt;

&lt;p&gt;Taking a break to move furniture...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Downloaded and installed Java 7.&lt;/p&gt;

&lt;p&gt;Now Matlab installer isn't able to communicate with server.  Arg...&lt;/p&gt;

&lt;p&gt;Working now... downloading...  Should be done in 45 minutes.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;p&gt;Migrating settings from old Matlab...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log, Week summary</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/26/work-log"/>
   <updated>2013-07-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;No work logs this week.  All week I've been splitting time between research and packing for the move to Phoenix on Monday.  I've made some progress on the training framework.&lt;/p&gt;

&lt;h2&gt;Miscellaneous&lt;/h2&gt;

&lt;p&gt;Due to periodic crashing of Matlab (user error!), I added a script &lt;code&gt;tmp_setup_workspace.m&lt;/code&gt;, which is intended to restore all the needed variables for whatever task I'm currently working on.&lt;/p&gt;

&lt;h2&gt;Optimized likelihood for training&lt;/h2&gt;

&lt;p&gt;Spend some time thinking about how to design a version of the marginal likelihood computation that is optimized for training.&lt;/p&gt;

&lt;p&gt;Some time was spent deriving the analytical ML gradient w.r.t. the training parameters.  I believe the result won't be too complicated, but deriving it was taking up too much time.  We'll use a numerical gradient for now; will return to analytical gradient if the numerical proves to be lacking.&lt;/p&gt;

&lt;p&gt;To save some computation during training, I precomputed the component matrices for the prior and likelihood.  Constructing the prior and likelihood covariance matrices will involve scaling and summing the component parts.  These cached matrices consume about &lt;strong&gt;740 MB&lt;/strong&gt; with the current training set.&lt;/p&gt;

&lt;p&gt;However, one unavoidable bottleneck continues to be the cholesky decomposition, which isn't improved by this precomutation.  I was hoping there would be some matrix inversion tricks involving linear combinations of matrices, but my research on this came up empty.  My last hope is to run Cholesky on the GPU (Matlab makes this trivial), but if that doesn't speed things up, I'll resign myself to waiting 10 hours for training.&lt;/p&gt;

&lt;p&gt;Nevertheless, the component caching still gives a 1.5x end-to-end speedup on the no_purturb_kernel  case, compared the to direct implementation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;div&gt;After optimizing, the bottlenecks are split evenly three ways: (1) constructing Prior kernel; (2) constructing ML covariance matrix; and (3) evaluating ML pdf.   The latter two are dominated mostly by \(O(n^3)\) matrix multiplication.  Surprisingly, Cholesky is significant, but not dominating.&lt;/div&gt;


&lt;h2&gt;Summary:&lt;/h2&gt;

&lt;p&gt;Training-optimized Marginal Likelihood is finished; in &lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;.  Results confirmed against reference implementation: &lt;code&gt;train/tr_curves_ml_ref.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;finish Training framework:

&lt;ul&gt;
&lt;li&gt;setup local minimizer with &lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;determine if numerical gradient is okay&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sanity check: compare the ML's of each trained model on training data.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/21/work-log"/>
   <updated>2013-07-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/21/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;: The likelihood covariance for the &quot;virtual observations&quot; scales linearly with the 2D likelihood variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt;: see &lt;code&gt;experiments/exp_2013_07_21_likelihood_covariance.m&lt;/code&gt;.  Constructs a likelihood using noise_variance of one and then scaling precisions afterward.  Compares to directly-constructed likelihood precisions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Negligible difference&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;: Practice matches theory--scaling likelihood precisions is equivalent to constructing likelihood with the scaled precision.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: This conclusion means that we can construct the likelihood precisions exactly once during training, and simply scale them as we modify the likelihood precision.  It would make sense to always use 1.0 when computing precisions, and refactor all existing code to scale the matrix by the reciprocal of the noise variance before using it.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Build training framework

&lt;ul&gt;
&lt;li&gt;fast likelihood evalautor

&lt;ul&gt;
&lt;li&gt;custom function for evaluating ML without recomputing stuff&lt;/li&gt;
&lt;li&gt;cache the three prior component matrices (smooth, linear, and offset)&lt;/li&gt;
&lt;li&gt;cache unscaled likelihood precision.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;wrap in a lambda

&lt;ul&gt;
&lt;li&gt;scales and combines all components&lt;/li&gt;
&lt;li&gt;depending on motion model, use different expression for purturbation coefficient&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pass to quasi-newton minimizer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat for background curves&lt;/li&gt;
&lt;li&gt;Heuristic pruning using background-subtraction.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log"/>
   <updated>2013-07-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing: ground-truth-to-data-labels.  See file &lt;code&gt;train/label_from_ground_truth.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finished.&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data labels to likelihood means/covariances&lt;/li&gt;
&lt;li&gt;Likelihood means/covariances to marginal likelihood&lt;/li&gt;
&lt;li&gt;training framework&lt;/li&gt;
&lt;li&gt;training&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Visualizing &lt;code&gt;labels_from_ground_truth()&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Wrote &lt;code&gt;tmp_get_max_posterior.m&lt;/code&gt;, a temporary script that computes the posterior mean from a possibly-overconstrained prior.  In this case, the posterior covariance is singular, but the mean can still be obtained.  The math behind it &lt;a href=&quot;/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance/&quot;&gt;is available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Below is a plot using the test dataset and the ground truth labels:&lt;/p&gt;

&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/foL28SUn1JM?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;This shows that given a good labeling, a quality 3D reconstruction can be obtained using only the fragmented curves output by the curve-detector.&lt;/p&gt;

&lt;p&gt;Notice that the curves at the base have missing parts.  There isn't sufficient edge data here, but this could probably be fixed by connecting them to the base of the main stem and using the Branching Gaussian Process prior to enforce connectivity.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Maximum posterior with singular prior covariance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance"/>
   <updated>2013-07-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Consider the scenerio of Bayesian inference with a linear-Gaussian prior and likelihood.&lt;/p&gt;

&lt;p&gt;It is sometimes the case that our prior has a singular covariance matrix, indicating that our variables are embedded in a lower-dimensional hyberplane, i.e. some dimensions are redundant.  In our 3D curve triangulation application, this situation arises when  the same 3D point is observed in from multiple views.&lt;/p&gt;

&lt;p&gt;We can still find the maximum posterior arising from such a prior, as long as the likelihood is non-singular.   We can interpret this as multiple observations of the rendant dimensions, and if we are careful with our math, we can handle it the same way as the case with a non-singular prior.&lt;/p&gt;

&lt;p&gt;Given a likelihood \( \mathcal{N}(\mu_l, \Sigma_l) \) and prior \( \mathcal{N}(\mu_0, \Sigma_0) \), recall that the posterior is given by  \( \mathcal{N(\mu_P, \Sigma_P)} \), where&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
\Sigma_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} \\
\mu_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} (\Sigma_l^{-1} \mu_l + \Sigma_0^{-1} \mu_0)
\end{align}

\]
&lt;/div&gt;


&lt;p&gt;However, if \(\Sigma_0 \) is singular, we must avoid inverting it when computing \( \mu_P  \).  An equivalent equation for \(\mu_P\) that satisfies this condition is:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;




&lt;div&gt;This should be computable as long as the likelihood precision matrix \(\Sigma_l^{-1}\) has no infinite eigenvalues.  &lt;/div&gt;


&lt;p&gt;To see an example result, see &lt;a href=&quot;/ksimek/research/2013/07/19/work-log/&quot;&gt;today's Work Log&lt;/a&gt; entry.&lt;/p&gt;

&lt;h2&gt;Implementation Notes&lt;/h2&gt;

&lt;p&gt;I tried implementing a version of this that uses Cholesky decomposition and backsubstitution instead of generic matrix inversion.  I needed a symmetric matrix, so I modified the equation for \(\mu_P\):&lt;/p&gt;

&lt;div&gt; \[
\mu_P = \Sigma_0 (\Sigma_0 \Sigma_l^{-1} \Sigma_0 + \Sigma_0)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;This was not noticibly faster than general matrix inversion, because it involves two additional large matrix multiplications.&lt;/p&gt;

&lt;p&gt;Surprisingly, I &lt;em&gt;was&lt;/em&gt; able to get a significant speedup (~7x) be using backsubstitution (Matlab's '\' operator) &lt;em&gt;without&lt;/em&gt; Cholesky decomposition.  I always assumed that the lower-triangular form was what made backsubstitution so fast, but it is apparently also fast with dense matrices.  So we can avoid the extra expensive dens-matrix multiplications, and also avoid expensive matrix inversion.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log"/>
   <updated>2013-07-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bootcamp demo session&lt;/li&gt;
&lt;li&gt;Ground truth labeling of curve fragments.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Bootcamp demo session&lt;/h2&gt;

&lt;p&gt;Ran bootcamp demo session.  Final code available at&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;svn+ssh://v01/misc/svn/src/bootcamp/kjb_demos/3d_demo
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Ground truth labeling of curve fragments&lt;/h1&gt;

&lt;p&gt;Goal: use 2D ground truth to automatically label bottom-up curve fragments.&lt;/p&gt;

&lt;h2&gt;Overview&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Draw ground truth curves using curve index&lt;/li&gt;
&lt;li&gt;dilate slightly&lt;/li&gt;
&lt;li&gt;For each rendered data curve, gather all GT indices&lt;/li&gt;
&lt;li&gt;keep most-occurring GT index.&lt;/li&gt;
&lt;li&gt;add to assoc list for that curve&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Issues:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Remember how to read and process ground truth.&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; See &lt;code&gt;../ground_truth/read_gt2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Symlinked to &lt;code&gt;train/read_gt2.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Which dataset does our example data come from?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/ler_5_36_0.jpg&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Where is ground truth data?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/resized_50%/ground_truth_2d.gt2&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Blah Blah&lt;/h2&gt;

&lt;p&gt;Refactored some code into new function &lt;code&gt;build_curve_maps.m&lt;/code&gt;, which renders each of the curves in a cell-array into a map containing their indices.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored as Bezier curves.  Symlinked the all bezier-related code into data_association_2.  Relevant function is &lt;code&gt;bezier/polybez_to_polyline.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored in OpenGL-style coordinates (bottom-left origin).  Converting to top-right using &lt;code&gt;tools/flip_y.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Summary &lt;/h2&gt;

&lt;p&gt;Got through item 2 in &quot;Overview&quot; above.  Will finish next time, and start building training framework.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log 2</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2"/>
   <updated>2013-07-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Experimenting with &lt;code&gt;ou_perturb_kernel.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Was getting weird results, where the ML approached infinity as &lt;code&gt;noise_variance&lt;/code&gt; approached zero.&lt;/p&gt;

&lt;p&gt;Realized that the set of precision matrices needs to be updated &lt;strong&gt;every time the noise sigma changes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is an oversight that has tripped me up before.  Need to try to avoid it in the future.&lt;/p&gt;

&lt;p&gt;It is possible (likely?) that this transformation is as simple as multiplying the precision matrices by \(\sigma_n* / \sigma_n\).  This would avoid a semi-expensive Hessian calculation for each point, which could be a bottleneck during training.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;p&gt;See previous entry&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log"/>
   <updated>2013-07-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14866&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Note: This entry marks then end of significant work on the new likelihood function.  The SVN revision is noted in the meta-box to the right.&lt;/p&gt;

&lt;h2&gt;Optimizing &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Attempting to use sparsity to speed up &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First attempt&lt;/strong&gt;: try building only the relevant elements of the prior matrix, K.&lt;br/&gt;
&lt;strong&gt;Resut&lt;/strong&gt;: Gains in multiplication are lost in construction of K.  Insignificant speedup.  Rolling back.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second attempt&lt;/strong&gt;: convert block-diagonal matrix &lt;em&gt;S&lt;/em&gt; to a sparse matrix.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Significant gains in multiplication; tolerable losses when constructing &lt;em&gt;S&lt;/em&gt;.  ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third Attempt&lt;/strong&gt;: Convert the individual blocks &lt;em&gt;S_i&lt;/em&gt; to sparse, &lt;em&gt;then&lt;/em&gt; construct &lt;em&gt;S&lt;/em&gt; from them.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Further speedup of ~2x&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fourth Attempt&lt;/strong&gt;:  Change &lt;code&gt;(eye(size(K)) + S * K * S')&lt;/code&gt; to &lt;code&gt;(speye(size(K)) + S * K * S')&lt;/code&gt; ; i.e. changing &lt;code&gt;eye&lt;/code&gt; to &lt;code&gt;speye&lt;/code&gt;.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: moderate speedup, ~1.5x.&lt;/p&gt;

&lt;p&gt;I think we've squeezed all we can from this function.  It's now 10x faster than the naive method on a problem with 4000-dimensions.&lt;/p&gt;

&lt;h2&gt;Results: &lt;/h2&gt;

&lt;p&gt;Running &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6362.9 ms)
Old ML: 207.041742

Computing marginal likelihood (new way, legacy correspondence)...
Done. (7655.2 ms)
New ML (Legacy corr): 207.700616

Computing marginal likelihood (new way)...
Done. (6537.7 ms)
New ML (New corr): 793.585038

Computing marginal likelihood (new way, no MIL)...
Done. (6330.3 ms)
New ML (New corr, no MIL): 793.600835

Computing marginal likelihood (direct method)...
Done. (667.9 ms)  &amp;lt;-------------------  10x speedup over previous
New ML (direct method): 793.300407 &amp;lt;--  0.04% error!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note speedup and negligible error compared the previous method.&lt;/p&gt;

&lt;h2&gt;General Observations Regarding &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;I've noticed that &lt;code&gt;markov_order&lt;/code&gt; must be much larger than I expected to avoid approximation error.&lt;/p&gt;

&lt;p&gt;Recall in &lt;code&gt;curve_ml2.m&lt;/code&gt;, we use the Markov assumption to break-down the prior covariance, and then combine them with the likelihood cliques.  In that case, we could use a Markov order between 2 and 5 without significant approximation error.&lt;/p&gt;

&lt;p&gt;In &lt;code&gt;curve_ml5.m&lt;/code&gt;, using a Markov order less than 100 results in unacceptable error.&lt;/p&gt;

&lt;p&gt;There are two reasons for this.  First, we're decomposing the marginal likelihood Gaussian, not the prior.  The prior is explicitly Markovian, whereas the marginal likelihood is not.  The ML Guassian adds extra uncertainty to every point, which means we need to consider more nearby points to avoid erroneous conclusions.&lt;/p&gt;

&lt;p&gt;Second, because the new approach creates a distinct index set for each view, indices are often repeated (multiple views of the same point) and considering them doesn't tell you anything about what direction the curve is heading.&lt;/p&gt;

&lt;p&gt;The first point will likely be mitigated when we start using the new curve model, which will allow the likelihood variance to decrease dramatically.  However, the markov assumption in the prior is less likely to hold under these models, so some experimentation will be needed.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The code for the new likelihood, &lt;code&gt;curve_ml5.m&lt;/code&gt; is now stable.  It is fast and accurate in its current implementation, assuming a reasonable value for &lt;code&gt;markov_order&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Medium-term goal: calibration/training for all curve models.&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ground-truth labelling of bottom-curve curve data&lt;/li&gt;
&lt;li&gt;set-up method for evaluating ground-truth using a given parameter-set, without re-computing indices each time.&lt;/li&gt;
&lt;li&gt;Run multi-dimensional optimization to fit for all models&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log"/>
   <updated>2013-07-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing new direct method for marginal likelihood.&lt;/h2&gt;

&lt;p&gt;Implemented in &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After initial attempt, the new method produced inconsistent results.&lt;/p&gt;

&lt;p&gt;Re-derived the result two more ways and the math comes out the same.  The theory looks right.&lt;/p&gt;

&lt;p&gt;Finally found the bug -- was computing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;instead of&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = 2 * log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the Cholesky decomposition is the square-root of the matrix Sigma, I was forgetting to double it's determinant to get the determinant of Sigma.&lt;/p&gt;

&lt;p&gt;Now getting the correct results; now time to make it fast.&lt;/p&gt;

&lt;h2&gt;Optimizing direct method for ML&lt;/h2&gt;

&lt;p&gt;Need to get O(n) runtime instead of O(n&lt;sup&gt;3).&lt;/sup&gt;  Will use existing code for markov-decomposed PDF evaluation.&lt;/p&gt;

&lt;p&gt;Bottleneck is 100% the Cholesky decomposition.  The direct method doesn't remove redundant dimensions, so it's slower than &lt;code&gt;curve_ml4.m&lt;/code&gt;, which does.  Recall that this fact makes &lt;code&gt;curvE_ml4.m&lt;/code&gt; not general enough to use the new foreground models with.&lt;/p&gt;

&lt;p&gt;...........&lt;/p&gt;

&lt;p&gt;After some investagation, the previous statement appears to be untrue.  The bottlenect is dense matrix multiplication, which is fixed by using sparse matrices.&lt;/p&gt;

&lt;p&gt;Now, cholesky &lt;strong&gt;is&lt;/strong&gt; the bottlenect, but not as huge as before.  The markov decompose method only gives a ~30% speedup at best, while introducing ~2.5% error and significant extra complexity. Is it worth it?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Direct Evaluation of the Marginal Likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood"/>
   <updated>2013-07-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Previously, we've always used hacks to evaluate the marginal likelihood, because evaluating it directly was apparently impossible because of an infinite normalization constant.  I've discovered that this isn't actually true; the infinite normalization constant arose due to our approximation of the likelihood.  The trick in correctly computing the ML is to replace the usual normalization constant with a corrected one.&lt;/p&gt;

&lt;p&gt;Our old method required knowledge of the maximum posterior solution, which was costly to compute.  The new approach is straightforward to describe, more accurate than our previous method, and can be evaluated in linear time by exploiting its Markovian structure.&lt;/p&gt;

&lt;p&gt;Below is a rough-draft writeup of my derivation of the marginal likelihood function.&lt;/p&gt;

&lt;h2&gt;Direct Evaluation of the Marginal likelihood&lt;/h2&gt;

&lt;p&gt;The 3D marginal likelihood function arises as the sum of a 3D curve process and a 3D perturbation process.  Both are gaussian (the second is approximate), this the result is a Gaussian function, the convolution of two Gaussians.  However, the perturbation process has infinite variance in the direction of backprojection, which arises from the fact that perturbation actually occurs in 2D, we are just backprojecting it to 3D for tractibility.  In other words, the ML isn't a distribution in 3D, it's a distribution in 2D, and we need to determine the appropriate normalization constant for the 3D function to it generates 2D ML densities.&lt;/p&gt;

&lt;p&gt;The likelihood function is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y | x) &amp;= \prod p(y_i | x_i) \\
         &amp;= \prod \mathcal{N}(y_i; \rho_I(x_i), \sigma_n^2 I) \\
         &amp;\approx \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \mathcal{G}(Y_i; x_i, S_i^{-1}) \\
         &amp;= \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \exp\{(Y_i - x_i)^\top S_i (Y_i - x_i)\} \\
         &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}^{2n}} \exp\{(Y - x)^\top S (Y - x)\} \\
         &amp;= \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\}
\end{align}
\]&lt;/div&gt;


&lt;p&gt;Where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\rho_i(x)\) is the projection of x into the I-th view,&lt;/li&gt;
&lt;li&gt;\(Y_i\) is the estimated backprojected position of observation \(y_i\),&lt;/li&gt;
&lt;li&gt;\(S_i\) is the curvature of the 3d likelihood function w.r.t. \(x_i\) evaluated at $Y_i$,&lt;/li&gt;
&lt;li&gt;\(Y\), \(S\) and \(x\) are the concatenation of \(Y_i\), and \(x\)&lt;/li&gt;
&lt;li&gt;\(S\) is the block-diagonal matrix of \(S_i\)'s&lt;/li&gt;
&lt;li&gt;\(\mathcal{G}\) is a Gaussian function (an unnormalized normal distribution).&lt;/li&gt;
&lt;li&gt;\(Z_l\) is a normalization constant.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We have transformed the likelihood into a log-linear function of x, by rewriting the PDF in terms of only 3D entities.  Note that \(S_i\) has zero curvature in the backprojection direction, resulting in infinite variance in the Gaussian function.   Also note that the normalization constant isn't the standard one for a 3D gaussian distribution, because this isn't a distribution over x.  The normalization constant is chosen so the approximate likelihood in 3D agrees with the exact 2D likelihood when perturbation is zero (i.e. when \(Y_i\) lies anywhere on the backprojection line).&lt;/p&gt;

&lt;p&gt;This approximation ignores the nonlinearity of projection; the likelihood function as a function of \(x_i\) would actually look like a cone whose axis-perpendicular slices are Gaussians, whereas our function is cylinder-shaped.  In practice, the approximation error has minimal effect on the marginal likelihood computation, assuming $Y_i$ is a good estimate of the posterior depth, and the posterior is reasonably peaked.  (A graphic to illustrate would be good here).&lt;/p&gt;

&lt;p&gt;The prior is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(x) &amp;= \mathcal{N}(x; \mathbf{0}, K) \\
     &amp;= \frac{1}{Z_p} \exp\{x^\top K^{-1} x\}
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;where \(K\) is the covariance matrix arising from the Gaussian process kernel, and Z_p is the standard Gaussian normalization constant.&lt;/p&gt;

&lt;p&gt;The marginal likelihood is defined as&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \int p(y|x) p(x) dx \\
     &amp;= \int \left ( \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\} \right ) \left ( \frac{1}{Z} \exp\{x^\top K^{-1} x\} \right ) \\
     &amp;= \frac{1}{Z_l Z_p}\int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;The expression within the integral is the convolution of two unnormalized zero-mean Gaussians, so the integral is a zero-mean Gaussian whose covariance is the sum of the inputs' covariances.  If the input Gaussians &lt;strong&gt;were&lt;/strong&gt; properly normalized, the normalization constant would be \(1/Z\);  since they are not, the normalization constant is \((Z_1 Z_2 / Z)\), where \(1/Z_1\) and \(1/Z_2\) are the would-be normalization constants for the first and second Gaussian, respectively.  Note that the second Gaussian's normalization constant is \(1/Z_p\), so this results in the cancellation we see below&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \frac{1}{Z_l Z_p} \int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     &amp;= \frac{1}{Z_l Z_p} \left ( \frac{Z_{l,3d} Z_p}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \} \right ) \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \} \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z}  \exp\{x^\top S(S + S K S)^{-1} S x \}
     \end{align}
\]&lt;/div&gt;




&lt;div&gt;
    Where \( Z_{l,3d} \) is the would-be 3D normalization constant for our likelihood Gaussian.  The last line avoids inverting the singular matrix \(S\) when it's singular.  Since \(S\) is rank-deficient, the normalization constants based on \(S^{-1}\) will be infinite (i.e. \(Z_{l,3d}\) and \(Z\)).  However, the terms involving determinants of \(S^{-1}\) should cancel in the ratio, resulting in a gaussian with infinite variance, but non-infinite normalization constant. **(Need to show this mathematically next time)** 
    &lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log"/>
   <updated>2013-07-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Markov-decomposed ML&lt;/h2&gt;

&lt;p&gt;The original plan was to markov-decompose the prior and posterior, but soon remembered that I also need to compute the mean of the posterior, which isn't as straighforward to compute.&lt;/p&gt;

&lt;p&gt;After some deliberation and whiteboarding, I decided to implement Bishop's forward/backward algorithm to simultaneously marginalize and compute the maximum.  Will test this against the clique-tree implementation to ensure correctness.&lt;/p&gt;

&lt;p&gt;........&lt;/p&gt;

&lt;p&gt;During testing I realized a fundamental problem with the current likelihood: there is no obvious way to consistently handle the &quot;redundant&quot; dimensions that arise from duplicated indices.  In some kernels, these are handled naturally, namely the kernels that treat different views as different indices.  The old kernel, however, does not, and the duplicated indices result in degenerate posterior distributions unless handled using hacks.&lt;/p&gt;

&lt;p&gt;We could handle this on a kernel-by-kernel basis, but it would be unmaintainable, and could hinder further research into new kernels.  It also isn't clear that the hacks I have in mind would actually give correct results.&lt;/p&gt;

&lt;p&gt;This problem is inherent to the &quot;candidates estimator&quot; of the marginal likelihood, because it involves a ratio of the posterior and prior, both of which are degenerate in these cases.&lt;/p&gt;

&lt;p&gt;The actual marginal likelihood function has no such degeneracies, because each observation is independent, given the underlying curve.  However, until recently it was unclear how to evaluate the marginal likelihood using the approximated likelihood function.  The approximate likelihood has a rank-deficient precision matrix, and its normalization constant is non-standard due to the transformation from 2D to 3D.  However, I think I've developed a way to evaluate it, which I describe in the next article.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log"/>
   <updated>2013-07-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Cleanup and speedup of curve_ml.&lt;/p&gt;

&lt;h2&gt;Task 1: Version without matrix-inversion lemma&lt;/h2&gt;

&lt;p&gt;Making a simpler version that doesn't exploit the matrix inversion lemma (MIL).&lt;/p&gt;

&lt;p&gt;The benefits of the MIL are likely mitigated by markov-decomposition (next task), and MIL
complicates the code and API design.&lt;/p&gt;

&lt;p&gt;Results (small test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (108.2 ms)
Old ML: 62.183969
Computing marginal likelihood (new way, legacy correspondence)...
Done. (120.8 ms)
New ML (Legacy corr): 63.908784
Computing marginal likelihood (new way)...
Done. (88.9 ms)
New ML (New corr): 72.304718           &amp;lt;----------  OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (97.3 ms)
New ML (New corr, no MIL): 72.304666   &amp;lt;----------  NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference is about 0.00007% -- seems good.  Slightly slower, possibly just noise.&lt;/p&gt;

&lt;p&gt;Results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6447.3 ms)
Old ML: 207.324848
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7665.1 ms)
New ML (Legacy corr): 209.199676
Computing marginal likelihood (new way)...
Done. (6205.6 ms)
New ML (New corr): 457529.406731  &amp;lt;---------- ?????
Computing marginal likelihood (new way, no MIL)...
Done. (5946.7 ms)
New ML (New corr, no MIL): 778723.327102  &amp;lt;---------- ?????
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both versions give CRAZY high results; significantly different from the &lt;a href=&quot;/ksimek/research/2013/07/05/work-log/&quot;&gt;same test on Friday&lt;/a&gt;.  Need to investigate...&lt;/p&gt;

&lt;h2&gt;Investigating new results&lt;/h2&gt;

&lt;p&gt;Observations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All results differ from friday, including old ML (slightly)&lt;/li&gt;
&lt;li&gt;New ML using legacy correspondence is still reasonable.&lt;/li&gt;
&lt;li&gt;Both crazy results are using new correspondence.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Look at pieces (log posterior, likelihood, prior)&lt;/li&gt;
&lt;li&gt;Inspect new correspondence&lt;/li&gt;
&lt;li&gt;Think about what changed since Friday?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:10:19 PM&lt;/p&gt;

&lt;p&gt;Ran old test, &lt;code&gt;test_ml_end_to_end.m&lt;/code&gt; and compared to old results in new test, &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt;.  Old test still gives old results, so I'll compare the two tests to determine what has changed.&lt;/p&gt;

&lt;p&gt;12:12:48 PM&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;mix&lt;/code&gt; parameter was set to 0.5 instead of 0.0.  I had forgotten I changed it at the end if Friday.&lt;/p&gt;

&lt;p&gt;Now getting &quot;good&quot; results again.  It raises a question that needs to be answered: why is ML sooo sensitive to evalaution position when using new correspondence?&lt;/p&gt;

&lt;h2&gt;Resuming&lt;/h2&gt;

&lt;p&gt;New results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6009.5 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7286.3 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6146.6 ms)
New ML (New corr): 793.585038             &amp;lt;---------- OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (6008.3 ms)
New ML (New corr, no MIL): 793.600835     &amp;lt;---------- NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.002% error, slightly faster.  Good.&lt;/p&gt;

&lt;h2&gt;Investigating Anomaly in new ML &lt;/h2&gt;

&lt;p&gt;So why is new ML so sensitive to where it is evaluated?&lt;/p&gt;

&lt;p&gt;Note that it's only extreme when using new correspondence.  Old correspondence is okay.&lt;/p&gt;

&lt;p&gt;Inspect difference between max likelihood and max posterior.  Maybe it's just more extreme than with old correspondence.&lt;/p&gt;

&lt;p&gt;12:20:59 PM&lt;/p&gt;

&lt;p&gt;Idea: bug in non-0.0 case?  Nope.&lt;/p&gt;

&lt;p&gt;Tried mixes: 0.0, 0.01, 0.1. Steady and dramatic increase in ML for new correspondence.  Old correspondence is nearly constant.  What is different between these correspodnences (other than the correspondences themselves).&lt;/p&gt;

&lt;p&gt;12:57:08 PM&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;: The max likelihood in the new correspondences is waaaaay into the tails of the prior.&lt;/p&gt;

&lt;p&gt;Since no real triangulation is done to ensure agreement between views, the maximum likelihood triangulation is very rough  (infinite likelihood variance allows this to happen without penalty).  This places the curve far into the tails of the prior, where evaluation is highly unstable.  Both the prior and likelihood are then extremely low (-1e-8 in log space), which should cancel, but don't due to numerical instability.  Hence, marginal likelihoods with huge magnitude.&lt;/p&gt;

&lt;p&gt;This is great news, since it means everything is working mostly as expected.  The numerical instability issue is easilly solved by always evaluating at the posterior, not the maximum likelihood.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The version of the new ML that doesn't use the matrix inversion lemma is accurate, so we can proceed to the markov-decomposed version next.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log"/>
   <updated>2013-07-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still trying to troubleshoot the difference between old and new likelihood implementations.&lt;/p&gt;

&lt;p&gt;Reviewing what is known&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prior is same&lt;/li&gt;
&lt;li&gt;Likelihood is same&lt;/li&gt;
&lt;li&gt;Posterior evaluates same in two different ways.&lt;/li&gt;
&lt;li&gt;mean curve is different&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Try using mean curve from old alg. in new alg.&lt;/p&gt;

&lt;p&gt;10:13:35 AM&lt;/p&gt;

&lt;p&gt;SOLVED.&lt;/p&gt;

&lt;p&gt;Yesterday I noticed that setting mix to 1.0 give decent results, but 0.0 was terrible.  Now I know why: 0.0 is a special case, in which a simplified calculation is used.  There was a bug in that special case, where the normalization constant didn't take into account the redundant dimensions that we eliminated.&lt;/p&gt;

&lt;h2&gt;ML Test&lt;/h2&gt;

&lt;p&gt;11:02:29 AM&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ml_test_end_to_end_2.m&lt;/code&gt; is now running&lt;/p&gt;

&lt;p&gt;Running on subset of correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...  
Done. (96.9 ms)  
Old ML: 62.176782

Computing marginal likelihood (new way, legacy correspondence)...  
Done. (112.3 ms)  
New ML (Legacy corr): 63.859940

Computing marginal likelihood (new way)...  
Done. (83.5 ms)  
New ML (New corr): 71.964026
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised the magnitude of difference between &quot;Old ML&quot; and &quot;New ML (Legacy corr)&quot;.  I guess the posterior variance is larger in this case, which means the posterior means can differ more.&lt;/p&gt;

&lt;p&gt;Running on full correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...
Done. (6400.1 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8328.4 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6349.2 ms)
New ML (New corr): 793.585038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that &quot;New ML (New Corr)&quot; is dramatiacally higher than the old and legacy ML's.  I suppose I should have been expected this, since this entire approach &lt;a href=&quot;/ksimek/research/2013/06/28/work-summary/&quot;&gt;was motivated by the terrible correspondences arising from the legacy correspondence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It's nice to see the ML for good correspondences getting even better.  Hopefully we'll see even more improvement as we try new curve models.&lt;/p&gt;

&lt;p&gt;Taking a break for lunch...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Next steps:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;fast ml_curve3

&lt;ul&gt;
&lt;li&gt;linear eval&lt;/li&gt;
&lt;li&gt;try without matrix inversion lemma&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;try new models (check that they're nonsingular; don't crash)&lt;/li&gt;
&lt;li&gt;set up training framework&lt;/li&gt;
&lt;li&gt;consider version without inversion lemma&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log"/>
   <updated>2013-07-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Continuing debugging of new likelihood&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Test:&lt;/strong&gt; &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;07:11:52 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Trouble with NaN error in &lt;code&gt;correspondence/corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;08:22:43 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Narrowed it down to gap-handling code-branch.&lt;/p&gt;

&lt;p&gt;08:53:34 AM&lt;/p&gt;

&lt;p&gt;Fixed. Arose from a nasty indexing scheme in the original &lt;code&gt;clean_correspondence.m&lt;/code&gt;, which I didn't handle well when adapting for &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;09:29:35 AM&lt;/p&gt;

&lt;p&gt;Nonsingular prior covariance matrix causing crash.  Apparently caused by same index appearing multiple times.&lt;/p&gt;

&lt;p&gt;In the past, we always merged such points.&lt;/p&gt;

&lt;p&gt;The new idea is different views of the same point would make the posterior covariance singular.  Either this isn't true, or there's a bug...&lt;/p&gt;

&lt;p&gt;Debugging this will take some serious coding and math brainpower and I fell debugging fatigue coming on.  Taking a brief break to refresh, then will tackle...&lt;/p&gt;

&lt;p&gt;11:56:21 AM&lt;/p&gt;

&lt;p&gt;Using a simple toy example, I (partially) confirmed my intuition that a degenerate prior matrix is okay, as long as its nullspace is spanned by the likelihood matrix.&lt;/p&gt;

&lt;p&gt;In the context of this bug, it means that two points sharing the same index value is fine as long as they arose from different views (assuming non-degenerate camera configuration, which is true in this case).&lt;/p&gt;

&lt;p&gt;Possible causes&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Numerical instability -- near-singular matrix.&lt;/li&gt;
&lt;li&gt;Error in Linear Algebra logic (seems unlikely, same code worked in old likelihood)&lt;/li&gt;
&lt;li&gt;Some bug earlier in the pipeline, causing invalid matrices here.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I'm guessing its 3; if not that, then maybe 2.&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;inspect the shared indices -- same view?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:48:49 PM&lt;/p&gt;

&lt;p&gt;Reversing my previous stance:  Degenerate prior with shared index will always result in a degenerate posterior.  The prior has two variables that are 100% correlated, and the posterior will also be 100% correlated.  This implies a degenerate covariance matrix.&lt;/p&gt;

&lt;p&gt;In such a case, the posterior's implied dimensionality is lower than it's apparent dimensionality.  The prior and posterior will be degenerate in the same way; this symmetry is aesthetically appealing, because the Candidates estimator for the marginal likelihood involves their ratio.  This suggests that eliminating the redundant dimensions for both pdfs is a sensible thing to do.&lt;/p&gt;

&lt;p&gt;How to resolve this?  Possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detect redundant indices early, eliminate them in the prior and posterior.  USe bookkeeping to remember which observations refer to the same model point.&lt;/li&gt;
&lt;li&gt;Handle reduntancies later, after degenerate posterior is formed, before evaluating posterior or prior .  The mean will automatically contain redundancies, which will avoid bookkeeping when comparing the mean to the data points (when &lt;code&gt;mix&lt;/code&gt; is != 0).&lt;/li&gt;
&lt;li&gt;Try to determine how identical indices arose in the first place.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Decided on 2 for now.  In retrospect, approach 1 may have had slightly less code and faster.&lt;/p&gt;

&lt;p&gt;01:45:00 PM&lt;/p&gt;

&lt;p&gt;ML is now running on legacy correspondence.  Results are significantly higher than the legacy ML algorithm (~ 2.25 times).  Not yet sure why yet.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6665.8 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8853.1 ms)
New ML (Legacy corr): 439.273126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Noticible slow-down, probably because of extra indices arising from not merging duplicate observations.&lt;/p&gt;

&lt;p&gt;possible causes of discrepancy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In old code, re-triangulation used simple averaging, not accounting for differing precisions of the points&lt;/li&gt;
&lt;li&gt;index sets differ between old and new code.  (update: checked, they are same)&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Plotting means for both to view differences...&lt;/p&gt;

&lt;p&gt;Investigation into ML discrepancy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Posterior means are similar, but obviously differ.&lt;/li&gt;
&lt;li&gt;both means are very non-smooth (need to re-address the smoothness issue)&lt;/li&gt;
&lt;li&gt;new mean has signficantly more points (~7%, 80 pts).  Could this alone affect ML?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;almost all difference is in posterior&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Maybe posterior kernel is different?  but prior is same
It's just the data covariance thats different.
but why??  There's more of them.&lt;/p&gt;

&lt;p&gt;New ml changes significantly when evaluation point changes.  This implies that the posterior curvature differs from the curvature of the joint distribution, which shouldn't happen.  A small amount of difference can arise naturally due to the approximation of the likelihood, but this is too large to be explained in that way.  Most noticibly, the new results look very similar to the old ones &lt;strong&gt;when evaluating at the likelihood mean&lt;/strong&gt;.  The largest difference occurs when evaluating at the posterior mean.  Not clear yet what conclusions to draw from this, or if this is just a coincidence.&lt;/p&gt;

&lt;p&gt;Maybe likelihood curvatures are differing.&lt;/p&gt;

&lt;p&gt;07:35:28 PM&lt;/p&gt;

&lt;p&gt;Was able to get identical likelihood value using matrix-form as I did with the original 2D geometric error form.  Seems like the precision matrices are good.&lt;/p&gt;

&lt;p&gt;Priors appear good too, since it evaluates to the same value as the reference implementation.&lt;/p&gt;

&lt;h2&gt;Reference implementation for new ML&lt;/h2&gt;

&lt;p&gt;Idea: direct implementation for quadform, but with hacked nomralization constant&lt;/p&gt;

&lt;h2&gt;In progress&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnosing errors in new likelihood &lt;code&gt;curve_ml_3.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;getting &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt; running.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log"/>
   <updated>2013-07-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Implementing covariance functions&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood&lt;/li&gt;
&lt;li&gt;Implement fast generalized marginal likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implement generalized marginal likelihood&lt;/h2&gt;

&lt;p&gt;Rewriting &lt;code&gt;curve_ml3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Based on &lt;code&gt;curve_ml.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Kernel now takes two sets of indices, curve-index and view-index.&lt;/p&gt;

&lt;p&gt;Kernel is now passed-in as a lambda.&lt;/p&gt;

&lt;p&gt;Need to decide whether to use matrix inversion lemma for global offset and linear prior.  Currently being used;  not using would be simpler; try both and compare results.&lt;/p&gt;

&lt;h2&gt;Testing new marginal likelihood&lt;/h2&gt;

&lt;p&gt;Creating new version of &lt;code&gt;test/test_ml_end_to_end.m&lt;/code&gt;, named &lt;code&gt;test/test_ml_end_to_end2.m&lt;/code&gt;.  This version uses the new likelihood format, and compares against the old.&lt;/p&gt;

&lt;h2&gt;New curve_likelihood&lt;/h2&gt;

&lt;p&gt;Need to write new version of &lt;code&gt;curve_likelihood.m&lt;/code&gt; to handle new likelihood format.&lt;/p&gt;

&lt;p&gt;It's going to be difficult to compare to older version, since we've changed the correspondence method...&lt;/p&gt;

&lt;p&gt;I'll have to add a flag that simulates the old correspondence method, but stores it in the new likelihood format...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;01:33:52 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I decided to write a separate function for the old correspondence method, called &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.  Moved &lt;code&gt;clean_correspondence3.m&lt;/code&gt; to &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The legacy function isn't exactly identical to the old function.  In particular, the index values might be slightly different, since it depends on the old function's global mean curve, which we don't compute in the new function.  If it's important, we can call the old function, and then call the new one.  This is a debugging function, so speed doesn't matter. For the moment, we'll accept the small difference, and use this function simply to confirm that we're in the ballpark.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;03:06:24 PM&lt;/em&gt;
Test is implemented.  Troubleshooting syntax errors...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;04:11:17 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Elusive NaN error in &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log"/>
   <updated>2013-07-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing marginal likelihood &lt;/h2&gt;

&lt;p&gt;Working on generalizing marginal likelihood for all three foreground models.&lt;/p&gt;

&lt;p&gt;Considered briefly how to use the matrix inversion lemma to improve numerical stability of all models.  Decided it could be difficult to build in a general way;  will proceed with the direct method for now, until there is evidence of numerical instability.&lt;/p&gt;

&lt;p&gt;Generalized how covariance is generated.  Wrote several new functions for generating differenct covariance matrices.  Will need to rework some due to the developments I describe later.&lt;/p&gt;

&lt;h2&gt;Theoretical Developments&lt;/h2&gt;

&lt;p&gt;Thought extensively about perturbation models I described yesterday.  I realized there is a serious problem with modeling motion using brownian motion -- the prior variance grows without bound as time approaches infinity.  Thus, the marginal prior for the curve in view 36 has much greater prior variance than the curve one in view 1.  This doesn't make sense -- ideally, they should all have the same marginal prior.&lt;/p&gt;

&lt;p&gt;This led to a reading session in Williams and Rasmussen, which led me to develop two new motion models, which I describe extensively in today's accompanying post.&lt;/p&gt;

&lt;h2&gt;Tomorrow&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish implementing new covariance functions.&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood.&lt;/li&gt;
&lt;li&gt;implement &lt;em&gt;fast&lt;/em&gt; generalized marginal likelihood (approximation)

&lt;ul&gt;
&lt;li&gt;Test against existing ML for old models.&lt;/li&gt;
&lt;li&gt;Test with new models.  Are the ML's higher? (will probably need training).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build training data set from hand-traced ground-truth.&lt;/li&gt;
&lt;li&gt;Write training code.  Train all three candidate models.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Rethinking covariance functions</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions"/>
   <updated>2013-07-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions</id>
   <content type="html">&lt;p&gt;Noticed a problem with the cubic spline perturbation model.  As the view number increases, the variance marginal prior of the curve in that view approaches infinity.  This means that the &quot;smoothness&quot; prior is ignored more and more in later views.&lt;/p&gt;

&lt;h2&gt;Squared Exponential perturbation model&lt;/h2&gt;

&lt;p&gt;A better model would have the same marginal prior for all views, but with correlation between nearby views.  This allows curves to revert to the prior as temporally correlated evidence becomes less informative.  I believe the squared exponential covariance function has this property.  Instead of adding the sq-exp covariance to the existing covariance function, we should multiply it, so self-covariance is unchanged, but pairwise correlation is non-zero.&lt;/p&gt;

&lt;p&gt;An added benefit of this is that it only has one tunable parameter.&lt;/p&gt;

&lt;p&gt;It should be easy to incorporate into our test-bed, along with the other two newly proposed foreground models.&lt;/p&gt;

&lt;h2&gt;Ornstein Uhlenbeck perturbation model&lt;/h2&gt;

&lt;p&gt;Digging deeper into Williams and Rasmussen, I found precisely the GP I was looking for a few days ago:  the OrnsteinUhlenbeck (OU) process.  This   process describes brownian motion under the influence of a regularizing force that pulls toward the mean.&lt;/p&gt;

&lt;p&gt;In other words, I can model correlation between views without affecting the marginal prior of the curve any particular view.  This is also accomplished by the squared-exponential model, but the OU process is probably more realistic, because the plant's motion looks non-smooth.&lt;/p&gt;

&lt;h2&gt;Modelling the mean or not?&lt;/h2&gt;

&lt;p&gt;I'm struggling with whether or not to explicitly model a &quot;mean&quot; curve with the SE and the OU processes.&lt;/p&gt;

&lt;p&gt;If I did model the mean, each curve's covariance function would be the sum of a standard covariance plus a perturbation covariance.  The standard covariance models the &quot;mean&quot; curve, and it would be the same for all views (100% correlated).  The perturbation covariance would be partially correlated between the views, using the SE or the OU process.  The bayes net has the following structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        O   &amp;lt;-- mean curve
     / /\ \
   /  /  \  \
 /   |    |   \
O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The alternative is to model the per-view curves directly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Under this model, each view's curve has a cubic-spline marginal distribution, and the SE or UO process controls the correlation between them.&lt;/p&gt;

&lt;p&gt;What isn't clear is whether the perturbations in the latter model will be independent between points.  We need to model within-view perturbations as correlated, otherwise the marginal likelihood will drop too low.  There is no explicit description of how perturbations of adjacent points correlate.&lt;/p&gt;

&lt;h2&gt;What follows is me thinking out-loud...&lt;/h2&gt;

&lt;p&gt;Intuitively, points nearby in curve-space (have similar \(\tau\)'s) can never be more correlated than they are when they appear in the same view.  Separating them in time (view) will decreases their correlation, until finally, there is no correlation; the only remaining correlation is between points in the same view.  The SE kernel modulates that correlation.  The SE kernel doesn't explicitly model correlation between perturbations, but this doesn't mean the correlation doesn't exist -- it is implicit in the original kernel.&lt;/p&gt;

&lt;p&gt;There is an analogy here with the classic krigging example.  In there models, the squared exponential over a 2D landscape (i.e. joint function over \(\mathbb{R}\)&lt;sup&gt;2&lt;/sup&gt; space) is equal to the product of 1D squared exponentials (i.e. two functions over \(\mathbb{R}\)&lt;sup&gt;1&lt;/sup&gt; space).  In other words, the 2D kernel is constructed by the product of 1D kernels.  There is no worry that the delta between nearby &quot;slices&quot; of the surface are uncorrelated, because the marginal covariance within that slice will enforce that smoothness.&lt;/p&gt;

&lt;p&gt;In our case, we also have a product of 1D covariance functions constructing a 2D covariance function.  The difference is that one of the kernels (the curve-space kernel) is a cubic-spline process, while the other (the time-dimension kernel) is squared exponential (or Ornstein-Uhlenbeck).  Despite this difference, my intuition is that the conclusions are the same - the deltas will be smooth (i.e. correlated), because they will be the difference between two smooth curves.&lt;/p&gt;

&lt;p&gt;Considering the marginals in both directions further illustrates why this works.  Obviously, a slice in the curve-direction will always look curve-like, since the marginals are all the same cubic-covariance GP prior.  In the time-direction, a single point will follow a GP or OU process, with initial conditions dictated by the first curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;...BUT...&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;...in the absence of data, each individual point will drift over time to the prior mean, I.e. &lt;strong&gt;zero&lt;/strong&gt;.  In other words, the 3D curve observed in view infinity gains no information from curve observed at time zero.&lt;/p&gt;

&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; realistic.  In reality, these points shouldn't drift far from some &quot;sensible&quot; center curve, implying that a mean curve exists.&lt;/p&gt;

&lt;p&gt;The time-slice of any particular point &lt;em&gt;should&lt;/em&gt; look like either a OU or SE process, but the mean needs to be explicit.  This implies an additive model, with distinct models for the unobserved &quot;mean curve&quot; and the deviations from it, which are summed to get the 3D curve seen in each view.&lt;/p&gt;

&lt;h2&gt;Modeling perturbation&lt;/h2&gt;

&lt;p&gt;So if we must model perturbation, how should we model it?&lt;/p&gt;

&lt;p&gt;One thing is clear: the marginal prior for the curve in any view &lt;strong&gt;must&lt;/strong&gt; still be a cubic-spline process.&lt;/p&gt;

&lt;p&gt;This implies that the perturbation must be a cubic-spline process, too.&lt;/p&gt;

&lt;p&gt;However, the variances for each component (offset, linear, and cubic) are likely to be different for the perturbation model, compared to the mean-curve model.  Most importantly, the magnitude of the offset variance must be large in the mean-curve model but will be &lt;em&gt;much&lt;/em&gt; lower (relative to linear and cubic variance) in the perturbation model.&lt;/p&gt;

&lt;p&gt;I was hoping to avoid adding four extra parameters to our model (perturbation offset, linear and cubic variance, plus perturbation scale-length).  The mean-free model only adds one parameter - scale-length.  I guess this is the price we pay for a better model -- and for higher marginal likelihoods.  Ideally, the occam's razor quality of the marginal likelihood will allow us to avoid overfitting this many parameters (7 total).  Any parameters that are superfluous should become near-zero during training.&lt;/p&gt;

&lt;p&gt;...I hope.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log"/>
   <updated>2013-07-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Background:  This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;p&gt;Also developed a new approach to evaluate marginal likelhoods thats much simpler, but need to confirm that it matches reference implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implement both new foreground models, plus old one in new &quot;kernel function&quot; way.&lt;/li&gt;
&lt;li&gt;Implement new evaluation method and test against reference.&lt;/li&gt;
&lt;li&gt;Learn parameters for all three models (needs some ground truthing).&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implementing new models&lt;/h2&gt;

&lt;p&gt;Worked on general framework for evaluating ML under general new model framework.&lt;/p&gt;

&lt;p&gt;After some struggling with covariance degeneracies inherent in backprojection, I realized that the 3D marginal likelihood is naturally degenerate, because it isn't the true likelihood function (which is in 2D).&lt;/p&gt;

&lt;p&gt;I'm kicking myself for not remembering that I struggled with this exact problem 5 months ago.  At that time, I realized the better approach is to use Candidate's estimator, which is the ratio of the unnormalized posterior to the normalized posterior.  The unnormalized posterior comes from the prior and 2D likelIhood;  the normalized posterior is obtainable from the 3D likelihood and the prior.&lt;/p&gt;

&lt;p&gt;This was already implemented in &lt;code&gt;curve_ml.m&lt;/code&gt;, but was  O(n&lt;sup&gt;3&lt;/sup&gt; ), so was all but abandoned in favor of the junction-tree method in &lt;code&gt;curve_ml2.m&lt;/code&gt;, which is O(n).&lt;/p&gt;

&lt;p&gt;However, it's recently become clear that the candidate's estimator should be evaluated in \(O(n)\)  by exploiting the Markovian nature of the covariance matrix.&lt;/p&gt;

&lt;p&gt;It should be easy to try both, by simply swapping out covariance matrices with ones arising from the new covariance functions.  Tomorrow...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Foreground Curve Models as Gaussian Process Covariance Function</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models"/>
   <updated>2013-07-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Background&lt;/h2&gt;

&lt;p&gt; This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;h2&gt;Covariance functions as models&lt;/h2&gt;

&lt;p&gt;I realized this weekend that all of the models I've considered are achievable by using different covariance functions.&lt;/p&gt;

&lt;p&gt;The original model is given by&lt;/p&gt;

&lt;div&gt; \[
k_1(i,j) = \sigma_s k_{\text{cubic}}(i,j) + \sigma_o  k_{\text{offset}}(i,j) + \sigma_r k_{\text{linear}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
k_{\text{cubic}}(i,j) &amp;= \frac{|\tau_i - \tau_j| \min(\tau_i, \tau_j)^2}{2} + \frac{\min(\tau_i, \tau_j)^3}{3} \\
k_{\text{linear}}(i,j) &amp;= \tau_i \tau_j  \\
k_{\text{offset}}(i,j) &amp;= 1 \\ 
\end{align}
\] &lt;/div&gt;


&lt;p&gt;The cubic model penalizes non-zero second derivative over the length of the curve.  The offset and linear model penalize zero and first derivative initial conditions.&lt;/p&gt;

&lt;h2&gt;White noise perturbation model&lt;/h2&gt;

&lt;p&gt;Both of the two new models expand on the original by modelling how the observed curve differs between views.  That is, the model for the curves are the same as before (they are cubic spline curves), but we additionally model &lt;strong&gt;how they are perturbed between views&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The first new model I call the &quot;White Noise&quot; perturbation model, which treats each view of the curve as arising from a white noise process that perturbs a &quot;master curve&quot;, I.e. the unobserved mean curve.  Its covariance is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{white}}(i,j) = k_1(i,j) + \delta(v_i-v_j) k_{\text{w}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(v_i\) is the view that captured the \(i\)th point and \(k_w\) is&lt;/p&gt;

&lt;div&gt; \[
k_w(i,j) = \sigma_{o,w}  k_{\text{offset}}(i,j) + \sigma_{r,w} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model adds extra covariance that is independent per-view.  This treats the perterbations from the mean curve as independent.&lt;/p&gt;

&lt;p&gt;The perturbations themselves are assumed to be purely to the curve's initial conditions, i.e. its position and direction.&lt;/p&gt;

&lt;p&gt;This model is motivated by the assumption that perturbations arise due to camera mis-calibrations, which result in mostly translation and small rotational changes.&lt;/p&gt;

&lt;h2&gt;Brownian motion perturbation model&lt;/h2&gt;

&lt;p&gt;The second model treats perturbations as arising from Brownian motion, i.e. each curve is independent, conditioned on the previous view's curve.  The covariance function is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{brown}}(i,j) = k_1(i,j) + \min(\tau_i, \tau_j) k_{\text{b}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(k_b\) is&lt;/p&gt;

&lt;div&gt; \[
k_b(i,j) = \sigma_{s,b} k_{\text{cubic}}(i,j) + \sigma_{o,b}  k_{\text{offset}}(i,j) + \sigma_{r,b} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model assumes perturbations arise by motion of the plant during imaging, like moving closer to a light source, or responding to a temperature gradient in the room.  &quot;View index&quot; is a surrogate for a time variable, since time between captures is roughly constant.  The use of Brownian motion means the magnitude of perturbation increases by the square root of the distance between the views (time).  \(k_b\) models the nature of the perturbation; we use the cubic-spline kernel which says that point-wise perturbations are strongly correlated and increase in magnitude the further they get from the base of the curve.&lt;/p&gt;

&lt;p&gt;This \(k_b\) is possibly overkill;  using the simpler \(k_w\) from the white-noise model might work just as well.  This simpler model implies curves drift in position and direction only, and these perturbations are correlated over time.&lt;/p&gt;

&lt;h2&gt;End stuff&lt;/h2&gt;

&lt;p&gt;After some consideration, I think implementing these models in the current system will will very simple.  Training them will be harder; some more ground truthing will be needed.&lt;/p&gt;

&lt;p&gt;Of the two new models, I suspect that the white noise model will be sufficient to get the gains in marginal likelihood we seek.  It is a much better explanation for misaligned data-points compared to the old model models all point perturbations as independent.&lt;/p&gt;

&lt;p&gt;The brownian motion model will give us something to compare against in evaluation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work summary</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary"/>
   <updated>2013-06-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finished editing new likelihood function.  See &lt;code&gt;correspondence/clean_correspondence3.m&lt;/code&gt; (filename likely to change soon).  Below is a summary of results and comparison to the old approach.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;Below is a plot of the maximum likelihood curves:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-likelihood_1.gif&quot; alt=&quot;Maximum likelihood reconstruction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each colored curve arises from a single 2D data curve.  The curve represents the mean of a gaussian process. Variance is not shown; it is infinite in the direction of backprojection.&lt;/p&gt;

&lt;p&gt;The dark-blue curve is an estimate of the posterior curve, which was used to backproject the 2D curves against.  It is estimated from the point-correspondences.&lt;/p&gt;

&lt;p&gt;Note that for clarity, x/y scale is significantly smaller than z-scale (up-direction).  When axis are equally-scaled, max-likelihood curves lie very close the to blue curve.&lt;/p&gt;

&lt;h2&gt;Improved correspondence&lt;/h2&gt;

&lt;p&gt;The old likelihood suffered from a small but non-negligible number of awful correspondences, which severely damaged both reconstruction and marginal likelihood values.  This was because the likelihood was derived from the point-to-point correspondences, which (a) is problematic at gaps, and (b) suffer bad correspondences which can't be fixed later.
The new approach uses the old approach as a starting point, but then recomputes all 2D curve correspondences against a rough reconstruction.  This dramatically improves correspondences as we see below.&lt;/p&gt;

&lt;p&gt;This is the old correspondence.  Blue points are points from the 2D data curve;  the teal line is the posterior 3D curve (projected to 2D); red lines show correspondences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug.png&quot; alt=&quot;Correspondence bug &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next is the fixed correspondence.  Notice how the red correspondence lines are much shorter, indicating a less costly correspondence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug-fixed.png&quot; alt=&quot;Correspondence bug, fixed &quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Per-data-curve likelihood&lt;/h2&gt;

&lt;p&gt;The old likelihood had a single GP curve that represented all of the different views.  Now we have a GP curve &lt;strong&gt;per data-curve&lt;/strong&gt;, which will be related by a GP prior.&lt;/p&gt;

&lt;p&gt;This will allow us to simultaneously track and triangulate, a key novelty to this approach.  More importantly, it will give us higher marginal likelihood numbers for true 3D curve observations, because we can make the independent noise component very small.&lt;/p&gt;

&lt;h2&gt;TODO - short-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt; - This new function adds an extra pass of DTW per data-curve, per iteration where previously, only one pass was needed.  This has introduced a significant performance bottleneck on the order of 10-50x.  I need to profile and optimize this function if we want runtimes that aren't measured in weeks.  This may motivate a full re-thinking of &lt;code&gt;merge_correspondence.m&lt;/code&gt;, to avoid a full DTW after every merge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Marginal Likelhood &lt;/strong&gt; - Need to build the new marginal likelihood for the foreground curve model.  This proposed version will take advantage of the new per-data-curve likelihood.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO - mid-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end&lt;/strong&gt; - Incorporate new likelhood and ML into gibbs sampler.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Swendsen wang cuts &lt;/strong&gt; - design SWC split/merge move&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log"/>
   <updated>2013-06-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Profiling and optimizing &lt;code&gt;clean_correspondence3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Huge bottleneck in &lt;code&gt;get_dtw_matches_()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Wrote MEX version of get_dtw_matches_(),  &lt;code&gt;get_dtw_matches_horiz.c&lt;/code&gt;.   ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New bottlenecks&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;projection_error_hessian&lt;/li&gt;
&lt;li&gt;&lt;code&gt;interp1&lt;/code&gt; - linear interpolation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;csaps&lt;/code&gt; - spline smoothin&lt;/li&gt;
&lt;li&gt;dtw&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Plan&lt;/h2&gt;

&lt;p&gt;can possibly eliminate some calls to interp1.&lt;/p&gt;

&lt;p&gt;could mex projecion_error_hessian.  should be a big win&lt;/p&gt;

&lt;p&gt;DTW can be done in two passes, or mex'd&lt;/p&gt;

&lt;h2&gt;Mexing &lt;code&gt;projection_error_hessian&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Tried using Matlab's &quot;Coder&quot; feature to generate mex code automatically.  The result was totally bloated (14 files!) and no faster than the matlab version.  Next I tried hand-coding the mex, and its ~10x faster.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/27/work-log"/>
   <updated>2013-06-27T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/27/work-log</id>
   <content type="html">&lt;p&gt;&lt;strong&gt; Thursday Overview &lt;/strong&gt;
* Continued refactoring of likelihood
* C++ Bootcamp
* VR lab tour and new user set-up&lt;/p&gt;

&lt;h2&gt;Continued refactoring of likelihood&lt;/h2&gt;

&lt;p&gt;Implemented &quot;alternative approach&quot; to handling tail points mentioned in last entry.  The old way didn't benefit from re-indexing; this way does.  This way handles negative index points correctly, too.&lt;/p&gt;

&lt;p&gt;In process of testing and debugging.  Possibly more outcomes later tonight.&lt;/p&gt;

&lt;h2&gt;C++ Bootcamp&lt;/h2&gt;

&lt;p&gt;today's session: inline and const-correctness&lt;/p&gt;

&lt;h2&gt;VR lab tour and new user set-up&lt;/h2&gt;

&lt;p&gt;By Angus's request, I showed showed the new postdoc Javier around the lab and set him up with an admin account.&lt;/p&gt;

&lt;p&gt;Lots of things still broken on VR01; biggest problem is video card #2 not displaying anything.  Game controller not set up yet.    Showed osgviewer demo, and by Angus's request, got his Processing demo running too.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/26/work-log"/>
   <updated>2013-06-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Likelihood construction, &lt;code&gt;clean_correspondence3.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;new tracking GP model&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Likelihood construction&lt;/h2&gt;

&lt;p&gt;Started and finished implementation today.  Need to design a test and then debug.&lt;/p&gt;

&lt;p&gt;~ 250 lines of Matlab code.  Logic overview:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;smooth triangulated points&lt;/li&gt;
&lt;li&gt;resample smoothed 3D curve uniformly&lt;/li&gt;
&lt;li&gt;handle tails (see below)&lt;/li&gt;
&lt;li&gt;project curve into each view and resample uniformly&lt;/li&gt;
&lt;li&gt;DTW to correspond 2d data curve to projected smooth curve  (see below)&lt;/li&gt;
&lt;li&gt;map corresponding projected curve points back to 3D points and indices&lt;/li&gt;
&lt;li&gt;triangulate 2d data points against corresponding 3d point&lt;/li&gt;
&lt;li&gt;compute likelihood hessian around that point&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;New DTW&lt;/h2&gt;

&lt;p&gt;Re-implemented a specialized version of DTW with following changes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;only horizontal steps accrue cost&lt;/li&gt;
&lt;li&gt;Hard-constraint on the number of vertical steps per horizontal step.&lt;/li&gt;
&lt;li&gt;keeps track of &quot;best&quot; match along vertical runs. no need for second pass

&lt;ul&gt;
&lt;li&gt;I think this is only possible because only horizontal steps accrue cost.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Tail points&lt;/h2&gt;

&lt;p&gt;Still iffy on the &quot;tail points&quot; case. Implemented late at night and likely needs review in the morning.  Still need to handle negative index values.&lt;/p&gt;

&lt;p&gt;Alternative implementation: only inspect the tails to determine the length of the 3D curve.  Then proceeed as usual. no special cases&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;review tail points code

&lt;ul&gt;
&lt;li&gt;consider alternative implementation (see above)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;handle negative index values&lt;/li&gt;
&lt;li&gt;think about hessian and transformation Jacobian&lt;/li&gt;
&lt;li&gt;testing, debugging, profiling&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Rethinking Likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/20/rethinking-likelihood"/>
   <updated>2013-06-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/20/rethinking-likelihood</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Testing and debugging &lt;code&gt;clean_correspodnence2.m&lt;/code&gt; has revealed some significant problems with the current approach to constructing the likelihood function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 5%-10% of fragements have correspondences that are nonsensical. &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I hypothesize that this is due to bad correspondences early on, when there is little evidence to drive a good correspodnence.  These bad correspodnences are propagated as new curves are added that could suggest a better correspodnence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Large lateral gaps in triangulation result in large axial gaps in posterior curve.  &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The problem is that the index set is computed from the triangulation.  The current fix for this -- smooth, re-index, repeat -- is very limited in the severity it can overcome.  In practice, most gaps are only partially reduced.&lt;/p&gt;

&lt;h2&gt;Rethinking &quot;Correspondence to Likelihood&quot;&lt;/h2&gt;

&lt;p&gt;Corresondence is good for constructing a decent-quality 3d curve, but isn't good for computing fine-grained pointwise likelihood, due to sporradic terrible correspondences and gaps.&lt;/p&gt;

&lt;p&gt;Instead of continuously Band-Aiding these issues that keep arising, its time to re-think how the likelhood is constructed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 1. the mean of 3d gaussians should project to the 2D position of the corresponding data point &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 2. the depth should be based on the corresponding position in the unobserved curve &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Two issues here.  First, how to localize the unobserved curve without having the likelihood already (chicken and egg).  Second, how to identify the corresponding point of the unobserved curve?&lt;/p&gt;

&lt;p&gt;In the old method, the answer to both was &quot;use the correspondence matrix.&quot;&lt;/p&gt;

&lt;p&gt;In the new method, we still use the correspondence matrix to triangulate, but we smooth the result using the prior and then throw away correspondence information.&lt;/p&gt;

&lt;h2&gt;Killing the correspondence grid&lt;/h2&gt;

&lt;p&gt;The corresponence matrix artificially forces points from different views to correspond to the same point.  This is out of necessity -- we need correspondence to achieve triangulation.  But we don't need to adhere to this to compute the likelhood.  Indeed, observed points may fall anywhere in the continuous index set, not into a discrete set of predefined cells.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Observations can correspond to any index in \([0 t_{end}]\).&lt;/li&gt;
&lt;li&gt;Unobserved curve is modelled at a uniform grid.&lt;/li&gt;
&lt;li&gt;Previously, the dimensionality of the unobserved curve grew with the number of observations. Now it grows with the range of the index set (i.e. the length of the curve).`&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Random thoughts&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;evaluate marginal likelihood directly?  then add extra normalization from triangulation (jacobian?)

&lt;ul&gt;
&lt;li&gt;evaluate linearly using markov conditional probabilities&lt;/li&gt;
&lt;li&gt;extend to poly model

&lt;ul&gt;
&lt;li&gt;grid-structured Bayes net&lt;/li&gt;
&lt;li&gt;topological sort for evaluation&lt;/li&gt;
&lt;li&gt;use scope variables to manage dependencies generally&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/17/work-log"/>
   <updated>2013-06-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing work on foreground curve model. Editing &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Logic overview&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;smooth triangulated points &amp;amp; estimate their indices&lt;/li&gt;
&lt;li&gt;fill-in untriangulatable points/indices&lt;/li&gt;
&lt;li&gt;resolve many-to-one data-to-3d-curve correspondences by picking best match&lt;/li&gt;
&lt;li&gt;triangulate each data point against 3d curve to get mean and covariance for point&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Logic detail&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;1. estimate indices by chord-length parameterization
2. repeat N times:
    2.1 smooth triangulated points using current index estimate (posterior mean approx.)
    2.2 re-estimate indices using chord-length parameterization
3. for each untriangulatable point
    3.1 triangulate against cubic interpolation of neighboring points (Newton's method)
    3.2 store resulting point and index with smoothed points from 2.1
4. for each 2D data curve
    4.1 triangulate against corresponding 3d point in smoothed curve to get likelihood mean
    4.2 compute curvature at this point to get likelihood precision
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;03:13:30 PM&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First run, addressing syntax errors.&lt;/li&gt;
&lt;li&gt;Plotting first test: triangulated curve w/ and wo/ gap-filling

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: gap-fill at beginning maps exactly to first non-gap point&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: interior gap point falls exactly on non-gap point&lt;/li&gt;
&lt;li&gt;need to merge points?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;04:06:03 PM&lt;/strong&gt;
wrapping up...&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnose coincident-point issue -- bug or not?&lt;/li&gt;
&lt;li&gt;Finish testing - plot per-view likelihood points&lt;/li&gt;
&lt;li&gt;Add point-merging&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/14/log-entry"/>
   <updated>2013-06-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/14/log-entry</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14528&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Working on foreground curve model.&lt;/p&gt;

&lt;p&gt;Begin of significant rework; SVN revision noted in infobox.&lt;/p&gt;

&lt;p&gt;Editing  &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt; which will replace &lt;code&gt;correspondence/clean_corespondence.m&lt;/code&gt;.  Drawing partially from &lt;code&gt;corr_to_bg_likelihood.m&lt;/code&gt;, esp. for &quot;best match&quot; logic.&lt;/p&gt;

&lt;p&gt;Probably will rename &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt; to &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt; before I finish.&lt;/p&gt;

&lt;p&gt;Will use &lt;code&gt;tests/test_ml_end_to_end&lt;/code&gt; to compare old and new implementations.&lt;/p&gt;

&lt;p&gt;Work still in progress...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/13/log"/>
   <updated>2013-06-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/13/log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;After investigating the false positives from the last entry, it seems clear that bad matches look good because missing data are not penalized.  For example&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-13-bad-match.png&quot; alt=&quot;Bad match&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the overlapping region of the blue and green curves, the distance between them is relatively low (less than 3 pixels, or 1.5px radius).  But the size of their overlap is so low that it would be hard to claim that they come from the same underlying curve with any confidence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Params &lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smoothing_variance_2d: 0.2500
    noise_variance_2d: 10
     position_mean_2d: [2x1 double]
 position_variance_2d: 1.3629e+04
     rate_variance_2d: 0.4962
   smoothing_variance: 1.0000e-04
       noise_variance: 10
        position_mean: [3x1 double]
    position_variance: 62500
        rate_variance: 2.2500
      smoothing_sigma: 0.2000
    noise_variance_bg: 0.1038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I would have expected that noise_variance_bg was low enough to discount this candidate, but the log ML ratio is 71.0.  The noise model must just look really bad...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Visualizing/Debugging BG ML</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/12/visualizingdebugging-bg-ml"/>
   <updated>2013-06-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/12/visualizingdebugging-bg-ml</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Experiment&lt;/h2&gt;

&lt;p&gt;We now have a new algorithm for computing background curve Marginal Likelihood.  Lowering noise sigma \(\sigma_n\) should rule out bad matches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: Re-run background candidate matching with new algorithm and roughly-trained \( \sigma_n \).&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;If the threshold is set right, the results are improved, but we still have some false-positives and false negatives.&lt;/p&gt;

&lt;p&gt;It's still unclear whether we can get good results without thresholding, since we haven't computed the noise ML using the new algorithm, so absolute numbers are meaningless.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Params&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smoothing_variance_2d: 0.2500
    noise_variance_2d: 10
     position_mean_2d: [2x1 double]
 position_variance_2d: 1.3629e+04
     rate_variance_2d: 0.4962
   smoothing_variance: 1.0000e-04
       noise_variance: 10
        position_mean: [3x1 double]
    position_variance: 62500
        rate_variance: 2.2500
      smoothing_sigma: 0.2000
       nlise_variance: 10
    noise_variance_bg: 0.1038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Calls&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data = offline_pair_candidates(data, params, 0, 1, 1, 'bg');
cands = tmp_get_cands(data);
visualize_bg_cands(data, cands, 250)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt; Plots &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Matched curves appear in white, unmatched appear in gray&lt;/p&gt;

&lt;p&gt;False negatives&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-12-bad-matches.png&quot; alt=&quot;bad matches&quot; /&gt;&lt;/p&gt;

&lt;p&gt;False positives:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-12-false-positive.png&quot; alt=&quot;bad matches&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;apply new indexing and cleanup algorithm to 3d and noise curves&lt;/li&gt;
&lt;li&gt;better training of foreground/background

&lt;ul&gt;
&lt;li&gt;ground truth curve fragments&lt;/li&gt;
&lt;li&gt;automatic training of background&lt;/li&gt;
&lt;li&gt;automatic training of noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;better foreground model

&lt;ol&gt;
&lt;li&gt;compute likelihood separately&lt;/li&gt;
&lt;li&gt;add smooth GP to likelihood&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;re-run experiment with trained noise model parameters&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Index set bug fixed, marginal likelihood curves improved</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/10/index-set-bug"/>
   <updated>2013-06-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/10/index-set-bug</id>
   <content type="html">&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Index Set Bug fixed &lt;/h2&gt;

&lt;p&gt;After fixing a problem with how index sets were estimated, the marginal likelihood (ML) curves are now much more sensible.&lt;/p&gt;

&lt;p&gt;Here are two marginal likelihood curves using the old approach with two different smoothing sigmas.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2012-06-10_ML_old.png&quot; alt=&quot;pre-bug ML curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how lowering the smoothing sigma \(\sigma_s\) causes the maximum ML to continuously improve, and results in a &lt;em&gt;huge&lt;/em&gt; improvement when noise sigma \(\sigma_n\) is low. There are two implications of this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ML continuously improves as \(\sigma_s\) approaches zero&lt;/li&gt;
&lt;li&gt;As \(\sigma_s\) approaches zeros, the optimal \(\sigma_n \) approaches zero&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;We &lt;em&gt;need&lt;/em&gt; to be able to use \(\sigma_n \) to control the cutoff for background curves, but this is not possible if ML is a monotonic function of \(\sigma_n\).&lt;/p&gt;

&lt;p&gt;The problem arose from the fact that index-set spacing grew in proportion to curve-noise, whereas it should have stayed roughly constant.  As a result, more noise made the curves look &lt;em&gt;smoother&lt;/em&gt;, because the unobserved points seemed to be farther apart.&lt;/p&gt;

&lt;p&gt;Obviously, this is the opposite of what we would want.  I rewrote the &quot;cleanup&quot; algorithm so index sets are now computed from an estimate of the posterior curve, not from the maximum likelihood curve.  This causes noise to be smoothed out of the curve before measuring the distance between points, so increasing noise will not significantly change the inter-point distance.&lt;/p&gt;

&lt;p&gt;Here are the ML curves after the change.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2012-06-10_ML_new.png&quot; alt=&quot;pre-bug ML curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Some observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The affect of changing \(\sigma_s\) is &lt;em&gt;far&lt;/em&gt; less dramatic&lt;/li&gt;
&lt;li&gt;As \(\sigma_n\) approaches zero, the ML &lt;em&gt;always&lt;/em&gt; drops below 0&lt;/li&gt;
&lt;li&gt;The position and value of the maximum is mostly unchanged, suggesting a &quot;natural&quot; noise-value that is independent of the smoothing value.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Point 2 is particularly important.  These ML values are actually log ratios between the ML under this model and the ML under the &quot;default&quot; noise model.  Values below zero indicate that the naive noise model is a better fit.  The fact that we can adjust \(sigma_n\) to control the trade-off between the two models is promising, and suggests that this new model can, indeed, be discriminitive.  Prior to these bugs, it was not clear that this was the case, because the background curve model &lt;em&gt;always&lt;/em&gt; looked better than noise.&lt;/p&gt;

&lt;h2&gt;Point merging&lt;/h2&gt;

&lt;p&gt;One additional complication that arose was that after smoothing, many curve points at appeared at nearly the same position.  As a result, the changes in the index set were very small and the resulting Gaussian Process covariance matrix became degenerate.  I added some code that merges points that are too close to each-other, and updates the likelihood function and index set accordingly.  My tests show that this causes a negligible decrease in the ML compared to the non-merged case, and eliminates the degeneracy problem in all the cases I encountered.&lt;/p&gt;

&lt;h2&gt;Smart Cleanup&lt;/h2&gt;

&lt;p&gt;During my investigations, I also rewrote the &quot;cleanup&quot; logic, which ensures that each point corresponds to the model curve exactly once.  It originally did this by naively taking the first correpsondence, and I thought that the problematic results from above were caused by this.  I wrote new logic that now chooses the &lt;em&gt;best&lt;/em&gt; correspondence, i.e. the correspondence that results in the lowest error.&lt;/p&gt;

&lt;h2&gt;Summary, Next steps&lt;/h2&gt;

&lt;p&gt;The new code is in &lt;code&gt;correspondence/corr_to_bg_likelihood.m&lt;/code&gt;, which now replaces the deprecated &lt;code&gt;clean_bg_correspondence.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Next test: can we distinguish BG curves from non-bg/foreground curves?&lt;/li&gt;
&lt;li&gt;Migrate this new logic to 3D curve model (clean_correspondence.m and maybe merge_correspondence.m)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 
</feed>
