<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>KLS Research Blog</title>
 <link href="http://vision.sista.arizona.edu/ksimek/research/atom.xml" rel="self"/>
 <link href="http://vision.sista.arizona.edu/ksimek/research"/>
 <updated>2013-10-27T22:59:47-07:00</updated>
 <id>http://vision.sista.arizona.edu/ksimek/research</id>
 <author>
   <name>Kyle Simek</name>
   <email>ksimek@email.arizona.edu</email>
 </author>

 
 <entry>
   <title>Work Log - Testing likelihood #2 (2-day)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/26/work-log"/>
   <updated>2013-10-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Client send/receive timeout &lt;/h2&gt;

&lt;p&gt;Tried to get send/receive to timeout if the server didn't respond.  It turns out, although this feature exists in native Unix sockets, the boost library abstractions render them useless.  To get this, I'll need to use asynchronous IO, which I'm not ready to jump to, yet (also not sure if callbacks will work in mex, since the callback code might not exist after the constructor function returns).&lt;/p&gt;

&lt;p&gt;Let's just take the approach that the server must always respond quickly or disconnect.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Testing &lt;code&gt;curve_tree_ml_2&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Need a reliable way to get a testing Trackset.  Write something for wacv -- &lt;code&gt;get_wacv_trackset&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Modified run_all_wacv to save Tracks;  re-running on all datasets.  &lt;code&gt;get_wacv_results&lt;/code&gt; now returns Tracks as well as means.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tweaked semantics&lt;/strong&gt; so this now &lt;em&gt;only&lt;/em&gt; computes the pixel likelihood.  The full likelihood is the sum of this and &lt;code&gt;curve_tree_ml&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tweaked &lt;code&gt;construct_attachment_covariance_2.m&lt;/code&gt; so the self-covariance matrices are computed in the function (if needed) instead of being passed in.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Spin-off &lt;code&gt;construct_attachment_covariance_3.m&lt;/code&gt; a fully general version that receives input and output indices, and optionally a pre-computed self-covariance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TODO: &lt;/strong&gt; make this the &quot;official&quot;/dispatch version, make other versions call this (or eliminate altogether)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;data matrix is HUGE.  8913 dimensions.  Is this right?  Also, we can move inversion outside of the loop (maybe).&lt;/p&gt;

&lt;p&gt;Consider 'subset of data' method, or other dimensionality-reduction method&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;&lt;strong&gt;Issue&lt;/strong&gt;: Can't do cholesky decomposition, because points branching from the base are redundant.  Using SVD instead.&lt;/h2&gt;

&lt;p&gt;Okay, it appears to be successfully running end-to-end.  Haven't confirmed results yet, but one thing is clear... its REALLLLLY slow (3 minutes and counting)&lt;/p&gt;

&lt;p&gt;Can do ancenstral sampling to exploit tree structure to speed it up.  can further use markov property to break up curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, not quite end-to-end success.  Some indexing, reshaping issues.   Other bugs found&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;wasn't adding posterior mean to sampled results&lt;/li&gt;
&lt;li&gt;missing transpose in posterior covariance equation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Debugging sampled curves&lt;/h2&gt;

&lt;p&gt;Message looks okay under inspection, but getting -inf (due to exception).  Dumping...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found a recent bug in code that builds the Gl_curve.  When I chnaged an assert to an if/throw, I forgot to negate the conditional.&lt;/p&gt;

&lt;p&gt;Getting finite values now.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Result looks in the ballpark, but some perturbations look questionable (considering the tight constraints on the WACV dataset).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-27-perturb-comparison.png&quot; alt=&quot;perturbed vs. original&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Doind a full dump-mode run...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found a bug in a recent refactor of dump-mode, causing segfaults.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;All views drift very far from their true values.  Possibly a math bug in the magnitude of the posterior covariance?&lt;/h2&gt;

&lt;p&gt;Refactored server's &quot;dump mode&quot;  to continuously dump each message as it's received, instead of running in offline mode and dumping only the passed-in model.&lt;/p&gt;

&lt;p&gt;Trying method2.  Got similar results, qualitatively; per-view curves still have some bizzarre features.  Interestingly, the perturbations between successive samples of method 2 (and between method2 and method 1) are relatively small, suggesting this is an issue with the mean, not the variance.&lt;/p&gt;

&lt;p&gt;Recall that we only tested the no-perturb model for wacv reconstruction; not the per-view reconstruction.&lt;/p&gt;

&lt;p&gt;Re-running WACV dataset 2 with OU-perturb model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Looks sensible.  So that rules out the parameter settings causing bad mu's.  We should be getting exactly WACV results;  can we get there?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;use K_star = K&lt;/li&gt;
&lt;li&gt;use zero covariance matrix (always sample at the mean)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Running...  (slow, because matrix multiplication is so much larger)&lt;/p&gt;

&lt;p&gt;I'm guessing the bug is in the full-tree covariance .&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Getting &quot;degenerate curve&quot; error.  Thinning points fixed.&lt;/p&gt;

&lt;p&gt;Dumped results look good.  This suggests that mean math is likely correct, as long as K_star is okay.  So K_star math is probably wrong.&lt;/p&gt;

&lt;p&gt;Need to write a test to confirm and start debugging.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done. Results match on symmetric and non-symmetric index sets.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Covariance matrix Rank-reduction&lt;/h2&gt;

&lt;p&gt;While previous test was running, read-up on reduced-rank approximations to K.  Nystrom method seems sensible to speed up matrix inversion.  Doesn't avoid the matrix multiplication  with K_star, or cholesky decomposition of the posterior covariance...  but using a smaller output index set seems like a reasonable approach to mitigating both of those.&lt;/p&gt;

&lt;p&gt;Curious how many non-negligible eigenvalues we have; how many data points we should use.  Recalling a plot of eignevalues from a few days ago, it looked like less than 0.1% of the dimensions are significant, but need to get a concrete number.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running wacv&lt;/p&gt;

&lt;h2&gt;Efficient sampling implementation&lt;/h2&gt;

&lt;p&gt;Lets do a decomposed implementation, and compare the results to this naive implementation.  This should resolve whether the aforementioned issue is a bug in implementation or in theory.  We need the efficient implementation anyway, and having it should help debugging faster.&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;get likelihood from multiple samples -- what is the variance of the MC estimator?&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>params, CVPR 2014</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/26/params-cvpr-2014"/>
   <updated>2013-10-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/26/params-cvpr-2014</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;/ksimek/research/CVPR2014/params.html&quot;&gt;See CVPR 2014 parameters page&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/24/work-log"/>
   <updated>2013-10-24T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/24/work-log</id>
   <content type="html">&lt;p&gt;Implementing one-object-per-view in likelihood_server&lt;/p&gt;

&lt;p&gt;Overloaded bd_mv_likelihood' evaluate and dump  functions to receive a sequence of renderables.&lt;/p&gt;

&lt;p&gt;Write a &quot;wrap_all_As_silhouette&quot; to, well, wrap all renderables in silhouette renderers.&lt;/p&gt;

&lt;p&gt;re-wrote message-to-curve function to decode the message to a vector-vector-vector, and convert that to a vector-of-gl_curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Compiling work from last 24 hours.&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;Generate a multi-model message from matlab (dummy)&lt;/li&gt;
&lt;li&gt;test message in likelihood_server_2.cpp

&lt;ol&gt;
&lt;li&gt;generate random samples from wacv data.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;Doing dummy message test.&lt;/p&gt;

&lt;p&gt;At first, accidentally sent to old implementation... and it didn't barf!  This is unsettling, because the message format has changed significantly.  Did I ever recompile the mex files?&lt;/p&gt;

&lt;p&gt;Nope.  got some mex compile errors to deal with.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Friday&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;okay, sending a random model using the new one-per-view message system isn't crashing.&lt;/p&gt;

&lt;p&gt;Questions&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the likelihood looks high, considering it's a ranomd model .  is a null model better?  is the ground-truth model better?&lt;/li&gt;
&lt;li&gt;visualize the received message;  is each view different?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Trying the null model from 1., the mex file crashed matlab.  Empty curvesset isn't handled.&lt;/p&gt;

&lt;p&gt;trying all-zeros.  Server crashed -- coincident points aren't handled gracefully.  Fixed (now it returns -inf for log likelihood).&lt;/p&gt;

&lt;p&gt;issues
server: error creating gl_curves causes client disconnect. fix exception handling
client: doesn't handle server crashes gracefully.  adding timeout
client: doesn't handle empty curveset&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;random sample from multi-view posterior (instead of single max posterior)&lt;/li&gt;
&lt;li&gt;get likelihood from multiple samples -- what is the variance of the MC estimator?&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - Implementing Two-term likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log"/>
   <updated>2013-10-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log</id>
   <content type="html">&lt;p&gt;Working on Matlab integration.&lt;/p&gt;

&lt;p&gt;Quick test shows we can get about 20 evaluations per second.  Not bad, considering each evaluation consists of 9 image likleihoods.  There's some room for improvement here, but probably not worth pursuing at the moment:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;leapfrog rendering: render (a) while evaluating on (b).&lt;/li&gt;
&lt;li&gt;cleaning up geometry and fragment shaders (fewer branches, less storage)&lt;/li&gt;
&lt;li&gt;try other GPU blurring routines.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;New Likelihood&lt;/h1&gt;

&lt;p&gt;Scenario: we have a decent likelihood that is linear-gaussian (and a gaussian prior), so we can compute the marginal likelihood in closed-form.  However, we'd like to incorporate additional sources of evidence whose likelihoods aren't gaussian.  We'll see how we can estimate the joint marginal likelihood with simple Monte-Carlo sampling (no MCMC needed, no gradient).&lt;/p&gt;

&lt;h2&gt;Derivation&lt;/h2&gt;

&lt;p&gt;The old marginal likelihood looked like this:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1) &amp;= \int p(x) p(D_1 | x) dx
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;After introducing the extra likelihood term, the joint probability is no longer linear-gaussian, so the exact marginal likelihood involves an intractible integral.  However, by  re-arranging, we see we can get a good monte-carlo approximation:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;= \int p(x) p(D_1 | x) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= \int p(x | D_1) p(D_1) p(D_2 | x) dx &amp; \left(\text{Bayes thm (see below)}\right) \\
p(D_1, D_2) &amp;= p(D_1) \int p(x | D_1) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= p(D_1) \frac{1}{N} \sum p\left(D_2 | x^{(*)}\right) &amp; \text{(Monte Carlo)}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;In the second line, I've replaced the first two terms using Bayes theorem.  In the last line, the x-stars are samples from \(p(x | D_1)\), which we have in closed-form due to linear-Gaussian prior and likelihood for \(D_1\).&lt;/p&gt;

&lt;p&gt;Thus, we see if at least one source of data yields a linear-gaussian likelihood, we can incorporate additional data with arbitrary likelihoods  in a principled way.  In many cases, \(p(x | D_1) \) has low variance, so a small number of Monte-Carlo samples are sufficient for a good estimate -- even a single sample could suffice.  Even if the estimates are bad, they are unbiased, so any MCMC involving the marginal likelihood will converge to the target distribution.&lt;/p&gt;

&lt;h2&gt;Importance Sampling version&lt;/h2&gt;

&lt;p&gt;We can alternatively derive this in terms of importance sampling, setting the proposal probability q(x) to \(p(x | D_1) \):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;\approx 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{1}{q(x)} \\
            &amp;= 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{p(D_1)}{p(x) p(D_1 | x)} \\
            &amp;= 1/N \sum p(D_2 | x) p(D_1) \\
            &amp;= p(D_1) 1/N \sum p(D_2 | x) 
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File:&lt;/strong&gt; curve_tree_ml_2.m&lt;/p&gt;

&lt;p&gt;Basic idea&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;construct a thinned output index set (optional, but smart)&lt;/li&gt;
&lt;li&gt;construct a posterior distribution over the thinned set&lt;/li&gt;
&lt;li&gt;Add perturbation variance to the posterior&lt;/li&gt;
&lt;li&gt;take average over n trials: sample curveset and evaluate pixel likelihood&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Step 2 required updating to construct_attachment_covariance, which only constructs symmetric covariance matrices.  We need the covariance between indices with observations and the desired output indices.  Fully refactored that function into &lt;code&gt;construct_attacment_covariance_2.m&lt;/code&gt;; confirmed correctenss in the case of the self-covariance by using an existing test for version 1 of that function.  If non-symmetric, the upper-triangular blocks are processed in a second pass, swapping indices so we can re-use existing code.&lt;/p&gt;

&lt;p&gt;Need to try view-specific sampling, i.e. sample 9 different curves from 9 different views.  This refactor affects likelihood server, client, and message format.  I'm worried about the performance hit, but probably not worth worrying about (or futile).  Coarse sampling of indices could mitigate.    In any case, view-specific sampling is probably necessary, because we're using such low blurring levels in the Bd_likelihood, so the reconstruction needs to fall near the data.  We've seen how plant motion and camera miscalibration cause &quot;good&quot; 3D curves to reproject up to 10 pixels away from the data in some views.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Side-note&lt;/strong&gt; - the tcp connection is working very reliably so far!  Even after the machine sleeping/resuming several times, and suspending the server job for serveral hours, the socket is still valid and communicating flawlessly!&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Implementing per-view sampling&lt;/h2&gt;

&lt;p&gt;need to refactor send_curves.m to send &lt;code&gt;num_views&lt;/code&gt; curves instead of one.&lt;/p&gt;

&lt;p&gt;Now receive as num_curves x num_views cell array.&lt;/p&gt;

&lt;p&gt;Coded vector&lt;sup&gt;3&lt;/sup&gt; to/from message.&lt;/p&gt;

&lt;p&gt;todo: rewrite likelihood server to receive vector3, somehow pass per-view models to likelihood&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;multi-view likelihood is out - it's one-model, multi-view.  We need multi-model, multi-view.

no, MV likelihood is okay, just add an extra operator() that receives a sequence of renderables 
whose size is equal to the number of views
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;p&gt;Some protocol for starting and loading the likelihood server from matlab code
Stress test likelihood server&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log"/>
   <updated>2013-10-22T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still searching for inf bug in libcudcvt blurred_difference_likelihood.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found it:  my GPU-based log_sum routine had a bug.  instead of subtracting the maximum value before exp-summing, it subtracted an arbitrary value.  The find-max loop looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Real pi = v[0];

#pragma unroll
for(unsigned int i = 1; i &amp;lt; N; ++i)
{
    pi = pi &amp;lt; v[0] ? v[0] : pi;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, the zeros should be i's.&lt;/p&gt;

&lt;p&gt;A great bug to have found and fixed, but really frustrating that I let it slip through in the first place.  So stupid!  But it didn't affect correctness in the common case, so my tests didn't catch it.  It makes a good case for randomized testing (as if a good argument was lacking...).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(semi-obvious) note to self: conditional gaussian mixture is not equal to mixture of conditional gaussians!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;okay, a few more cleanup tasks, then on to real goals:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cleanup and commit version 0.1 of the likelihood server -- done&lt;/li&gt;
&lt;li&gt;split training into it's own directory. -- done&lt;/li&gt;
&lt;li&gt;Get likelihood server connecting reliably with matlab on different computers.&lt;/li&gt;
&lt;li&gt;implement likelihood importance sampling into maltlab's curve_ml procedure&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Thought some on birth moves.  It seems like there are some possibilities for births larget than two by using the adjacency graph created by pair candidates.&lt;/p&gt;

&lt;p&gt;for now, start with greedy&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - Linux NVidia/Cuda/X11 erorrs; Cuda server; matlab integration</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log"/>
   <updated>2013-10-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log</id>
   <content type="html">&lt;p&gt;Getting my (cuda-equipped) desktop system running with likelihod_server.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Trouble building makefiles. Csh issue with new makefiles.  Kobus fixed&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still can't build makefiles. Probilems in the ./lib subdirectory is apparently blocking kjb_add_makefiles from finishing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Some makefiles were missing in ./lib.  Adding them was problematic, because (like before) kjb_add_makefiles wasn't finishing, because dependency directories were missing makefiles.  Also had some issues in kjb /lib, out of date from svn and some compile bugs I introduced in my last commit.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Compiling now, but shader is giving errors at runtime (opengl can't compile it, but no error message is given.&lt;/p&gt;

&lt;p&gt;Rebooting...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;No desktop.  Missing disk issue?  Tweaked fstab, still no X11.  Must be a driver issue.  Couldn't find driver from NVidia's I downloaded from web site months ago.  Using driver &quot;Ubuntu-X&quot; PPA.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still no X.  /var/log/Xorg.0.log says driver and client have different versions.  Got a tip online:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo dpkg --get-selections | grep nvidia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lots of extra crap there (old versions, experimental drivers, etc).  'sudo apt-get purged' all the non-essentials.  Now we're getting to login screen, but login is failing (simply returns to login screen).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found solution:&lt;/p&gt;

&lt;p&gt;   sudo  rm ~/.Xauthority&lt;/p&gt;

&lt;p&gt;Booting successfully.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running likelihood_server...  shader is now compiling successfully!   Previous issues must have been driver issues as a result of the recent 'sudo apt-get upgrade' without restart.&lt;/p&gt;

&lt;p&gt;Getting segfault when dumping pixels, though.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Caused by trying to read from cuda memory as if it were main memory.  Should be dumping if we're using GPU (or use a different dump function).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Fixed;  if using gpu, copy from cuda to temp buffer, then call normal routine.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting nan's from likelihood.  Need to dump from GPU likleihood, which means digging into libcudcvt.  Got some build errors resulting from a boost bug from 8 months ago (which arose because I updated GCC).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;cudcvt updated to grab blurred frames for (some method names changed to better match the analogous CPU likelihood in KJB.).&lt;/p&gt;

&lt;p&gt;Moved &quot;dump&quot; routines from Bd_mv_likelihood_cpu to the base class Bd_mv_likelihood as pure virtual function, and implemented a version in Bd_mv_likelihood_gpu to call cudcvt's new frame-grabbing code.  So &quot;dump&quot; mode now works on both cpu and gpu version.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Dumped data from CUDA likelihood; looks fine.  So I'm still clueless why we're getting nan values.&lt;/p&gt;

&lt;p&gt;Sanity check -- check that tests in libcudcvt are still passing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;test is failing on an assert.  Looks like an overzealous assert with a low percent-error threshold.&lt;/p&gt;

&lt;p&gt;Lowered threshold, test finished; GMM model doesn't match between gpu and cpu versions...&lt;/p&gt;

&lt;p&gt;what's changed?  Rolling back to first releast to see if tests pass.  Is it possible I never validated this?  Or maybe different GGM's being used between cpu and gpu?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Old version crashed and burned hard.  GPU is returning 'inf'.  No help here...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, svn version has lots of uncommitted changes, including bugfixes.  No wonder it was no help.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;found issue: introduced a regression when troubleshooting last bug.  Grrr.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still getting 'inf'.  Checked:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GPU buffers contain valid data for data and model&lt;/li&gt;
&lt;li&gt;CPU version gives finite results.&lt;/li&gt;
&lt;li&gt;GPU can give finite results (e.g. in test application)&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Modified libcudcvt test to receive arbitrary data and model images.  It's giving finite results, so it must be something about how I'm calling it in the likelihood server program.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;issue:&lt;/strong&gt; Bd likelihood in GMM mode gives lower values for ground truth model than random model.&lt;/p&gt;

&lt;p&gt;Bug. fixed.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Digging deeper into 'inf' issue.&lt;/p&gt;

&lt;p&gt;Dug all the way into the thrust::inner_product call.  Probed both buffers -- look okay.  mixture components look okay&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Got it!&lt;/strong&gt;  Was able to reproduce the 'inf's in the likelihood test program in libcudcpp.  It was so hard to reproduce because it only occurred when&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;old GMM values were used&lt;/li&gt;
&lt;li&gt;&lt;em&gt;unblurred&lt;/em&gt; model &lt;em&gt;and&lt;/em&gt; data were used&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Regarding 2, previosuly I just used the unblurred model, since it was at hand from a cuda dump, but used the blurred data from a training dump.&lt;/p&gt;

&lt;p&gt;Blurring doesn't seem to matter; both 2.0 and 5.0 give inf.&lt;/p&gt;

&lt;p&gt;Interestingly, the perfect model doesn't oveflow, but the random (almost null) model does.&lt;/p&gt;

&lt;p&gt;Also, for all the 'inf' cases, the cpu results aren't terribly high, so float overflow seems unlikely.&lt;/p&gt;

&lt;p&gt;Most likely it's the conditioning, where the joint pdf is divided by the marginal.  Maybe we're getting some bizzare model values that are off the charts?  But using the model image with the blurred data image is no problem, so that suggests the model values aren't a problem.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Interesting:&lt;/strong&gt; Tried removing border edges and the problem disappeared.  Is it possible that the blurring routine is not robust to near-edge pixels?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;idea: dump image after scale-offset, but before reduce&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;p&gt;Probability maps look sensible when a 2-pixel margin is added (--data-margin=2).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_2.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 2.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 5.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice what happens when we disable the data margin (rendered dimmer and with blue padding to emphasize effect):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_no_margin.tiff.jpg&quot; alt=&quot;&quot; /&gt;
(2.0 blurring, 0 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice the white pixels around the border, which presumably correspond to inf values.&lt;/p&gt;

&lt;p&gt;Inspecting the blurred data image, it looks like these pixels are, indeed, the brightest in the image.  It's possible we were lucky enough to have found the maximum range of this gmm.&lt;/p&gt;

&lt;p&gt;It looks like evaluating the bivariate normal is underflowing.  It could have been brought back down to size during conditioning, but we never made it that far.  could refactor by computing conditional covariance matrix beforehand, instaed of taking a ratio of computed values&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Pre-test all incoming data images: evaluate against an empty image and a full image and check for NaN and inf.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - KJB EM GMM</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log"/>
   <updated>2013-10-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log</id>
   <content type="html">&lt;p&gt;Still struggling with the EM GMM algorithm.  Found/fixed one bug where responsibilities aren't initialized if NULL is passed to the function.&lt;/p&gt;

&lt;p&gt;Ran overnight and it hadn't finished at the end.  Found/fixed a bug where the entire @M element responsibility matrix is rescaled 2M times (once per point).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting rank-deficient errors.  Looks like this is a result of the huge dataset; covariance matrix is divided by 1e6 (the number of soft-members of the cluster).&lt;/p&gt;

&lt;p&gt;For now, hack by trying to thin the dataset.  Long-term, adding a minimum offset to the emperical covariance seems to be the common solution.&lt;/p&gt;

&lt;p&gt;Possibly normalizing the data would be good.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Added offset to covariance (command-line options and file-static variable already existed, just needed to add it to the full_GMM logic).&lt;/p&gt;

&lt;p&gt;Added &lt;code&gt;read_gmm.m&lt;/code&gt; matlab routine, and plotted along with scatter plot of data.  Results look pretty good:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-20-kjb-gmm-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Moving trained gmm to ~/data/arabidopsis/training/bd_likelihood_gmm/blur_2.0.gmm&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;re-train GMM on held-out dataset.&lt;/li&gt;
&lt;li&gt;implement matlab likelihood sampling&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;how to sync datasets on matlab and c++ server?&lt;/li&gt;
&lt;li&gt;Wacv multi-view reconstruction for training&lt;/li&gt;
&lt;li&gt;saving wacv to files for training&lt;/li&gt;
&lt;li&gt;fixing issues in wacv reconstructions&lt;/li&gt;
&lt;li&gt;evaluation methodology&lt;/li&gt;
&lt;li&gt;split/merge&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/19/work-log"/>
   <updated>2013-10-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/19/work-log</id>
   <content type="html">&lt;h2&gt;Training likelihood&lt;/h2&gt;

&lt;p&gt;Fitting a three-component GMM resulted in a null model having a higher likelihood than the ground truth model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Spent a lot of time troubleshooting this and finally realized I had scaled all values up by 100x so Matlab wouldn't choke, but I wasn't accounting for this when evaluating.  Blame it on this blasted cold, killing my focus!&lt;/p&gt;

&lt;p&gt;Ground truth model looks much better than null model now.&lt;/p&gt;

&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;Trying different blurring levels.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.0 is to small.&lt;/li&gt;
&lt;li&gt;2.0 seems okay&lt;/li&gt;
&lt;li&gt;5.0 gives a wide variance for true-positives.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In the end, running max likleihood on this will be best, but this gives us a ballpark.&lt;/p&gt;

&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;Trying to get KJB EM algorithm working on my 2M element dataset.  Getting NaN errors at the moment.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - Uninformative likelihood?</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/17/work-log"/>
   <updated>2013-10-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15701&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Likelihood sanity check (ctd)&lt;/h2&gt;

&lt;p&gt;An empty model should have a significantly worse likelihood than the ground truth model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ground truth model:  -3.38064e+08
empty model: -3.28279e+08
full model: -1.20572e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both empty and full are better than ground truth.  Why??&lt;/p&gt;

&lt;p&gt;At least possiblities here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Noise level is so high that fitting is impossible (remember the paper &quot;Fundamental limits of Bayesian Inference: Order Parameters and Phase Transitions for Road Tracking&quot;)&lt;/li&gt;
&lt;li&gt;Bug.  maybe renderings are just waaaay off?&lt;/li&gt;
&lt;li&gt;Bad calibration.  Looking at the GMM plot from yesterday (see below) it looks like the one-sigma contour is very very large.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-bd.gmm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Case 1 is what's left over once 2 and 3 are ruled out.&lt;/p&gt;

&lt;h2&gt;Rendering sanity check.&lt;/h2&gt;

&lt;p&gt;This seems to be a culprit.&lt;/p&gt;

&lt;p&gt;Here's first data view:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-data-view-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Not a good match to the actual first data image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-ler_2_36_0.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But a good match to the second data image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-ler_2_36_1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Must be an off-by-one error when reading data.&lt;/p&gt;

&lt;p&gt;The cameras are not off-by one.  Here's the rendering of the model under the first camera:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-model-view-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Good match to the first data image above.&lt;/p&gt;

&lt;p&gt;Bug is in the config file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;view-indices=1:4:36  # bug here
view-indices=0:4:35  # should be this
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Good news, bad news.&lt;/p&gt;

&lt;p&gt;Good news: after fixing config file, data and rendering look good (use slider to swap between images)&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-10-17-data.jpg&quot;,
        &quot;/ksimek/research/img/2013-10-17-model.jpg&quot;
        ]

    construct_animation($(&quot;#data-model-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;data-model-anim&quot; style=&quot;width:530px&quot;&gt; &lt;/div&gt;


&lt;p&gt;Bad news: likelihood gets &lt;strong&gt;worse&lt;/strong&gt;!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fixed likelhood: -3.38298e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like the rendered model is lighter than the data model.  This might account for the issues we're seeing.  We're in the region to the right of the diagonal &quot;true positive&quot; component in the GMM (see contour plot above).  In this region, the noise model has a stronger pull than the true positive model, which partially explains why we get better when fewer model pixels are on the screen.   It doesn't explain why the model gets better when we shift left and right -- maybe it fits better in the tails of the blurred data, but we'd expect it to eventually get worse.  Lets test it.&lt;/p&gt;

&lt;p&gt;Recall that this GMM was trained on a manipulated version of the data image, not a rendered 3D model image.  The process taken was (a) removing all background pixels from the data image and (b) perturbing the foreground pixels. We may have removed a few foreground pixels to add false positives, but the amount was small, if memory serves.  Now we have far fewer model pixels, so we expect the &quot;true positive&quot; region to be slanted more upward.&lt;/p&gt;

&lt;h2&gt;Optimality test (redux)&lt;/h2&gt;

&lt;p&gt;Code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x_values=&quot;-80 -40 -20 -10 -5 -2 -1 0 1 2 5 10 20 40 80&quot;
for x in $x_values; do
    echo -n &quot;$x &quot;;
    ./likelihood_server \
        --config=test.conf  \
        --image-bounds &quot;0 530 397 0 200 2000&quot;  \
        --cam-convention &quot;1 0 0&quot;  \
        --dbg_save_frames  \
        --dbg_load_message  \
        --dbg-ppo-x-offset=$x 2&amp;gt; /dev/null | grep ^3 | sed -e 's/3 //' -e 's/,//';
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output (delta-x vs log-likelihood):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-80 -3.36866e+08
-40 -3.36987e+08
-20 -3.37434e+08
-10 -3.37864e+08
-5 -3.38169e+08
-2 -3.38279e+08
-1 -3.38294e+08
0 -3.38298e+08
1 -3.38292e+08
2 -3.38275e+08
5 -3.38172e+08
10 -3.37912e+08
20 -3.37479e+08
40 -3.36958e+08
80 -3.36991e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-likelihood_plot_vs_x.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;&quot;Full&quot; model issue&lt;/h2&gt;

&lt;p&gt;One of the strangest things is the fact that a rendering full of &quot;1.0's&quot; performs far better than the ground truth or empty model.  This area lies along the x-axis in the GMM contour plot, where there is no support.&lt;/p&gt;

&lt;p&gt;Lets debug this.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found an insane bug.  Below is the loop that compares all the pixels of the data and rendered model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    for(size_t i = 0; i &amp;lt;= size_; ++i)
    {
        kjb::Vector2 x;
        x[0] = model[i];
        x[1] = data_[i] + 1;

        ...  // compute p(x[1] | x[0])
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have no idea why we're adding 1.0 to &lt;code&gt;data_[i]&lt;/code&gt;, but this explains everything.  In the &quot;full&quot; model, it puts us right on the diagonal of the GMM.  In the empty model, we're right on the x-axis.  Both are well-supported regions.  In the ground truth model, we're lying between these extremes, which is a no-man's land of near-zero support.&lt;/p&gt;

&lt;p&gt;Spending 5 minutes to determine how this got added...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;It was added between November 11 and December 13 last year.   No obvious reason.  Oh well.&lt;/p&gt;

&lt;p&gt;New sanity check numbers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;full model -5.2217e+08
ground truth model 4.03273e+06
empty model: 4.01252e+06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much more sensible.  Missing data looks terrible, noise looks okay but not great,  ground truth looks best.  Lets do an offset plot just to be safe.&lt;/p&gt;

&lt;h2&gt;Optimality test (redux&lt;sup&gt;2)&lt;/sup&gt;&lt;/h2&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-80 3.1039e+06
-40 3.27557e+06
-20 3.58156e+06
-10 3.77333e+06
-5 3.93159e+06
-2 4.01459e+06
-1 4.02845e+06
0 4.03273e+06
1 4.02704e+06
2 4.01209e+06
5 3.93021e+06
10 3.78782e+06
20 3.60557e+06
40 3.26585e+06
80 3.26894e+06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plot&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-likelihood_plot_vs_x_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Oh Yeah!!&lt;/p&gt;

&lt;h2&gt;BD Likelihood Data Dump Mode.&lt;/h2&gt;

&lt;p&gt;add a mode by which pixel likelihood will save pixel data to files for analysis or debugging&lt;/p&gt;

&lt;p&gt;changes will appear in:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;lib/evaluate/bd_likelihood.h - Bd_pixel_likelihood_cpu&lt;/li&gt;
&lt;li&gt;lib/evaluate/bd_likelihood.h - Bd_mv_likeliood_cpu&lt;/li&gt;
&lt;li&gt;./likelihood_server.cpp - Options&lt;/li&gt;
&lt;li&gt;./likelihood_server.cpp - main()&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;Added dump mode.  Probably should move this into a separate directory, or at least a different program, since it has almost nothing to do with the likleihood server.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;rename matlab &quot;render_client&quot; code to &quot;likelihood_client&quot;&lt;/li&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;remove edges around image border.&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;train likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;how to sync datasets on matlab and c++ server?&lt;/li&gt;
&lt;li&gt;Wacv multi-view reconstruction for training&lt;/li&gt;
&lt;li&gt;saving wacv to files for training&lt;/li&gt;
&lt;li&gt;fixing issues in wacv reconstructions&lt;/li&gt;
&lt;li&gt;evaluation methodology&lt;/li&gt;
&lt;li&gt;split/merge&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - silhouettes, training likelihood, evaluating likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/16/work-log"/>
   <updated>2013-10-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Enabled glPolygonOffset and it improved results somewhat.  Still getting stippling 10% of the time.&lt;/p&gt;

&lt;p&gt;Adding edge-detection code to show stippled edges helps, but adds some internal edges for some reason.&lt;/p&gt;

&lt;p&gt;Apparently, we have point spacing too close together.  This introduces lots of little ridges, which makes silhouette edges appear on the interior of the object incorrectly.  We should pro-process curves in matlab to force spacing to be greater than or equal to to the curve radius.&lt;/p&gt;

&lt;h2&gt;Likelihood rendering bugs&lt;/h2&gt;

&lt;p&gt;Image dumps from our likelihood show all edges are being rendered (not just silhouette edges).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;seems to be an issue with Camera_render_wrapper.  Possibly the y-axis flipping?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Yep.  Reverses the handedness of the projected model.&lt;/p&gt;

&lt;p&gt;Shader assumes orientation of forward-facing triangles isn't affected by projection.  Thus, cross-product can be used to determine whether a face is forward-facing (a fundamental operation for classifying silhouette edges).&lt;/p&gt;

&lt;p&gt;Can we solve by doing visibility test in world coordinates, which avoids the projection matrix altogether?&lt;/p&gt;

&lt;p&gt;Yes, but there's a bug causing a few faulty silhouette edges to appear.  Why?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It turns out, we were assuming the normal vector of opposite faces are parallel, and used this to .  Apparently not always the case, especially when sharp corners occur (which happens routinely when point spacing is close).&lt;/p&gt;

&lt;p&gt;During testing, we somehow corrupted the GPU's memory.  System sprites (e.g. cursor) are starting to be corrupted, and program keeps crashing  from GPU errors.  Probably constructed too many shaders in a single session.  Will reboot and continue.&lt;/p&gt;

&lt;p&gt;Summary: found silhouette rendering bug caused by projection matrix that didn't preserve handedness.  Rewrote silhouette pass 2 geometry shader to handle this case better.  Now renders correctly in both cases.&lt;/p&gt;

&lt;h2&gt;Next: determine cause of likleihood NaN's; train the likelihood.&lt;/h2&gt;

&lt;h2&gt;Blurred-difference (bd) likelihood issues&lt;/h2&gt;

&lt;p&gt;Getting &quot;NaN&quot; when evaluating.  Checking the result of blurring the model shows inf's everywhere.  Weird, because I can confirm that the input data is succesfully blurring:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-blurred_data.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Same blurrer is used for data and model&lt;/li&gt;
&lt;li&gt;It isn't the input (tried substituting data in for model and still got corrupt stuff back).&lt;/li&gt;
&lt;li&gt;re-initializing blurrer doesn't seem to help&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Stepping through in GDB...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After convolution, values are on the order of 1e+158...  clearly invalid.  Converting to float causes inf.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Aside:  the padding added before convolution isn't zeros!  Fixing bug in lib/evaluate/convolve.h:fftw_convolution class.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Aside:  Need to remove edges around image border:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-border_edges.jpg&quot; alt=&quot;border edges bug&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Aside: different blurrer for each view?  unnecessary!&lt;/p&gt;

&lt;p&gt;Wrong, same blurrer, different internal buffers.  Caused by calling blurrer.init_fttw_method() every time we add a view to the multi-view likelihood.  Removed that call.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Apparently, calling blurrer_.init_fftw_method() is wreaking all kinds of havok on the blurring results.  I have idea why.  Possibly calling init_fftw_method() more than once is simply not supported.  But that call simply destroys and object and creates a new one, so why is that different from the first time we called it?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Theory: fftw wisdom is messing with us?&lt;/p&gt;

&lt;p&gt;Results: nope&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We have a working case and a non-working case.  Can we walk from one case to the other and find the exact change that causes the error?&lt;/p&gt;

&lt;p&gt;Aside: Here's a bizarre result wer'e getting during failure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-weird_results.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kind of cool!&lt;/p&gt;

&lt;p&gt;Actually, there might be a hint here... Notice the eliptical shapes that are about the same size as the turntable, but shifted and screwed up.&lt;/p&gt;

&lt;p&gt;It's possible what we're seeing is convolution with a random mask.  the two main ellipses we're seeing are two random bright pixels in the mask, and the miscellaneous roughness might be ca combinariton of small positive and negative values causing the texture we see.  Next step: inspect mask&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bingo!  We're working with a random mask.&lt;/p&gt;

&lt;p&gt;Let's step back to the original failing case and see if the mask is still random.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Confirmed.  Here's an eample mask:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-tmp.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here it is dimmer:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-bunk_maks_dimmer.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Anything look familiar?  It's derived from our plant image (notice the ellipses from the turntable).&lt;/p&gt;

&lt;p&gt;Theory: somehow the mask is getting overwritten with our blurred model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Solved.  Simple bug but was obscured by several issues.&lt;/p&gt;

&lt;p&gt;The fundamental issue was that when the blurring mask was being padded, the padding wasn't being filled with zeros.  Thus, the padded mask consisted of a small section of real mask, surrounded by huge amounts of uninitialized memory.&lt;/p&gt;

&lt;p&gt;This was a bug that I discovered and fixed 12 months ago, but it got rolled back when Andrew revamped the FFTW convolution class.  Ultimately, I was able to restore my changes to the FFTW class without much trouble, but for the six hours that followed, these changes weren't being compiled, because they were in a header file, and the build system only rebuilds object files if the &quot;cpp&quot; file is changed.&lt;/p&gt;

&lt;p&gt;Further, there was a red-herring that appeared as &quot;re-initializing the blurrer causes the errors&quot;.  While this was true, it was only because re-initializing also re-initialized the blurring mask, and it was only during re-initialization did the uninitialized mask padding have junk -- on the first initialization, the memory just happened to be blank.  Go figure.&lt;/p&gt;

&lt;p&gt;This was an absolutely essential bug to find and fix.  Glad its fixed, just wish it hadn't regressed in the first place.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Okay time to reset.  Let's confirm that the rendered models are blurring correctly.  Then confirm non-zero log-likleihood result.    Then on to training.&lt;/p&gt;

&lt;p&gt;Model blurring: CHECK
Finite likelihood: CHECK  (result: -3.38064e+08)
Likelihood is near-optimal: &lt;strong&gt;FAIL&lt;/strong&gt;. see below&lt;/p&gt;

&lt;h2&gt;Optimality test&lt;/h2&gt;

&lt;p&gt;Added debugging option to shift rendering left/right/up/down in the image: &lt;code&gt;--dbg-ppo-x-offset&lt;/code&gt; and &lt;code&gt;--dbg-ppo-y-offset&lt;/code&gt;.   Ideally, the likleihood should be optimized at zero offset&lt;/p&gt;

&lt;p&gt;Log likelihood vs x-offset&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 0  -3.38064e+08
+1  -3.38069e+08
+2  -3.38068e+08
+5  -3.38033e+08
+10 -3.37881e+08
-2  -3.38039e+08
-5  -3.37968e+08
-10 -3.37786e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-likleihood_plot_vs_x.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weird... Why is it at a local &lt;em&gt;minimum&lt;/em&gt; at the center?&lt;/p&gt;

&lt;p&gt;Log likelihood vs y-offset&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 0  -3.38064e+08
-2  -3.38062e+08
-5  -3.38033e+08
-10 -3.3789e+08
-20 -3.37321e+08
-40 -3.36139e+08
-80 -3.33693e+08
 2  -3.38052e+08
 5  -3.38025e+08
 10 -3.37983e+08
 20 -3.37955e+08
 40 -3.3785e+08
 80 -3.37523e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-likelihood_plot_vs_y.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that as y decreases, the top of the plant starts to fall off the screen. This suggests that having fewer model points might be over-preferred.  Need to sanity-check the GMM model we're using -- I trained it months ago and never really leaned on it hard, so it's the first place to look for problems.&lt;/p&gt;

&lt;p&gt;Also, might be a scaling issue.  Double-check that blurred model and blurred data have similar scale.&lt;/p&gt;

&lt;h2&gt;GMM sanity check&lt;/h2&gt;

&lt;p&gt;Plotting first and second standard deviation of the joint distribution of model-pixel-intensity and data-pixel-intensity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-bd.gmm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This looks sensible.  The first component is the diagonal one, representing true postives (in the presence of random perturbations).&lt;/p&gt;

&lt;p&gt;The second component is at the origin -- true negatives.&lt;/p&gt;

&lt;p&gt;The third component is along the y-axis, representing false positives in the data, aka noise.&lt;/p&gt;

&lt;p&gt;The first component has roughly 10x less weight than the second and third.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;No component for the false negatives (aka missing data).  As I recall, running EM on a GMM with four or more components always resulted in redundant components.  Possibly hand-initializing the components in each of the four positions might help.&lt;/p&gt;

&lt;p&gt;I also recall some sort of GPU limitation that prevented me from evaluating more than three components in hardware.  That seems unusual though, and I may be misremembering.&lt;/p&gt;

&lt;p&gt;Does this provide any insight as to why we prefer our model to drift off of the screen?  This introduces more &quot;negative&quot; model pixels, pushing toward the well-supported y-axis are of our model.  Possibly this is the result of poor calibration?  Or wrong blurring sigma?&lt;/p&gt;

&lt;p&gt;TODO: re-run the x/y offset experiment with smaller blurring sigma.&lt;/p&gt;

&lt;h2&gt;Data Sanity check&lt;/h2&gt;

&lt;p&gt;To do.  Approach: instead of evaluating likelihood pixels against data, dump model/data pixel pairs into a list and (a) plot them or (b) train on them.  Maybe just dump to a file and do it in matlab?&lt;/p&gt;

&lt;h2&gt;Blooper reel&lt;/h2&gt;

&lt;p&gt;While writing miscellaneous blocks of memory to disk as images, got some interesting but totally wrong results.&lt;/p&gt;

&lt;p&gt;The following is partially due to rendering an array of doubles as floats.  Interesting banding pattern.  A mini-Mondrian!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-fun-mess-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next is just uninitialized memory.  Iteresting patterns :-)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-fun-mess-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;remove edges around image border.&lt;/li&gt;
&lt;li&gt;share blurrer between views&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;why is it crashing on exit?&lt;/li&gt;
&lt;li&gt;Data sanity check - does it roughly match the GMM's distribution?&lt;/li&gt;
&lt;li&gt;train likelihood&lt;/li&gt;
&lt;li&gt;add &quot;dump&quot; mode to bd-likelihood which saves all image pairs to disk for analysis or debugging.&lt;/li&gt;
&lt;li&gt;test empty model likelihood&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - Finishing Likelihood Server, integrating to sampling engine</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/15/work-log"/>
   <updated>2013-10-15T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/15/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Old silhouette rendering technique is failing.  Point spacing is much tighter than before.  Our old algorithm relied on segment-ids matching between the first-pass and second pass to work.&lt;/p&gt;

&lt;p&gt;Have a replacement algorithm that works reasonably well.  Basically edge-detection on the depth image.  The down side is we're getting double-edges in some areas, and you have to adjust a threshold.  It'll have to do for now, hopefully we can train a the likelihood to be robust to these issues.&lt;/p&gt;

&lt;h2&gt;Training Bd_likelihood&lt;/h2&gt;

&lt;p&gt;added 'train_bd_likelihood()' function to 'lib/evaluate/bd_likelihood.h'.  Uses 'kjb_c::get_full_GMM()'  to fit a three-component gaussian mixture model to model the relationship between model and data pixels.&lt;/p&gt;

&lt;h2&gt;Refactoring&lt;/h2&gt;

&lt;p&gt;Shader objects from './shader.h' are now in the project library, under 'lib/ogl/shader_2.h'.  Not the greatest name, but shader.h already exists and conflicts.  Consder refactoring them later.&lt;/p&gt;

&lt;p&gt;Silhouetter rendering from './render_util.h' are now in 'lib/graphics/silhouette_renderer.h'.  Much better encapsulation now, so we can use it in other projects.&lt;/p&gt;

&lt;p&gt;In the process of updating './likelihood_server.cpp' to reflect these changes.  At the moment, getting an &quot;invalid operation&quot; when rendering.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It was a stray &lt;code&gt;glUseProgram(notzero)&lt;/code&gt; in the shader loading code.&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - Troubleshooting silhouette rendering</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/14/work-log"/>
   <updated>2013-10-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/14/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Silhouettes are rendering badly&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-13-dump_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looks like internal edges are rendering.  Likely an issue with shader_src/silhouette2.gs.glsl.&lt;/p&gt;

&lt;p&gt;Need a quick way to tweak shader and re-run.&lt;/p&gt;

&lt;h2&gt;Building shader debugging mode&lt;/h2&gt;

&lt;p&gt;if enabled, results will display in viewer instead of being passed to likelihood.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - Four days of OpenGL Debugging</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/11/work-log"/>
   <updated>2013-10-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/11/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Status&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Server is now running, connecting, receiving and decoding messages successfully.&lt;/li&gt;
&lt;li&gt;Matlab client is constructing, sending messages, and destroying successfully.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Since we've sent our first message and saved it to a file, we no longer need matlab to debug the server, using the --dbg-load-message flag.&lt;/p&gt;

&lt;h2&gt;OpenGL crashing &lt;/h2&gt;

&lt;p&gt;In render_util.cpp -&gt; render_silhouettes(), crash is occurring somewhere in these lines.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;39      GLenum draw_buffers[2] = {GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1};
(gdb) list
40      glDrawBuffers(2, draw_buffers);
41
42      glClearColor(0.0,0.0,0.0,0.0);
43      glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
44      glEnable(GL_DEPTH_TEST);
45      DBG_GL_ETX();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The dreaded &quot;invalid operation&quot;&lt;/p&gt;

&lt;p&gt;I'm guessing opengl isn't in a valid rendering state yet; we may not have bound the fbo, or forgot to enable a rendering program.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;yep, fbo wasn't bound.&lt;/p&gt;

&lt;p&gt;now running to completion, but result is NaN.&lt;/p&gt;

&lt;p&gt;Probably because we're using random numbers for our curves.  Lets grab some real curve data and re-run.  We'll probably need to visualize the silhouette output soon.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;added function &lt;code&gt;wacv-2012/get_wacv_result.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Code to get curves and send them to server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp = get_wacv_result(2);
tmp = cellfun(@(x) [x; ones(1,size(x,2))], tmp, 'UniformOutput', false);
socket = construct_render_client('localhost', '12345')'
send_curves(socket, tmp);
result = send_curves(socket, tmp);
destroy_render_client(socket)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Got first dumps.  issues&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;all black, no content&lt;/li&gt;
&lt;li&gt;reversed wrong aspect ratio&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Possible causees for 1.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;bad camera files&lt;/li&gt;
&lt;li&gt;bad code for converting intrinsic matrix to opengl&lt;/li&gt;
&lt;li&gt;bad curves?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;2 seems most likely.  Can debug 1 and 2 by trying old camera object and see if they work;  if so, compare matrices with new matrices.&lt;/p&gt;

&lt;h2&gt;Possibly the mods I made to the multi-view likelihoods are the culprit.  I'm using raw matrices now instead of cmara objects in obj.add_view() and Camera_view_wrapper.&lt;/h2&gt;

&lt;p&gt;Added command-line option &quot;--dbg-save-frames&quot;.  Writes all rendered views to disk as &quot;dump_1.png, dump_2.png,...&quot;.  Only the most recent 10 are kept at any time.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;re-running using turntable camera; passing intrinsic and extrinsic matrices to likelihood.&lt;/p&gt;

&lt;p&gt;Renders are still black.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Forgot to set white color?&lt;/p&gt;

&lt;p&gt;No; its in the render_silhouette function.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found something: wacv results were aritficially centered on zero.  fixed.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Did matlab test on modelview and projection matrices and curve points -- z coordinate falls outside of [-1 1].  Beyond far plane.&lt;/p&gt;

&lt;p&gt;reason: not negating z-coordinate in test code.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;changing gears.  running matlab test on original input modelview and projection matrices.&lt;/p&gt;

&lt;p&gt;Modelview looks okay. Projection is way off.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;intrinsic matrix?&lt;/li&gt;
&lt;li&gt;NDC matrix?&lt;/li&gt;
&lt;li&gt;bounds?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Intrinsic looks reasonable.&lt;/p&gt;

&lt;p&gt;probably NDC/bounds issue&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bug in ndc math&lt;/p&gt;

&lt;p&gt;projecting outside from far plane&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;needed to flip axes to handle convention mismatch.&lt;/p&gt;

&lt;p&gt;Now getting something, but the positions are looking weird.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;was re-negating ppo; already flipped during convention resolution.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;now getting decent results, but off-center and smaller:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-12-dump_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;rough original to rendering offsets:  up by 56, left by 115.&lt;/p&gt;

&lt;h2&gt;non-uniform scaling; width is smaller than height.&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;Projecting plant base using
(a) intrinsic and extrinsic matrices from matlab
(b) modelview and projection matrices pulled from opengl

results:
(a) projects to the right place (tested using image and pixelstick)
(b) NDC coordinates look okay.  I manually remapped (scale and offset), and they look okay.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since NDC look okay, the remapping must be wrong.  glViewport issue?  I'm manually setting it, but maybe it needs to be set for each shader?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Or maybe it's a geometry shader issue?  I'm using different modelview and projection matrices now.&lt;/p&gt;

&lt;p&gt;=--&lt;/p&gt;

&lt;p&gt;Found it (Sunday night) -- Somehow the viewport transformation is getting reset. Possibly viewport is a shader-specific state that needs to be set each time?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;now centered, need to diagnose silhouette issues:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-13-dump_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;rename matlab &quot;render_client&quot; code to &quot;likelihood_client&quot;&lt;/li&gt;
&lt;li&gt;rename likelihood_server/sampler2.cpp to something not nonsense.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/09/work-log"/>
   <updated>2013-10-09T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/09/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Spent most of the day finishing coding and compiling likelihood server.&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;build config file and make first run.

&lt;ul&gt;
&lt;li&gt;build camera files&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;write matlab TCP client mex code&lt;/li&gt;
&lt;li&gt;send ground truth model from matlab to server&lt;/li&gt;
&lt;li&gt;debug:

&lt;ul&gt;
&lt;li&gt;check rendering: silhouettes look okay?&lt;/li&gt;
&lt;li&gt;check y-axis flipping issue&lt;/li&gt;
&lt;li&gt;check likelihood vs. perturbed model vs. null model vs. overexpressive model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Issue: stem radius&lt;/h2&gt;

&lt;p&gt;Until now, our model and data have been infinitesimally thin curve (medial axis).&lt;/p&gt;

&lt;p&gt;Need to consider how to add width.  Options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Train?&lt;/li&gt;
&lt;li&gt;Fixed?  (pass as CLI parameter to server)&lt;/li&gt;
&lt;li&gt;Marginalize over?&lt;/li&gt;
&lt;li&gt;Optimize?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Since we're on short time, I'm leaning toward 2 for simplicity.&lt;/p&gt;

&lt;h2&gt;Building config file&lt;/h2&gt;

&lt;p&gt;Done, but need to double-check which dataset we're using in matlab at the moment.&lt;/p&gt;

&lt;h2&gt;Issues&lt;/h2&gt;

&lt;p&gt;When running batches of several datasets, need a way to ensure matlab and likelihood server are running on the same dataset.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/08/work-log"/>
   <updated>2013-10-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Working on compiling render server.&lt;/p&gt;

&lt;p&gt;Do I want to compute likelihood in C++ too?&lt;/p&gt;

&lt;p&gt;Code exists for it.  Maybe best to just re-use code from curve_sampling.cpp and abandon render server.&lt;/p&gt;

&lt;p&gt;Lets do that.&lt;/p&gt;

&lt;h2&gt;Reviewing old curve sampling code&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Pixel store&lt;/li&gt;
&lt;li&gt;renderer&lt;/li&gt;
&lt;li&gt;multi-view likelihood&lt;/li&gt;
&lt;li&gt;pixel likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Pixel likelihood receives a pointer to linear array of reals that represent the pixel matrix.&lt;/p&gt;

&lt;p&gt;Multi-view likelihood receives a renderable and handles all the rendering and pixel-retreival, and dispatches to pixel likelihood objects for computing.  is constructed with one or more cameras.  On evaluation it receives a renderable object, projects it under each camera, retreives the pixels, and passes to a pixel likelihood.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Refactor&lt;/strong&gt;: Separate rendering into a &quot;renderer&quot; object, which can be split into its own process if needed.  It will also make leapfrog pixel-reading simpler.&lt;/p&gt;

&lt;p&gt;End product will&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;receive message from matlab containing curves&lt;/li&gt;
&lt;li&gt;decode curves into vectors&lt;/li&gt;
&lt;li&gt;convert into gl_curves (renderables)&lt;/li&gt;
&lt;li&gt;pass to likelihood object&lt;/li&gt;
&lt;li&gt;convert log-posterior to message&lt;/li&gt;
&lt;li&gt;transmit back to matlab&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;all functionality is already implemented, just need to weed out code rot and put it together.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting weird link errors.  For some reason, the build system is using gcc instead of g++ to link my main program.  Will try re-running kjb_add_makefiles
&lt;strong&gt;Refactor&lt;/strong&gt;: specify all parameters at command-line (width, height, cameras, etc)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Meanwhile, I'm planning the program&lt;/p&gt;

&lt;p&gt;inputs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;* dimensions
* cameras
* likelihood parameters
* 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Steps&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;* initialize likelihood
* initialize TCP server
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Abandoning GL_context from sampler_cpp.  writing one-off offscreen-buffer class.  writing new GL_context struct with just the stuff we need&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Reducing dependence on perspective_camera object in favor of using modelview and projection matrices.  We'll be working with matrices from matlab.&lt;/p&gt;

&lt;p&gt;Updated Camera_render_wrapper to use matrices instead of camera.&lt;/p&gt;

&lt;p&gt;Updated Abstract_mv_likelihood to use matrices in addition to cameras (for backward compatibility).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/03/work-log"/>
   <updated>2013-10-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Thought more on split/merge.  Could make merge a special case of split, and the move becomes a split/split process.  At every step, pick a track, pick a subset of observations from that track, then reassign them all to an existing curve or a new curve.  It's symmetric, which is nice, but the probability of a merge being selected becomes vanishingly small very quickly.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Looking into issues with offline pair-candidate generation.  So many bad candidates are coming up, and their ML values look better than the good candidates.&lt;/p&gt;

&lt;p&gt;A big issue is that we aren't penalizing missing data (gaps/tails), and we don't enforce multiple-view consistency (only two views need to match).&lt;/p&gt;

&lt;p&gt;Option 1: use a stronger likelihood to rule out background curves.  Use per-pixel foreground/background classifier?  Use color consistency metric?  Sample from posterior, project, check pixels, repeat -- gets a monte-carlo of marginal likelihood.&lt;/p&gt;

&lt;p&gt;Option 2: use foreground/background classifier to classify fragments; misusing a background fragment in a foreground pair results in some penalty.&lt;/p&gt;

&lt;p&gt;Option 1 is nice theoretically, but has lots of moving parts (new features, training, monte-carlo issue, need to know how thick to make branches.)  Unlikely to get working in two weeks.  Also isn't clear what the role of the detector curves are.  Are they data?  data-driven proposals?&lt;/p&gt;

&lt;p&gt;Options 2 is a bit weird, but doesn't introduce dimensionality issues.  But also doesn't specifically address the issue of bizzare candidates being introduced.  For example, a bad pair could still be proposed as long as they're both foreground curves.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/02/work-log"/>
   <updated>2013-10-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/02/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Debugging split/merge toy sampler.&lt;/p&gt;

&lt;p&gt;Split from 5 to 6 groups:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; split_alpha = -p_merge + p_split - q_split + q_merge - log(psplitmv) + log(0.5);
 K&amp;gt;&amp;gt; p_merge

 p_merge =

   -1.4437e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;-1.4437e+03 + -1.6060e+03 - -125.6828 + -2.7081&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; K&amp;gt;&amp;gt; p_split

 p_split =

   -1.6060e+03

   K&amp;gt;&amp;gt; q_split

   q_split =

    -125.6828

    K&amp;gt;&amp;gt; q_merge

    q_merge =

       -2.7081

       K&amp;gt;&amp;gt; log(psplitmv)

  ans =

     -0.6931

     K&amp;gt;&amp;gt; log(0.5)

  ans =

     -0.6931

     K&amp;gt;&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Merge from 6 to 5 groups:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;merge_alpha = p_merge - p_split + q_split - q_merge - pmergemv + psplitmv ;
K&amp;gt;&amp;gt; p_merge

p_merge =

  -1.4437e+03

K&amp;gt;&amp;gt; p_split

p_split =

  -1.6060e+03

K&amp;gt;&amp;gt; q_split

q_split =

 -208.8604

K&amp;gt;&amp;gt; q_merge

q_merge =

   -2.7081

K&amp;gt;&amp;gt; pmergemv

pmergemv =

   -0.6931

K&amp;gt;&amp;gt; psplitmv

psplitmv =

   -0.6931
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that q_split doesn't match between the two moves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After reflection, this shouldn't match, because we're comparing two different cases.  Specifically, the post-split results in two small groups, where as the pre-merge model has 6 groups of equal size.  Smaller groups have smaller number of ways to split, so the proposal probability is higher.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Killed myself today thinking about this problem and getting exactly nowhere.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/30/work-log"/>
   <updated>2013-09-30T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/30/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Have some problems with offline correspondence pair_matching.&lt;/p&gt;

&lt;p&gt;Is it a bug introduced in the reconstruction code?  Lots of refactors there, recently.&lt;/p&gt;

&lt;p&gt;Test by re-running wacv results.&lt;/p&gt;

&lt;h2&gt;WACV Reconstruction (revisited)&lt;/h2&gt;

&lt;p&gt;Now that datasets six through eleven have been re-ground-truthed, should confirm them by running wacv reconstruction code on them.  Also this should confirm whether we've introduced any reconstruction bugs in the last week or two.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Problem with dataset six:  curve #4 isn't connected to its parent.&lt;/p&gt;

&lt;p&gt;Problem with dataset 1: really crooked.  Inspecting the ground-truth doesn't show any obvious problems that would cause this.  Maybe reversed curves? (which should be handled, but maybe aren't)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset1.png&quot; alt=&quot;Dataset 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dataset 6: base curve attaches from wrong side (red curve below):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset6.png&quot; alt=&quot;Dataset 6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dataset 7: Wacky top curves, missing connections at top, no base curves:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset7.png&quot; alt=&quot;Dataset 7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dataset 8: missed connections, bad top curves&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset8.png&quot; alt=&quot;Dataset 8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dataset 9: problems...&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset9.png&quot; alt=&quot;Dataset 9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Datset 10:  Good!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset10.png&quot; alt=&quot;Dataset 10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Datset 11: Bad top-curve associations&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset11.png&quot; alt=&quot;Dataset 11&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Revenge of the Pre-curves&lt;/h2&gt;

&lt;p&gt;When investigating issues from dataset 6, discovered that results improved when ensuring that lateral curves don't go past the parent curve.  For example&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   |   /      |   /     |   /
   |  /       |  /      |  /
   | /        |_/      _|_/
   |          |         |
   |          |         |
   |          |         |
 Good       Best       Bad
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Priors and proposal distributions&lt;/h2&gt;

&lt;p&gt;Thinking about split proposal distributions.&lt;/p&gt;

&lt;p&gt;I've been thinking about the combinitorics of partitions and realized I'v been counting wrong.  Given M curves and N observations, I was counting the number of partitions as \(M&lt;sup&gt;N\).&lt;/sup&gt;  This doesn't account for the fact that order of cluster id's doesn't matter, so a better count would be \(M&lt;sup&gt;N&lt;/sup&gt; / N! \), which divides by the number of permutations of the cluster id's.    However, this is still wrong, because it counts assignments with empty clusters (which is indistinquishible from a model with N-1 clusters.  The real number of partitions is given by &lt;a href=&quot;http://en.wikipedia.org/wiki/Stirling_number_of_the_second_kind&quot;&gt;Stirling numbers of the second kind&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The same goes for the partitioning that occurs during a split move.&lt;/p&gt;

&lt;p&gt;Implemented &lt;code&gt;tools/build_stirling2.m&lt;/code&gt;, which builds an NxN matrix of the log of Stirling numbers of the second kind.&lt;/p&gt;

&lt;p&gt;Want to know that probability of accepting a merge move in various configurations, assuming the likelihood is constant.  Here's the test code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% number of observations: 918
% number of tracks: n
% number of observations in track i: k_i

p_merged  = @(n,k) -sm(918,n-1);
p_split = @(n,k) -sm(918,n);
q_split = @(n,k) log(1/(n-1)) - sm(k, 2);
q_merge = @(n,k) -log(nchoosek(n, 2));
alpha = @(n,k) p_merged(n,k) - p_split(n,k) + q_split(n,k) - q_merge(n,k);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running in several scenarios from plant-modelling problem:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% small number of curves; L,M,S number of observations
&amp;gt;&amp;gt; alpha(5, 50)

ans =

  169.0363

&amp;gt;&amp;gt; alpha(5, 20)

ans =

  189.8307

&amp;gt;&amp;gt;  alpha(5, 5)

ans =

  200.2747

% medium number of curves; L,M,S number of observations
&amp;gt;&amp;gt; alpha(10, 50)

ans =

   60.8395

&amp;gt;&amp;gt; alpha(10, 20)

ans =

   81.6339

&amp;gt;&amp;gt; alpha(10, 5)

ans =

   92.0779

% huge number of curves; L,M,S number of observations
&amp;gt;&amp;gt; alpha(100, 50)

ans =

  -26.7355

&amp;gt;&amp;gt; alpha(100, 20)

ans =

   -5.9411

&amp;gt;&amp;gt; alpha(100, 5)

ans =

    4.5029
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: These numbers are slightly off, due to a bug in the stirling number computation (now fixed).  The resulting alphas aren't substantially different.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In most realistic-model scenarios, the probability of accepting a merge move (ignoring the likelihood; assuming constant) is 100% (i.e. positive alpha).&lt;/p&gt;

&lt;p&gt;When the number of curves is huge (e.g. early in the sampler, when we need merges the most), the probability of acceptance is non-negligible only if the number of observations in the resulting curve is relatively small.  This is usually the case, and continuously merging short curves will result the acceptance probability getting larger and larger.&lt;/p&gt;

&lt;p&gt;It is interesting that in the presence of a constant likelihood, the sampler would ever prever fewer tracks to more.&lt;/p&gt;

&lt;p&gt;In these cases, the sampler is preferring to split up curves with abnormally large number of observations (compared to other pertitions).&lt;/p&gt;

&lt;p&gt;Would it be interesting to sample over partitions and see how the partition numbers and sizes evolve over time?  Heck yes!&lt;/p&gt;

&lt;h1&gt;Toy problem: sampling partitions&lt;/h1&gt;

&lt;p&gt;Prior: Uniform over number of groups.  Given a group, uniform over partitions.&lt;/p&gt;

&lt;p&gt;Likelihood: uniform.  For simplicity, I'd like the sampler's exploration to not be influenced by which split or merge we pick.&lt;/p&gt;

&lt;p&gt;We implement a split-merge move.  When splitting, a group is picked at random, and its members are randomly split into two groups.  On merge, two groups are picked at random and merged.&lt;/p&gt;

&lt;p&gt;The prior of a model with N observations and k groups is \(1/s(N,k)\) where s the number of ways to partition N into k groups.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;When initialized with a single monolithic group (k=1), the sampler jumps between k=1 and k=2, rejecting about 50% of the time.  &lt;strong&gt;The sampler never jumps to k=3.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a problem, because we'd expect to spend the same amount of time in the k=3 state as any other, since the target distribution is uniform over number of tracks.&lt;/p&gt;

&lt;p&gt;Either this is a bug, a flaw in my assumptions, or &quot;simply&quot; a mixing problem (a huge practical issue, but theoretically still correct).&lt;/p&gt;

&lt;p&gt;After exhausting myself bug-hunting, I decided to explore the mixing issue.&lt;/p&gt;

&lt;p&gt;Instead of initializing to k=1, I initialized to k=3.  Both the split and merge move have an acceptance probability of exp(-51.0), meaning it accept once in every 7.1e23 samples.  This sampler &lt;strong&gt;definitely has bad mixing&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Aside&lt;/em&gt;. It seemed notable that the split and merge move had the exact same acceptance probability for k=3.  At first this seems sensible, since the sampler should spend as much time in a two-group model as in a four-group. It turns out not to be true for any other \(k \neq 3\) I tried, but all exhibited poor mixing.  In fact, this asymmetry is natural, as the cardinality of the split proposals doesn't grow at exactly the same rate as the cardinality of the data-partitions.  This is balanced by the fact that you might transition to a lower model less often than the higher model, but you might spend more time in the lower model once you're there.&lt;/p&gt;

&lt;p&gt;Note that although we've used a uniform likelihood, this mixing problem likely can't be fixed by using a different likelihood.  The likelihood could drive the sampler in a specific direction, but the reverse direction would be even harder to explore.  So the likelihood only makes matters worse.&lt;/p&gt;

&lt;p&gt;Q: Can I compute the expected length spent in each state?&lt;/p&gt;

&lt;p&gt;A: yes.  1/p_leave, where p_leave = (0.5 * split_alpha + 0.5 * merge_alpha)&lt;/p&gt;

&lt;p&gt;Q: Can I compute the expected direction of transition between states?
A: yes.  split_alpha / (split_alpha + merge_alpha);&lt;/p&gt;

&lt;p&gt;Q: Can I compute the expected ratio between an N-group model and an N+1 group model?
A: Do MCMC sampling.  When you reach a state, add its expected number of rejected samples to its tally, then transition up or down according to the direction distribution.&lt;/p&gt;

&lt;h2&gt;New toy sampler: Expected ratio between model probabilities&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% compute expected samples
% compute up probability
% (state, tallies)
% random walk between states, 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Fix dataset six: connect curve 4 to parent&lt;/li&gt;
&lt;li&gt;Add confirmation code to check for multiple connected components in ground truth.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/28/work-log"/>
   <updated>2013-09-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/28/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finished re-doing Liu's ground-truth.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/26/work-log"/>
   <updated>2013-09-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Problem with revamped index-estimation code:  it fails miserably on poorly-triangulatable hypotheses.&lt;/p&gt;

&lt;p&gt;The heuristic triangulation explodes in size, probably because triangulated points are all over the map (need to confirm this).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Adding error codes; if any part of the pipeline can't complete for any reason, set error code and return.  On error, the MCMC move will simply reject the proposal.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Issue with &lt;code&gt;build_full_correspondence&lt;/code&gt;: input track is destroyed and replaced.&lt;/p&gt;

&lt;p&gt;Refactored to replace make_correspondence with init_track.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make_correspondence -&amp;gt; init_track
make_trivial_correspondence -&amp;gt; init_trivial_corespondence
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Running &lt;code&gt;offline_pair_candidates_2&lt;/code&gt;.  Seems to be running without crashing.&lt;/p&gt;

&lt;p&gt;Takes about 10 minutes;  seems fast considering all the new logic we've added (re-triangulation, 2D DTW). Probably the mex'd DTW is helping.  Some profiling will likely identify some low-hanging optimization fruit here, too.&lt;/p&gt;

&lt;p&gt;Need to inspect triangulated curves when done.&lt;/p&gt;

&lt;p&gt;Need to see if background subtraction classifier will help prune.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, spoke too soon.  Crash at about 75% completion.&lt;/p&gt;

&lt;p&gt;Random bug in candidate-proposal generator.  wasn't handling the &quot;no candidates&quot; case, resulted in index-out-of-bounds error.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>todo</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/26/todo"/>
   <updated>2013-09-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/26/todo</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;TESTS

&lt;ul&gt;
&lt;li&gt;Three levels of branching

&lt;ul&gt;
&lt;li&gt;visual test: reconstruction&lt;/li&gt;
&lt;li&gt;numeric test: fast ML vs reference&lt;/li&gt;
&lt;li&gt;numeric test: fast Covariance matrix vs. reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finish ground truthing&lt;/li&gt;
&lt;li&gt;implement recursive update&lt;/li&gt;
&lt;li&gt;end-to-end sampler

&lt;ul&gt;
&lt;li&gt;test birth/death move&lt;/li&gt;
&lt;li&gt;finish swap move&lt;/li&gt;
&lt;li&gt;implement attachment moves&lt;/li&gt;
&lt;li&gt;re-order move (see &lt;a href=&quot;/ksimek/research/2013/09/24/todo/&quot;&gt;here&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;evaluation&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Track stages</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/26/reference"/>
   <updated>2013-09-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/26/reference</id>
   <content type="html">&lt;p&gt;A track's &quot;stage&quot; is a description of where it is in the processing pipeline.&lt;/p&gt;

&lt;p&gt;Each stage has one or more track fields associated with it.  No stage may modify fields from the previous stages.&lt;/p&gt;

&lt;h2&gt;Stage 0&lt;/h2&gt;

&lt;p&gt;Track is ready to be processed.   At this stage, the only valid fields are those set by the user, namely:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Track.assoc
Track.reversed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To construct a stage-zero track:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;init_track
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 1&lt;/h2&gt;

&lt;p&gt;A stage 1 track has passed the correspondence stage.&lt;/p&gt;

&lt;p&gt;Three functions can prepare a track for stage 1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;init_trivial_track
merge_correspondence
build_full_correspondence
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated fields are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;corr
means
precisions
cov_error
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 2:&lt;/h2&gt;

&lt;p&gt;In stage 2, raw curve likelihood fields have been constructed, and are ready for post-processing.&lt;/p&gt;

&lt;p&gt;The transition from stage 1 to stage 2 is relatively expensive, as every point requires backprojection, a pass of dynamic time warping, and a few iterations newton's method to find the index set.&lt;/p&gt;

&lt;p&gt;Associated fields&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ll_means
ll_precisions
ll_distances
sm_lambda
curve_sm
curve_sm_t
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated functions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;corr_to_likelihood
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 3&lt;/h2&gt;

&lt;p&gt;Stage three consists of inexpensive post-processing of the likelihood fields.  Curve reversal is handled here; flattening and sorting is lumped into this stage too.  Reversing and re-evaluating curves is a common use-case, and this can be done efficiently by keeping stage 3 separate from stage 2.&lt;/p&gt;

&lt;p&gt;This is also where the likelihood covariance blocks are computed; since this is somewhat time-costly, it may be moved into stage 2 in the future.&lt;/p&gt;

&lt;p&gt;Associated fields&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ll_views_flat
ll_means_flat
ll_precisions_flat
ll_distances_flat
ll_S
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated functions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flatten_sort_and_reverse
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 4: Attachment&lt;/h2&gt;

&lt;p&gt;Stage 4 is where topology is handled.  The predictive distribution of the branch point is computed and stored for efficient computation of the marginal likelihood later.&lt;/p&gt;

&lt;p&gt;Stage 4 needs to be applied recursively to all children.&lt;/p&gt;

&lt;p&gt;Associated fields&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;parent_ci
start_index
prior_K
branch_distance
mu_b
Sigma_b
branch_mu
branch_K
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated functions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;attach
detach
att_set_branch_distance (called from attach)
att_set_start_index (called from attach/detach)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 5&lt;/h2&gt;

&lt;p&gt;Marginal likelihood has been computed for this track, conditioned on its parent.&lt;/p&gt;

&lt;p&gt;Note: in the current implementation, ml field is not set, and curve_ml simply returns the ml value.  This will change in the near future to comply with the multi-stage model described in this post.&lt;/p&gt;

&lt;p&gt;Associated fields&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated function&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curve_ml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Running all stages&lt;/h2&gt;

&lt;p&gt;In many cases, it's not necessary to construct tracks from scratch.   Reversing a curve only requires re-running stages 3 through 5.  Attaching or detaching a curve only requires re-running Stages 4 and 5.&lt;/p&gt;

&lt;p&gt;However, some cases require a full end-to-end running of stages 1 through 5.  The function that does this is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build_track
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function is also a nice reference of how to run each stage.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/25/work-log"/>
   <updated>2013-09-25T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/25/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Coding swap and birth/death moves.&lt;/p&gt;

&lt;p&gt;Lots of refactoring.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;new function &lt;code&gt;attachment/detach.m&lt;/code&gt;; moved some code from attach to detach function.&lt;/li&gt;
&lt;li&gt;Reworked &lt;code&gt;kernel/&lt;/code&gt; directory.

&lt;ul&gt;
&lt;li&gt;added function: &lt;code&gt;kernel/get_base_covariance.m&lt;/code&gt;.  Used to implement detach.&lt;/li&gt;
&lt;li&gt;new function: &lt;code&gt;kernel/get_model_temporal_kernel.m&lt;/code&gt;.  Returns ind, ou, or sqexp kernel&lt;/li&gt;
&lt;li&gt;Eliminated &lt;code&gt;kernel/*_perturb_model.m&lt;/code&gt;.  &lt;code&gt;Get_model_kernel&lt;/code&gt; now constructs these directly using &lt;code&gt;get_model_temporal_kernel&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Reworked &lt;code&gt;kernel/get_model_kernel.m&lt;/code&gt; to use get_model_temporal_kernel.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Birth/Death Move&lt;/h2&gt;

&lt;p&gt;Implemented birth/death move.  New concept: birth and death candidate sets; on birth, candidate is moved from one to the other.&lt;/p&gt;

&lt;h2&gt;Migrating legacy sampling code&lt;/h2&gt;

&lt;p&gt;Reworking old sampler code&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;sampling context&quot; object; (see &lt;code&gt;inference/ctx_*&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;offline construction of curve-pair (see &lt;code&gt;inference/offline_*&lt;/code&gt;);&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Refactoring Tracks (previously Corrs)&lt;/h2&gt;

&lt;p&gt;Continuing to update the codebase to use the new &quot;Tracks&quot; structure instead  of &quot;Corrs&quot;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;New function &lt;code&gt;correspondence/build_empty_curveset.m&lt;/code&gt;.  This is useful in two ways: (a) constructing an initial object before sampling begins, and (b) constructing a single track, which is now defined as a trackset of length 1.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;To be continued...&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;need to test &lt;code&gt;mcmc_birth_death.m&lt;/code&gt;.

&lt;ul&gt;
&lt;li&gt;need to construct context &lt;code&gt;ctx_init.m&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;need to construct offline pair candidates: &lt;code&gt;offline_pair_candidates_2&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Current task: constructing offline pair candidates.  Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data_2 = offline_pair_candidates_2(data_, params, 0, 1, 3);
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/24/work-log"/>
   <updated>2013-09-24T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/24/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Struggling with re-assigning associations.&lt;/p&gt;

&lt;p&gt;Yesterday, I was planning to re-sample attachments after every change to association.  The bookkeeping necessary to ensure reversibility of this is proving to be a nightmare, and it's questionable whether it actually improves results.  The counter-argument is that MH moves should be cheap to propose, but re-sampling attachments has significant cost.  All that cost is wasted if the proposed move is a bad one, which it often will be.  Also, most curves only change slightly when association changes, so changing the branching is probably a bad idea.&lt;/p&gt;

&lt;p&gt;Two special cases need to be handled.  First is when a change to association causes a curve to become shorter, and an attachment point disappears.  In this case, we allow it, but during evaluation, clamp to the endpoint.&lt;/p&gt;

&lt;p&gt;However, when birth/death occurs, re-doing attachment is absolutely necessary.  In this case, we take a simpler apprach than yesterday's strategy:  on death, child curves become root curves; on birth, new curve gathers children from root curves.  Or alternatively, birth/death is illegal if curve has children.&lt;/p&gt;

&lt;p&gt;The preceeding dicusson about avoiding re-attachment is only valid for re-assignment moves.  For split/merge moves, we must update attachment.  But this is easy too: on merge, child sets are merged; on split, sample A vs. B based on distance.&lt;/p&gt;

&lt;h2&gt;Approach #2&lt;/h2&gt;

&lt;p&gt;When re-assigning associations, if it results in a dangling curve, reject&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>todo</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/24/todo"/>
   <updated>2013-09-24T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/24/todo</id>
   <content type="html">&lt;h2&gt;Track order issue&lt;/h2&gt;

&lt;p&gt;There are situations where ties between tracks are broken by giving priority to the track with lower index. For this reason, we should have a move where tracks randomly change order.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;p&gt;Quick&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;test three-levels of branching

&lt;ul&gt;
&lt;li&gt;reconstruction&lt;/li&gt;
&lt;li&gt;ML vs. reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cleanup Corr fields

&lt;ul&gt;
&lt;li&gt;group fields by processing stage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;map-out processing pipeline&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Medium&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;finish ground-truthing (Friday night, Saturday)&lt;/li&gt;
&lt;li&gt;implement recursive update&lt;/li&gt;
&lt;li&gt;code for inferring branching parameters.&lt;/li&gt;
&lt;li&gt;Finish training

&lt;ul&gt;
&lt;li&gt;infer branching parameters&lt;/li&gt;
&lt;li&gt;re-write training ML&lt;/li&gt;
&lt;li&gt;re-train prior parameters with full ML

&lt;ul&gt;
&lt;li&gt;jointly train FG and BG using same&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MCMCDA sampler

&lt;ul&gt;
&lt;li&gt;association moves&lt;/li&gt;
&lt;li&gt;split/merge moves&lt;/li&gt;
&lt;li&gt;re-order moves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;evaluation code&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/23/work-log"/>
   <updated>2013-09-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/23/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Updating data structures to Version 2&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;* Refactored several variable names, some functions may need updating.
* Corrs was changed from cell-of-structs to a cell-array.  Some functions may still need updating.
* Removed old, unused variables
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Thinking on sampling&lt;/h2&gt;

&lt;p&gt;Did a writeup on MCMCDA with attachments, see associated &quot;strategy&quot; entry&lt;/p&gt;

&lt;h2&gt;Thinking on priors&lt;/h2&gt;

&lt;p&gt;Did a writeup on association priors, see associated &quot;reference&quot; entry&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Split/Merge moves and Association Priors</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/23/splitmerge-moves-and-association-priors"/>
   <updated>2013-09-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/23/splitmerge-moves-and-association-priors</id>
   <content type="html">&lt;p&gt;Thinking again about split/merge moves, and the problem of the exploding number of split moves as the nubmer of observations increases.&lt;/p&gt;

&lt;p&gt;Part of the problem is the fact that we're treating all associations as equal under the prior.  This has the side-effect that the prior strongly prefers more curves in a model as opposed to fewer.  Consider a scenario with N observations, and M curves.  There are M&lt;sup&gt;N&lt;/sup&gt; number of ways to assign these observations to M curves.  Compare this with a model with one curve -- there's only one possible assigment.  Since all assignments are equal under the prior, the model with M curves is more favored by the prior by a factor of M&lt;sup&gt;N.&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;For this reason, I propose that a more sensible prior is one that is uniform over the number of curves in the scene.  Then, given the number of curves, the prior over associations is uniform.  In other words, given an association, it's prior should be 1/M&lt;sup&gt;N,&lt;/sup&gt; where M is the number of curves represented in the association.&lt;/p&gt;

&lt;p&gt;When running a split move, the number of ways to split is 2&lt;sup&gt;K,&lt;/sup&gt; where K is the number of observations associated with the original curve.  If chosen uniformly, the Metropolis-Hastings proposal probability is 2&lt;sup&gt;K.&lt;/sup&gt;  This should cancel nicely with the prior term in the MH acceptance ratio.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>MCMCDA Sampling with Attachment</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/23/mcmcda-sampling-with-attachment"/>
   <updated>2013-09-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/23/mcmcda-sampling-with-attachment</id>
   <content type="html">&lt;p&gt;The old MCMCDA gibbs sampler runs into trouble now that we allow attachment.&lt;/p&gt;

&lt;p&gt;Previously, for any observation, we would compute the ML of grouping it with each candidate, and select a candidate from those weights.&lt;/p&gt;

&lt;p&gt;The problem now is that regrouping an observation sometimes causes it's old group to vanish, and the question of what to do with the group's children is unanswered.  The same problem occurs if a regrouping makes a curve shorter, and children were attached to the now-missing structure.&lt;/p&gt;

&lt;h2&gt;Metropolis Hastings&lt;/h2&gt;

&lt;p&gt;Things become much easier if we move away from Gibbs sampling.  Gibbs was nice for a proof of concept, because we never failed to pick embarrassingly good candidates, and we almost always moved toward a better result with each step.  However, it was very expensive, and now we have a question of how to compute the ML of a candidate whose attachment property is unknown.&lt;/p&gt;

&lt;p&gt;So, we'll move to Metropolis Hastings.  We resolve the attachment issue by using a simple rule:  every time a track's association changes, we re-sample its the attachment and the attachment of its former children.&lt;/p&gt;

&lt;h2&gt;Sampling Associations.&lt;/h2&gt;

&lt;p&gt;When sampling associations, we'll use two types of moves:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;swap&lt;/strong&gt;: re-assign a single observation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;split/merge&lt;/strong&gt; re-assign a group of observations&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The swap move will be similar to the existing Gibbs move, except we'll choose from candidates uniformly at random.  Split/merge proposals will be naive; we'll incorporate an association prior to counter the exploding number of split moves.&lt;/p&gt;

&lt;h2&gt;Re-sampling attachment&lt;/h2&gt;

&lt;p&gt;After sampling a new association, we'll re-sample attachment.  For this, we'll introduce a new function &lt;code&gt;sample_attachment&lt;/code&gt;, which will be responsible for constructing a list of reasonable candidates and attachment parameters (Start point and branch position), and selecting one at random (probably based on geometry, i.e. not uniform).  It returns the sampled attachment, along with the proposal probability.  It can optionally receive a &lt;code&gt;hint&lt;/code&gt; parameter, which is a list of attachment candidates, along with a guess as to the attachment parameters.  The candidate of &quot;no attachment&quot; should always be an option.&lt;/p&gt;

&lt;p&gt;We'll also need a function that returns the probabiliy of selecting a specific attachment, according to &lt;code&gt;sample_attachment&lt;/code&gt;, for computing the reverse move.&lt;/p&gt;

&lt;h2&gt;Topology moves&lt;/h2&gt;

&lt;p&gt;In addition to association moves, we'll also have topology moves.  This will consist of simply calling &lt;code&gt;sample_attachment&lt;/code&gt; without a sampling an association beforehand.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Branching ML Done</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/20/branching-ml-done"/>
   <updated>2013-09-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/20/branching-ml-done</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finally finished implementing and confirming the multi-view branching ML.  Next steps:&lt;/p&gt;

&lt;p&gt;Quick&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;re-run tests with non-zero start index&lt;/li&gt;
&lt;li&gt;test three-levels of branching

&lt;ul&gt;
&lt;li&gt;reconstruction&lt;/li&gt;
&lt;li&gt;ML vs. reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cleanup Corr fields

&lt;ul&gt;
&lt;li&gt;eliminate clean_ fields&lt;/li&gt;
&lt;li&gt;group fields by processing stage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;map-out processing pipeline&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Medium&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;finish ground-truthing (Friday night, Saturday)&lt;/li&gt;
&lt;li&gt;implement recursive update&lt;/li&gt;
&lt;li&gt;code for inferring branching parameters.&lt;/li&gt;
&lt;li&gt;Finish training

&lt;ul&gt;
&lt;li&gt;infer branching parameters&lt;/li&gt;
&lt;li&gt;re-write training ML&lt;/li&gt;
&lt;li&gt;re-train prior parameters with full ML

&lt;ul&gt;
&lt;li&gt;jointly train FG and BG using same&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gibbs sampler&lt;/li&gt;
&lt;li&gt;evaluation code&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Reach&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;split-merge moves&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>refactoring; dependencies</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/19/work-log"/>
   <updated>2013-09-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;investigate difference between ML implementations&lt;/li&gt;
&lt;li&gt;handle nonzero start-index in branching. re-run confirmation&lt;/li&gt;
&lt;li&gt;ground truth - trace datasets 7 through 10&lt;/li&gt;
&lt;li&gt;retrain using attachment&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Invstigating consistent ML implementations&lt;/h1&gt;

&lt;p&gt;Discovered issue:  was calling &lt;code&gt;curve_tree_ml&lt;/code&gt; instead of &lt;code&gt;curve_tree_ml_2&lt;/code&gt;.  Working now&lt;/p&gt;

&lt;h2&gt;Aside: common mistakeis&lt;/h2&gt;

&lt;p&gt;Losing a too much time to stupid organizational mistakes --&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;indexing,&lt;/li&gt;
&lt;li&gt;passing wrong copies of variables to code,&lt;/li&gt;
&lt;li&gt;leaving random debugging code in miscellaneous functions,&lt;/li&gt;
&lt;li&gt;running wrong debugging branch in code,&lt;/li&gt;
&lt;li&gt;running wrong version of function.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Code has gotten complex enough that managing all of these little sharp edges is too expensive.  Need to start being more disciplined.  Solutions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;indexing&lt;/strong&gt;: use functions for common indexing tasks

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;one_d_to_three_d&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;to_block_index&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;miscellaneous variables&lt;/strong&gt;: clean-up workspace every evening&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Debugging cruft&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;if code is an in-line test, refactor questionable code into functions; write a real test.&lt;/li&gt;
&lt;li&gt;if code is a &lt;code&gt;plot&lt;/code&gt;, &lt;code&gt;imagesc&lt;/code&gt;, or &lt;code&gt;printf&lt;/code&gt;, always wrap in a DEBUGGING block, even if its a two-minute test.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wrong debugging branch&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;if using &lt;code&gt;method = 1&lt;/code&gt; give names like &lt;code&gt;method = MARKOV&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;display warnings for non-standard methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;wrong version of function&lt;/strong&gt;: depends on future of old code

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;deprecation&lt;/strong&gt;: add block to old reference code: &lt;code&gt;fprintf('running legacy code (press enter)'); pause;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reference implementation&lt;/strong&gt;: rename to *_ref.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Discussion: start_index and branch_index&lt;/h1&gt;

&lt;p&gt;In previous days, I was torn over whether I should add start_index to branch_index before evaluating.&lt;/p&gt;

&lt;p&gt;The answer is yes, because it greatly simplifies the optimization of start_index, because you don't have to re-adjust the children's branch_indices.&lt;/p&gt;

&lt;p&gt;how is markov blanket determined (pre or post offset indexing?)&lt;/p&gt;

&lt;p&gt;Do we need to recurse after attach?&lt;/p&gt;

&lt;p&gt;RAW VALUES should be stored, not derived values.  branch_index is a raw value.&lt;/p&gt;

&lt;p&gt;*Example: * C is attached to B.  We want to attach B to A, with start index of 10.  Assume branch_index is stored relative to the zero-index (as opposed to the first observed point).  After attaching, we need to update C's branch point; if we later detach B from A, B's branch point need to be updated again.  There's potential for drift to accumulate after all of these upates.&lt;/p&gt;

&lt;p&gt;To avoid confusion, replace branch_index to branch_distance; will convert to an index value before computing. Also &lt;code&gt;prior_indices&lt;/code&gt; needs updating?  Or just eliminate&lt;/p&gt;

&lt;h2&gt;recursive Updating after attachment&lt;/h2&gt;

&lt;p&gt;Consider attachment:&lt;/p&gt;

&lt;p&gt;Before&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(D -&amp;gt; C -&amp;gt; B)    (A)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(D -&amp;gt; C -&amp;gt; B -&amp;gt; A) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visually,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;             A   B C D         A B C D 
             |   | | |         | | | |
             |   | |/          | | |/
             |   | +           | | +  
             |   |/            | |/ 
             |   +       ==&amp;gt;   | +
             |                 |/         
             |                 +      
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;After attaching B to A,

&lt;ul&gt;
&lt;li&gt;C's branch index changes, so...

&lt;ul&gt;
&lt;li&gt;C's branch distribution changes, so...

&lt;ul&gt;
&lt;li&gt;D's branch distribution changes so...

&lt;ul&gt;
&lt;li&gt;D's ML changes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;C's ML changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;C's prior_K doesnt change.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As far as stored fields go, it looks like branch distributions change recursively after attachment, etc.&lt;/p&gt;

&lt;h1&gt;Dependencies&lt;/h1&gt;

&lt;p&gt;Did some thinking about dependencies; what needs to be updated when parents are changed.  Scan of the notes are available below.  Also did &lt;a href=&quot;/ksimek/research/reference/2013/09/19/dependencies/&quot;&gt;a reference writeup&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/ksimek/research/img/2013-09-19-dependency_notes.jpg&quot;&gt;Dependency hand-written notes&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;Refactoring&lt;/h1&gt;

&lt;p&gt;Changed &lt;code&gt;Corr.branch_index&lt;/code&gt; to &lt;code&gt;Corr.branch_distance&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Replaced all instances of &lt;code&gt;Corrs(i).branch_index&lt;/code&gt; with &lt;code&gt;get_branch_index(Corrs, i)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Replaced Corr.ll_indices* with Corr.ll_distances.&lt;/p&gt;

&lt;p&gt;removed Corr.prior_indices&lt;/p&gt;

&lt;p&gt;TODO: replace kernel(XX,YY) with eval_kernel(asdf)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;see att_set_start_index_2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;TODO: replace Corrs cell array with Corrs structure array&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;p&gt;Minor&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;re-run tests with non-zero start index&lt;/li&gt;
&lt;li&gt;test three-levels of branching

&lt;ul&gt;
&lt;li&gt;reconstruction&lt;/li&gt;
&lt;li&gt;ML vs. reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cleanup Corr fields

&lt;ul&gt;
&lt;li&gt;eliminate clean_ fields&lt;/li&gt;
&lt;li&gt;group fields by processing stage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;map-out processing pipeline&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Medium&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;finish ground-truthing (Friday night, Saturday)&lt;/li&gt;
&lt;li&gt;implement recursive update&lt;/li&gt;
&lt;li&gt;code for inferring branching parameters.&lt;/li&gt;
&lt;li&gt;Finish training

&lt;ul&gt;
&lt;li&gt;infer branching parameters&lt;/li&gt;
&lt;li&gt;re-write training ML&lt;/li&gt;
&lt;li&gt;re-train prior parameters with full ML&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MCMCDA sampler&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Summary of Dependency relationships</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/19/dependencies"/>
   <updated>2013-09-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/19/dependencies</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;The dependency between individual variables is a bit complicated, but if we group variables together, the relationship between the groups is simple.  We group parameters into &quot;self&quot; parameters and &quot;inherited&quot; parameters.&lt;/p&gt;

&lt;p&gt;Self parameters are immune to changes to parents or children.
Inherited parameters are affected by any changes to parent.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SELF PARAMETERS

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;reconstruction fields&lt;/em&gt;: assoc, corr, ll_*,&lt;/li&gt;
&lt;li&gt;start index&lt;/li&gt;
&lt;li&gt;prior_K&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;INHERITED PARAMETERS

&lt;ul&gt;
&lt;li&gt;branch distance, branch_index&lt;/li&gt;
&lt;li&gt;&lt;em&gt;branch distribution&lt;/em&gt;:  mu_b, mu_Sigma, branch_mu, branch_Sigma&lt;/li&gt;
&lt;li&gt;Marginal likelihood.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This can be summarized as follows:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Any time a curve is changed in any way, we must recursively update all inherited parameters.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This &quot;if anything changes, update everything&quot; rule is a bit broad, and we can use a finer-grained definition to update fewer inherited fields, but in general, we only avoid updating fields that are inexpensive to update anyway.  For heavy-wieght fields (e.g. branch distribution and ML), they are affected by everything, so we're forced to update them after every change.  Thus, the simpler rule is only nominally less efficeint, and much easier to implement and understand.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Testing full-tree covariance matrix</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/18/work-log"/>
   <updated>2013-09-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/18/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Running covariance confirmation for model_type = 3.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Results don't match.&lt;/p&gt;

&lt;p&gt;Off-diagonals are okay.&lt;/p&gt;

&lt;p&gt;on-diagonal's are larger in reference impl; on order of 10.&lt;/p&gt;

&lt;p&gt;Reference implementation has all positive eigenvalues.  Likely bug is in testing implementation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Open question: Does branch_index need to be incremented by start_index?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;also, is prior_K including start_index?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Root curve is block incorrect, which means we can focus on either &lt;code&gt;build_root_object&lt;/code&gt; or &lt;code&gt;build_sibling block&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Got it&lt;/strong&gt;: prior_K was computed with model_type = 1.  It contains no perturb covariance.  But root block does contain perturb covariance.&lt;/p&gt;

&lt;p&gt;Re-attach using:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wacv_Corrs_ratt = wacv_Corrs;
for i = 1:numel(wacv_Corrs_reatt)
    C = wacv_Corrs_reatt{i};
    wacv_Corrs_reatt = attach_2(wacv_Corrs_reatt, i, C.parent_ci, C.start_index, C.branch_index,params);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now results are identical for the testing set&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;results differ on the full set.&lt;/p&gt;

&lt;p&gt;in testing set, all branches occured from the zero point on the parent.  the full set has branches from other points, and we're getting different results.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found the cause of the problem: it was leftover cruft code I added during debugging.  Caused non-symmetric matrix.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;TODO: Move &quot;confirm full covariance against reference implementation&quot; into its own test folder&lt;/p&gt;

&lt;p&gt;TODO: handle nonzero start-index in branching. re-run confirmation&lt;/p&gt;

&lt;p&gt;TODO: finish &quot;confirm curve-tree ML against reference implementation&quot;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Struggling with curve_ml5, where Chol is failing.  Looks like the problem is in att_Set_Branch_index; branch conditional covariance is not positive definite.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found: indexing bug in &lt;code&gt;attachment/attach_2.m&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Comparing three methods:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% method 1
ml1 = curve_tree_ml_ref(wacv_Corrs_reatt, params, data_)
% method 2
ml2 = curve_tree_ml(wacv_Corrs_reatt, params, data_)
% method 3
ml3 = 0;
for i = 1:numel(wacv_Corrs_reatt)
    ml = ml + curve_ml5( ...
            wacv_Corrs_reatt{i}, ...
            data_, ...
            params, ...
            get_model_kernel(params, params.model_type), ...
            params.ml_block_size, ...
            params.ml_markov_order);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All three currently differ significantly.&lt;/p&gt;

&lt;p&gt;Also compared against an indendent model, whose ML was about 300 points lower.&lt;/p&gt;

&lt;p&gt;At this point, method 2 and method 3 should be giving the same results, but aren't.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Moved covariance matrix test to &lt;code&gt;test/test_construct_attachment_covariance.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Re-ran and now it's failing :-/&lt;/p&gt;

&lt;p&gt;Modified it to save the output; re-running to debug.&lt;/p&gt;

&lt;p&gt;Needed to re-run &lt;code&gt;attach_2()&lt;/code&gt;, because prior_K was stale&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;troubleshoot covariance test&lt;/li&gt;
&lt;li&gt;investigate difference between method 2 and method 3 above.&lt;/li&gt;
&lt;li&gt;handle nonzero start-index in branching. re-run confirmation&lt;/li&gt;
&lt;li&gt;ground truth - trace datasets 7 through 10&lt;/li&gt;
&lt;li&gt;retrain using attachment&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Building reference implementation of full-tree covariance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/17/work-log"/>
   <updated>2013-09-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;




&lt;div&gt;
Building reference version of `build_attachment_covariance()`.  It does full recursion for every element, taking \(O(n^2 \log n)\) time.
&lt;/div&gt;


&lt;hr /&gt;

&lt;p&gt;Running now with model_type = 1;  estimated runtime: 45 minutes.  (compare this to less than 5 seconds in the fast version).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Results don't match.&lt;/p&gt;

&lt;p&gt;Issue 1: topology indices array was reversed; handled inheritance wrongly.
Issue 2: forgot about start_index; should add to all curve indices.&lt;/p&gt;

&lt;p&gt;Results now match when model type = 1.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Matching results suggest that the covariance matrix is correctly implemented as designed, but why are we getting negative eigenvalues?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Next: re-run with model_type = 3.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Full-tree covariance; Run on WACV dataset</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/16/work-log"/>
   <updated>2013-09-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finished testing &lt;code&gt;construct_attachment_covariance.m&lt;/code&gt; against legacy method for constructing covariance matrix.  Constructed new test &lt;code&gt;wacv_2014/run_wacv_2.m&lt;/code&gt; to compare new method against old method in &lt;code&gt;wacv_2014/run_wacv.m&lt;/code&gt;.  After significant amount of debugging, results match.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tweak to Corr.prior_K: &lt;/strong&gt; don't replicate until needed.  New function &lt;code&gt;tools/one_d_to_trhee_d&lt;/code&gt; helps with this.&lt;/p&gt;

&lt;p&gt;Now, to test new covariance matrix algorithm against the existing branching ML code to confirm that the latter is correct...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Wrote &lt;code&gt;curve_tree_ml_ref&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Having trouble running  on wacv dataset.  The covariance matrix constructed by &lt;code&gt;construct_attachment_covariance&lt;/code&gt; has several &lt;em&gt;negative eigenvalues&lt;/em&gt; (magnitude on the order of 1000).  These don't appear when attachments don't exist.&lt;/p&gt;

&lt;p&gt;Old method of constructing the covariance matrix has several negative eigenvalues, but they're on the order of 1e-9.  (were'nt they identical?)&lt;/p&gt;

&lt;p&gt;Lets go back to the WACV example and run everything on those...&lt;/p&gt;

&lt;p&gt;The big question to answer is: &lt;strong&gt;is WACV ML better with attachments or worse?&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;On second look, the test and reference prior matricies &lt;strong&gt;don't&lt;/strong&gt; match.  Going back to debugging &lt;code&gt;construct_attachment_covaraince.m&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;found bug in bugfix in attach.m.   results now match&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Now covariancaes from wacv and curve_tree_ml_ref don't match.  Furthermore, curve_tree_ml_ref crashes because the matrix isn't positive definite.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found issue: wacv program forced model_type to be 1 (no-perturb model).  This accounts for the difference between wacv reconstruction and the reference ML code.&lt;/p&gt;

&lt;p&gt;Still don't know why we're getting negative eigenvalues when model_type != 1.&lt;/p&gt;

&lt;h2&gt;Medium-term planning&lt;/h2&gt;

&lt;p&gt;Q: What is needed to get end-to-end running?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Efficient ML for branching model.&lt;/li&gt;
&lt;li&gt;inference of branch points, given attachment (part of proposal mechanism?)&lt;/li&gt;
&lt;li&gt;training with attachment

&lt;ul&gt;
&lt;li&gt;updated training ML&lt;/li&gt;
&lt;li&gt;updated training procedure&lt;/li&gt;
&lt;li&gt;joint training of foreground and background, with shared noise parameter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;re-write sampling infrastructure

&lt;ul&gt;
&lt;li&gt;pair candidates&lt;/li&gt;
&lt;li&gt;gibbs moves&lt;/li&gt;
&lt;li&gt;merge/split (using Swendsen Wang?)&lt;/li&gt;
&lt;li&gt;attach/detach&lt;/li&gt;
&lt;li&gt;HACKS

&lt;ul&gt;
&lt;li&gt;background subtraction?&lt;/li&gt;
&lt;li&gt;&quot;cheating&quot; (nonreversible) merge/split&lt;/li&gt;
&lt;li&gt;forced attachment?&lt;/li&gt;
&lt;li&gt;heuristic initialization?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish debugging reference ML: &lt;code&gt;curve_tree_ml_ref.m&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;why are we getting negative eigenvalues in prior matrix when model_type &gt; 1?&lt;/li&gt;
&lt;li&gt;compare against long-hand version of matrix (slow, full recursive version).  Does it still have negative eigenvalues?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Confirm that WACV dataset has better ML when attachments are modeled.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Branching prior covariance; implementing</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/12/work-log"/>
   <updated>2013-09-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/12/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Goals:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ground truth - trace datasets 7 through 10&lt;/li&gt;
&lt;li&gt;build new attachment covariance&lt;/li&gt;
&lt;li&gt;reconstruct ground truth using new attachment covariance&lt;/li&gt;
&lt;li&gt;work attachment covariance function into ml&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Building Full Attachment Covariance Matrix&lt;/h2&gt;

&lt;p&gt;See &lt;code&gt;attachment/construct_attachment_covariance.m&lt;/code&gt; (filename still in flux).&lt;/p&gt;

&lt;p&gt;Issue: The recursive definition of the attached curve kernel that we outlined yesterday is too expensive to implement naively.  Can implement by re-using precomputed values, but the branch point isn't necessarilly one of the observed points, so it's not necesarilly represented in the curve's &lt;code&gt;prior_K&lt;/code&gt;.  This means we need to construct covariance entries between each of the attachment points and their parent and child curves.  Then we can run the &quot;memoized&quot; recursive function.&lt;/p&gt;

&lt;h2&gt;Base self covariance&lt;/h2&gt;

&lt;p&gt;Key idea: each child curve has a &quot;base self covariance&quot; that is extra covariance inherited from the branch point on parent that is added to all points of the curve self-covariance matrix.  That curve, in turn, has a base covariance, that &lt;em&gt;it&lt;/em&gt; inherits, all the way to the root, whose base covariance is the &quot;position variance&quot; parameter.  Thus, the leaves have a base covariance that is the sum of all the incremental variance of all the branches to the root.&lt;/p&gt;

&lt;p&gt;The base covariance is an NxN matrix, where N is the number of views.  We construct each in isolation, and then cumulatively sum them from root to leaf.&lt;/p&gt;

&lt;p&gt;Every element of the curve's observed-point covariance matrix is increased by the base_covariance relative to it's entry; e.g. if element (k_{ij}) is the covariance between a point in view 3 and a point in view 5, you'll add the (3,5) element of the base covariance matrix.&lt;/p&gt;

&lt;h2&gt;Base Parent covariance&lt;/h2&gt;

&lt;p&gt;Each child curve also hase a &quot;parent covariance&quot; (for lack of a better term), which is the set of covariances between the branch point and each of the parent points.  Again, this is inherited recursively; just as the curve's covariance has it's self-covariance added, the parent covariance has the paren'ts self-covariance added.&lt;/p&gt;

&lt;p&gt;This will be used for the off-diagonal blocks of the full attachment covariance matrix.&lt;/p&gt;

&lt;h2&gt;Implementation Misc&lt;/h2&gt;

&lt;p&gt;Issues during implementation and testing of &lt;code&gt;construct_attachment_covariance&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;replacing Cell-of-structs with struct-array:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Corrs = cell2mat(Corrs);
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;The trivial code path of calling &lt;code&gt;blkdiag&lt;/code&gt; takes 500ms. Wow!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running on the WACV dataset is giving bad results.&lt;/p&gt;

&lt;p&gt;Looks like the covariance matrix is different from the one in the reference implementation.&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Need to convert from &quot;index of branch index&quot; to simply &quot;branch index&quot;.&lt;/li&gt;
&lt;li&gt;More debugging.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Branching prior covariance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/10/work-log"/>
   <updated>2013-09-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/10/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;In the past, we've formulated the branching prior covariance matrix by starting with an independent-curve block-diagonal covariance matrix and multiplying by an &quot;attachment matrix.&quot;  This works when the branching point appears in the index set, but we'd like to allow branching from any continuous point along the curve.  Stated differently, we currently model branching using a discritized model, but we'd prefer to express branching using the underlying continuous gaussian process.  This means modelling branching using the covariance function, rather than by manipulating the covariance matrix.&lt;/p&gt;

&lt;div&gt;
&lt;p&gt;
The key observation here is that in addition to each point's smooth curvature variance, it inherits the covariance of its parent's branch point.   In order to model correlation between curves, we extend the definition of &quot;index&quot; to add the curve index, \(c_i\), in addition to point index and time/view index.  Thus, index is now a triple: (c_i, p_i, t_i).  Note that we've changed our notation; previously point index was \(t_i\) and time/view index was \(v_i\).  The new notation uses \(p_i\) and \(t_i\) respectively, to avoid confusion between spacial and temporal dimensions.  
&lt;/p&gt;

&lt;p&gt;
Let's introduce two functions, \(\text{branch_curve}(c_i)\) and \(\text{branch_point}(c_i)\), which return the curve index and point index of curve \(c_i\)'s branch point, resepectively, or zero if \(c_i\) is a root curve.  We'll also use the convenience function \(\text{branch}(i)\), which is shorthand for full branch index, i.e. the tuple \(\text{branch_curve}(c_i), \text{branch_point}(p_i), t_i)\).  Note that the time-index \(t_i\) is preserved when getting the branch index.
&lt;/p&gt;

&lt;p&gt;
Without loss of generality, we'll assume curves are indexed in topological order, with parents appearing occurring before descendants.  If there are multiple connected components in the graph, it will be important that all root nodes have a lower index than non-root nodes, so we can add a sentinal node with index \(c_i = 0\), and attach all root curves to it before ordering. (Note to self: breadth-first search order might be simpler to describe).   The branching covariance function is now:
&lt;/p&gt;

\[
\begin{align}
k\left( i, j \right) 
     &amp;= 
    k\left( (c_i, p_i, t_i), (c_j, p_j, t_j) \right)  \\
     &amp;=  
     \begin{cases}
     k(j, i) &amp; \text{if } c_i &gt; c_j = 0 \\
     k_\text{base} ( i, j ) + k_o(ti, tj)  &amp; \text{if } \text{isroot}(c_i) \text{ and } \text{isroot}(c_j) \text{ and } c_i = c_j &amp;&amp;  \\
     k_\text{base} ( i, j ) + 0 &amp; \text{if } \text{isroot}(c_i) \text{ and } \text{isroot}(c_j) \text{ and } c_i \neq c_j \\
     k_\text{base} ( i, j ) + k(i, \text{branch}(j)) &amp; \text{otherwise} 
    \end{cases}
\end{align}
\]



&lt;p&gt;
For convenience, we've used the \(k(i,j) \) to stand-in for \(k\left((c_i, p_i, t_i), (c_j, p_j, t_j)\right)\).
  Here, \(k_\text{base}(i,j)\) is the independent curve covariance (e.g. cubic spline covariance w/ perturbations).   In our case, \(k_\text{base}(\dot)\) is zero if \(c_i \neq c_j\), but this isn't necessarilly required.
  &lt;/p&gt;

&lt;p&gt;
Here, \(k_o\) is to marginal covariance of the first curve point.  All points in the plant are measured relative to this point, so this covariance ends up in every element of the covariance matrix.  Since it only measures covariance of a specific point with respect to itself, curve and point index are irrelevant to \(k_o\).  Only the temporal is received, which allows us to model how the point moves over time.

In the case of the Ornstein Uhlenbech process, \(k_o\) is

\[
k_o = \sigma_o + \exp(|t_i - t_j|) \sigma_{o,b})
\]
&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;Lets examine each case in the piecewise function.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first case just simplifies the definition by ensuring that if i and j are related, the ancestor always appears in the first position.&lt;/li&gt;
&lt;li&gt;The second case handles the offset variance that allow root curves to translate away from the origin.&lt;/li&gt;
&lt;li&gt;The third case ensures zero covariance between disconnected trees.&lt;/li&gt;
&lt;li&gt;The fourth case inherits covariance from the second parameter's parent curve.  Note that \(c_j\) is always a non-root curve; if it were, one of the first three cases would handle it.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Testing &lt;/h2&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish tracing wacv datasets 7 through 11.&lt;/li&gt;
&lt;li&gt;Test the new covariance function against a matrix-based approach.&lt;/li&gt;
&lt;li&gt;Test the new covariance function by reconstructing 9-view sequence using WACV datasets.&lt;/li&gt;
&lt;li&gt;implement attachment-based ml using new covariance function&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>WACV results</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/06/work-log"/>
   <updated>2013-09-06T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/06/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;WACV deadline has passed, but still working on reconstructing ground truth, because it's  a good test on the end-to-end system.&lt;/p&gt;

&lt;p&gt;Open issues:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;weird issue with branching&lt;/li&gt;
&lt;li&gt;clear up math issue with branching&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Branching bug&lt;/h1&gt;

&lt;p&gt;Fixed!  It was a bug in &lt;code&gt;infer_branch_points()&lt;/code&gt; function.  I was assuming Corr.curve_sm used the same  index set as Corr.ll_means_flat, which it was not.&lt;/p&gt;

&lt;p&gt;I wanted to use &lt;code&gt;Corr.curve_sm&lt;/code&gt;, because it is precomputed in &lt;code&gt;corr_to_likelihood()&lt;/code&gt;, and it would save me an extra semi-expensive call to the cubic spline smoother, &lt;code&gt;csaps()&lt;/code&gt;.  In the end, I modified &lt;code&gt;corr_to_likelihood&lt;/code&gt; to save the smoothing amount and used it to call csaps inside &lt;code&gt;run_wacv()&lt;/code&gt; to re-smooth the curves.  At least &lt;code&gt;csaps&lt;/code&gt; is faster (linear time) than computing the posterior mean (quadratic).&lt;/p&gt;

&lt;h2&gt;Test Results&lt;/h2&gt;

&lt;p&gt;Results on dataset 2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-06-result_dataset_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looks good!&lt;/p&gt;

&lt;h2&gt;All Results&lt;/h2&gt;

&lt;p&gt;Running on all 11 datasets...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Matlab is running out of memory on the second dataset (apparently)&lt;/p&gt;

&lt;p&gt;When running directly on second dataset, it's fine.  Is the pass on the first dataset leaking memory or something?&lt;/p&gt;

&lt;p&gt;Maybe one of the mex files?&lt;/p&gt;

&lt;p&gt;It's weird, because we've run on dataset #2 tens of times without this problem...&lt;/p&gt;

&lt;p&gt;Now running directly on dataset #2 is resulting in a curve of over 1 million points.  investigating...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Solution: the image dimensions were reversed (should be nrows X ncols, not width x height).  The triangulations were nutso, and after projecting and resampling, the number of points was in the millions.  It's interesting that it's so easy to make this algorithm blow up; are there other; will need to keep an eye out for more benign ways to trigger this kind of explosion.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Ground truth issues&lt;/h2&gt;

&lt;p&gt;Encountered this error message when running on all ground truth datasets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Warning: file wacv-2012/datasets/3/ground_truth_2d.gt2
Missing curves in view 6: 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 2: 2, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 3: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 4: 2, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 6: 2, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 8: 1, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 10: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 15: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 19: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 23: 2, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 27: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 31: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 35: 3,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dataset 6 looks to be totally crap; need to investigate.&lt;/p&gt;

&lt;h2&gt;Found a bug in &lt;code&gt;corr_to_likelihood&lt;/code&gt; that led to out-of-bound indexing.&lt;/h2&gt;

&lt;p&gt;Looks like Liu never finished ground-truthing dataset &lt;code&gt;2010-01-26/arab_1_36&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Skipping for now; lets see if others are okay...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Dataset 9 is broken. investigating...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Dataset 9 seems not terrible, actually.  A few missing views on the hard-to-trace views of a few curves.&lt;/p&gt;

&lt;p&gt;Some cuves were so short that they contained only one point, which violated an assert that requires all curves to be at least 2 points long.  Adjusted the bezier-to-sampled-curve code so that the final point is always included.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Finished running all.  issues&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dataset 1 is very rough&lt;/li&gt;
&lt;li&gt;datasets 6 was skipped&lt;/li&gt;
&lt;li&gt;datasets 7 - 11 are worthless.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Need to re-trace 6 through 11.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Finished re-trace of 6.  Whew, 90 minutes... that sucked.  Reconstruction looks good now.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish tracing datasets 7 through 11.&lt;/li&gt;
&lt;li&gt;Work on attached-curve-multi-view prior and reconstruction.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>WACV Deadline</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/05/work-log"/>
   <updated>2013-09-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/05/work-log</id>
   <content type="html">&lt;p&gt;Late-hour realization: not sure how to handle attachment matrix when multiple-views are allowed.  Deriving the correct equations will take all afternoon; no time.  Decided to remove the tracking part from the paper.&lt;/p&gt;

&lt;h1&gt;Reconstructions&lt;/h1&gt;

&lt;p&gt;code in &lt;code&gt;wacv-2012/run_wacv.m&lt;/code&gt;.  Example call:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gt_paths = arrayfun(@(x) sprintf('wacv-2012/datasets/%d', x), 1:11, 'UniformOutput', false);
gt_cam_fmt = [gt_paths{i} '/cal/calib_%d.txt'];
gt_fname = [gt_paths{i} '/ground_truth_2d.gt2'];
[mu, lengths] = run_wacv(gt_fname, gt_cam_fmt, 1:4:36, [397,530]', params, 1);
mu = reshape(mu, 3, []);
mu = mat2cell(mu, 3, lengths);
colors = lines(numel(mu));
for i = 1:numel(mu);
    c = mu{i};
    col = colors(i, :);
    plot3(c(1,:), c(2,:), c(3,:), '-o', 'Color', col);
    hold on
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Dataset #1&lt;/em&gt;: curve #1 has some weird shape.  Probably and error in GT&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dataset #2&lt;/em&gt;: again weirdness in curve #1.  Now I'm guessing it's an attachment issue:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-05-bad_result_c2_pass1.png&quot; alt=&quot;bad result&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Run again without attachments.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dataset #2&lt;/em&gt;: curve 1 issue is better; now exhibiting some weird stray points.  few views, bad indices?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-05-c2-pass2.png&quot; alt=&quot;result 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like the base curves are trying to attach to the &lt;em&gt;end&lt;/em&gt; of curve #1, instead of the beginning.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Branching ML, debugging, training</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/03/work-log"/>
   <updated>2013-09-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Blockwise precision decomposition issue&lt;/h1&gt;

&lt;p&gt;Bug: when attaching, was getting incorrect results for estimated location of attachment.  Our copmutation used a subset of data points on the parent curve nearby the branch point to estimate the distribution of the branch point position.  We were simply taking a submatrix of the full (decomposed) precision matrix, which wasn't correct, due to the way the full decomposed matrix was constructed.&lt;/p&gt;

&lt;h2&gt;Submatrix of decomposition != decomposition of submatrix&lt;/h2&gt;

&lt;p&gt;When using subset of variables, need to be able to get submatrix of noise precision and prior.&lt;/p&gt;

&lt;p&gt;Let \((s' s) = S\), and let \(P\) be a matrix that selects rows of \(S\).  if you want to decompose the submatrix \(P S P'\), in general, you cannot simply take the submatrix \(P s P'\).  There are at-least two special case where this is valid:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(s\) is a Cholesky decomposition of \(S\).  But since \(S\) is degenerate in our case, we can't use Cholesky.&lt;/li&gt;
&lt;li&gt;\(S\) is block-diagonal, and \(P\) doesn't split-up the blocks.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I thought that condition 2 was being satisfied, because I foolishly assumed the eigenvector matrix produced by eigenvalue decomposition was preserving block structure.  I had to modify &lt;code&gt;correspondence/flatten_sort_and_reverse.m&lt;/code&gt; to perform eigenvalue decomposition on each block individually.  I had previously determined that operating on groups of five blocks at once is faster than operating individually, but we'll have to sacrifice this speed-up for now.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Okay, fixed the precision problem.  After attaching stem 2 to stem 1, Corrs{2}.mu_b now looks reasonable.&lt;/p&gt;

&lt;p&gt;Still getting really low ML's...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bingo!  Bug in &lt;code&gt;curve_tree_ml_2.m&lt;/code&gt; that was introduced when we started allowing non-zero means when evaluating our conditional normal distribution.   Recall the equation for marginal likelihood (reproduced from &lt;a href=&quot;/ksimek/research/2013/07/12/marginal-likelihood/&quot;&gt;this post&lt;/a&gt;):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
     p(y) &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z}  \exp\{x^\top S(S + S K S)^{-1} S x \} \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z}  \exp\{x^\top s^\top(I + s K s^\top)^{-1} s x \} \\
     &amp;= \frac{Z_{l,3d} }{Z_l} \mathcal{N}\left (sx ; 0, (I + s K s^\top) \right )
\end{align}
\]
&lt;/div&gt;




&lt;div&gt;Aside: in the second line, we've substituded the decomposition, \(s^\top s = S\), which we use in practice.  &lt;/div&gt;


&lt;p&gt;However, this equation assume zero mean.  If we include a nonzero mean, the \(s\) matrix distributes to both \(y\) and \(\mu\):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
     p(y) &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z}  \exp\{(x-\mu)^\top s^\top(I + s K s^\top)^{-1} s (x - \mu) \} \\
     &amp;= \frac{Z_{l,3d} }{Z_l} \mathcal{N}\left (sx ; s \mu, (I + s K s^\top) \right )
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;In our implementation, we weren't multiplying \(\mu\) by \(s\).  Fixing this seems to fix the low ML issue:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Corrs = attach(Corrs, 2,1,12, -12, params);
curve_tree_ml_2(Corrs, params, data_)

    ans =

       2.0903e+04

Corrs_ind = attach(Corrs, 2,0,0,0, params);
curve_tree_ml_2(Corrs_ind, params, data_)

    ans =

       2.0804e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the case above, ML is roughly maximized when branch index is -12 and start index is 12, which seems to agree with the data, visually.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Next:
* get attachment, reversal, and branch points from ground truth
* store ml_2d with corr. update on merge. use during ml computation instead of data_
* branching in training ML
* traing with branching
* branch-wise reconstruction.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Misc.</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/26/work-log"/>
   <updated>2013-08-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Monday IVILAB infrastructure meeting&lt;/h1&gt;

&lt;p&gt;I'll be organizing the Computational Intelligence seminar this semester.&lt;/p&gt;

&lt;p&gt;Will need to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Arrange volunteers&lt;/li&gt;
&lt;li&gt;send announcements&lt;/li&gt;
&lt;li&gt;send reminders to speakers&lt;/li&gt;
&lt;li&gt;Set up seminar web page&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Building efficient curve-tree ML.&lt;/h1&gt;

&lt;p&gt;Significant rework.&lt;/p&gt;

&lt;p&gt;New field: branch_K and branch_mu to store mean and covariance of all points in curve.
Prior_k now does not include offset covariance; stored in branch_K.
mu_b and Sigma_b now store the branch point means and covariances for all views&lt;/p&gt;

&lt;p&gt;fix kernel to not include offset index&lt;/p&gt;

&lt;h1&gt;Issues&lt;/h1&gt;

&lt;p&gt;Finished implementation.  Has bugs.&lt;/p&gt;

&lt;h2&gt;Start-point has no effect&lt;/h2&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;start_pt = [0 100];
ml = [];
for i = 1:2
    Corrs = attach(Corrs, 2, 1, start_pt(i), 0, params);
    ml(i) = curve_tree_ml_2(Corrs, params, data_);
end
assert(ml(1) == ml(2));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Fixed&lt;/em&gt; - stupid bug in &lt;code&gt;attachment/attach.m&lt;/code&gt; - was index offset was hard-coded to zero due to a refactoring mishap.&lt;/p&gt;

&lt;h2&gt;Optimal Branch point isn't correct&lt;/h2&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;start_pt = [0 10 50 100 1000 5000];
ml = [];
for i = 1:2
    Corrs = attach(Corrs, 2, 1, start_pt(i), 0, params);
    ml(i) = curve_tree_ml_2(Corrs, params, data_);
end
assert(all(diff(ml) &amp;gt; 0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Solved&lt;/em&gt; - curves were reversed.&lt;/p&gt;

&lt;h1&gt;Handling reversed curves&lt;/h1&gt;

&lt;p&gt;Should add a &lt;code&gt;reversed&lt;/code&gt; flag, which reverses indices before building likelihood and prior.  currently, constructing the likelihood occurs during the &quot;backproject and re-index&quot; phase, in file &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt;.  Should refactor likelihood construction into its own function, so we don't have to re-backproject when we don't need to.&lt;/p&gt;

&lt;p&gt;Let's review the data-flow so we can see more clearly where everything happens.&lt;/p&gt;

&lt;h1&gt;Overview: end-to-end curve construction&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;propose association and attachment

&lt;ul&gt;
&lt;li&gt;By sampling (no code yet)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;train/labels_from_ground_truth&lt;/code&gt; - propose from ground truth.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: assoc {}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Construct track

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correspondence/make_correspondence&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Corrs {};  Corr.ml_2d&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Correspondence and triangulation

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correspondence/build_full_correspondence.m&lt;/code&gt; - build from scratch&lt;/li&gt;
&lt;li&gt;&lt;code&gt;correspondence/merge_correspondence_2.m&lt;/code&gt; - merge two pieces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;corr&lt;/code&gt;, &lt;code&gt;means&lt;/code&gt;, &lt;code&gt;precisions&lt;/code&gt;, &lt;code&gt;cov_error&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;backproject and estimate curvature

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt; -&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;ll_{means, precisions, indices}&lt;/code&gt;, &lt;code&gt;curve_sm*&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;construct likelihood (flatten, sort, and reverse if needed)

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correspondence/flatten_sort_and_reverse.m&lt;/code&gt; - (doesn't exist yet)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;ll_{means_flat, precisions_flat, indices_flat, S}&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;handle attachment recursively

&lt;ol&gt;
&lt;li&gt;compute conditional prior

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_start_index.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;start_index&lt;/code&gt;, &lt;code&gt;prior_K&lt;/code&gt;, &lt;code&gt;prior_indices&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;compute branch point posterior

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_branch_index.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;branch_index&lt;/code&gt;, &lt;code&gt;mu_b&lt;/code&gt;, &lt;code&gt;sigma_b&lt;/code&gt;, &lt;code&gt;branch_K&lt;/code&gt;, &lt;code&gt;branch_mu&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Compute (ML, argmax, etc)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Consider renaming step 4.  backproject against rough triangulation; estimate curvature at each point; determine index set.  At this point, the order of points don't matter, because the index set hasn't been put to use.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Action: reverse curve&lt;/strong&gt; - detach?, rerun step 5, 6.1 &amp;amp; 6.2 on self, update branch point &amp;amp; rerun 6.2 for children.&lt;/p&gt;

&lt;h1&gt;Test: optimize branch point and start index&lt;/h1&gt;

&lt;p&gt;(TODO)&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;optimize test&lt;/li&gt;
&lt;li&gt;get attachment, reversal, and branch points from ground truth&lt;/li&gt;
&lt;li&gt;store &lt;code&gt;ml_2d&lt;/code&gt; with corr.  update on merge.  use during ml computation instead of data_&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Attachment ML Math (ctd); Implementing Attach()/Detach()</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/22/work-log"/>
   <updated>2013-08-22T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/22/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15169&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Note new working path: &lt;code&gt;data_association_3&lt;/code&gt;&lt;/strong&gt;&lt;br/&gt;
Fixed corrupt svn working copy.&lt;/p&gt;

&lt;h1&gt;Computing \(\mu_b\) and \(\Sigma_b\)&lt;/h1&gt;

&lt;div&gt; Since \(S\) is usually singular, we can't invert it, so we need to tweak yesterday's equations.  In what follows, the precision matrix \(S\) is decomposed into \(s s^\top\), which will allow us to keep the covariance matrix symmetric. 

&lt;div&gt;
The updated forumula for \(\mu_b\) is:&lt;/div&gt;
\[
    \begin{align}
    \mu_b &amp;= K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} y_{MB}  \\
    \tag{1}
          &amp;= K_*^\top s^\top \left(s K_{\mathcal{MB}} s^\top + I\right)^{-1} s y_{MB}  \\
    \end{align}
\]

The formula for \(\Sigma_b\) is:

\[
    \Sigma_b = K_b - K_*^\top s^\top \left(s K_{\mathcal{MB}} s^\top + I\right)^{-1} s K_*
\]

Recall that \(K_b\) is the prior covariance of the branch point.
&lt;/div&gt;


&lt;br /&gt;


&lt;h2&gt;Predictive covariance question&lt;/h2&gt;

&lt;div&gt;
&lt;p&gt;
I realized during implementation that it isn't clear what the predictive covariance, \(K_*\), should be.  I know it's the covariance between the true branch point and the observed points in the markov blanket, but the observed points have associated view-indices, while the branch point does not.  After some thought, I realized that the covariance arising from view-indices is essentially *likelihood* variance in this context (i.e. imaging noise), and according to Williams and Rasmussen, these parts of the likelihood should be omitted when computing the covariance between data and true unobserved points.  (See equation (2.21), notice that off-diagonal elements don't include the noise variance, \(\sigma_n I\)).
&lt;/p&gt;
&lt;p&gt;
To accomodate the use-case where you want to compute the covariance while ignoring view-index, I tweaked `kernel/get_model_kernel`; now, if you pass-in a `model_index` of zero, it returns a two-parameter no-perturb kernel where only spacial-indices are received.
&lt;/p&gt;
&lt;/div&gt;


&lt;h2&gt;&lt;code&gt;ll_indices_flat&lt;/code&gt; vs. &lt;code&gt;prior_indices&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;There's some ambiguity in these two fields of a curve-track.  The difference is simply that in &lt;code&gt;ll_indices_flat&lt;/code&gt;, indices &lt;em&gt;always&lt;/em&gt; start at zero, whereas in &lt;code&gt;prior_indices&lt;/code&gt; the offset &lt;code&gt;start_index&lt;/code&gt; is added to all values.  In practice, there are rules for where these two fields should be used:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;when passing indices to a kernel, always use &lt;code&gt;prior_indices&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;When dealing with geometry (e.g. referring to a point along a curve), always use ll_indices.  That way, the position of the referred point is unchanged if &lt;code&gt;start_index&lt;/code&gt; ever changes.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Task progress&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Finish &lt;code&gt;attachment/att_set_branch_index.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;rough test of attachment stuff - (does it parse? does it run?)

&lt;ul&gt;
&lt;li&gt;It runs, but with buggy output for \(\mu_b\) and \(\Sigma_b\).&lt;/li&gt;
&lt;li&gt;Found bug - incorrect re-indexing (xxyyzz to xyzxyz)&lt;/li&gt;
&lt;li&gt;Now giving apparently good results.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Implement efficient tree ML&lt;/li&gt;
&lt;li&gt;implement naive ML&lt;/li&gt;
&lt;li&gt;write tests

&lt;ol&gt;
&lt;li&gt;compare against &lt;code&gt;curve_ml5.m&lt;/code&gt; for independent curves&lt;/li&gt;
&lt;li&gt;compare against naive ML using full-covariance-matrix.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;invalidate children after calling attach/detach&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Misc notes&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;should place a condition on \(\Sigma_b\).  If it's variance is too high, need to expand markov-blanket.&lt;/li&gt;
&lt;li&gt;Add &lt;code&gt;model_type to params&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Have &quot;make_correspondence&quot; initialize start_index, parent_index, branch_index.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;chain of affects&lt;/h1&gt;

&lt;p&gt;How are children affected when properties of the parent change?&lt;/p&gt;

&lt;div&gt;
&lt;table border=&quot;1&quot;&gt;
&lt;tr&gt;
&lt;th width=&quot;33%&quot;&gt;parent property&lt;/th&gt;
&lt;th width=&quot;33%&quot;&gt;self affects&lt;/th&gt;
&lt;th&gt;child affects&lt;/th&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Assoc/Correspondence&lt;/td&gt;
&lt;td&gt;start_index (and inherited)&lt;/td&gt;
&lt;td&gt;Branch_index (and inherited.  easy to re-estimate deterministically), marginal likelihood, (not start index)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Start index&lt;/td&gt;
&lt;td&gt;prior_K, prior_indices, ML&lt;/td&gt;
&lt;td&gt;\(\mu_b\)*, \(\Sigma_b\)*, ML*&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Branch index&lt;/td&gt;
&lt;td&gt;\(\mu_b\), \(\Sigma_b\), &lt;/td&gt;
&lt;td&gt;none.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;
* Updating these might not be necessary
&lt;/div&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Implement Tree ML (2 ways, &quot;naive&quot; and &quot;fast&quot;)&lt;/li&gt;
&lt;li&gt;Test&lt;/li&gt;
&lt;li&gt;Get attachment for ground truth&lt;/li&gt;
&lt;li&gt;Re-train using attachment&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Clique-tree math (ctd)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/21/work-log"/>
   <updated>2013-08-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/21/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Conditional clique node&lt;/h1&gt;

&lt;p&gt;Continuing from yesterday, let's convert our conditional gaussian distribution to a gaussian function over \(x_c\) and \(\mu_b\).&lt;/p&gt;

&lt;p&gt;Let \(II\) represent a stack of identity matrices:&lt;/p&gt;

&lt;div&gt;
\[
II = \left( \begin{array}{c} I\\I\\...\\I \end{array}\right)
\]

The exponential expression for the conditional Gaussian distribution is

    \[
    -\frac{1}{2} (x_C - II \mu_b)^\top \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}(x_C - II \mu_b)
\]

Let's convert this to a linear function over \((x_C, \mu_b)\), instead of just \(x_C\):
        
    \[
    -\frac{1}{2} \left(\begin{array}{c}x_C \\ \mu_b \end{array} \right )^\top \left ( \begin{array}{cc}I &amp; -II\end{array}\right )^\top \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}\left ( \begin{array}{cc}I &amp; -II\end{array}\right )\left(\begin{array}{c}x_C \\ \mu_b \end{array}\right )
\]
&lt;/div&gt;


&lt;p&gt;Recall that \(\mu_b\) is a linear function of the data-points in the Markov-blanket, \(y_\mathcal{MB}\) (reproduced from &lt;a href=&quot;/ksimek/research/2013/08/19/work-log/&quot;&gt;yesterday's post&lt;/a&gt;)&lt;/p&gt;

&lt;div&gt;
\[
    \begin{align}
    \mu_b &amp;= K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} y_{MB}  \\
        &amp;= K_*^\top K^{-1}_{y_\mathcal{MB}}  y_\mathcal{MB}  \\
    \end{align}
\]

Rewriting the expression as a function of \((x_C, y_\mathcal{MB})\):

    \[
    \begin{align}
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\ K_* K^{-1}_{y_\mathcal{MB}}  y_\mathcal{MB} \end{array} \right )^\top
    \left ( \begin{array}{cc}I &amp; -II\end{array}\right )^\top 
    \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}
    \left ( \begin{array}{cc}I &amp; -II\end{array}\right )
    \left(\begin{array}{c}x_C \\ K_* K^{-1}_{y_\mathcal{MB}}  y_\mathcal{MB} \end{array}\right )
    \\
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\   y_\mathcal{MB} \end{array} \right )^\top
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}} \end{array}\right )^\top 
    \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}}\end{array}\right )
    \left(\begin{array}{c}x_C \\ y_\mathcal{MB} \end{array}\right )
    \end{align}
\]

And expanding \(\Sigma_b\):
 
\[
    \begin{align}
    \Sigma_b &amp;= K_b - K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} K_* \\
             &amp;= K_b - K_*^\top K^{-1}_{y_\mathcal{MB}} K_*

    \end{align}
\]

The final expression is:

    \[
    \begin{align}
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\   y_\mathcal{MB} \end{array} \right )^\top
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}} \end{array}\right )^\top 
    \left( \Sigma_C + II \left(K_b - K_*^\top K^{-1}_{y_\mathcal{MB}} K_* \right)  II^\top \right )^{-1}
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}}\end{array}\right )
    \left(\begin{array}{c}x_C \\ y_\mathcal{MB} \end{array}\right ) \\
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\   y_\mathcal{MB} \end{array} \right )^\top
    \Lambda_{C \mid \mathcal{MB}}
    \left(\begin{array}{c}x_C \\ y_\mathcal{MB} \end{array}\right )
    \end{align}
\]

the normalization constant is:
    
\[
    Z = 
    (2 \pi)^\frac{k}{2} \left | \Sigma_C + II \left(K_b - K_*^\top K^{-1}_{y_\mathcal{MB}} K_* \right)  II^\top \right |^\frac{1}{2}
\]
&lt;/div&gt;


&lt;h2&gt;Test experiment&lt;/h2&gt;

&lt;p&gt;See  &lt;code&gt;exp_2013_08_21_clique_tree_test.m&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;sample N points for curve 1, \(C_1\)&lt;/li&gt;
&lt;li&gt;sample N points for curve 2, \(C_2\)&lt;/li&gt;
&lt;li&gt;offset curve 1: \(C_2 = C_2 + C_1(:,5)\)&lt;/li&gt;
&lt;li&gt;add noise to \(C_1\) and \(C_2\) to get data \(y_1\),\(y_2\)&lt;/li&gt;
&lt;li&gt;add noise to \(C_1\) and \(C_2\) to get data \(y_1\), \(y_2\).&lt;/li&gt;
&lt;li&gt;Construct full prior, \(\Sigma\) (see below for definition).&lt;/li&gt;
&lt;li&gt;Evaluate ML directly from \(p(y_1, y_2) = \mathcal{N}(0, (\Sigma + \sigm_n I))\).&lt;/li&gt;
&lt;li&gt;Construct ML decomposed: \(p(y_2 | y_1) p(y_1) \)&lt;/li&gt;
&lt;li&gt;(didn't implement) Construct clique tree using

&lt;ul&gt;
&lt;li&gt;Node 1: \((0, \Sigma_1)\)&lt;/li&gt;
&lt;li&gt;Node 2: \((0, inv(\Lambda_{C\mid \mathcal{MB}}))\)&lt;/li&gt;
&lt;li&gt;multiply by corresponding noise nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(didn't implement) Marginalize clique tree.  compare against result in 7.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I originally thought the result from 8 was only an approximation (mainly because I hadn't originally written it out that way).  In fact, it's an exact computation, but it isn't very useful in this form, because deep trees still exhibit linear growth of the condition-set, meaning cubic growth in running time.  In practice, we can replace \(y_1\) to the data markov-blanket, \(y_{1,\mathcal{MB}}\).  The data markov-blanket is naturally larger than the prior m.b., which is a single point, but if the noise is low relative to the point-spacing, the data m.b. should still be relatively small.  Thus, we can approximate the ML with a decomposed form that avoids cubic growth.&lt;/p&gt;

&lt;p&gt;The alternative is to use the clique tree from step 9., but the former is simpler to implement, because we don't have to use the crazy linear form we developed above, and we don't have to do any message-passing.  We just need \(\Sigma_b\) and \(\mu_b\).&lt;/p&gt;

&lt;h2&gt;Computing max posterior with attachments&lt;/h2&gt;

&lt;p&gt;Since this operation won't be run during production, we can just implement it the naive way.&lt;/p&gt;

&lt;p&gt;But if we wanted a fast version, we could to a forward-pass approximation, i.e. find the max for the root curve, and then pass that as data to the child curves.&lt;/p&gt;

&lt;p&gt;A full forward-backward algorithm probably wouldn't be too hard, but probably not worth the trouble at the moment.&lt;/p&gt;

&lt;h1&gt;Build Tasks&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;apply attacment to trackset&lt;/li&gt;
&lt;li&gt;guess branch spacing&lt;/li&gt;
&lt;li&gt;construct independent cliques, given branch spacing.&lt;/li&gt;
&lt;li&gt;Guess branch point&lt;/li&gt;
&lt;li&gt;construct markov-blanket, \(\mathcal{MB}\).&lt;/li&gt;
&lt;li&gt;construct \(sigma_b\) and \(\mu_b\) from parent \(\mathcal{MB}\).&lt;/li&gt;
&lt;/ol&gt;


&lt;h1&gt;TODO &amp;amp; In-progress&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/attach.m&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_start_index.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_branch_index.m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Next

&lt;ul&gt;
&lt;li&gt;pre-flatten and pre_sort ll_*.

&lt;ul&gt;
&lt;li&gt;remove all calls to flatten_inputs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;precompute likelihood covariance matrix.

&lt;ul&gt;
&lt;li&gt;remove logic from &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;compute full branching ML using cached values.

&lt;ul&gt;
&lt;li&gt;identify connected components&lt;/li&gt;
&lt;li&gt;topological sort&lt;/li&gt;
&lt;li&gt;root-to-leaf evaluation over each CC.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>cleanup</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/21/cleanup"/>
   <updated>2013-08-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/21/cleanup</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Note new working path: &lt;code&gt;data_association_3&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Started realizing that there are a huge number of dead/obsolete files in the &lt;code&gt;data_association_2&lt;/code&gt; directory.  It's time to migrate to a clean code-base, &lt;code&gt;data_association_3&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Reorganization&lt;/h1&gt;

&lt;h2&gt;Setups&lt;/h2&gt;

&lt;p&gt;Realized that &lt;code&gt;tmp_setup_workspace.m&lt;/code&gt; is very valuable, but will probably need to change and evolve over time.  Created new directory called &lt;code&gt;setups/&lt;/code&gt;, where setup scripts will be stored, organized by date.  Related files that the setup scrip needs will be stored in the same directory, with a similar name to the script itself.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tmp_setup_workspace.m&lt;/code&gt; is now in &lt;code&gt;setups/setup_workspace_2013_08_21.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Format is: &lt;code&gt;setup_&amp;lt;type&amp;gt;_&amp;lt;date&amp;gt;.m&lt;/code&gt;.  Currently only type is &quot;workspace&quot;.
Related &quot;load&quot; files have format: &lt;code&gt;setup_&amp;lt;type&amp;gt;_&amp;lt;date&amp;gt;.mat&lt;/code&gt;   or if multiple files, &lt;code&gt;setup_&amp;lt;type&amp;gt;_&amp;lt;date&amp;gt;.&amp;lt;N&amp;gt;.mat&lt;/code&gt;, where N is an increasing integer starting at 1.&lt;/p&gt;

&lt;h2&gt;Mex files&lt;/h2&gt;

&lt;p&gt;Propose creating a new file &lt;code&gt;compile_mex_scripts.m&lt;/code&gt;.  If called with no arguments, it will compile all scripts that are uncompiled.  Optional &quot;force_recompile&quot; parameter.&lt;/p&gt;

&lt;h1&gt;Misc Notes/Issues&lt;/h1&gt;

&lt;h2&gt;split_correspondence&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;split_correspondence.m&lt;/code&gt; is no longer available.  It became obsolete some time ago, around when we moved from &lt;code&gt;clean_correspondence.m&lt;/code&gt; to &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When we return to end-to-end sampling, we'll need something like it.  Alternatively,  maybe full-rebuilding of the split curves is fast enough, now that we've mexed the bottlenecks?&lt;/p&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Not totally done, but we can at least run the workspace setup file.&lt;/p&gt;

&lt;p&gt;Will save the current workspace, re-open matlab, and see what issues arise as we continue to work toward the short-term goal of implementing branching-curve marginal likelihood.&lt;/p&gt;

&lt;p&gt;Workspace saved to &lt;code&gt;tmp/workspace_2013_08_21.mat&lt;/code&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Branching curve clique-tree</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/19/work-log"/>
   <updated>2013-08-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Thinking about attachments.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Organize into connected components.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Reviewing ML code, with considerations to generalizing for attachments.&lt;/p&gt;

&lt;p&gt;Can we exploit the structure in the branching-prior covariance matrix to speed-up inversion in the marginal likelihood?  No obvious way.&lt;/p&gt;

&lt;p&gt;Why did we abandon the clique-tree method?  Because it wasn't clear that the perturb-models would be compatible with it.  Now, it seems like it might be, as long as we set the markov-order high enough.  More importantly, it might be &lt;em&gt;necessary&lt;/em&gt;, because after adding attachment, the covariance matrix is growing too large to allow direct evaluation.&lt;/p&gt;

&lt;p&gt;Construct clique tree per-track, conditioned on parent point.  In case of base-curves, the parent point is the prior.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;New fields&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;branch parent

&lt;ul&gt;
&lt;li&gt;curve index&lt;/li&gt;
&lt;li&gt;point index&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;branch distance

&lt;ul&gt;
&lt;li&gt;i.e. starting index of self&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gaussian structure

&lt;ul&gt;
&lt;li&gt;in canonical form&lt;/li&gt;
&lt;li&gt;no position variance&lt;/li&gt;
&lt;li&gt;no branching variance&lt;/li&gt;
&lt;li&gt;yes branch distance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Inferring branch distance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The reconstructed lateral branch curves are usually have gaps of several millimeters from their parent branch.  Assigning an index of zero to the first point will almost certainly result in a worse marginal likelihood, because these gaps are not well-modelled.&lt;/p&gt;

&lt;p&gt;Instead, we should attempt to infer the branch distance(or better yet, try to marginalize over it).&lt;/p&gt;

&lt;p&gt;Spent some time trying to derive a &quot;fast update&quot; for covariance matrices for inferring branch distance.  In this scheme, the matrices are computed assuming branch distance is zero, and then a easily computed delta matrix is added to account for non-zero branch distance.&lt;/p&gt;

&lt;p&gt;After some work, it looks like the smoothing variance kernel is too complicated to permit such a method.  It's just easier to recompute the entire covariance matrix.&lt;/p&gt;

&lt;h2&gt;Constructing branching clique-tree&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Construct individual cliques trees, assuming zero position variance (but including branch distance).  Call this the &quot;raw prior.&quot;&lt;/li&gt;
&lt;li&gt;Given a branch-point-index, compute the markov-blanket of the data points on the parent.&lt;/li&gt;
&lt;li&gt;Given raw prior and markov-blanket, construct conditional clique  node from raw clique node.&lt;/li&gt;
&lt;li&gt;Given clique tree, multiply and marginalize from tips to root.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;All cliques will now be stored in track structure (currently named &lt;code&gt;Corrs&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to construct conditional prior?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\mathcal{MB}\) - indices of the parent curve markov blanket, centered at branch point.&lt;/li&gt;
&lt;li&gt;\(y_\mathcal{MB}\) - observations in the markov blanket&lt;/li&gt;
&lt;li&gt;\(x_b\) - branch point&lt;/li&gt;
&lt;li&gt;\(\Sigma_b\) - branch point predictive covariance, conditioned on markov blanket&lt;/li&gt;
&lt;li&gt;\(x_c\) - child curve points, relative to branch point.&lt;/li&gt;
&lt;li&gt;\(x_C\) - child curve points, relative to world origin&lt;/li&gt;
&lt;li&gt;\(N\) - number of points in child curve.&lt;/li&gt;
&lt;li&gt;\(\Sigma_c\) - curve predictive covariance, conditioned on branch point&lt;/li&gt;
&lt;li&gt;\(\Sigma_C\) - curve predictive covariance, conditioned on markov blanket&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;div&gt;
The branch point distribution is given by the GP predictive distribution:
    
\[
p(x_b | y_\mathcal{MB}) = \mathcal{N}(\mu_b, \Sigma_b) \]

where 

\[
\begin{align}
    \mu_b &amp;= K_* \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} y_{MB} \\
    \Sigma_b &amp;= K_b - K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} K_*

\end{align}
\]

Here, \(K_*\) is the kernel of the branch index vs. the markov blanket indices, \(k(t_\mathcal{MB}, t_b)\).
&lt;/div&gt;


&lt;p&gt;The conditional child curve distribution is&lt;/p&gt;

&lt;div&gt;
\[ p(x_C \mid x_b) = p(x_c) = \mathcal{N}(x_b, \Sigma_c) \]

Here, \(Sigma_c\) is the raw curve covariance, whatever it may be.
&lt;br /&gt;
The marginal over the child curve arises due to the linear combination of random variables \(x_c\) and \(x_b\):

\[
x_C = x_c + \left ( \begin{array}{c}I\\I\\ \ldots \\I \end{array}\right) x_b
\]

The corresponding distribution is:

\[
p(x_C | y_\mathcal{MB}) = \mathcal{N}(\mu_C, \Sigma_C)
\]

where

\[
\begin{align}
\mu_C &amp;= \left( \begin{array}{c}I\\I\\ \ldots \\I \end{array}\right) \mu_b \\
\Sigma_C &amp;= \Sigma_c + \left( \begin{array}{c}I\\I\\ \ldots \\I \end{array}\right) \Sigma_b \left ( I \; I \ldots I \right )
\end{align}
\]
&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>Saturday Thoughts - Enabling Non-gaussian models by using Gaussian models as proposal distributions</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/17/work-log"/>
   <updated>2013-08-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/17/work-log</id>
   <content type="html">&lt;p&gt;The different between the models we're fitting and the models we'd really like to use is that our model doesn't have a way to prefer certain branch angles or internode-distance.&lt;/p&gt;

&lt;p&gt;Unfortunately, adding those features would break the Gaussian-ness of our model.  So we couldn't use it to evaluate marginal likelhoods, which we need to evaluate curve-fragemnt correspondences, i.e. triangulation.&lt;/p&gt;

&lt;p&gt;However, what's notable is that is that our models are invriably more permissive than the models we'd prefer.  So any model allowed under our preferred models would certainly be permitted by our &lt;em&gt;worse-but-Gaussian&lt;/em&gt; model.&lt;/p&gt;

&lt;p&gt;So even though we can't distinguish between different species with our weaker model, we can triangulate with it.  And once we have a triangulation, we can switch to a stronger model to do classification.  Since the data is so strong, the posterior under the weak model is still extremely peaked.  We can do importance sampling to marginalize the strong model, using the weak model as a proposal distribution.&lt;/p&gt;

&lt;p&gt;This also gives us an approach to using non-gaussian likelihoods (e.g. pixel-based likelihoods).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;use matlab to construct gaussian model.&lt;/li&gt;
&lt;li&gt;save and read into C++ code&lt;/li&gt;
&lt;li&gt;Use C++ code for sampling full (non-Gaussian) model&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Improved indexing; Retraining; Distinguishing between camera and plant motion</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/16/work-log"/>
   <updated>2013-08-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Improved Indexing (ctd)&lt;/h1&gt;

&lt;p&gt;Finished debugging changes to &lt;code&gt;corr_to_likelihood&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Recap: since projected model curves are discretely sampled at coarse intervals, multiple observed points may correspond to the same model point.  The results below show this. The projected model curve is sampled every 2 pixels (&lt;code&gt;index_delta_2d&lt;/code&gt;), so each model point has between 2 and 3 corresponding data points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-16-bad-indexing.png&quot; alt=&quot;old indexing results in aliasing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The new scheme post-processes the indexes by linearly interpolating the model curve and projecting the data point onto the neighboring line segments.  Resulting indices are much improved:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-16-better-indexing.png&quot; alt=&quot;new indexing scheme permits continuous (between-point) correspondences, which results in better indexing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that viewing angle distorts the correspondence angles somewhat.  Non-perpendicular correspondence lines may be simply due to non-orthogonal viewing direction.&lt;/p&gt;

&lt;p&gt;Since coarse sampling is no longer an issue, we can increase the 2D sample period and still get good results.  Below is the result after increasing 2D sampling period from 2 to 5:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-16-better-indexing-in-spite.png&quot; alt=&quot;new indexing scheme permits continuous (between-point) correspondences, which results in better indexing&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Improved training&lt;/h2&gt;

&lt;p&gt;This has implications on training results.  Re-running training using &lt;code&gt;exp_2013_08_11_train_all&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0030
            noise_variance: 1.0805
         position_variance: 1.6270e+04
             rate_variance: 0.2904
perturb_smoothing_variance: 1
     perturb_rate_variance: 1
 perturb_position_variance: 1
             perturb_scale: 2.5000

Final ML: -9.094636e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Ind-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0034
            noise_variance: 0.3472
         position_variance: 1.6458e+04
             rate_variance: 0.2605
perturb_smoothing_variance: 1.4186e-06
     perturb_rate_variance: 3.0555e-04
 perturb_position_variance: 0.5467
             perturb_scale: 2.5000

Final ML: -6.203953e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;OU-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0035
            noise_variance: 0.3486
         position_variance: 1.6440e+04
             rate_variance: 0.2587
perturb_smoothing_variance: 1.4874e-06
     perturb_rate_variance: 3.6269e-04
 perturb_position_variance: 0.7241
             perturb_scale: 2.3364

Final ML: -6.156721e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;SqExp-Perurb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0035
            noise_variance: 0.3479
         position_variance: 1.6246e+04
             rate_variance: 0.2745
perturb_smoothing_variance: 1.5495e-06
     perturb_rate_variance: 4.1614e-04
 perturb_position_variance: 0.6613
             perturb_scale: 0.9654

Final ML: -6.159716e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Awesome news:  perturb smoothing variance is now non-negligible!&lt;/strong&gt;  There must have been so much IID noise resulting from bad indexing that it totally masked the perturb smoothing variance.&lt;/p&gt;

&lt;p&gt;The totally validates our efforts to fix indexing.  Before, the model was fundamentally broken; bad indexing was preventing us from making any correct inferences beyond a certain level of granularity.  By fixing indexing, we're suddenly able to everything clearly, whereas before we were squinting through a noisy haze.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other observations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ML is much lower compared to the badly indexed results, which were one the order of -8000 (&lt;a href=&quot;/ksimek/research/2013/08/14/work-log/&quot;&gt;according to this post&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Noise variance dropped from 0.68 to 0.35.&lt;/li&gt;
&lt;li&gt;Smoothing variance has increased, probably because we attribute fewer deviations to IID noise.  Great!&lt;/li&gt;
&lt;li&gt;Global rate variance is lower, while perturb rate variance roughly tripled.&lt;/li&gt;
&lt;li&gt;Perturb scale dropped slightly.  Since noise variance can't explain independent deviations, the perturb-model takes over, becomes closer to independent.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Lets see if anything interesting comes out of our reconstructions...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;OU-perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az = 24;
el = 16;
axis_ = [ 70.0000  110.0000   50.0000  110.0000   47.8040  224.0467 ]

exp_2013_08_11_reconstruct_for_web(test_Corrs_ll_2, retraining_results{3}, 3, axis_, el, az, num_views, '/Users/ksimek/src/research_blog/img/2013-08-16-ou-model-%d.png', '/ksimek/research/img/2013-08-16-ou-model-%d.png', 'ou-reconstruct-anim', true)
&lt;/code&gt;&lt;/pre&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-16-ou-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-anim&quot; style=&quot;width:137px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;SqExp-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-9.png&quot;
        ]

    construct_animation($(&quot;#sqexp-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;sqexp-reconstruct-anim&quot; style=&quot;width:136px&quot;&gt; &lt;/div&gt;


&lt;h2&gt;Removing camera-based motion&lt;/h2&gt;

&lt;p&gt;We can remove perturbations that arise from poor camera calibration by assuming it is captured in the linear and offset perturbations; under this assumption, the remaining cubic-spline smooth perturbations capture the true plant motion.&lt;/p&gt;

&lt;div&gt;Removing linear and offset perturbations is as simple as removing their contributions to \(K^*\) in yesterday's equation for the mean of the predictive distribution.&lt;/div&gt;


&lt;p&gt;Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reverse = false(1,num_tracks);
reverse([1 2 4 5 6 8 9 10 11 12 14 15]) = true;
% error above, should omit 11:
% reverse([1 2 4 5 6 8 9 10 12 14 15]) = true;
exp_2013_08_16_visualize_smooth_perturbations( ...
        test_Corrs_ll_2, ...
        retraining_results{3},  ...
        3, axis_, el, az, num_views,  ...
        '/Users/ksimek/src/research_blog/img/2013-08-16-ou-model-smooth-%d.png', ...
        '/ksimek/research/img/2013-08-16-ou-model-smooth-%d.png', ...
        'ou-reconstruct-smooth-anim', reverse)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;removing-camera-perturbation&quot;&gt;&lt;/a&gt;
&lt;em&gt;OU-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-smooth-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-smooth-anim&quot; style=&quot;width:250px&quot;&gt; &lt;/div&gt;


&lt;p&gt;It's notable that the little curves at the top don't move.  Attaching them to the large main step will allow them to move, which should improve ML.&lt;/p&gt;

&lt;p&gt;It should also significantly affect training if we train with attachments in place.  Perturb_position_variance should be responsible for less of the variance, and perturb_smoothing_variance should explain more.&lt;/p&gt;

&lt;h1&gt;Attachments&lt;/h1&gt;

&lt;p&gt;Tasks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Create new attachments structure.  Indicates which curve and index the curve is attached to.&lt;/li&gt;
&lt;li&gt;Algorithm to convert N attachment to M track-sets, where M is the number of connected components in the attachment structure.  (in practice, attachment structure might be irrelevant, given the track-set structure).&lt;/li&gt;
&lt;li&gt;New function to construct prior matrix for track-sets, as opposed to individual tracks.&lt;/li&gt;
&lt;li&gt;New function to evaluate ML over track-sets.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Add attachments before training.&lt;/li&gt;
&lt;li&gt;Complete end-to-end training

&lt;ul&gt;
&lt;li&gt;input: path of training and curves&lt;/li&gt;
&lt;li&gt;automatically add attachments&lt;/li&gt;
&lt;li&gt;automatically reverse curves as needed&lt;/li&gt;
&lt;li&gt;save trained parameters somewhere&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Index Refinement; Mean-curve Reconstruction</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/15/work-log"/>
   <updated>2013-08-15T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/15/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing from yesterday.&lt;/p&gt;

&lt;p&gt;Tried changing index_delta from 0.5 to 0.25.  ML suddenly dropped a &lt;em&gt;lot&lt;/em&gt;.  Realized that oversampling the smoothed curves causes significant degree of many-to-one, and our code prevents skipping more than one or two points during correspondences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Idea:&lt;/strong&gt; Sampling period should be equal to (or close to) the data point-spacing.&lt;/p&gt;

&lt;h1&gt;Index-refinement&lt;/h1&gt;

&lt;p&gt;Allow indices to correspond to continuous values between the samples.&lt;/p&gt;

&lt;p&gt;Modified &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt; to post-process indices to search the neighboring line segments for a better value. Re-ran &lt;code&gt;tr_prep_likelihood&lt;/code&gt; to reconstruct Corrs collection.  ML improves after fix.&lt;/p&gt;

&lt;p&gt;Need to confirm improved indexing by visualizing results.  See &lt;code&gt;experiments/exp_2013_08_15_visualize_indices.m&lt;/code&gt;, still in progress.  Some apparent bugs in aforementioned changes, causing bad results.  Still investigating...&lt;/p&gt;

&lt;h1&gt;Mean-curve reconstruction&lt;/h1&gt;

&lt;p&gt;Last week, I derived the equation for finding the maximum posterior per-view reconstruction.  Now we have a formula for the unobserved mean curve, i.e. the curve that each view is perturbed from.  Iterestingly, they both have the same form:&lt;/p&gt;

&lt;div&gt;
\[
    \mu = K^* (S K + I)^{-1} S y
\]
&lt;/div&gt;


&lt;p&gt;Note that this is slightly different from the form used in a previous post.  This form is equivalent, and better reflects the form used by Williams and Rasmussen.&lt;/p&gt;

&lt;div&gt;Here, \(K\) is the prior covariance, \(S\) is the likelihood precision matrix, and \(y\) are the virtual observations in 3D.  \(K^*\) is the covariance between the observed points (columns) and the points to be predicted (rows). The difference between the per-view and the mean reconstruction is the form of \(K^*\):  for the per-view reconstruction, \(K^*\) uses the full prior covariance, whereas with the mean reconstruction, the perturb covariances are set to zero.&lt;/div&gt;


&lt;h2&gt;Usage&lt;/h2&gt;

&lt;p&gt;Modified &lt;code&gt;curve_max_posterior_2.m&lt;/code&gt; by adding an extra optional parameter, &lt;code&gt;kernel_2&lt;/code&gt;.  If set, it assumes you want a mean reconstruction instead of a per-view reconstruction, and uses &lt;code&gt;kernel_2&lt;/code&gt; to build \(K*\).&lt;/p&gt;

&lt;p&gt;Added an extra flag to &lt;code&gt;experiments/exp_2013_08_11_reconstruct_for_web.m&lt;/code&gt;, which if set to true, will also plot the mean curve.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Background ML bugs; Why is Foreground Noise Variance so large?</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/14/work-log"/>
   <updated>2013-08-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/14/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;ML Validity Testing&lt;/h1&gt;

&lt;p&gt;In yesterday's test, I hadn't realized that the training ML didn't include all of the curves, only the foreground curves were included.  Rerunning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference ML: -5.1552e+04
Training ML: -5.0786e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Okay, we're back in the ballpark -- Within 1.5% of the reference.&lt;/p&gt;

&lt;p&gt;Found the other issue: the roundabout way I was using to generate the training covariance matrices was ignoring the user-specified position_variance_2d.  Results now match:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference ML: -5.1552e+04
Training ML: -5.1552e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, none of this explains why training is causing noise variance to collapse so low.  All discovered problems were merely bugs in the validation logic.  At least we know the training ML logic is valid.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Quick inspection shows that 2D curves are, indeed, pre-smoothed.  This means that noise variance can collapse to near-zero when fitting 2D curve.&lt;/p&gt;

&lt;p&gt;The new question becomes: why doesn't it occur in the foreground (3D) model?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Try re-indexing...&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% update params related to smoothing variance
params_2 = tbc_.params;
params_2.smoothing_variance = training_results{1}.smoothing_variance;
params_2.noise_variance = training_results{1}.noise_variance;
% re-construct likelihood (including indicies)
test_Corrs_ll_2 = tr_prep_likelihood(test_Corrs, data_, params_2);
% re-run training
train_params_done = tr_train(test_Corrs_ll_2, training_results{2}, 400, 3);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0011
            noise_variance: 0.6846
         position_variance: 1.3597e+04
             rate_variance: 0.3042
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.3013e-06
 perturb_position_variance: 0.6621
             perturb_scale: 2.9795

Final ML: -7840.140322
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare against results prior to re-indexing: version:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654

Final ML: -8049.9e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we've improved, but nowhere near the background model's ML of 4611.9.  Note that curves got smoother and less noisy.  More correlation, more variance pushed into the perturbations.  (why the f*** perturb_smoothing_variance is just sitting there like an idiot is still beyond me).&lt;/p&gt;

&lt;h2&gt;Miscellaneous thoughts&lt;/h2&gt;

&lt;p&gt;Need to visualize ll_means against smoothed curve.  The perturbations should be correlated.  Maybe even plot them?  Note the perturbations need to be considered only in the directions parallel to the image plane.  &lt;strong&gt; This has never been done, and is necessary to validate the index-estimation in &lt;code&gt;correspondence/corr_to_likelihood&lt;/code&gt;.&lt;/strong&gt; Can we visualize after removing rate and offset components?  Yes: difference between ll_means and per-view reconstructed curve.&lt;/p&gt;

&lt;p&gt;Consider smarter smoothing in &lt;code&gt;corr_to_likelihood&lt;/code&gt; -- using posterior max instead of &lt;code&gt;csaps&lt;/code&gt;.    Could give better index estimation if the visualization test shows problems.&lt;/p&gt;

&lt;p&gt;What if we were using a 3D likelhood for background curves too? Could we still expect the BG ML to be insanely high, and the noise variance to be insanely peaked?  Backproject, estimate 3d noise variance, estimate index set.  The farther away it gets, the more variance in the correlated points.  Which means lower ML, right?  But training will push variance lower.&lt;/p&gt;

&lt;p&gt;Note that larger noise variance in FG model will explain away bad triangulations.  Perturb variances also explain to some extent, but maybe they aren't sufficient to explain enough of it.&lt;/p&gt;

&lt;p&gt;Why is perturb_smoothing_variance basically zero?  If it was higher, it could explain more of the traingulation error, and allow noise variance to drop.  Should we be using a different pertubation model?  Maybe Brownian motion instead of integrated brownian motion?  Visualizing perturbations would be informative here.&lt;/p&gt;

&lt;p&gt;Do smooth perturbations follow a different dynamics model than linear and offset perturbations?  Can we force it to be larger?  what if it was the only option for modelling perturbations?   It's true that a small amount of smoothing variance can result in a huge amount of marginal point variance, and large variances kill ML's.  Probably a mean-reverting model is more sensible -- Ornstein Ulenbeck process, perhaps?  Or SqExp?  I avoided these in the past, because it changes the form of the marginal curve covariances -- they're no longer purely cubic-spline processes.  But I never considered the fact that we need to model triangulation error.&lt;/p&gt;

&lt;p&gt;Observations: setting perturb_smoothing_variance to exactly zero has no change in ML.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Consider tying foreground and background noise variance during training.&lt;/strong&gt;  &amp;lt;---  This is the most pragmatic solution.  Avoids getting mired in details, and acknowledges what we know to be true: image noise arises from the same process in foreground and background models.&lt;/p&gt;

&lt;p&gt;Possibly the fact that we allow a nonzero position_mean in 2D but not in 3D is the issue?&lt;/p&gt;

&lt;h1&gt;Finer-grained index estimation&lt;/h1&gt;

&lt;p&gt;Got it!  In &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;, we have two parameters that determine how fine-grained the sampling is along the smoothed curve.  Each observed curve is then matched against the sampled curve.  The sampling period is 2 pixels, which means there's an average error of about 1 pixel in each index estimate.&lt;/p&gt;

&lt;p&gt;Reducing the sampling period to 1 pixel and re-training gives:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0014
            noise_variance: 0.3986
         position_variance: 1.3555e+04
             rate_variance: 0.3100
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 3.3992e-04
 perturb_position_variance: 0.6874
             perturb_scale: 2.3823

Final ML: -6357.585946
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Although we only slightly changed the sampling period, the final ML improved significantly.  The noise variance dropped from 0.7 to 0.4, too.  Perturb rate variance changed by 2 orders of magnitude!&lt;/p&gt;

&lt;p&gt;Reducing sampling period to 0.5:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0018
            noise_variance: 0.3060
         position_variance: 1.3499e+04
             rate_variance: 0.3098
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 4.3411e-04
 perturb_position_variance: 0.8152
             perturb_scale: 2.4095

Final ML: -5664.35
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The upward ML trend continues, but the noise variance appears to be flattening out.  The perturb_position_variance jumped upward unexpectedly.&lt;/p&gt;

&lt;p&gt;This might explain all of the disparity between the 2D and 3D noise variances.  Unfortunately, we can't reduce the sampling period to 0.0004, because the runtime complexity of the matching  is O(N&lt;sup&gt;2),&lt;/sup&gt; where N is the number of sampled points.&lt;/p&gt;

&lt;p&gt;Better idea: after finding the optimal region in the matching algorithm, improve it by projecting the point onto the line segments neighboring the matched point.  Constant-time, and significantly better!&lt;/p&gt;

&lt;p&gt;Another thought: if the rasterization error was approx. 1 pixel, the sampling error could be reduced to the point where the rasterization error dominated (possibly a sampling period of  0.5 or 0.01 would achieve this).  That way, both 2D and 3D noise sigma would be dominated by rasterization error, and would train to similar values.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;implement post-match index improvement.&lt;/li&gt;
&lt;li&gt;Plot reconstruction residuals, look for correlation model.

&lt;ul&gt;
&lt;li&gt;Goal: determine if residuals are truely independent, and belong in the noise bucket.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try training BG and FG together, with the same noise variance.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Theoretical Rate variance bug; Training background curve model</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log"/>
   <updated>2013-08-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Curve reversing thoughts&lt;/h1&gt;

&lt;p&gt;The reversed curve issue only really matters during training.  Our tests show that curve-flip moves will mix well, even if the maximum isn't always correct.  Adding connections between parent and child curves should resolve these issues.&lt;/p&gt;

&lt;p&gt;During training, derive curve direction from ground-truth.&lt;/p&gt;

&lt;h1&gt;Theoretical Rate Variance (take 2)&lt;/h1&gt;

&lt;p&gt;Realized that my last attempt at this had two bugs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I used &lt;code&gt;rand()&lt;/code&gt; instead of &lt;code&gt;randn()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;I was normalizing by the &lt;em&gt;squared&lt;/em&gt; vector magnitude.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Fixing this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir = randn(3,10000000);
dir = bsxfun(@times, dir, 1./sqrt(sum(dir.^2)));
var(dir(:))

    ans =
        0.3332
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare this to earlier theoretical results of ~0.23.&lt;/p&gt;

&lt;p&gt;This new result is interesting, because it is 25% higher than the emperical results we've been getting.  I'm guessing that the fact all of the curves point upward reduces the variance. To prove, we'll force all points to be in the top hemishphere:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; dir = randn(3,10000000);
 dir(3,:) = abs(dir(3,:));
 dir = bsxfun(@times, dir, 1./sum(dir.^2));

 var(dir(:))

    ans =
        0.3149
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep.  And in practive, our values take on an even smaller range of directions.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Repeating for the 2D case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir = randn(2,10000000);
dir = bsxfun(@times, dir, 1./sqrt(sum(dir.^2)));
var(dir(:))

    ans =
        0.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This strongly suggests a pattern of variance being 1/D.&lt;/p&gt;

&lt;h1&gt;Connection test&lt;/h1&gt;

&lt;p&gt;does connecting each of the curves result in better ML?  Do we need to marginalize?&lt;/p&gt;

&lt;h1&gt;training background model&lt;/h1&gt;

&lt;p&gt;construct training ML for background ML
construct mats for bg curve models.&lt;/p&gt;

&lt;p&gt;Result: &lt;code&gt;train/tr_train_bg.m&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 1.7837e+04
     rate_variance_2d: 0.5000
    noise_variance_2d: 9.4597e-04
smoothing_variance_2d: 0.0157
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that noise_variance_2d is so low.  We expected it to be on the order of 1 pixel.  More discussion on this later.&lt;/p&gt;

&lt;p&gt;I retrained the BG model for &lt;em&gt;only&lt;/em&gt; the foreground curves, and evaluated the ML under it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 5.8116e+03
     rate_variance_2d: 0.5000
    noise_variance_2d: 4.8105e-04
smoothing_variance_2d: 0.0053
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Smaller noise variance, smaller smoothing variance.  This shrinking of variance with smaller training set is typical overfitting behavior.  Not of much concern.&lt;/p&gt;

&lt;p&gt;Here's the comparison against the ML for the trained foreground model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bg model = 4611.886746
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not good.  The FG model on true foreground curves should have a better marginal likelihood than the same curves under the BG model.&lt;/p&gt;

&lt;p&gt;Some questions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Why is bg noise variance so low??

&lt;ul&gt;
&lt;li&gt;did we smooth the detected curves before storing them?&lt;/li&gt;
&lt;li&gt;If so, why isn't the foreground model lower?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why is the background model so much better than the foreground model?

&lt;ul&gt;
&lt;li&gt;We expect foreground curves to have a higher marginal likelihood under the foreground model than the background.&lt;/li&gt;
&lt;li&gt;could it be an indexing issue?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This warrants further investigation.&lt;/p&gt;

&lt;h2&gt;Other observations&lt;/h2&gt;

&lt;p&gt;If I force the noise variance to be equal to that of the fg model (0.72), the ML drops significantly (fg results reprinted for convenience):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; bg model = -9413.1e+03
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we're back in business.  This is a good sanity check, but it doesn't explain why we can't get similar noise variances for both models when training.&lt;/p&gt;

&lt;p&gt;Possibly the smoothing variance would need to change in this case.  Retraining, with noise_variance forced to 0.72:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 5.8116e+03
     rate_variance_2d: 0.5000
    noise_variance_2d: 0.7204
smoothing_variance_2d: 2.7394e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Smoothing variance dropped dramatically.  ML comparison (fg results reprinted for convenience):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bg model = -9214.632874
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that bg ML didn't significantly change (-2%) after optimizing noise variance (which did change a lot).&lt;/p&gt;

&lt;p&gt;Might be worthwhile visualizing the optimal fits with these parameters.  Are we oversmoothing?  undersmoothing?  These would suggest a bug.&lt;/p&gt;

&lt;h2&gt;ML Validity Testing&lt;/h2&gt;

&lt;p&gt;Running reference ml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data_2 = init_data_curves_ml(data_, bg_train_params_done_force)
sum([data_2.curves_ml{:}])

    ans =
      -5.1552e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Very different from the training implementation.  Need to dig deeper to determine the cause.&lt;/p&gt;

&lt;h2&gt;Misc Thoughts&lt;/h2&gt;

&lt;p&gt;Do we need to re-estimate the index set during training of the FG model?&lt;/p&gt;

&lt;p&gt;Iterate: train, re-index, repeat.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;investigate disparity between training ML and reference ML for background curves.&lt;/li&gt;
&lt;li&gt;Further investigate the FG vs. BG marginal-likelihood issue.&lt;/li&gt;
&lt;li&gt;test the re-indexing approach to FG model training.&lt;/li&gt;
&lt;li&gt;re-build end-to-end sampler.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Re-run training, Re-reconstruction, Curve-Flipping</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log"/>
   <updated>2013-08-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Re-training&lt;/h1&gt;

&lt;p&gt;Re-ran training after several bug-fixes.&lt;/p&gt;

&lt;h2&gt;New Files:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;train/tr_train_all.m&lt;/code&gt;  - Utility method for training all four models.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;experiments/exp_2013_08_11_train_all.m&lt;/code&gt;  - end-to-end training example; recreates results here.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;All results generated by &lt;code&gt;exp_2013_08_11_train_all.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0024
            noise_variance: 1.2308
         position_variance: 1.6072e+04
             rate_variance: 0.2743
perturb_smoothing_variance: 1
     perturb_rate_variance: 1
 perturb_position_variance: 1
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7192
         position_variance: 1.6132e+04
             rate_variance: 0.2451
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1292e-06
 perturb_position_variance: 0.4849
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;OU-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0018
            noise_variance: 0.7207
         position_variance: 1.6117e+04
             rate_variance: 0.2480
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1355e-06
 perturb_position_variance: 0.5172
             perturb_scale: 0.9202
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised to see that noise-variance only changed by a factor of 10, not 100.  However, the resulting noise_variance is right in the range that you'd expect arising from pixel-grid rasterization error.&lt;/p&gt;

&lt;p&gt;OU perturb-scale is lower than in the last case, and perturb position and rate variance is lower, too.&lt;/p&gt;

&lt;p&gt;SqExp perturb-scale is higher than in the last case, and perturb rate variance is lower.&lt;/p&gt;

&lt;p&gt;Lower position and rate variance makes sense after correcting curve-reversals.&lt;/p&gt;

&lt;p&gt;However, since we trimmed the pre-tails, a higher global and perturb position variance should result.  The result we're seeing is a combination of these competing effects.&lt;/p&gt;

&lt;h1&gt;Reconstructions&lt;/h1&gt;

&lt;p&gt;Some curves are flipped; need to an approach that will detect and correct flipped curves.&lt;/p&gt;

&lt;p&gt;Images and javascript generated by &lt;code&gt;../experiments/exp_2013_08_11_reconstruct_for_web.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-ind-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-9.png&quot;
        ]

    construct_animation($(&quot;#ind-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ind-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;OO-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-ou-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-9.png&quot;
        ]

    construct_animation($(&quot;#sqexp-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;sqexp-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;h1&gt;Detecting and Flipping Curves&lt;/h1&gt;

&lt;p&gt;Experiment: &lt;code&gt;../experiments/exp_2013_08_11_flip_curves.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Result: doesn't really work.  Lots of false negatives.&lt;/p&gt;

&lt;p&gt;Algorithm output: 2     4     5     6     8    10    12
Ground Truth: 1 2 4 5 6 8 9 10 11 12 14 15&lt;/p&gt;

&lt;p&gt;Not really sure why this is failing.  After flipping, most of these curves are closer to the origin, which is promoted by position_variance.  And any tip-perturbations should be better modelled after flipping.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Think more on curve-reversing&lt;/li&gt;
&lt;li&gt;Central curve extraction&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;li&gt;end-to-end correspondence sampling&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Pre-tails fix</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/10/work-log"/>
   <updated>2013-08-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/10/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Pre-tails issue&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Bug&lt;/strong&gt;: Smallest index of reconstructed curves is significantly greater than 0.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp_setup_workspace
min([test_Corrs_ll{1}.ll_indices{:}])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This results in long pre-tails, as seen in this image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-10-pretails.jpg&quot; alt=&quot;pretails&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recall that reconstruction occurs by doing (1) rough triangulation, (2) smoothing, then (3) re-triangulating against the smoothed curve.  The initial triangulation usually results in a very bad index set, with spacing far larger than it should be, due to poor localization by maximum likelihood.  The subsequent smoothing causes the curve to stretch out longer than it should, so when re-triangulation occurs, ends are cut off.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Solution &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After re-triangulating, re-index so the minimum index is zero.&lt;/p&gt;

&lt;p&gt;Change to &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;145  % re-index by subtracting minimum index
146  min_index = min([Corr.ll_indices{:}]);
147  Corr.ll_indices = cellfun(@(x) x - min_index, Corr.ll_indices, 'UniformOutput', false);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt; Fallout &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This likely had an effect on training results, because marginal prior variance of the initial point was over-estimated, because it's index was over-estimated.&lt;/p&gt;

&lt;h1&gt;Cleanup&lt;/h1&gt;

&lt;p&gt;I'm afraid I made a mess of things yesterday when I was addressing the noise_variance issue in training.  Need to review the end-to-end systems for training, reconstruction and marginal likelihood evaluation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is training world-variance scaled everywhere?&lt;/li&gt;
&lt;li&gt;is training world-variance projecting to 1.0?&lt;/li&gt;
&lt;li&gt;is training-ml equal to inference-ml?&lt;/li&gt;
&lt;li&gt;are the reconstructed results sensible?&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;handle reversed curves&lt;/li&gt;
&lt;li&gt;Retrain all models since the following changes

&lt;ul&gt;
&lt;li&gt;reversal fixes&lt;/li&gt;
&lt;li&gt;noise variance fix&lt;/li&gt;
&lt;li&gt;&quot;pre-tail&quot; index fix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;central curve extraction&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;li&gt;end-to-end sampling system&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Refactoring, cleanup, bug fixes</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/09/work-log"/>
   <updated>2013-08-09T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/09/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15169&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Goals&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Move &lt;code&gt;test/tmp*&lt;/code&gt; to &lt;code&gt;experiment/&lt;/code&gt; directory.&lt;/li&gt;
&lt;li&gt;Overlay reconstruction onto images.&lt;/li&gt;
&lt;li&gt;Test effect of reversing curves on ML.&lt;/li&gt;
&lt;li&gt;Train background curve model.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Git Blog Mess&lt;/h1&gt;

&lt;p&gt;Got sidetracked after screwing up a git commit of the research blog.  Not sure the cause, but several files were deleted from the &quot;source&quot; branch.  Changed the &quot;preview&quot; rake target so it builds to /tmp/research_blog_site, instead of the source directory.  Hopefully this will avoid these issues in the future.&lt;/p&gt;

&lt;h1&gt;End-to-end experiment&lt;/h1&gt;

&lt;p&gt;Created an experiment file that recreates yesterday's results from scratch: &lt;code&gt;exp_2013_08_09_animated_reconstruction.m&lt;/code&gt;.  Since it runs training, it takes about 5 minutes to run.&lt;/p&gt;

&lt;p&gt;Also broke out reconstruction code into function in &lt;code&gt;reconstruction/reconstruct_views.m&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Overlay reconstruction onto images&lt;/h1&gt;

&lt;p&gt;See &lt;code&gt;test/tmp_vis_overlay.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I'm seeing some weirdness in the reconstructions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Curves are too stiff&lt;/li&gt;
&lt;li&gt;long pre-tails on a couple of curves.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Stiff Curves Solved&lt;/h1&gt;

&lt;p&gt;Figured out what was causing stiff curves.  I forgot that during training, all precisions are stored with noise_variance fixed at 1.0, and then are scaled on the fly.  However, during visualization, that scaling doesn't occur; the precisions are assumed to be stored at the desired scale.  i.e. a dumb bug.&lt;/p&gt;

&lt;p&gt;The new reconstructions now show a moderate amount of curvature, compared to their pre-bug stiff counterparts.&lt;/p&gt;

&lt;p&gt;Found another bug:  when constructing the &quot;unscaled&quot; precisions in &lt;code&gt;tr_prep_likelihood.m&lt;/code&gt;, I called &lt;code&gt;corr_to_likelihood&lt;/code&gt; with &lt;code&gt;params.noise_variance&lt;/code&gt; instead of 1.0.  Thus, if I understand correctly, the reported training value for noise-variance is 100x lower than it should be.&lt;/p&gt;

&lt;p&gt;That means the no-perturb model has a noise standard deviation on the order of 3.4 pixels and the perturb models are around 2.7.  This is closer to the range I was expecting, but I was hoping the perturb model stddev would be closer to 0.5, because it should arise only from pixel rasterization.  However, other sources of noise could be the curve detector, and also the cubic spline model might not be expressive enough to capture the model variance.&lt;/p&gt;

&lt;p&gt;I think I need to re-run end-to-end training and reconstruction to make sure there aren't any side-effects of these fixes.&lt;/p&gt;

&lt;p&gt;Committed to revision 15169&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Determine cause of long pre-tails.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Visualizing Results; New training method</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/08/work-log"/>
   <updated>2013-08-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Visualization&lt;/h1&gt;

&lt;p&gt;I reconstructed the curves using the models I trained yesterday.  I was able to recover both the overall structure and track it's motion over 9 views.  For each of these results, use the slider below to change between each of the 9 views (the changes are subtle).&lt;/p&gt;

&lt;p&gt;I'm still struggling with smoothing variance being too low, which causes curves to be too straight.  For all of these results, I manually changed &lt;code&gt;smoothing_variance&lt;/code&gt; to be 0.1.&lt;/p&gt;

&lt;script&gt;
$(function(){
    var ind_urls = [
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#ind-reconstruction&quot;), ind_urls);


    var ou_urls = [
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#ou-reconstruction&quot;), ou_urls);

    var sqexp_urls = [
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#sqexp-reconstruction&quot;), sqexp_urls);

});
&lt;/script&gt;


&lt;h2&gt;Ind-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Below is the reconstruction for the independent-perturbation model.  The curves in each view are independently perturbed versions of central mean curves (not shown).&lt;/p&gt;

&lt;div id=&quot;ind-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;p&gt;It is unclear how much of this motion is due to camera miscalibration, and how much is actual plant motion.  Nevertheless, this shows that we can use the perturbation models to simultaneously triangulate and track over time.&lt;/p&gt;

&lt;p&gt;First, although each &lt;em&gt;view's&lt;/em&gt; perturbation is independent of the others, the perturbation is correlated between nearby points within the same view.  In other words, perturbations don't violate the smoothness constraint.&lt;/p&gt;

&lt;h2&gt;OU-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Next is the Ornstein-Ulenbeck perturbation model.  As opposed to the previous model, which modeled each view's perturbations as white noise, this model assumes Brownian motion over time.  Thus, we see a more naturally evolving time-series.&lt;/p&gt;

&lt;div id=&quot;ou-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;p&gt;The OU process models brownian motion&lt;/p&gt;

&lt;h2&gt;SQEXP-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Finally we have the squared-exponential perturbation model.  Now Brownian motion has been replaced by smooth motion.  I'm doubtful that this is a natural motion model for these plants.  The scale parameter is so low, I question whether it has any significant effect.&lt;/p&gt;

&lt;div id=&quot;sqexp-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;h1&gt;New Training Method&lt;/h1&gt;

&lt;p&gt;Need to determine why learned smoothing variance is so low.&lt;/p&gt;

&lt;p&gt;Is it even valid to do max-likelihood to train the parameters of the covariance function?&lt;/p&gt;

&lt;p&gt;Should we be training the smoothness parameter using the noiseless ground-truth data?&lt;/p&gt;

&lt;h2&gt;Two-pass Learning Procedure&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;First learn using noiseless data:

&lt;ol&gt;
&lt;li&gt;Set rate-variance to fixed ~0.23 (see &lt;a href=&quot;/ksimek/research/2013/08/07/work-log/#optimal-rate-variance&quot;&gt;yesterday's results&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Estimate  position_variance by emperical distribution over all point positions in ground truth.&lt;/li&gt;
&lt;li&gt;Estimate smoothing_variance by maximum likelihood over ground truth.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Learn the noise variance for the no-perturb model.

&lt;ol&gt;
&lt;li&gt;Find point correspondence between detected curves and corresponding ground truth curve.&lt;/li&gt;
&lt;li&gt;Compute variance between projected points and observed points.  This should maximize \(p(D \mid \Theta_0) \), where \(\Theta_0\) are the ground truth curves.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Learn perturb model parameters: noise_variance, perturb_{smoothing_variance, rate_varaince, position_variance}

&lt;ol&gt;
&lt;li&gt;Use nonlinear optimization to maximize \(p(D \mid \Theta_0) \), as defined below.  Start with no-perturb parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;div&gt;Let \(\theta_0\) be the ground-truth curve, and \(\{\theta_i\}\) be the set of all (unobserved) per-view curves \(\theta_i\). Let \(S_i\) be the virtual point precision matrices, assuming noise variance \(\sigma_n^2 = 1\).  The likelihood conditioned on the ground truth data \(p(D \mid \theta_0)\) is given by: &lt;/div&gt;




&lt;div&gt; 
\begin{align}
p(D \mid \theta_0) &amp;= \int_{\{\theta_i\}}  p(D_i, \{\theta_i\} \mid \theta_0)  d\{\theta_i\} \\
                   &amp;= \int_{\{\theta_i\}} \prod_{i=1}^N p(\theta_i \mid \theta_0) p(D_i | \theta_i) d\{\theta_i\} \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  p(\theta_i \mid \theta_0) p(D_i | \theta_i) d\theta_i \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  \mathcal{N}(\theta_i; \theta_0, \Sigma_p) \mathcal{N}(D_i;  \theta_i, \sigma_n^2 S_i^{-1}) d\theta_i \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  \mathcal{N}(D_i ; \theta_0, \Sigma_p + \sigma_n^2 S_i^{-1})
\end{align}
&lt;/div&gt;




&lt;div&gt; where  \(\Sigma_p\) is the perturbation variance. &lt;/div&gt;


&lt;p&gt;This should be an improvement over the current method, where we use only ground-truth labellings (not positions) and fit all parameters simultaneously.  This method assumed too much noise variance and not enough smoothness variance.&lt;/p&gt;

&lt;p&gt;Since we actually know the noiseless curves, we should train to that, to avoid the confounding of smoothness variance and noise variance.  The ground truth curves are aso stronger sources of evidence, compared to the curves reconstructed from data.&lt;/p&gt;

&lt;h1&gt;TODO:&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;reproject per-view reconstructions and overlay with original image.&lt;/li&gt;
&lt;li&gt;obtain &amp;amp; visualize unobserved &quot;central curve&quot;&lt;/li&gt;
&lt;li&gt;investigate the low smoothness variance.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Later TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;automatic curve reversing?&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Training, Reversed Curves, and Theoretical Rate Variance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/07/work-log"/>
   <updated>2013-08-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/07/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Visualized results after capping likelihood variance.  As expected, degree of spreading stops growing as perturb_rate_variance continues to grow.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Tasks&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;manually flip some curves and see if model changes&lt;/li&gt;
&lt;li&gt;try automatically determining which to curves need flipping&lt;/li&gt;
&lt;li&gt;try to get training and visualization to agree&lt;/li&gt;
&lt;li&gt;visualize curves moving through space over time&lt;/li&gt;
&lt;li&gt;train background curve model

&lt;ul&gt;
&lt;li&gt;is background model better than foreground?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Long term goals&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sampling framework

&lt;ul&gt;
&lt;li&gt;try using background pixel modeling to prune background curves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Visualizing curve-direction revealed a bizarre artifact: most curves start somewhere in the middle of the reconstructed curve!&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Found issue - wasn't sorting by index when reconstructing.&lt;/p&gt;

&lt;h2&gt;Breaking Change&lt;/h2&gt;

&lt;p&gt;Modified &lt;code&gt;tr_curves_ml&lt;/code&gt; to &lt;em&gt;not&lt;/em&gt; include the background curve ml into the computation.  Recall that the normal ML computation code doesn't actually return ML, but a &lt;em&gt;ratio&lt;/em&gt; of the foreground curve ML and the background curve ML.  This indicates how much the model improves over the &quot;null model&quot;.&lt;/p&gt;

&lt;p&gt;Since the background curve ml is constant during training, this shouldn't affect results.  However, if you want to confirm the correctness of &lt;code&gt;tr_curves_ml&lt;/code&gt; against the reference implementation in &lt;code&gt;curve_ml5.m&lt;/code&gt;, you'll need to manually divide by a constant.  See the documentation for &lt;code&gt;tr_curves_ml&lt;/code&gt; for more details.&lt;/p&gt;

&lt;h2&gt;Reversing Curves&lt;/h2&gt;

&lt;p&gt;Investigating the effect of reversing curves.&lt;/p&gt;

&lt;p&gt;Visually determined which curves were reversed.  See modified version of &lt;code&gt;test/tmp_visualize_test&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Hacked &lt;code&gt;train/tr_construct_matrices.m&lt;/code&gt; with hard-coded list of curves to flip.  Re-ran training for &lt;em&gt;IND-Perturb&lt;/em&gt; model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;: we should see larger values for perturb_rate_variance and/or perturb_smoothing_variance, and smaller values for perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0020
            noise_variance: 0.0720
         position_variance: 1.3414e+04
             rate_variance: 0.2378
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.5332e-06
 perturb_position_variance: 0.4662

Final ML: -95.736042
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare against old results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0718
         position_variance: 1.6706e+04
             rate_variance: 0.2135
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4942e-06
 perturb_position_variance: 0.4886

Final ML: -97.463243
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary of changes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: +2.09%
            noise_variance: 0.17%
         position_variance: -19.7%
             rate_variance: +11.3%
perturb_smoothing_variance: 0 
     perturb_rate_variance: +2.61%
 perturb_position_variance: -4.59%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As expected, global position variance dropped; perturb rate grew while perturb position variance decreased.&lt;/p&gt;

&lt;p&gt;Unexpected increase in rate_variance; expected it to stay constant.  Possibly due to random fluctuations; both old and new values (0.214 and 0.238, respectively) are near the theoretical optimum (0.23, see next section).&lt;/p&gt;

&lt;p&gt;Also unexpected small increase in global smoothing variance (expected to be constant); also possibly due to random fluctuations.&lt;/p&gt;

&lt;p&gt;Literally no change to perturb smoothing variance.  I'm starting to suspect something weird is going on with this value...&lt;/p&gt;

&lt;h2 id=&quot;optimal-rate-variance&quot;&gt;Theoretical Rate Variance&lt;/h2&gt;


&lt;p&gt;Was curious what the rate variance should be, assuming the rate vectors are drawn from a uniform distribution over the unit sphere.&lt;/p&gt;

&lt;p&gt;Determined empirically that rate variance should be somewhere between 0.220 and 0.235.  Code below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% generate 10,000 3-vectors with distribution over direction
dir = rand(3,10000);
% normalize to lie on unit sphere
dir = bsxfun(@times, dir, 1./sum(dir.^2));
% Get the emperical covariance of the vectors
Sigma = cov(dir')

        ans =

            0.2105    0.0556    0.0580
            0.0556    0.2297    0.0680
            0.0580    0.0680    0.3735
% take the average of the diagonals
mean(diag(Sigma))

        ans =

            0.2290
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This strongly suggests that the global rate variances we've seen in training are consistent with the theoretical value.  Great!&lt;/p&gt;

&lt;h2&gt;Visualizing Curve Motion&lt;/h2&gt;

&lt;p&gt;Attempting to visualize perturbations between views.&lt;/p&gt;

&lt;p&gt;First attempt: tweak &lt;code&gt;test/test_visualize_test&lt;/code&gt; to only display points from a particular view.  Doesn't work great, because only part of the plant is visible in each view, and those parts differ between views.&lt;/p&gt;

&lt;p&gt;Next attempt: tweak &lt;code&gt;curve_max_posterior.m&lt;/code&gt;  to define a canonical index set for the curve, and then reconstruct for each view.&lt;/p&gt;

&lt;p&gt;More tomorrow...&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;p&gt;Training Background model&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Singular Regions Issue; Training</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/06/work-log"/>
   <updated>2013-08-06T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/06/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Investigating the &quot;Spreading&quot; issue with increases to perturb_rate_variance.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Confirmed the same phenomenon with increase to perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Setting perturb_position_variance to 1000:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-1.png&quot; alt=&quot;perturb_position_variance = 1000&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Setting perturb_position_variance to 1000000:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-2.png&quot; alt=&quot;perturb_position_variance = 1000000&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spreading appears to increase monotonically with perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Again, this is surprising, because you'd expect them to revert to the maximum likelihood solution.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;However, recall that the per-view likelihood has infinite variance in the back-projection direction.  The spreading appears to be occuring in this direction.  The infinite variance means that any influence from the prior will overcome the likelihood.&lt;/p&gt;

&lt;p&gt;But isn't the prior centered at zero?  Why is the result drifting so far from zero?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;One thing is clear: with high perturb&lt;em&gt;* values, the correlation between nearby views becomes negligible compared to the within-view variance.  And since the likelihood variance has an infinite component, the posterior variance grows with perturb&lt;/em&gt;*.  While we can inspect the maximum posterior curve, it is relatively meaningless because the variance is so great.&lt;/p&gt;

&lt;p&gt;Even so, why doesn't it just revert to the mean?&lt;/p&gt;

&lt;p&gt;Mean rate is zero, but it can't be exactly zero, because the likelihood requires that the curve be near the data.  But the data's position is only known in two dimensions, so the posterior is free to manipulate the third dimension so that the rate is minimized.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Model is trying to use a linear-perturbation model to explain per-view deviations from the mean model.  Since the deviations don't arise from pure scaling, it has to contort into bizarre shapes to explain it.&lt;/p&gt;

&lt;p&gt;But the bizarre shapes fit the data better, so it's worth it.&lt;/p&gt;

&lt;h1&gt;&quot;Singular Regions&quot;&lt;/h1&gt;

&lt;p&gt;GOT IT!  Recall that the likelihood variance is measured in world units, even though they are really image-based values.  As the curve moves toward the camera, the likelihood variance &lt;em&gt;should&lt;/em&gt; ideally reduce, since the same size of world perturbations result in larger image perturbations.  But in our model they don't, and so curves can stray farther from the data in the image, but still look nearby the data according to the Gaussian.&lt;/p&gt;

&lt;p&gt;In the extreme case, all the points end up near the camera pinhole, and they will be in the center of the data Gaussian.  In practice, any point within \(\sigma&lt;sup&gt;2\)&lt;/sup&gt; of the camera will be well supported, where \(\sigma&lt;sup&gt;2\)&lt;/sup&gt; is the average noise variance in 3D.  I'll call this the &quot;Singular Region&quot;, where all the of likelihood's Gaussian &quot;cylinders&quot; (degenerate cones) intersect and overlap.&lt;/p&gt;

&lt;p&gt;In terms of the marginal likelihood, this can cause counterintuitive behavior.  For example, large perturb_rate_variance might be good, because it allows the curve to wander into the &quot;singular zone&quot; near the camera.  Thinking of in spherical coordinates, there is a wedge of the sphere that points toward the camera, and as the perturb_rate_variance increases, this wedge remains relatively constant in angular size, but gets longer and longer.  The longer it gets, the more of singular zone it overlaps. Even though greater variance means there are more possible model configurations, there is a period during which the proportion of these configurations that are well-supported by the likelihood doesn't necessarily decrease, so the ML doesn't necessarily decrease, either.&lt;/p&gt;

&lt;p&gt;This explains the phenomenon we saw during training, where the plot of ML vs. perturb_rate_variance (reproduced below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-training-plot.png&quot; alt=&quot;ML vs. perturb_rate_variance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The lump to the left is the result of the singular region giving false support near the camera.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Need to somehow place penalty for any point that strays too far from the mean curve.  Can this be done without radically distorting the model?&lt;/p&gt;

&lt;p&gt;What if I placed a limit on the likelihood variance, instead of letting it be infinite?  It will prevent the prior of taking credit for lousy configurations during the ML marginalization.&lt;/p&gt;

&lt;h1&gt;Experiments&lt;/h1&gt;

&lt;p&gt;Modified &lt;code&gt;train/tr_construct_matrices.m&lt;/code&gt; to clamp the likelihood's maximum variance to some multiple of the largest finite eigenvalue (see local function &quot;fix_precisions&quot;).  ML shouldn't change much when using reasonable values.&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference (no cap): 2.2675e+04  (inf)
100x cap:           2.2109e+04  (1.2 mm)
1000x cap:          2.2360e+04  (3.8 mm)
10000x cap:         2.2510e+04  (12 mm)
100000x cap:        2.2596e+04  (38 mm)
1000000x cap:       2.2654e+04  (12 cm) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Capping the variance to 10000x the triangulation variance results in a standard deviation of about 12mm in practice, which seems very reasonable.&lt;/p&gt;

&lt;p&gt;We have to raise standard deviation to 12 cm for it to be accurate to three significant digits, which seems somewhat high.  Possibly, even with reasonable model parameters, we're still seeing some influence from the &quot;singular zone,&quot; so it may be a good thing that we aren't seeing the full reference value.&lt;/p&gt;

&lt;h2 id=&quot;training-results&quot;&gt;Training Results&lt;/h2&gt;


&lt;p&gt;Running training using clamped likelihoods.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0025
            noise_variance: 0.1231
         position_variance: 1.6658e+04
             rate_variance: 0.2207

Final ML: 2.371 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Exactly the same result as the non-clamped version.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ind Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0719
         position_variance: 1.6729e+04
             rate_variance: 0.2422
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4918e-06
 perturb_position_variance: 0.4801

Final ML:  2.512 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Old non-clamped training resulted in perturb_rate_variance exploding.  The new perturb_rate_variance looks very reasonable.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;OU Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0721
         position_variance: 1.6681e+04
             rate_variance: 0.2146
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4711e-06
 perturb_position_variance: 0.7793
             perturb_scale: 3.7353

Final ML:  2.516 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of the global variance is can be pushed into the perturb_variance, since they are now correlated.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SQEXP Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0018
            noise_variance: 0.0720
         position_variance: 1.6689e+04
             rate_variance: 0.2122
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4952e-06
 perturb_position_variance: 0.5130
             perturb_scale: 0.8425

Final ML: 2.516 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;General observations&lt;/h2&gt;

&lt;p&gt;It's still somewhat weird that perturb_smoothing_variance is so low.  I'm pretty sure there are non-negligible deformations occurring during the imaging process.  Maybe it's just the Ind-perturb model?  More likely it's because the curves that deform are reversed...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Which of the perturb-components are really correlated?  Seems like position variance is probably independent, but rate variance might not be.  Definitely smoothing_variance (i.e. nonrigid deformations) should be correlated.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;try visualization with truncated likelihoods

&lt;ul&gt;
&lt;li&gt;does increasing rate variance eventually have no effect?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;remove perturb_scale from ind model (better inference)&lt;/li&gt;
&lt;li&gt;hand-pick parameters and fix them, to reduce dimensionality of search space.&lt;/li&gt;
&lt;li&gt;Handle &quot;flipped&quot; curves.  Try to infer direction&lt;/li&gt;
&lt;li&gt;Does the visualized max posterior look good for the trained values (I'm guessing not -- too strict of variances, overfitting)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Training Bugs</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log"/>
   <updated>2013-08-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14852&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing training of OU perturb model.&lt;/p&gt;

&lt;p&gt;Yesterday, we had problems with the perturb scale exploding.  Have switched to using a logistic sigmoid instead of an exponential to map perturb_scale, which sets a lower-bound and and upper-bound during optimization.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;perturb_scale is now staying low, but stil getting weird results.&lt;/p&gt;

&lt;p&gt;perturb_position_variance wants to be 9000+ ???&lt;/p&gt;

&lt;p&gt;perturb_rate_variance wants to be ~40?&lt;/p&gt;

&lt;p&gt;perturb_smoothing_variance wants to be ~0.0.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Trying new kernel: ind_kernel.  Like OU and SQEXP kernels, but no correlation between perturbations.  It's an intermediate step between no-perturb and ou-perturb, since it allows perturbations, but doesn't need a scale-length parameter.&lt;/p&gt;

&lt;p&gt;Training results for ind_kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_3 = 

            smoothing_variance: 0.0024
                noise_variance: 0.0132
             position_variance: 1.5747e+04
                 rate_variance: 0.1709
    perturb_smoothing_variance: 0.0020
         perturb_rate_variance: 26.3458
     perturb_position_variance: 0.8770
                 perturb_scale: 1.7199
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Very weird that perturb_rate variance&lt;/p&gt;

&lt;p&gt;Consider limiting number of dimensions somehow...&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Force both smoothing_variances to be the same&lt;/li&gt;
&lt;li&gt;remove perturb_scale parameter&lt;/li&gt;
&lt;li&gt;???&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;....&lt;/p&gt;

&lt;p&gt;recall that handling view-index in kernels has never been thoroughly tested...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Let's visualize results for the ind_perturb model, and see if everything looks reasonable.&lt;/p&gt;

&lt;p&gt;Created &lt;code&gt;test/tmp_visualize_test.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Interesting...  Some curves are &quot;reversed&quot;, i.e. the base is the &quot;end&quot; of the curve and the tip is the &quot;beginning&quot;.  This has some unintended consequences when applying the perturb model, because the tips are where perturbations are greatest, but when curves are reversed, the tips aren't affected by most of the modelled perturbations.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Weird.  As perturb_rate_variance increases, the per-view curves spread out radially like flower petals.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-1.png&quot; alt=&quot;high perturb_rate_variance causes radial spreading of curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would have expected that the likelihood would take over, but this is clearly not happening.  Lets look at the likelihood...&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-2.png&quot; alt=&quot;Maximum-likelihood solution.  Posterior should revert to this as variance increases asymtotically.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We must be running up against numerical precision errors.  Let's look at the equation for maximum posterior:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;When \(\Sigma_0\) has huge eigenvalues, this equation basically reduces to&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
\mu_P = (\Sigma_0 \Sigma_l^{-1})^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l 
     &amp;= \Sigma_l \Sigma_0^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l
     &amp;= \Sigma_l \Sigma_l^{-1} \mu_l
     &amp;= \mu_l
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;However, the effective cancellation of \Sigma_0 can't occur, because the expression \((\Sigma_0 \Sigma_l&lt;sup&gt;{-1}&lt;/sup&gt; + I)\) has a huge condition number, which makes inverting it an unstable operation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Anyways, that's visualization.  Is the same issue arising in ML computations?  Higher rate variance can result in significantly larger condition numbers, and IIRC, we don't take any special steps to handle such issues in the ML  computation anymore (for example, using the matrix inversion lemma).&lt;/p&gt;

&lt;p&gt;Maybe we should force rate variance to be within a reasonable range.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Even when perturb_rate_variance is reasonable (1.0), we still get drifting:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-3.png&quot; alt=&quot;perturb_rate_variance == 1.0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The degree of this phenomenon seems to be a smooth function of perturb_rate_variance.  If it was a numerical instability issue, we'd see it arise abruptly, indicating we had entered the unstable regime.&lt;/p&gt;

&lt;p&gt;I'm now thinking this is a real issue with the model, not simply an artifact of computation (maybe it's both?).  Need to think more about it.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Determine why spreading happens when &lt;code&gt;perturb_rate_variance&lt;/code&gt; increases.&lt;/li&gt;
&lt;li&gt;Determine why huge &lt;code&gt;perturb_rate_variance&lt;/code&gt; values are promoted by the ML.&lt;/li&gt;
&lt;li&gt;Handle &quot;reversed curves&quot; issue, where perturbation is applied to the wrong end.&lt;/li&gt;
&lt;li&gt;Training

&lt;ul&gt;
&lt;li&gt;Finish &quot;ind&quot; model&lt;/li&gt;
&lt;li&gt;Traing &quot;ou&quot; and &quot;sqexp&quot; models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log"/>
   <updated>2013-08-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting training of OU perturbation model (&lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Found bug in &lt;code&gt;tr_curves_ml.m::ou_kernel&lt;/code&gt; -- &lt;code&gt;smoothing_variance&lt;/code&gt; was used where &lt;code&gt;perturb_smoothing_variance&lt;/code&gt; should have been (copy-paste bug).  Also fixed in &lt;code&gt;sqexp_kernel&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Optimizer appears to be running much more smoothly now.  Probably because there previously was a strong correlation, due to &lt;code&gt;smoothing_variance&lt;/code&gt; appearing in two different parts of the equation.  The resulting ridge could have caused slow progress.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Initial params:&lt;/p&gt;

&lt;p&gt;train_params =&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.2500
                noise_variance: 100
             position_variance: 90000
                 rate_variance: 2.2500
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 2.2500
     perturb_position_variance: 90000
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stopped after 3 iterations.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.4501
                noise_variance: 2.5982e-04
             position_variance: 6.5359e+04
                 rate_variance: 3.9645
    perturb_smoothing_variance: 3.3437e-06
         perturb_rate_variance: 0.7268
     perturb_position_variance: 3.0785e+04
                 perturb_scale: 0.5867

Final ML: 28423.438936
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final ML (2.84e4) is greater than the no-pertrub model (2.37e4).  This is expected, since this model has more parameters, so we can get a tighter fit (maybe overfitting).&lt;/p&gt;

&lt;p&gt;I like that noise variance is much smaller (close to the minimum--do I need to lower the floor?).  This is expected, because the only source of IID noise is the rasterization process.&lt;/p&gt;

&lt;p&gt;Position standard deviation is much higher than before (255 vs. 126), as is rate standard deviation (2.0 vs. 0.47).&lt;/p&gt;

&lt;p&gt;Perturb variances are harder to explain.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is low, which is nice to see, because the worst of the perturbations are arising from miscalibrations (rate and position perturbations), not deformations.  But it's still far smaller than I expected.  hOnestly, I was expecting it to be almost the same as &lt;code&gt;smoothing_variance&lt;/code&gt;, but I realize now it makes sense for it to be smaller.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perturb position variance is much, much larger than I expected.&lt;/strong&gt;  Standard deviation is 175.4, which is basically saying each plant has 17 cm of IID perturbations from the unobserved mean plant.   I have no good explanation for this, expect maybe that the initial value was ridiculously large.&lt;/p&gt;

&lt;p&gt;Pertub scale seems reasonable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Test sensitivity to initialization.  Initial values:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 0.0500
         perturb_rate_variance: 0.1000
     perturb_position_variance: 1
                 perturb_scale: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First four were initialized from the trained no-perturb model.&lt;/p&gt;

&lt;p&gt;Note the biggest change: perturb_position_variance changed from 9000 to 1.&lt;/p&gt;

&lt;p&gt;Terminated after 46 iterations; much better than the 3 iterations from last attempt.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0019
                noise_variance: 0.0721
             position_variance: 1.6267e+04
                 rate_variance: 0.2050
    perturb_smoothing_variance: 6.7471e-10
         perturb_rate_variance: 1.3201e-04
     perturb_position_variance: 403.7082
                 perturb_scale: 2.4888e+03

Final ML: 25163.231449
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Weird that this is lower than that poorly-initialized model.  Consider initializing using a combination of Attempt #1 and Attempt #2's initial values.&lt;/p&gt;

&lt;p&gt;Smoothing variance is much smaller than before, noise variance is larger.&lt;/p&gt;

&lt;p&gt;Position variance is in the same order-of-magnitude, but about 1/5 the magnitude.&lt;/p&gt;

&lt;p&gt;Rate variance is an OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is still near-zero.&lt;/p&gt;

&lt;p&gt;Perturb rate variance is 3 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb position variance is 2 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb scale is sooooooo high.  Basically flat, so the regular and perturb variances sum.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: Result is very sensitive to initialization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: When perturb scale is this high, there exists a ridge, due to perturb variances playing the same role as normal variances.  Note that when perturb scale is infinity, this model degenerates to the no-perturb model.  Notice that Final ML isn't much better than the no-perturb.  Possibly when perturb scale exploded, the model could no longer improve, so constraining perturb scale to some small set of values may be a good idea (sigmoid transform).&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Try smaller changes to initialization.&lt;/li&gt;
&lt;li&gt;Try sigmoid transform on perturb scale.&lt;/li&gt;
&lt;li&gt;Try training using standard deviations instead of variances.&lt;/li&gt;
&lt;li&gt;Try changing one or more trained values to sensible defaults and retrain.&lt;/li&gt;
&lt;li&gt;Work with standard deviations, not variances&lt;/li&gt;
&lt;li&gt;Fix some 'known' values and optimize others.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log"/>
   <updated>2013-08-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Running optimizer...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issue&lt;/strong&gt;: Likelihood variance is collapsing to zero.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Guess&lt;/strong&gt;: Training ML is different from naive ML and isn't penalizing noise correctly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test 1&lt;/strong&gt;: save params, compare training ml and naive ml&lt;/p&gt;

&lt;p&gt;Same result.&lt;/p&gt;

&lt;h2&gt;Discussion&lt;/h2&gt;

&lt;div&gt; The marginal likelihood is monotonically increasing as the noise variance \(\sigma_n^2\) approaches zero.  This shouldn't be happening, because as the likelihood variance collapses to a delta, the marginal likelihood should become equal to the prior evaluated at the (virtual) observed locations.&lt;/div&gt;




&lt;div&gt;
\begin{align}
    \lim_{\sigma_n \to 0} p(D) &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; p_l(D \mid x) dx &amp; \text{(Marginalization)}\\
         &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; f(D - x) dx &amp;   \text{(Convolution)} \\
         &amp;= \int p_0(x) \delta(D - x) dx \\
         &amp;= p_0(D) 
\end{align}
&lt;/div&gt;


&lt;p&gt;This implies there's a bug in the code causing this phenomenon; the mathematical model is not the cause.&lt;/p&gt;

&lt;h2&gt;Solved&lt;/h2&gt;

&lt;div&gt;Found the bug.  When computing the likelihood precision \(S\) from the unscaled precision \(S_0\), the noise variance \(\sigma_n^2\) was multiplied instead of divided:&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;S = S0 * sigma_n; // incorrect
S = S0 * (1/sigma_n); // corrected version
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Resuming Training&lt;/h2&gt;

&lt;p&gt;Had some trouble with noise sigmas being too low.  Solved by in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Attempts 1-3: clamping to a minimum value and then adding a penalty depending on the amount that was clamped.&lt;/li&gt;
&lt;li&gt;Attempt 4:  offset by minimum value before transforming&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = tr_train(test_Corrs_ll, train_params, data_, 400);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Handle extreme values by incuring a penalty for variances smaller than 1e-5.  Result stored in &lt;code&gt;train_params_done_1&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprisingly small smoothness sigma.  Surprisingly low rate variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the magnitude of the penalty term.  Scaled up penalty by 1000.  Results in &lt;code&gt;train_params_done_2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the threshold where the penalty term starts to be incurred.  Set MIN_NOISE_VARIANCE to 1e-3 and MIN_SMOOTHING_VARIANCE to 1e-7 (previously both 1e-5).  Results in &lt;code&gt;train_params_done_3&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_3 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Attempt #4 &lt;/strong&gt;
Handled small variances by offsetting before transforming:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% converting param to state variable
x(1) = log(smoothing_variance - MIN_SMOOTING_VARIANCE);

% converting state variable to param
x(1) = exp(smoothing_variance) + MIN_SMOOTING_VARIANCE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is more elegant than the penalty hack used in the last three attempts.   Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;h2&gt;No-Perturb model summary:&lt;/h2&gt;

&lt;p&gt;Successfully trained model.&lt;br/&gt;
Required &lt;strong&gt;115 function evaluations&lt;/strong&gt;, taking &lt;strong&gt;112 s&lt;/strong&gt;.&lt;br/&gt;
Optimal marginal likelihood:  &lt;strong&gt;23714.937760&lt;/strong&gt;.&lt;br/&gt;
Optimal parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised at how small noise_variance was, considering the calibration noise.  However, I guess the maximum-likelihood reconstruction looked pretty good, if unity-apect-ratio axis scaling is used.&lt;/p&gt;

&lt;h2&gt;Training OU-Perturb model&lt;/h2&gt;

&lt;p&gt;Getting weird results.  Halted after second iteration; second iteration took 250 function evaluations; perturb_smoothing_variance gradient is zero.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 1.5003e-05
                noise_variance: 1.0087e-04
             position_variance: 7.9393e+04
                 rate_variance: 0.7879
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 1.5183
     perturb_position_variance: 2.5159e+04
                 perturb_scale: 48.6163
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Recall that new model ML's aren't deeply tested; probably a bug in there (in the kernel implementation?  in the kernel theory? in the conversion from mats to kernel?).  Will continue tomorrow.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log"/>
   <updated>2013-08-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Morning&lt;/h2&gt;

&lt;p&gt;Researched telecommuting strategies.&lt;/p&gt;

&lt;p&gt;Set up Google hangout: &lt;a href=&quot;https://plus.google.com/hangouts/_/89759369dd280ff225c298a7a4291745134e1d6f&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Set up IRC chat room: &lt;a href=&quot;http://webchat.freenode.net/?channels=ivilab&amp;amp;uio=d4&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Probably IRC will be best for general chat, Google Hangouts for screen sharing or group video chat.&lt;/p&gt;

&lt;h2&gt;Afternoon&lt;/h2&gt;

&lt;p&gt;Setting up training minimizer using Matlab's &lt;code&gt;fminunc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Got all params in-place, but Matlab version is too old.&lt;/p&gt;

&lt;p&gt;Trying to upgrade matlab, but Java is out of date.&lt;/p&gt;

&lt;p&gt;Taking a break to move furniture...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Downloaded and installed Java 7.&lt;/p&gt;

&lt;p&gt;Now Matlab installer isn't able to communicate with server.  Arg...&lt;/p&gt;

&lt;p&gt;Working now... downloading...  Should be done in 45 minutes.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;p&gt;Migrating settings from old Matlab...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>, Week summary</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/26/work-log"/>
   <updated>2013-07-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;No work logs this week.  All week I've been splitting time between research and packing for the move to Phoenix on Monday.  I've made some progress on the training framework.&lt;/p&gt;

&lt;h2&gt;Miscellaneous&lt;/h2&gt;

&lt;p&gt;Due to periodic crashing of Matlab (user error!), I added a script &lt;code&gt;tmp_setup_workspace.m&lt;/code&gt;, which is intended to restore all the needed variables for whatever task I'm currently working on.&lt;/p&gt;

&lt;h2&gt;Optimized likelihood for training&lt;/h2&gt;

&lt;p&gt;Spend some time thinking about how to design a version of the marginal likelihood computation that is optimized for training.&lt;/p&gt;

&lt;p&gt;Some time was spent deriving the analytical ML gradient w.r.t. the training parameters.  I believe the result won't be too complicated, but deriving it was taking up too much time.  We'll use a numerical gradient for now; will return to analytical gradient if the numerical proves to be lacking.&lt;/p&gt;

&lt;p&gt;To save some computation during training, I precomputed the component matrices for the prior and likelihood.  Constructing the prior and likelihood covariance matrices will involve scaling and summing the component parts.  These cached matrices consume about &lt;strong&gt;740 MB&lt;/strong&gt; with the current training set.&lt;/p&gt;

&lt;p&gt;However, one unavoidable bottleneck continues to be the cholesky decomposition, which isn't improved by this precomutation.  I was hoping there would be some matrix inversion tricks involving linear combinations of matrices, but my research on this came up empty.  My last hope is to run Cholesky on the GPU (Matlab makes this trivial), but if that doesn't speed things up, I'll resign myself to waiting 10 hours for training.&lt;/p&gt;

&lt;p&gt;Nevertheless, the component caching still gives a 1.5x end-to-end speedup on the no_purturb_kernel  case, compared the to direct implementation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;div&gt;After optimizing, the bottlenecks are split evenly three ways: (1) constructing Prior kernel; (2) constructing ML covariance matrix; and (3) evaluating ML pdf.   The latter two are dominated mostly by \(O(n^3)\) matrix multiplication.  Surprisingly, Cholesky is significant, but not dominating.&lt;/div&gt;


&lt;h2&gt;Summary:&lt;/h2&gt;

&lt;p&gt;Training-optimized Marginal Likelihood is finished; in &lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;.  Results confirmed against reference implementation: &lt;code&gt;train/tr_curves_ml_ref.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;finish Training framework:

&lt;ul&gt;
&lt;li&gt;setup local minimizer with &lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;determine if numerical gradient is okay&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sanity check: compare the ML's of each trained model on training data.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/21/work-log"/>
   <updated>2013-07-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/21/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;: The likelihood covariance for the &quot;virtual observations&quot; scales linearly with the 2D likelihood variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt;: see &lt;code&gt;experiments/exp_2013_07_21_likelihood_covariance.m&lt;/code&gt;.  Constructs a likelihood using noise_variance of one and then scaling precisions afterward.  Compares to directly-constructed likelihood precisions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Negligible difference&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;: Practice matches theory--scaling likelihood precisions is equivalent to constructing likelihood with the scaled precision.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: This conclusion means that we can construct the likelihood precisions exactly once during training, and simply scale them as we modify the likelihood precision.  It would make sense to always use 1.0 when computing precisions, and refactor all existing code to scale the matrix by the reciprocal of the noise variance before using it.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Build training framework

&lt;ul&gt;
&lt;li&gt;fast likelihood evalautor

&lt;ul&gt;
&lt;li&gt;custom function for evaluating ML without recomputing stuff&lt;/li&gt;
&lt;li&gt;cache the three prior component matrices (smooth, linear, and offset)&lt;/li&gt;
&lt;li&gt;cache unscaled likelihood precision.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;wrap in a lambda

&lt;ul&gt;
&lt;li&gt;scales and combines all components&lt;/li&gt;
&lt;li&gt;depending on motion model, use different expression for purturbation coefficient&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pass to quasi-newton minimizer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat for background curves&lt;/li&gt;
&lt;li&gt;Heuristic pruning using background-subtraction.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log"/>
   <updated>2013-07-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing: ground-truth-to-data-labels.  See file &lt;code&gt;train/label_from_ground_truth.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finished.&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data labels to likelihood means/covariances&lt;/li&gt;
&lt;li&gt;Likelihood means/covariances to marginal likelihood&lt;/li&gt;
&lt;li&gt;training framework&lt;/li&gt;
&lt;li&gt;training&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Visualizing &lt;code&gt;labels_from_ground_truth()&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Wrote &lt;code&gt;tmp_get_max_posterior.m&lt;/code&gt;, a temporary script that computes the posterior mean from a possibly-overconstrained prior.  In this case, the posterior covariance is singular, but the mean can still be obtained.  The math behind it &lt;a href=&quot;/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance/&quot;&gt;is available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Below is a plot using the test dataset and the ground truth labels:&lt;/p&gt;

&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/foL28SUn1JM?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;This shows that given a good labeling, a quality 3D reconstruction can be obtained using only the fragmented curves output by the curve-detector.&lt;/p&gt;

&lt;p&gt;Notice that the curves at the base have missing parts.  There isn't sufficient edge data here, but this could probably be fixed by connecting them to the base of the main stem and using the Branching Gaussian Process prior to enforce connectivity.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Maximum posterior with singular prior covariance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance"/>
   <updated>2013-07-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Consider the scenerio of Bayesian inference with a linear-Gaussian prior and likelihood.&lt;/p&gt;

&lt;p&gt;It is sometimes the case that our prior has a singular covariance matrix, indicating that our variables are embedded in a lower-dimensional hyberplane, i.e. some dimensions are redundant.  In our 3D curve triangulation application, this situation arises when  the same 3D point is observed in from multiple views.&lt;/p&gt;

&lt;p&gt;We can still find the maximum posterior arising from such a prior, as long as the likelihood is non-singular.   We can interpret this as multiple observations of the rendant dimensions, and if we are careful with our math, we can handle it the same way as the case with a non-singular prior.&lt;/p&gt;

&lt;p&gt;Given a likelihood \( \mathcal{N}(\mu_l, \Sigma_l) \) and prior \( \mathcal{N}(\mu_0, \Sigma_0) \), recall that the posterior is given by  \( \mathcal{N(\mu_P, \Sigma_P)} \), where&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
\Sigma_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} \\
\mu_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} (\Sigma_l^{-1} \mu_l + \Sigma_0^{-1} \mu_0)
\end{align}

\]
&lt;/div&gt;


&lt;p&gt;However, if \(\Sigma_0 \) is singular, we must avoid inverting it when computing \( \mu_P  \).  An equivalent equation for \(\mu_P\) that satisfies this condition is:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;




&lt;div&gt;This should be computable as long as the likelihood precision matrix \(\Sigma_l^{-1}\) has no infinite eigenvalues.  &lt;/div&gt;


&lt;p&gt;To see an example result, see &lt;a href=&quot;/ksimek/research/2013/07/19/work-log/&quot;&gt;today's Work Log&lt;/a&gt; entry.&lt;/p&gt;

&lt;h2&gt;Implementation Notes&lt;/h2&gt;

&lt;p&gt;I tried implementing a version of this that uses Cholesky decomposition and backsubstitution instead of generic matrix inversion.  I needed a symmetric matrix, so I modified the equation for \(\mu_P\):&lt;/p&gt;

&lt;div&gt; \[
\mu_P = \Sigma_0 (\Sigma_0 \Sigma_l^{-1} \Sigma_0 + \Sigma_0)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;This was not noticibly faster than general matrix inversion, because it involves two additional large matrix multiplications.&lt;/p&gt;

&lt;p&gt;Surprisingly, I &lt;em&gt;was&lt;/em&gt; able to get a significant speedup (~7x) be using backsubstitution (Matlab's '\' operator) &lt;em&gt;without&lt;/em&gt; Cholesky decomposition.  I always assumed that the lower-triangular form was what made backsubstitution so fast, but it is apparently also fast with dense matrices.  So we can avoid the extra expensive dens-matrix multiplications, and also avoid expensive matrix inversion.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log"/>
   <updated>2013-07-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bootcamp demo session&lt;/li&gt;
&lt;li&gt;Ground truth labeling of curve fragments.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Bootcamp demo session&lt;/h2&gt;

&lt;p&gt;Ran bootcamp demo session.  Final code available at&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;svn+ssh://v01/misc/svn/src/bootcamp/kjb_demos/3d_demo
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Ground truth labeling of curve fragments&lt;/h1&gt;

&lt;p&gt;Goal: use 2D ground truth to automatically label bottom-up curve fragments.&lt;/p&gt;

&lt;h2&gt;Overview&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Draw ground truth curves using curve index&lt;/li&gt;
&lt;li&gt;dilate slightly&lt;/li&gt;
&lt;li&gt;For each rendered data curve, gather all GT indices&lt;/li&gt;
&lt;li&gt;keep most-occurring GT index.&lt;/li&gt;
&lt;li&gt;add to assoc list for that curve&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Issues:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Remember how to read and process ground truth.&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; See &lt;code&gt;../ground_truth/read_gt2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Symlinked to &lt;code&gt;train/read_gt2.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Which dataset does our example data come from?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/ler_5_36_0.jpg&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Where is ground truth data?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/resized_50%/ground_truth_2d.gt2&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Blah Blah&lt;/h2&gt;

&lt;p&gt;Refactored some code into new function &lt;code&gt;build_curve_maps.m&lt;/code&gt;, which renders each of the curves in a cell-array into a map containing their indices.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored as Bezier curves.  Symlinked the all bezier-related code into data_association_2.  Relevant function is &lt;code&gt;bezier/polybez_to_polyline.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored in OpenGL-style coordinates (bottom-left origin).  Converting to top-right using &lt;code&gt;tools/flip_y.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Summary &lt;/h2&gt;

&lt;p&gt;Got through item 2 in &quot;Overview&quot; above.  Will finish next time, and start building training framework.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title> 2</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2"/>
   <updated>2013-07-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Experimenting with &lt;code&gt;ou_perturb_kernel.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Was getting weird results, where the ML approached infinity as &lt;code&gt;noise_variance&lt;/code&gt; approached zero.&lt;/p&gt;

&lt;p&gt;Realized that the set of precision matrices needs to be updated &lt;strong&gt;every time the noise sigma changes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is an oversight that has tripped me up before.  Need to try to avoid it in the future.&lt;/p&gt;

&lt;p&gt;It is possible (likely?) that this transformation is as simple as multiplying the precision matrices by \(\sigma_n* / \sigma_n\).  This would avoid a semi-expensive Hessian calculation for each point, which could be a bottleneck during training.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;p&gt;See previous entry&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log"/>
   <updated>2013-07-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14866&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Note: This entry marks then end of significant work on the new likelihood function.  The SVN revision is noted in the meta-box to the right.&lt;/p&gt;

&lt;h2&gt;Optimizing &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Attempting to use sparsity to speed up &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First attempt&lt;/strong&gt;: try building only the relevant elements of the prior matrix, K.&lt;br/&gt;
&lt;strong&gt;Resut&lt;/strong&gt;: Gains in multiplication are lost in construction of K.  Insignificant speedup.  Rolling back.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second attempt&lt;/strong&gt;: convert block-diagonal matrix &lt;em&gt;S&lt;/em&gt; to a sparse matrix.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Significant gains in multiplication; tolerable losses when constructing &lt;em&gt;S&lt;/em&gt;.  ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third Attempt&lt;/strong&gt;: Convert the individual blocks &lt;em&gt;S_i&lt;/em&gt; to sparse, &lt;em&gt;then&lt;/em&gt; construct &lt;em&gt;S&lt;/em&gt; from them.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Further speedup of ~2x&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fourth Attempt&lt;/strong&gt;:  Change &lt;code&gt;(eye(size(K)) + S * K * S')&lt;/code&gt; to &lt;code&gt;(speye(size(K)) + S * K * S')&lt;/code&gt; ; i.e. changing &lt;code&gt;eye&lt;/code&gt; to &lt;code&gt;speye&lt;/code&gt;.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: moderate speedup, ~1.5x.&lt;/p&gt;

&lt;p&gt;I think we've squeezed all we can from this function.  It's now 10x faster than the naive method on a problem with 4000-dimensions.&lt;/p&gt;

&lt;h2&gt;Results: &lt;/h2&gt;

&lt;p&gt;Running &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6362.9 ms)
Old ML: 207.041742

Computing marginal likelihood (new way, legacy correspondence)...
Done. (7655.2 ms)
New ML (Legacy corr): 207.700616

Computing marginal likelihood (new way)...
Done. (6537.7 ms)
New ML (New corr): 793.585038

Computing marginal likelihood (new way, no MIL)...
Done. (6330.3 ms)
New ML (New corr, no MIL): 793.600835

Computing marginal likelihood (direct method)...
Done. (667.9 ms)  &amp;lt;-------------------  10x speedup over previous
New ML (direct method): 793.300407 &amp;lt;--  0.04% error!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note speedup and negligible error compared the previous method.&lt;/p&gt;

&lt;h2&gt;General Observations Regarding &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;I've noticed that &lt;code&gt;markov_order&lt;/code&gt; must be much larger than I expected to avoid approximation error.&lt;/p&gt;

&lt;p&gt;Recall in &lt;code&gt;curve_ml2.m&lt;/code&gt;, we use the Markov assumption to break-down the prior covariance, and then combine them with the likelihood cliques.  In that case, we could use a Markov order between 2 and 5 without significant approximation error.&lt;/p&gt;

&lt;p&gt;In &lt;code&gt;curve_ml5.m&lt;/code&gt;, using a Markov order less than 100 results in unacceptable error.&lt;/p&gt;

&lt;p&gt;There are two reasons for this.  First, we're decomposing the marginal likelihood Gaussian, not the prior.  The prior is explicitly Markovian, whereas the marginal likelihood is not.  The ML Guassian adds extra uncertainty to every point, which means we need to consider more nearby points to avoid erroneous conclusions.&lt;/p&gt;

&lt;p&gt;Second, because the new approach creates a distinct index set for each view, indices are often repeated (multiple views of the same point) and considering them doesn't tell you anything about what direction the curve is heading.&lt;/p&gt;

&lt;p&gt;The first point will likely be mitigated when we start using the new curve model, which will allow the likelihood variance to decrease dramatically.  However, the markov assumption in the prior is less likely to hold under these models, so some experimentation will be needed.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The code for the new likelihood, &lt;code&gt;curve_ml5.m&lt;/code&gt; is now stable.  It is fast and accurate in its current implementation, assuming a reasonable value for &lt;code&gt;markov_order&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Medium-term goal: calibration/training for all curve models.&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ground-truth labelling of bottom-curve curve data&lt;/li&gt;
&lt;li&gt;set-up method for evaluating ground-truth using a given parameter-set, without re-computing indices each time.&lt;/li&gt;
&lt;li&gt;Run multi-dimensional optimization to fit for all models&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log"/>
   <updated>2013-07-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing new direct method for marginal likelihood.&lt;/h2&gt;

&lt;p&gt;Implemented in &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After initial attempt, the new method produced inconsistent results.&lt;/p&gt;

&lt;p&gt;Re-derived the result two more ways and the math comes out the same.  The theory looks right.&lt;/p&gt;

&lt;p&gt;Finally found the bug -- was computing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;instead of&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = 2 * log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the Cholesky decomposition is the square-root of the matrix Sigma, I was forgetting to double it's determinant to get the determinant of Sigma.&lt;/p&gt;

&lt;p&gt;Now getting the correct results; now time to make it fast.&lt;/p&gt;

&lt;h2&gt;Optimizing direct method for ML&lt;/h2&gt;

&lt;p&gt;Need to get O(n) runtime instead of O(n&lt;sup&gt;3).&lt;/sup&gt;  Will use existing code for markov-decomposed PDF evaluation.&lt;/p&gt;

&lt;p&gt;Bottleneck is 100% the Cholesky decomposition.  The direct method doesn't remove redundant dimensions, so it's slower than &lt;code&gt;curve_ml4.m&lt;/code&gt;, which does.  Recall that this fact makes &lt;code&gt;curvE_ml4.m&lt;/code&gt; not general enough to use the new foreground models with.&lt;/p&gt;

&lt;p&gt;...........&lt;/p&gt;

&lt;p&gt;After some investagation, the previous statement appears to be untrue.  The bottlenect is dense matrix multiplication, which is fixed by using sparse matrices.&lt;/p&gt;

&lt;p&gt;Now, cholesky &lt;strong&gt;is&lt;/strong&gt; the bottlenect, but not as huge as before.  The markov decompose method only gives a ~30% speedup at best, while introducing ~2.5% error and significant extra complexity. Is it worth it?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Direct Evaluation of the Marginal Likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood"/>
   <updated>2013-07-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Previously, we've always used hacks to evaluate the marginal likelihood, because evaluating it directly was apparently impossible because of an infinite normalization constant.  I've discovered that this isn't actually true; the infinite normalization constant arose due to our approximation of the likelihood.  The trick in correctly computing the ML is to replace the usual normalization constant with a corrected one.&lt;/p&gt;

&lt;p&gt;Our old method required knowledge of the maximum posterior solution, which was costly to compute.  The new approach is straightforward to describe, more accurate than our previous method, and can be evaluated in linear time by exploiting its Markovian structure.&lt;/p&gt;

&lt;p&gt;Below is a rough-draft writeup of my derivation of the marginal likelihood function.&lt;/p&gt;

&lt;h2&gt;Direct Evaluation of the Marginal likelihood&lt;/h2&gt;

&lt;p&gt;The 3D marginal likelihood function arises as the sum of a 3D curve process and a 3D perturbation process.  Both are gaussian (the second is approximate), this the result is a Gaussian function, the convolution of two Gaussians.  However, the perturbation process has infinite variance in the direction of backprojection, which arises from the fact that perturbation actually occurs in 2D, we are just backprojecting it to 3D for tractibility.  In other words, the ML isn't a distribution in 3D, it's a distribution in 2D, and we need to determine the appropriate normalization constant for the 3D function to it generates 2D ML densities.&lt;/p&gt;

&lt;p&gt;The likelihood function is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y | x) &amp;= \prod p(y_i | x_i) \\
         &amp;= \prod \mathcal{N}(y_i; \rho_I(x_i), \sigma_n^2 I) \\
         &amp;\approx \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \mathcal{G}(Y_i; x_i, S_i^{-1}) \\
         &amp;= \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \exp\{(Y_i - x_i)^\top S_i (Y_i - x_i)\} \\
         &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}^{2n}} \exp\{(Y - x)^\top S (Y - x)\} \\
         &amp;= \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\}
\end{align}
\]&lt;/div&gt;


&lt;p&gt;Where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\rho_i(x)\) is the projection of x into the I-th view,&lt;/li&gt;
&lt;li&gt;\(Y_i\) is the estimated backprojected position of observation \(y_i\),&lt;/li&gt;
&lt;li&gt;\(S_i\) is the curvature of the 3d likelihood function w.r.t. \(x_i\) evaluated at $Y_i$,&lt;/li&gt;
&lt;li&gt;\(Y\), \(S\) and \(x\) are the concatenation of \(Y_i\), and \(x\)&lt;/li&gt;
&lt;li&gt;\(S\) is the block-diagonal matrix of \(S_i\)'s&lt;/li&gt;
&lt;li&gt;\(\mathcal{G}\) is a Gaussian function (an unnormalized normal distribution).&lt;/li&gt;
&lt;li&gt;\(Z_l\) is a normalization constant.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We have transformed the likelihood into a log-linear function of x, by rewriting the PDF in terms of only 3D entities.  Note that \(S_i\) has zero curvature in the backprojection direction, resulting in infinite variance in the Gaussian function.   Also note that the normalization constant isn't the standard one for a 3D gaussian distribution, because this isn't a distribution over x.  The normalization constant is chosen so the approximate likelihood in 3D agrees with the exact 2D likelihood when perturbation is zero (i.e. when \(Y_i\) lies anywhere on the backprojection line).&lt;/p&gt;

&lt;p&gt;This approximation ignores the nonlinearity of projection; the likelihood function as a function of \(x_i\) would actually look like a cone whose axis-perpendicular slices are Gaussians, whereas our function is cylinder-shaped.  In practice, the approximation error has minimal effect on the marginal likelihood computation, assuming $Y_i$ is a good estimate of the posterior depth, and the posterior is reasonably peaked.  (A graphic to illustrate would be good here).&lt;/p&gt;

&lt;p&gt;The prior is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(x) &amp;= \mathcal{N}(x; \mathbf{0}, K) \\
     &amp;= \frac{1}{Z_p} \exp\{x^\top K^{-1} x\}
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;where \(K\) is the covariance matrix arising from the Gaussian process kernel, and Z_p is the standard Gaussian normalization constant.&lt;/p&gt;

&lt;p&gt;The marginal likelihood is defined as&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \int p(y|x) p(x) dx \\
     &amp;= \int \left ( \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\} \right ) \left ( \frac{1}{Z} \exp\{x^\top K^{-1} x\} \right ) \\
     &amp;= \frac{1}{Z_l Z_p}\int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;The expression within the integral is the convolution of two unnormalized zero-mean Gaussians, so the integral is a zero-mean Gaussian whose covariance is the sum of the inputs' covariances.  If the input Gaussians &lt;strong&gt;were&lt;/strong&gt; properly normalized, the normalization constant would be \(1/Z\);  since they are not, the normalization constant is \((Z_1 Z_2 / Z)\), where \(1/Z_1\) and \(1/Z_2\) are the would-be normalization constants for the first and second Gaussian, respectively.  Note that the second Gaussian's normalization constant is \(1/Z_p\), so this results in the cancellation we see below&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \frac{1}{Z_l Z_p} \int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     &amp;= \frac{1}{Z_l Z_p} \left ( \frac{Z_{l,3d} Z_p}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \} \right ) \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \} \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z}  \exp\{x^\top S(S + S K S)^{-1} S x \}
     \end{align}
\]&lt;/div&gt;




&lt;div&gt;
    Where \( Z_{l,3d} \) is the would-be 3D normalization constant for our likelihood Gaussian.  The last line avoids inverting the singular matrix \(S\) when it's singular.  Since \(S\) is rank-deficient, the normalization constants based on \(S^{-1}\) will be infinite (i.e. \(Z_{l,3d}\) and \(Z\)).  However, the terms involving determinants of \(S^{-1}\) should cancel in the ratio, resulting in a gaussian with infinite variance, but non-infinite normalization constant. **(Need to show this mathematically next time)** 
    &lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log"/>
   <updated>2013-07-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Markov-decomposed ML&lt;/h2&gt;

&lt;p&gt;The original plan was to markov-decompose the prior and posterior, but soon remembered that I also need to compute the mean of the posterior, which isn't as straighforward to compute.&lt;/p&gt;

&lt;p&gt;After some deliberation and whiteboarding, I decided to implement Bishop's forward/backward algorithm to simultaneously marginalize and compute the maximum.  Will test this against the clique-tree implementation to ensure correctness.&lt;/p&gt;

&lt;p&gt;........&lt;/p&gt;

&lt;p&gt;During testing I realized a fundamental problem with the current likelihood: there is no obvious way to consistently handle the &quot;redundant&quot; dimensions that arise from duplicated indices.  In some kernels, these are handled naturally, namely the kernels that treat different views as different indices.  The old kernel, however, does not, and the duplicated indices result in degenerate posterior distributions unless handled using hacks.&lt;/p&gt;

&lt;p&gt;We could handle this on a kernel-by-kernel basis, but it would be unmaintainable, and could hinder further research into new kernels.  It also isn't clear that the hacks I have in mind would actually give correct results.&lt;/p&gt;

&lt;p&gt;This problem is inherent to the &quot;candidates estimator&quot; of the marginal likelihood, because it involves a ratio of the posterior and prior, both of which are degenerate in these cases.&lt;/p&gt;

&lt;p&gt;The actual marginal likelihood function has no such degeneracies, because each observation is independent, given the underlying curve.  However, until recently it was unclear how to evaluate the marginal likelihood using the approximated likelihood function.  The approximate likelihood has a rank-deficient precision matrix, and its normalization constant is non-standard due to the transformation from 2D to 3D.  However, I think I've developed a way to evaluate it, which I describe in the next article.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log"/>
   <updated>2013-07-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Cleanup and speedup of curve_ml.&lt;/p&gt;

&lt;h2&gt;Task 1: Version without matrix-inversion lemma&lt;/h2&gt;

&lt;p&gt;Making a simpler version that doesn't exploit the matrix inversion lemma (MIL).&lt;/p&gt;

&lt;p&gt;The benefits of the MIL are likely mitigated by markov-decomposition (next task), and MIL
complicates the code and API design.&lt;/p&gt;

&lt;p&gt;Results (small test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (108.2 ms)
Old ML: 62.183969
Computing marginal likelihood (new way, legacy correspondence)...
Done. (120.8 ms)
New ML (Legacy corr): 63.908784
Computing marginal likelihood (new way)...
Done. (88.9 ms)
New ML (New corr): 72.304718           &amp;lt;----------  OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (97.3 ms)
New ML (New corr, no MIL): 72.304666   &amp;lt;----------  NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference is about 0.00007% -- seems good.  Slightly slower, possibly just noise.&lt;/p&gt;

&lt;p&gt;Results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6447.3 ms)
Old ML: 207.324848
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7665.1 ms)
New ML (Legacy corr): 209.199676
Computing marginal likelihood (new way)...
Done. (6205.6 ms)
New ML (New corr): 457529.406731  &amp;lt;---------- ?????
Computing marginal likelihood (new way, no MIL)...
Done. (5946.7 ms)
New ML (New corr, no MIL): 778723.327102  &amp;lt;---------- ?????
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both versions give CRAZY high results; significantly different from the &lt;a href=&quot;/ksimek/research/2013/07/05/work-log/&quot;&gt;same test on Friday&lt;/a&gt;.  Need to investigate...&lt;/p&gt;

&lt;h2&gt;Investigating new results&lt;/h2&gt;

&lt;p&gt;Observations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All results differ from friday, including old ML (slightly)&lt;/li&gt;
&lt;li&gt;New ML using legacy correspondence is still reasonable.&lt;/li&gt;
&lt;li&gt;Both crazy results are using new correspondence.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Look at pieces (log posterior, likelihood, prior)&lt;/li&gt;
&lt;li&gt;Inspect new correspondence&lt;/li&gt;
&lt;li&gt;Think about what changed since Friday?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:10:19 PM&lt;/p&gt;

&lt;p&gt;Ran old test, &lt;code&gt;test_ml_end_to_end.m&lt;/code&gt; and compared to old results in new test, &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt;.  Old test still gives old results, so I'll compare the two tests to determine what has changed.&lt;/p&gt;

&lt;p&gt;12:12:48 PM&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;mix&lt;/code&gt; parameter was set to 0.5 instead of 0.0.  I had forgotten I changed it at the end if Friday.&lt;/p&gt;

&lt;p&gt;Now getting &quot;good&quot; results again.  It raises a question that needs to be answered: why is ML sooo sensitive to evalaution position when using new correspondence?&lt;/p&gt;

&lt;h2&gt;Resuming&lt;/h2&gt;

&lt;p&gt;New results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6009.5 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7286.3 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6146.6 ms)
New ML (New corr): 793.585038             &amp;lt;---------- OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (6008.3 ms)
New ML (New corr, no MIL): 793.600835     &amp;lt;---------- NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.002% error, slightly faster.  Good.&lt;/p&gt;

&lt;h2&gt;Investigating Anomaly in new ML &lt;/h2&gt;

&lt;p&gt;So why is new ML so sensitive to where it is evaluated?&lt;/p&gt;

&lt;p&gt;Note that it's only extreme when using new correspondence.  Old correspondence is okay.&lt;/p&gt;

&lt;p&gt;Inspect difference between max likelihood and max posterior.  Maybe it's just more extreme than with old correspondence.&lt;/p&gt;

&lt;p&gt;12:20:59 PM&lt;/p&gt;

&lt;p&gt;Idea: bug in non-0.0 case?  Nope.&lt;/p&gt;

&lt;p&gt;Tried mixes: 0.0, 0.01, 0.1. Steady and dramatic increase in ML for new correspondence.  Old correspondence is nearly constant.  What is different between these correspodnences (other than the correspondences themselves).&lt;/p&gt;

&lt;p&gt;12:57:08 PM&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;: The max likelihood in the new correspondences is waaaaay into the tails of the prior.&lt;/p&gt;

&lt;p&gt;Since no real triangulation is done to ensure agreement between views, the maximum likelihood triangulation is very rough  (infinite likelihood variance allows this to happen without penalty).  This places the curve far into the tails of the prior, where evaluation is highly unstable.  Both the prior and likelihood are then extremely low (-1e-8 in log space), which should cancel, but don't due to numerical instability.  Hence, marginal likelihoods with huge magnitude.&lt;/p&gt;

&lt;p&gt;This is great news, since it means everything is working mostly as expected.  The numerical instability issue is easilly solved by always evaluating at the posterior, not the maximum likelihood.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The version of the new ML that doesn't use the matrix inversion lemma is accurate, so we can proceed to the markov-decomposed version next.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log"/>
   <updated>2013-07-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still trying to troubleshoot the difference between old and new likelihood implementations.&lt;/p&gt;

&lt;p&gt;Reviewing what is known&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prior is same&lt;/li&gt;
&lt;li&gt;Likelihood is same&lt;/li&gt;
&lt;li&gt;Posterior evaluates same in two different ways.&lt;/li&gt;
&lt;li&gt;mean curve is different&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Try using mean curve from old alg. in new alg.&lt;/p&gt;

&lt;p&gt;10:13:35 AM&lt;/p&gt;

&lt;p&gt;SOLVED.&lt;/p&gt;

&lt;p&gt;Yesterday I noticed that setting mix to 1.0 give decent results, but 0.0 was terrible.  Now I know why: 0.0 is a special case, in which a simplified calculation is used.  There was a bug in that special case, where the normalization constant didn't take into account the redundant dimensions that we eliminated.&lt;/p&gt;

&lt;h2&gt;ML Test&lt;/h2&gt;

&lt;p&gt;11:02:29 AM&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ml_test_end_to_end_2.m&lt;/code&gt; is now running&lt;/p&gt;

&lt;p&gt;Running on subset of correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...  
Done. (96.9 ms)  
Old ML: 62.176782

Computing marginal likelihood (new way, legacy correspondence)...  
Done. (112.3 ms)  
New ML (Legacy corr): 63.859940

Computing marginal likelihood (new way)...  
Done. (83.5 ms)  
New ML (New corr): 71.964026
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised the magnitude of difference between &quot;Old ML&quot; and &quot;New ML (Legacy corr)&quot;.  I guess the posterior variance is larger in this case, which means the posterior means can differ more.&lt;/p&gt;

&lt;p&gt;Running on full correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...
Done. (6400.1 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8328.4 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6349.2 ms)
New ML (New corr): 793.585038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that &quot;New ML (New Corr)&quot; is dramatiacally higher than the old and legacy ML's.  I suppose I should have been expected this, since this entire approach &lt;a href=&quot;/ksimek/research/2013/06/28/work-summary/&quot;&gt;was motivated by the terrible correspondences arising from the legacy correspondence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It's nice to see the ML for good correspondences getting even better.  Hopefully we'll see even more improvement as we try new curve models.&lt;/p&gt;

&lt;p&gt;Taking a break for lunch...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Next steps:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;fast ml_curve3

&lt;ul&gt;
&lt;li&gt;linear eval&lt;/li&gt;
&lt;li&gt;try without matrix inversion lemma&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;try new models (check that they're nonsingular; don't crash)&lt;/li&gt;
&lt;li&gt;set up training framework&lt;/li&gt;
&lt;li&gt;consider version without inversion lemma&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log"/>
   <updated>2013-07-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Continuing debugging of new likelihood&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Test:&lt;/strong&gt; &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;07:11:52 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Trouble with NaN error in &lt;code&gt;correspondence/corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;08:22:43 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Narrowed it down to gap-handling code-branch.&lt;/p&gt;

&lt;p&gt;08:53:34 AM&lt;/p&gt;

&lt;p&gt;Fixed. Arose from a nasty indexing scheme in the original &lt;code&gt;clean_correspondence.m&lt;/code&gt;, which I didn't handle well when adapting for &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;09:29:35 AM&lt;/p&gt;

&lt;p&gt;Nonsingular prior covariance matrix causing crash.  Apparently caused by same index appearing multiple times.&lt;/p&gt;

&lt;p&gt;In the past, we always merged such points.&lt;/p&gt;

&lt;p&gt;The new idea is different views of the same point would make the posterior covariance singular.  Either this isn't true, or there's a bug...&lt;/p&gt;

&lt;p&gt;Debugging this will take some serious coding and math brainpower and I fell debugging fatigue coming on.  Taking a brief break to refresh, then will tackle...&lt;/p&gt;

&lt;p&gt;11:56:21 AM&lt;/p&gt;

&lt;p&gt;Using a simple toy example, I (partially) confirmed my intuition that a degenerate prior matrix is okay, as long as its nullspace is spanned by the likelihood matrix.&lt;/p&gt;

&lt;p&gt;In the context of this bug, it means that two points sharing the same index value is fine as long as they arose from different views (assuming non-degenerate camera configuration, which is true in this case).&lt;/p&gt;

&lt;p&gt;Possible causes&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Numerical instability -- near-singular matrix.&lt;/li&gt;
&lt;li&gt;Error in Linear Algebra logic (seems unlikely, same code worked in old likelihood)&lt;/li&gt;
&lt;li&gt;Some bug earlier in the pipeline, causing invalid matrices here.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I'm guessing its 3; if not that, then maybe 2.&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;inspect the shared indices -- same view?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:48:49 PM&lt;/p&gt;

&lt;p&gt;Reversing my previous stance:  Degenerate prior with shared index will always result in a degenerate posterior.  The prior has two variables that are 100% correlated, and the posterior will also be 100% correlated.  This implies a degenerate covariance matrix.&lt;/p&gt;

&lt;p&gt;In such a case, the posterior's implied dimensionality is lower than it's apparent dimensionality.  The prior and posterior will be degenerate in the same way; this symmetry is aesthetically appealing, because the Candidates estimator for the marginal likelihood involves their ratio.  This suggests that eliminating the redundant dimensions for both pdfs is a sensible thing to do.&lt;/p&gt;

&lt;p&gt;How to resolve this?  Possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detect redundant indices early, eliminate them in the prior and posterior.  USe bookkeeping to remember which observations refer to the same model point.&lt;/li&gt;
&lt;li&gt;Handle reduntancies later, after degenerate posterior is formed, before evaluating posterior or prior .  The mean will automatically contain redundancies, which will avoid bookkeeping when comparing the mean to the data points (when &lt;code&gt;mix&lt;/code&gt; is != 0).&lt;/li&gt;
&lt;li&gt;Try to determine how identical indices arose in the first place.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Decided on 2 for now.  In retrospect, approach 1 may have had slightly less code and faster.&lt;/p&gt;

&lt;p&gt;01:45:00 PM&lt;/p&gt;

&lt;p&gt;ML is now running on legacy correspondence.  Results are significantly higher than the legacy ML algorithm (~ 2.25 times).  Not yet sure why yet.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6665.8 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8853.1 ms)
New ML (Legacy corr): 439.273126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Noticible slow-down, probably because of extra indices arising from not merging duplicate observations.&lt;/p&gt;

&lt;p&gt;possible causes of discrepancy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In old code, re-triangulation used simple averaging, not accounting for differing precisions of the points&lt;/li&gt;
&lt;li&gt;index sets differ between old and new code.  (update: checked, they are same)&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Plotting means for both to view differences...&lt;/p&gt;

&lt;p&gt;Investigation into ML discrepancy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Posterior means are similar, but obviously differ.&lt;/li&gt;
&lt;li&gt;both means are very non-smooth (need to re-address the smoothness issue)&lt;/li&gt;
&lt;li&gt;new mean has signficantly more points (~7%, 80 pts).  Could this alone affect ML?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;almost all difference is in posterior&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Maybe posterior kernel is different?  but prior is same
It's just the data covariance thats different.
but why??  There's more of them.&lt;/p&gt;

&lt;p&gt;New ml changes significantly when evaluation point changes.  This implies that the posterior curvature differs from the curvature of the joint distribution, which shouldn't happen.  A small amount of difference can arise naturally due to the approximation of the likelihood, but this is too large to be explained in that way.  Most noticibly, the new results look very similar to the old ones &lt;strong&gt;when evaluating at the likelihood mean&lt;/strong&gt;.  The largest difference occurs when evaluating at the posterior mean.  Not clear yet what conclusions to draw from this, or if this is just a coincidence.&lt;/p&gt;

&lt;p&gt;Maybe likelihood curvatures are differing.&lt;/p&gt;

&lt;p&gt;07:35:28 PM&lt;/p&gt;

&lt;p&gt;Was able to get identical likelihood value using matrix-form as I did with the original 2D geometric error form.  Seems like the precision matrices are good.&lt;/p&gt;

&lt;p&gt;Priors appear good too, since it evaluates to the same value as the reference implementation.&lt;/p&gt;

&lt;h2&gt;Reference implementation for new ML&lt;/h2&gt;

&lt;p&gt;Idea: direct implementation for quadform, but with hacked nomralization constant&lt;/p&gt;

&lt;h2&gt;In progress&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnosing errors in new likelihood &lt;code&gt;curve_ml_3.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;getting &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt; running.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log"/>
   <updated>2013-07-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Implementing covariance functions&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood&lt;/li&gt;
&lt;li&gt;Implement fast generalized marginal likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implement generalized marginal likelihood&lt;/h2&gt;

&lt;p&gt;Rewriting &lt;code&gt;curve_ml3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Based on &lt;code&gt;curve_ml.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Kernel now takes two sets of indices, curve-index and view-index.&lt;/p&gt;

&lt;p&gt;Kernel is now passed-in as a lambda.&lt;/p&gt;

&lt;p&gt;Need to decide whether to use matrix inversion lemma for global offset and linear prior.  Currently being used;  not using would be simpler; try both and compare results.&lt;/p&gt;

&lt;h2&gt;Testing new marginal likelihood&lt;/h2&gt;

&lt;p&gt;Creating new version of &lt;code&gt;test/test_ml_end_to_end.m&lt;/code&gt;, named &lt;code&gt;test/test_ml_end_to_end2.m&lt;/code&gt;.  This version uses the new likelihood format, and compares against the old.&lt;/p&gt;

&lt;h2&gt;New curve_likelihood&lt;/h2&gt;

&lt;p&gt;Need to write new version of &lt;code&gt;curve_likelihood.m&lt;/code&gt; to handle new likelihood format.&lt;/p&gt;

&lt;p&gt;It's going to be difficult to compare to older version, since we've changed the correspondence method...&lt;/p&gt;

&lt;p&gt;I'll have to add a flag that simulates the old correspondence method, but stores it in the new likelihood format...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;01:33:52 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I decided to write a separate function for the old correspondence method, called &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.  Moved &lt;code&gt;clean_correspondence3.m&lt;/code&gt; to &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The legacy function isn't exactly identical to the old function.  In particular, the index values might be slightly different, since it depends on the old function's global mean curve, which we don't compute in the new function.  If it's important, we can call the old function, and then call the new one.  This is a debugging function, so speed doesn't matter. For the moment, we'll accept the small difference, and use this function simply to confirm that we're in the ballpark.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;03:06:24 PM&lt;/em&gt;
Test is implemented.  Troubleshooting syntax errors...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;04:11:17 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Elusive NaN error in &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log"/>
   <updated>2013-07-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing marginal likelihood &lt;/h2&gt;

&lt;p&gt;Working on generalizing marginal likelihood for all three foreground models.&lt;/p&gt;

&lt;p&gt;Considered briefly how to use the matrix inversion lemma to improve numerical stability of all models.  Decided it could be difficult to build in a general way;  will proceed with the direct method for now, until there is evidence of numerical instability.&lt;/p&gt;

&lt;p&gt;Generalized how covariance is generated.  Wrote several new functions for generating differenct covariance matrices.  Will need to rework some due to the developments I describe later.&lt;/p&gt;

&lt;h2&gt;Theoretical Developments&lt;/h2&gt;

&lt;p&gt;Thought extensively about perturbation models I described yesterday.  I realized there is a serious problem with modeling motion using brownian motion -- the prior variance grows without bound as time approaches infinity.  Thus, the marginal prior for the curve in view 36 has much greater prior variance than the curve one in view 1.  This doesn't make sense -- ideally, they should all have the same marginal prior.&lt;/p&gt;

&lt;p&gt;This led to a reading session in Williams and Rasmussen, which led me to develop two new motion models, which I describe extensively in today's accompanying post.&lt;/p&gt;

&lt;h2&gt;Tomorrow&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish implementing new covariance functions.&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood.&lt;/li&gt;
&lt;li&gt;implement &lt;em&gt;fast&lt;/em&gt; generalized marginal likelihood (approximation)

&lt;ul&gt;
&lt;li&gt;Test against existing ML for old models.&lt;/li&gt;
&lt;li&gt;Test with new models.  Are the ML's higher? (will probably need training).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build training data set from hand-traced ground-truth.&lt;/li&gt;
&lt;li&gt;Write training code.  Train all three candidate models.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Rethinking covariance functions</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions"/>
   <updated>2013-07-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions</id>
   <content type="html">&lt;p&gt;Noticed a problem with the cubic spline perturbation model.  As the view number increases, the variance marginal prior of the curve in that view approaches infinity.  This means that the &quot;smoothness&quot; prior is ignored more and more in later views.&lt;/p&gt;

&lt;h2&gt;Squared Exponential perturbation model&lt;/h2&gt;

&lt;p&gt;A better model would have the same marginal prior for all views, but with correlation between nearby views.  This allows curves to revert to the prior as temporally correlated evidence becomes less informative.  I believe the squared exponential covariance function has this property.  Instead of adding the sq-exp covariance to the existing covariance function, we should multiply it, so self-covariance is unchanged, but pairwise correlation is non-zero.&lt;/p&gt;

&lt;p&gt;An added benefit of this is that it only has one tunable parameter.&lt;/p&gt;

&lt;p&gt;It should be easy to incorporate into our test-bed, along with the other two newly proposed foreground models.&lt;/p&gt;

&lt;h2&gt;Ornstein Uhlenbeck perturbation model&lt;/h2&gt;

&lt;p&gt;Digging deeper into Williams and Rasmussen, I found precisely the GP I was looking for a few days ago:  the OrnsteinUhlenbeck (OU) process.  This   process describes brownian motion under the influence of a regularizing force that pulls toward the mean.&lt;/p&gt;

&lt;p&gt;In other words, I can model correlation between views without affecting the marginal prior of the curve any particular view.  This is also accomplished by the squared-exponential model, but the OU process is probably more realistic, because the plant's motion looks non-smooth.&lt;/p&gt;

&lt;h2&gt;Modelling the mean or not?&lt;/h2&gt;

&lt;p&gt;I'm struggling with whether or not to explicitly model a &quot;mean&quot; curve with the SE and the OU processes.&lt;/p&gt;

&lt;p&gt;If I did model the mean, each curve's covariance function would be the sum of a standard covariance plus a perturbation covariance.  The standard covariance models the &quot;mean&quot; curve, and it would be the same for all views (100% correlated).  The perturbation covariance would be partially correlated between the views, using the SE or the OU process.  The bayes net has the following structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        O   &amp;lt;-- mean curve
     / /\ \
   /  /  \  \
 /   |    |   \
O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The alternative is to model the per-view curves directly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Under this model, each view's curve has a cubic-spline marginal distribution, and the SE or UO process controls the correlation between them.&lt;/p&gt;

&lt;p&gt;What isn't clear is whether the perturbations in the latter model will be independent between points.  We need to model within-view perturbations as correlated, otherwise the marginal likelihood will drop too low.  There is no explicit description of how perturbations of adjacent points correlate.&lt;/p&gt;

&lt;h2&gt;What follows is me thinking out-loud...&lt;/h2&gt;

&lt;p&gt;Intuitively, points nearby in curve-space (have similar \(\tau\)'s) can never be more correlated than they are when they appear in the same view.  Separating them in time (view) will decreases their correlation, until finally, there is no correlation; the only remaining correlation is between points in the same view.  The SE kernel modulates that correlation.  The SE kernel doesn't explicitly model correlation between perturbations, but this doesn't mean the correlation doesn't exist -- it is implicit in the original kernel.&lt;/p&gt;

&lt;p&gt;There is an analogy here with the classic krigging example.  In there models, the squared exponential over a 2D landscape (i.e. joint function over \(\mathbb{R}\)&lt;sup&gt;2&lt;/sup&gt; space) is equal to the product of 1D squared exponentials (i.e. two functions over \(\mathbb{R}\)&lt;sup&gt;1&lt;/sup&gt; space).  In other words, the 2D kernel is constructed by the product of 1D kernels.  There is no worry that the delta between nearby &quot;slices&quot; of the surface are uncorrelated, because the marginal covariance within that slice will enforce that smoothness.&lt;/p&gt;

&lt;p&gt;In our case, we also have a product of 1D covariance functions constructing a 2D covariance function.  The difference is that one of the kernels (the curve-space kernel) is a cubic-spline process, while the other (the time-dimension kernel) is squared exponential (or Ornstein-Uhlenbeck).  Despite this difference, my intuition is that the conclusions are the same - the deltas will be smooth (i.e. correlated), because they will be the difference between two smooth curves.&lt;/p&gt;

&lt;p&gt;Considering the marginals in both directions further illustrates why this works.  Obviously, a slice in the curve-direction will always look curve-like, since the marginals are all the same cubic-covariance GP prior.  In the time-direction, a single point will follow a GP or OU process, with initial conditions dictated by the first curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;...BUT...&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;...in the absence of data, each individual point will drift over time to the prior mean, I.e. &lt;strong&gt;zero&lt;/strong&gt;.  In other words, the 3D curve observed in view infinity gains no information from curve observed at time zero.&lt;/p&gt;

&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; realistic.  In reality, these points shouldn't drift far from some &quot;sensible&quot; center curve, implying that a mean curve exists.&lt;/p&gt;

&lt;p&gt;The time-slice of any particular point &lt;em&gt;should&lt;/em&gt; look like either a OU or SE process, but the mean needs to be explicit.  This implies an additive model, with distinct models for the unobserved &quot;mean curve&quot; and the deviations from it, which are summed to get the 3D curve seen in each view.&lt;/p&gt;

&lt;h2&gt;Modeling perturbation&lt;/h2&gt;

&lt;p&gt;So if we must model perturbation, how should we model it?&lt;/p&gt;

&lt;p&gt;One thing is clear: the marginal prior for the curve in any view &lt;strong&gt;must&lt;/strong&gt; still be a cubic-spline process.&lt;/p&gt;

&lt;p&gt;This implies that the perturbation must be a cubic-spline process, too.&lt;/p&gt;

&lt;p&gt;However, the variances for each component (offset, linear, and cubic) are likely to be different for the perturbation model, compared to the mean-curve model.  Most importantly, the magnitude of the offset variance must be large in the mean-curve model but will be &lt;em&gt;much&lt;/em&gt; lower (relative to linear and cubic variance) in the perturbation model.&lt;/p&gt;

&lt;p&gt;I was hoping to avoid adding four extra parameters to our model (perturbation offset, linear and cubic variance, plus perturbation scale-length).  The mean-free model only adds one parameter - scale-length.  I guess this is the price we pay for a better model -- and for higher marginal likelihoods.  Ideally, the occam's razor quality of the marginal likelihood will allow us to avoid overfitting this many parameters (7 total).  Any parameters that are superfluous should become near-zero during training.&lt;/p&gt;

&lt;p&gt;...I hope.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log"/>
   <updated>2013-07-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Background:  This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;p&gt;Also developed a new approach to evaluate marginal likelhoods thats much simpler, but need to confirm that it matches reference implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implement both new foreground models, plus old one in new &quot;kernel function&quot; way.&lt;/li&gt;
&lt;li&gt;Implement new evaluation method and test against reference.&lt;/li&gt;
&lt;li&gt;Learn parameters for all three models (needs some ground truthing).&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implementing new models&lt;/h2&gt;

&lt;p&gt;Worked on general framework for evaluating ML under general new model framework.&lt;/p&gt;

&lt;p&gt;After some struggling with covariance degeneracies inherent in backprojection, I realized that the 3D marginal likelihood is naturally degenerate, because it isn't the true likelihood function (which is in 2D).&lt;/p&gt;

&lt;p&gt;I'm kicking myself for not remembering that I struggled with this exact problem 5 months ago.  At that time, I realized the better approach is to use Candidate's estimator, which is the ratio of the unnormalized posterior to the normalized posterior.  The unnormalized posterior comes from the prior and 2D likelIhood;  the normalized posterior is obtainable from the 3D likelihood and the prior.&lt;/p&gt;

&lt;p&gt;This was already implemented in &lt;code&gt;curve_ml.m&lt;/code&gt;, but was  O(n&lt;sup&gt;3&lt;/sup&gt; ), so was all but abandoned in favor of the junction-tree method in &lt;code&gt;curve_ml2.m&lt;/code&gt;, which is O(n).&lt;/p&gt;

&lt;p&gt;However, it's recently become clear that the candidate's estimator should be evaluated in \(O(n)\)  by exploiting the Markovian nature of the covariance matrix.&lt;/p&gt;

&lt;p&gt;It should be easy to try both, by simply swapping out covariance matrices with ones arising from the new covariance functions.  Tomorrow...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Foreground Curve Models as Gaussian Process Covariance Function</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models"/>
   <updated>2013-07-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Background&lt;/h2&gt;

&lt;p&gt; This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;h2&gt;Covariance functions as models&lt;/h2&gt;

&lt;p&gt;I realized this weekend that all of the models I've considered are achievable by using different covariance functions.&lt;/p&gt;

&lt;p&gt;The original model is given by&lt;/p&gt;

&lt;div&gt; \[
k_1(i,j) = \sigma_s k_{\text{cubic}}(i,j) + \sigma_o  k_{\text{offset}}(i,j) + \sigma_r k_{\text{linear}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
k_{\text{cubic}}(i,j) &amp;= \frac{|\tau_i - \tau_j| \min(\tau_i, \tau_j)^2}{2} + \frac{\min(\tau_i, \tau_j)^3}{3} \\
k_{\text{linear}}(i,j) &amp;= \tau_i \tau_j  \\
k_{\text{offset}}(i,j) &amp;= 1 \\ 
\end{align}
\] &lt;/div&gt;


&lt;p&gt;The cubic model penalizes non-zero second derivative over the length of the curve.  The offset and linear model penalize zero and first derivative initial conditions.&lt;/p&gt;

&lt;h2&gt;White noise perturbation model&lt;/h2&gt;

&lt;p&gt;Both of the two new models expand on the original by modelling how the observed curve differs between views.  That is, the model for the curves are the same as before (they are cubic spline curves), but we additionally model &lt;strong&gt;how they are perturbed between views&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The first new model I call the &quot;White Noise&quot; perturbation model, which treats each view of the curve as arising from a white noise process that perturbs a &quot;master curve&quot;, I.e. the unobserved mean curve.  Its covariance is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{white}}(i,j) = k_1(i,j) + \delta(v_i-v_j) k_{\text{w}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(v_i\) is the view that captured the \(i\)th point and \(k_w\) is&lt;/p&gt;

&lt;div&gt; \[
k_w(i,j) = \sigma_{o,w}  k_{\text{offset}}(i,j) + \sigma_{r,w} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model adds extra covariance that is independent per-view.  This treats the perterbations from the mean curve as independent.&lt;/p&gt;

&lt;p&gt;The perturbations themselves are assumed to be purely to the curve's initial conditions, i.e. its position and direction.&lt;/p&gt;

&lt;p&gt;This model is motivated by the assumption that perturbations arise due to camera mis-calibrations, which result in mostly translation and small rotational changes.&lt;/p&gt;

&lt;h2&gt;Brownian motion perturbation model&lt;/h2&gt;

&lt;p&gt;The second model treats perturbations as arising from Brownian motion, i.e. each curve is independent, conditioned on the previous view's curve.  The covariance function is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{brown}}(i,j) = k_1(i,j) + \min(\tau_i, \tau_j) k_{\text{b}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(k_b\) is&lt;/p&gt;

&lt;div&gt; \[
k_b(i,j) = \sigma_{s,b} k_{\text{cubic}}(i,j) + \sigma_{o,b}  k_{\text{offset}}(i,j) + \sigma_{r,b} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model assumes perturbations arise by motion of the plant during imaging, like moving closer to a light source, or responding to a temperature gradient in the room.  &quot;View index&quot; is a surrogate for a time variable, since time between captures is roughly constant.  The use of Brownian motion means the magnitude of perturbation increases by the square root of the distance between the views (time).  \(k_b\) models the nature of the perturbation; we use the cubic-spline kernel which says that point-wise perturbations are strongly correlated and increase in magnitude the further they get from the base of the curve.&lt;/p&gt;

&lt;p&gt;This \(k_b\) is possibly overkill;  using the simpler \(k_w\) from the white-noise model might work just as well.  This simpler model implies curves drift in position and direction only, and these perturbations are correlated over time.&lt;/p&gt;

&lt;h2&gt;End stuff&lt;/h2&gt;

&lt;p&gt;After some consideration, I think implementing these models in the current system will will very simple.  Training them will be harder; some more ground truthing will be needed.&lt;/p&gt;

&lt;p&gt;Of the two new models, I suspect that the white noise model will be sufficient to get the gains in marginal likelihood we seek.  It is a much better explanation for misaligned data-points compared to the old model models all point perturbations as independent.&lt;/p&gt;

&lt;p&gt;The brownian motion model will give us something to compare against in evaluation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work summary</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary"/>
   <updated>2013-06-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finished editing new likelihood function.  See &lt;code&gt;correspondence/clean_correspondence3.m&lt;/code&gt; (filename likely to change soon).  Below is a summary of results and comparison to the old approach.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;Below is a plot of the maximum likelihood curves:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-likelihood_1.gif&quot; alt=&quot;Maximum likelihood reconstruction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each colored curve arises from a single 2D data curve.  The curve represents the mean of a gaussian process. Variance is not shown; it is infinite in the direction of backprojection.&lt;/p&gt;

&lt;p&gt;The dark-blue curve is an estimate of the posterior curve, which was used to backproject the 2D curves against.  It is estimated from the point-correspondences.&lt;/p&gt;

&lt;p&gt;Note that for clarity, x/y scale is significantly smaller than z-scale (up-direction).  When axis are equally-scaled, max-likelihood curves lie very close the to blue curve.&lt;/p&gt;

&lt;h2&gt;Improved correspondence&lt;/h2&gt;

&lt;p&gt;The old likelihood suffered from a small but non-negligible number of awful correspondences, which severely damaged both reconstruction and marginal likelihood values.  This was because the likelihood was derived from the point-to-point correspondences, which (a) is problematic at gaps, and (b) suffer bad correspondences which can't be fixed later.
The new approach uses the old approach as a starting point, but then recomputes all 2D curve correspondences against a rough reconstruction.  This dramatically improves correspondences as we see below.&lt;/p&gt;

&lt;p&gt;This is the old correspondence.  Blue points are points from the 2D data curve;  the teal line is the posterior 3D curve (projected to 2D); red lines show correspondences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug.png&quot; alt=&quot;Correspondence bug &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next is the fixed correspondence.  Notice how the red correspondence lines are much shorter, indicating a less costly correspondence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug-fixed.png&quot; alt=&quot;Correspondence bug, fixed &quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Per-data-curve likelihood&lt;/h2&gt;

&lt;p&gt;The old likelihood had a single GP curve that represented all of the different views.  Now we have a GP curve &lt;strong&gt;per data-curve&lt;/strong&gt;, which will be related by a GP prior.&lt;/p&gt;

&lt;p&gt;This will allow us to simultaneously track and triangulate, a key novelty to this approach.  More importantly, it will give us higher marginal likelihood numbers for true 3D curve observations, because we can make the independent noise component very small.&lt;/p&gt;

&lt;h2&gt;TODO - short-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt; - This new function adds an extra pass of DTW per data-curve, per iteration where previously, only one pass was needed.  This has introduced a significant performance bottleneck on the order of 10-50x.  I need to profile and optimize this function if we want runtimes that aren't measured in weeks.  This may motivate a full re-thinking of &lt;code&gt;merge_correspondence.m&lt;/code&gt;, to avoid a full DTW after every merge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Marginal Likelhood &lt;/strong&gt; - Need to build the new marginal likelihood for the foreground curve model.  This proposed version will take advantage of the new per-data-curve likelihood.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO - mid-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end&lt;/strong&gt; - Incorporate new likelhood and ML into gibbs sampler.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Swendsen wang cuts &lt;/strong&gt; - design SWC split/merge move&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log"/>
   <updated>2013-06-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Profiling and optimizing &lt;code&gt;clean_correspondence3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Huge bottleneck in &lt;code&gt;get_dtw_matches_()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Wrote MEX version of get_dtw_matches_(),  &lt;code&gt;get_dtw_matches_horiz.c&lt;/code&gt;.   ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New bottlenecks&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;projection_error_hessian&lt;/li&gt;
&lt;li&gt;&lt;code&gt;interp1&lt;/code&gt; - linear interpolation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;csaps&lt;/code&gt; - spline smoothin&lt;/li&gt;
&lt;li&gt;dtw&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Plan&lt;/h2&gt;

&lt;p&gt;can possibly eliminate some calls to interp1.&lt;/p&gt;

&lt;p&gt;could mex projecion_error_hessian.  should be a big win&lt;/p&gt;

&lt;p&gt;DTW can be done in two passes, or mex'd&lt;/p&gt;

&lt;h2&gt;Mexing &lt;code&gt;projection_error_hessian&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Tried using Matlab's &quot;Coder&quot; feature to generate mex code automatically.  The result was totally bloated (14 files!) and no faster than the matlab version.  Next I tried hand-coding the mex, and its ~10x faster.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/27/work-log"/>
   <updated>2013-06-27T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/27/work-log</id>
   <content type="html">&lt;p&gt;&lt;strong&gt; Thursday Overview &lt;/strong&gt;
* Continued refactoring of likelihood
* C++ Bootcamp
* VR lab tour and new user set-up&lt;/p&gt;

&lt;h2&gt;Continued refactoring of likelihood&lt;/h2&gt;

&lt;p&gt;Implemented &quot;alternative approach&quot; to handling tail points mentioned in last entry.  The old way didn't benefit from re-indexing; this way does.  This way handles negative index points correctly, too.&lt;/p&gt;

&lt;p&gt;In process of testing and debugging.  Possibly more outcomes later tonight.&lt;/p&gt;

&lt;h2&gt;C++ Bootcamp&lt;/h2&gt;

&lt;p&gt;today's session: inline and const-correctness&lt;/p&gt;

&lt;h2&gt;VR lab tour and new user set-up&lt;/h2&gt;

&lt;p&gt;By Angus's request, I showed showed the new postdoc Javier around the lab and set him up with an admin account.&lt;/p&gt;

&lt;p&gt;Lots of things still broken on VR01; biggest problem is video card #2 not displaying anything.  Game controller not set up yet.    Showed osgviewer demo, and by Angus's request, got his Processing demo running too.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/26/work-log"/>
   <updated>2013-06-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Likelihood construction, &lt;code&gt;clean_correspondence3.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;new tracking GP model&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Likelihood construction&lt;/h2&gt;

&lt;p&gt;Started and finished implementation today.  Need to design a test and then debug.&lt;/p&gt;

&lt;p&gt;~ 250 lines of Matlab code.  Logic overview:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;smooth triangulated points&lt;/li&gt;
&lt;li&gt;resample smoothed 3D curve uniformly&lt;/li&gt;
&lt;li&gt;handle tails (see below)&lt;/li&gt;
&lt;li&gt;project curve into each view and resample uniformly&lt;/li&gt;
&lt;li&gt;DTW to correspond 2d data curve to projected smooth curve  (see below)&lt;/li&gt;
&lt;li&gt;map corresponding projected curve points back to 3D points and indices&lt;/li&gt;
&lt;li&gt;triangulate 2d data points against corresponding 3d point&lt;/li&gt;
&lt;li&gt;compute likelihood hessian around that point&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;New DTW&lt;/h2&gt;

&lt;p&gt;Re-implemented a specialized version of DTW with following changes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;only horizontal steps accrue cost&lt;/li&gt;
&lt;li&gt;Hard-constraint on the number of vertical steps per horizontal step.&lt;/li&gt;
&lt;li&gt;keeps track of &quot;best&quot; match along vertical runs. no need for second pass

&lt;ul&gt;
&lt;li&gt;I think this is only possible because only horizontal steps accrue cost.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Tail points&lt;/h2&gt;

&lt;p&gt;Still iffy on the &quot;tail points&quot; case. Implemented late at night and likely needs review in the morning.  Still need to handle negative index values.&lt;/p&gt;

&lt;p&gt;Alternative implementation: only inspect the tails to determine the length of the 3D curve.  Then proceeed as usual. no special cases&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;review tail points code

&lt;ul&gt;
&lt;li&gt;consider alternative implementation (see above)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;handle negative index values&lt;/li&gt;
&lt;li&gt;think about hessian and transformation Jacobian&lt;/li&gt;
&lt;li&gt;testing, debugging, profiling&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Rethinking Likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/20/rethinking-likelihood"/>
   <updated>2013-06-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/20/rethinking-likelihood</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Testing and debugging &lt;code&gt;clean_correspodnence2.m&lt;/code&gt; has revealed some significant problems with the current approach to constructing the likelihood function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 5%-10% of fragements have correspondences that are nonsensical. &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I hypothesize that this is due to bad correspondences early on, when there is little evidence to drive a good correspodnence.  These bad correspodnences are propagated as new curves are added that could suggest a better correspodnence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Large lateral gaps in triangulation result in large axial gaps in posterior curve.  &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The problem is that the index set is computed from the triangulation.  The current fix for this -- smooth, re-index, repeat -- is very limited in the severity it can overcome.  In practice, most gaps are only partially reduced.&lt;/p&gt;

&lt;h2&gt;Rethinking &quot;Correspondence to Likelihood&quot;&lt;/h2&gt;

&lt;p&gt;Corresondence is good for constructing a decent-quality 3d curve, but isn't good for computing fine-grained pointwise likelihood, due to sporradic terrible correspondences and gaps.&lt;/p&gt;

&lt;p&gt;Instead of continuously Band-Aiding these issues that keep arising, its time to re-think how the likelhood is constructed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 1. the mean of 3d gaussians should project to the 2D position of the corresponding data point &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 2. the depth should be based on the corresponding position in the unobserved curve &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Two issues here.  First, how to localize the unobserved curve without having the likelihood already (chicken and egg).  Second, how to identify the corresponding point of the unobserved curve?&lt;/p&gt;

&lt;p&gt;In the old method, the answer to both was &quot;use the correspondence matrix.&quot;&lt;/p&gt;

&lt;p&gt;In the new method, we still use the correspondence matrix to triangulate, but we smooth the result using the prior and then throw away correspondence information.&lt;/p&gt;

&lt;h2&gt;Killing the correspondence grid&lt;/h2&gt;

&lt;p&gt;The corresponence matrix artificially forces points from different views to correspond to the same point.  This is out of necessity -- we need correspondence to achieve triangulation.  But we don't need to adhere to this to compute the likelhood.  Indeed, observed points may fall anywhere in the continuous index set, not into a discrete set of predefined cells.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Observations can correspond to any index in \([0 t_{end}]\).&lt;/li&gt;
&lt;li&gt;Unobserved curve is modelled at a uniform grid.&lt;/li&gt;
&lt;li&gt;Previously, the dimensionality of the unobserved curve grew with the number of observations. Now it grows with the range of the index set (i.e. the length of the curve).`&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Random thoughts&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;evaluate marginal likelihood directly?  then add extra normalization from triangulation (jacobian?)

&lt;ul&gt;
&lt;li&gt;evaluate linearly using markov conditional probabilities&lt;/li&gt;
&lt;li&gt;extend to poly model

&lt;ul&gt;
&lt;li&gt;grid-structured Bayes net&lt;/li&gt;
&lt;li&gt;topological sort for evaluation&lt;/li&gt;
&lt;li&gt;use scope variables to manage dependencies generally&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/17/work-log"/>
   <updated>2013-06-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing work on foreground curve model. Editing &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Logic overview&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;smooth triangulated points &amp;amp; estimate their indices&lt;/li&gt;
&lt;li&gt;fill-in untriangulatable points/indices&lt;/li&gt;
&lt;li&gt;resolve many-to-one data-to-3d-curve correspondences by picking best match&lt;/li&gt;
&lt;li&gt;triangulate each data point against 3d curve to get mean and covariance for point&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Logic detail&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;1. estimate indices by chord-length parameterization
2. repeat N times:
    2.1 smooth triangulated points using current index estimate (posterior mean approx.)
    2.2 re-estimate indices using chord-length parameterization
3. for each untriangulatable point
    3.1 triangulate against cubic interpolation of neighboring points (Newton's method)
    3.2 store resulting point and index with smoothed points from 2.1
4. for each 2D data curve
    4.1 triangulate against corresponding 3d point in smoothed curve to get likelihood mean
    4.2 compute curvature at this point to get likelihood precision
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;03:13:30 PM&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First run, addressing syntax errors.&lt;/li&gt;
&lt;li&gt;Plotting first test: triangulated curve w/ and wo/ gap-filling

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: gap-fill at beginning maps exactly to first non-gap point&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: interior gap point falls exactly on non-gap point&lt;/li&gt;
&lt;li&gt;need to merge points?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;04:06:03 PM&lt;/strong&gt;
wrapping up...&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnose coincident-point issue -- bug or not?&lt;/li&gt;
&lt;li&gt;Finish testing - plot per-view likelihood points&lt;/li&gt;
&lt;li&gt;Add point-merging&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/14/log-entry"/>
   <updated>2013-06-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/14/log-entry</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14528&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Working on foreground curve model.&lt;/p&gt;

&lt;p&gt;Begin of significant rework; SVN revision noted in infobox.&lt;/p&gt;

&lt;p&gt;Editing  &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt; which will replace &lt;code&gt;correspondence/clean_corespondence.m&lt;/code&gt;.  Drawing partially from &lt;code&gt;corr_to_bg_likelihood.m&lt;/code&gt;, esp. for &quot;best match&quot; logic.&lt;/p&gt;

&lt;p&gt;Probably will rename &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt; to &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt; before I finish.&lt;/p&gt;

&lt;p&gt;Will use &lt;code&gt;tests/test_ml_end_to_end&lt;/code&gt; to compare old and new implementations.&lt;/p&gt;

&lt;p&gt;Work still in progress...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/13/log"/>
   <updated>2013-06-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/13/log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;After investigating the false positives from the last entry, it seems clear that bad matches look good because missing data are not penalized.  For example&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-13-bad-match.png&quot; alt=&quot;Bad match&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the overlapping region of the blue and green curves, the distance between them is relatively low (less than 3 pixels, or 1.5px radius).  But the size of their overlap is so low that it would be hard to claim that they come from the same underlying curve with any confidence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Params &lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smoothing_variance_2d: 0.2500
    noise_variance_2d: 10
     position_mean_2d: [2x1 double]
 position_variance_2d: 1.3629e+04
     rate_variance_2d: 0.4962
   smoothing_variance: 1.0000e-04
       noise_variance: 10
        position_mean: [3x1 double]
    position_variance: 62500
        rate_variance: 2.2500
      smoothing_sigma: 0.2000
    noise_variance_bg: 0.1038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I would have expected that noise_variance_bg was low enough to discount this candidate, but the log ML ratio is 71.0.  The noise model must just look really bad...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Visualizing/Debugging BG ML</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/12/visualizingdebugging-bg-ml"/>
   <updated>2013-06-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/12/visualizingdebugging-bg-ml</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Experiment&lt;/h2&gt;

&lt;p&gt;We now have a new algorithm for computing background curve Marginal Likelihood.  Lowering noise sigma \(\sigma_n\) should rule out bad matches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: Re-run background candidate matching with new algorithm and roughly-trained \( \sigma_n \).&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;If the threshold is set right, the results are improved, but we still have some false-positives and false negatives.&lt;/p&gt;

&lt;p&gt;It's still unclear whether we can get good results without thresholding, since we haven't computed the noise ML using the new algorithm, so absolute numbers are meaningless.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Params&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smoothing_variance_2d: 0.2500
    noise_variance_2d: 10
     position_mean_2d: [2x1 double]
 position_variance_2d: 1.3629e+04
     rate_variance_2d: 0.4962
   smoothing_variance: 1.0000e-04
       noise_variance: 10
        position_mean: [3x1 double]
    position_variance: 62500
        rate_variance: 2.2500
      smoothing_sigma: 0.2000
       nlise_variance: 10
    noise_variance_bg: 0.1038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Calls&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data = offline_pair_candidates(data, params, 0, 1, 1, 'bg');
cands = tmp_get_cands(data);
visualize_bg_cands(data, cands, 250)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt; Plots &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Matched curves appear in white, unmatched appear in gray&lt;/p&gt;

&lt;p&gt;False negatives&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-12-bad-matches.png&quot; alt=&quot;bad matches&quot; /&gt;&lt;/p&gt;

&lt;p&gt;False positives:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-12-false-positive.png&quot; alt=&quot;bad matches&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;apply new indexing and cleanup algorithm to 3d and noise curves&lt;/li&gt;
&lt;li&gt;better training of foreground/background

&lt;ul&gt;
&lt;li&gt;ground truth curve fragments&lt;/li&gt;
&lt;li&gt;automatic training of background&lt;/li&gt;
&lt;li&gt;automatic training of noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;better foreground model

&lt;ol&gt;
&lt;li&gt;compute likelihood separately&lt;/li&gt;
&lt;li&gt;add smooth GP to likelihood&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;re-run experiment with trained noise model parameters&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Index set bug fixed, marginal likelihood curves improved</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/10/index-set-bug"/>
   <updated>2013-06-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/10/index-set-bug</id>
   <content type="html">&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Index Set Bug fixed &lt;/h2&gt;

&lt;p&gt;After fixing a problem with how index sets were estimated, the marginal likelihood (ML) curves are now much more sensible.&lt;/p&gt;

&lt;p&gt;Here are two marginal likelihood curves using the old approach with two different smoothing sigmas.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2012-06-10_ML_old.png&quot; alt=&quot;pre-bug ML curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how lowering the smoothing sigma \(\sigma_s\) causes the maximum ML to continuously improve, and results in a &lt;em&gt;huge&lt;/em&gt; improvement when noise sigma \(\sigma_n\) is low. There are two implications of this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ML continuously improves as \(\sigma_s\) approaches zero&lt;/li&gt;
&lt;li&gt;As \(\sigma_s\) approaches zeros, the optimal \(\sigma_n \) approaches zero&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;We &lt;em&gt;need&lt;/em&gt; to be able to use \(\sigma_n \) to control the cutoff for background curves, but this is not possible if ML is a monotonic function of \(\sigma_n\).&lt;/p&gt;

&lt;p&gt;The problem arose from the fact that index-set spacing grew in proportion to curve-noise, whereas it should have stayed roughly constant.  As a result, more noise made the curves look &lt;em&gt;smoother&lt;/em&gt;, because the unobserved points seemed to be farther apart.&lt;/p&gt;

&lt;p&gt;Obviously, this is the opposite of what we would want.  I rewrote the &quot;cleanup&quot; algorithm so index sets are now computed from an estimate of the posterior curve, not from the maximum likelihood curve.  This causes noise to be smoothed out of the curve before measuring the distance between points, so increasing noise will not significantly change the inter-point distance.&lt;/p&gt;

&lt;p&gt;Here are the ML curves after the change.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2012-06-10_ML_new.png&quot; alt=&quot;pre-bug ML curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Some observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The affect of changing \(\sigma_s\) is &lt;em&gt;far&lt;/em&gt; less dramatic&lt;/li&gt;
&lt;li&gt;As \(\sigma_n\) approaches zero, the ML &lt;em&gt;always&lt;/em&gt; drops below 0&lt;/li&gt;
&lt;li&gt;The position and value of the maximum is mostly unchanged, suggesting a &quot;natural&quot; noise-value that is independent of the smoothing value.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Point 2 is particularly important.  These ML values are actually log ratios between the ML under this model and the ML under the &quot;default&quot; noise model.  Values below zero indicate that the naive noise model is a better fit.  The fact that we can adjust \(sigma_n\) to control the trade-off between the two models is promising, and suggests that this new model can, indeed, be discriminitive.  Prior to these bugs, it was not clear that this was the case, because the background curve model &lt;em&gt;always&lt;/em&gt; looked better than noise.&lt;/p&gt;

&lt;h2&gt;Point merging&lt;/h2&gt;

&lt;p&gt;One additional complication that arose was that after smoothing, many curve points at appeared at nearly the same position.  As a result, the changes in the index set were very small and the resulting Gaussian Process covariance matrix became degenerate.  I added some code that merges points that are too close to each-other, and updates the likelihood function and index set accordingly.  My tests show that this causes a negligible decrease in the ML compared to the non-merged case, and eliminates the degeneracy problem in all the cases I encountered.&lt;/p&gt;

&lt;h2&gt;Smart Cleanup&lt;/h2&gt;

&lt;p&gt;During my investigations, I also rewrote the &quot;cleanup&quot; logic, which ensures that each point corresponds to the model curve exactly once.  It originally did this by naively taking the first correpsondence, and I thought that the problematic results from above were caused by this.  I wrote new logic that now chooses the &lt;em&gt;best&lt;/em&gt; correspondence, i.e. the correspondence that results in the lowest error.&lt;/p&gt;

&lt;h2&gt;Summary, Next steps&lt;/h2&gt;

&lt;p&gt;The new code is in &lt;code&gt;correspondence/corr_to_bg_likelihood.m&lt;/code&gt;, which now replaces the deprecated &lt;code&gt;clean_bg_correspondence.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Next test: can we distinguish BG curves from non-bg/foreground curves?&lt;/li&gt;
&lt;li&gt;Migrate this new logic to 3D curve model (clean_correspondence.m and maybe merge_correspondence.m)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 
</feed>
