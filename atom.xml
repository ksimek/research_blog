<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>KLS Research Blog</title>
 <link href="http://vision.sista.arizona.edu/ksimek/research/atom.xml" rel="self"/>
 <link href="http://vision.sista.arizona.edu/ksimek/research"/>
 <updated>2015-01-20T23:26:27-07:00</updated>
 <id>http://vision.sista.arizona.edu/ksimek/research</id>
 <author>
   <name>Kyle Simek</name>
   <email>ksimek@email.arizona.edu</email>
 </author>

 
 <entry>
   <title>Articulated gaussian processes (part 2)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/20/reference2"/>
   <updated>2015-01-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/20/reference2</id>
   <content type="html">&lt;p&gt;Continued from &lt;a href=&quot;/ksimek/research/2015/01/20/reference/&quot;&gt;part 1&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Covariance functions&lt;/h2&gt;

&lt;p&gt;Recall the 1D cubic-spline covariance:&lt;/p&gt;

&lt;div&gt;
\[
k(x,x&#39;) = |x-x&#39;| \min(x,x&#39;)^2 / 2 + \min(x,x&#39;)^3/3.
\]
&lt;/div&gt;


&lt;p&gt;We can generalize this to ND indices as&lt;/p&gt;

&lt;div&gt;
\[
k(\boldsymbol(x),\boldsymbol(x)&#39;) = \sum_i |x_i-x_i&#39;| min(x_i,x_i&#39;)^2 / 2 + min(x_i,x_i&#39;)^3/3.
\]
&lt;/div&gt;


&lt;p&gt;When using a cubic-spline covariance function, the only dimensions that are nonzero are those corresponding to shared ancestors.  The position along the curve only matters when comparing points on the same subgraph.  For trees, this exactly corresponds to the branching Gaussian process covariance I derived in my dissertation proposal.  Plate regions act like thin-plate splines.&lt;/p&gt;

&lt;p&gt;When using a radial-basis covariance function like squared exponential, the squared L2 distance has a nice interpretation.  Take any path between two nodes, and split it at articulation points into a sequence of subpaths.  The squared L2 distance is the sum of the squared local distances between articulation points, plus the squared distance between the endpoints and their nearest articulation points.  This is similar to the squared geodesic distance between the points, but modified to restart the distance measurement from zero at each articulation point.&lt;/p&gt;

&lt;h2&gt;Generalizations&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-20-plate_in_chain_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It may be interesting to model plates as being superimposed on an underlying chain. For example, in the image above,  1,2,3,4 is probably well modelled by a smooth curve (i.e. a single chain), but 2,3,5 is also clearly a plate.   We might want to let point 5&#39;s position influence points 2 and 3, without violating smoothness of 1,2,3,4.  To handle this, we relax the requirement that the plate and chain must lie in orthogonal hyperplanes.  We modify equation (1) so instead of vertex 5 inheriting the index of its predecessor (e.g. vertex 3), we replace the first term with the linear interpolation between indexes 3 and 4.  This lets subgraph 2,3,5 act a little bit like a chain and a little bit like a plate.  How to implement this linear interpolation is up for debate, but two possibilities are: (a) relative euclidean distance, or (b) relative geodesic distance.&lt;/p&gt;

&lt;h2&gt;Altnerative definition&lt;/h2&gt;

&lt;p&gt;The definition of the index-space below is equivalent to the one above, but requires some extra proofs to explain (like proving all DFS paths between vertices within a subgraph are fully contained within the subgraph).  It was just too cumbersome, rhetorically, but quite convenient to implmement. I&#39;m including it here so I remember it during implementation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;For each vertex \(v_i \in V\), we assign an index \(x_i \in \mathbb{R}^{|G^c| + 2\,|G^p|} \). We arbitrarilly pick a vertex \(v_0\) to be the graph&#39;s root, and set its index to \(x_0 = \mathbf{0}\).  For each vertex \(v_i \in V \setminus v_0\), let \(p(i)\) be the index of its predecessor in a depth-first search.  We then define the index of \(v_i\) to be&lt;/p&gt;

&lt;div&gt;
\[
x_i = x_{p(i)} + d(v_i, v_{p(i)})  \text{  (1)}
\]
&lt;/div&gt;


&lt;p&gt;where \(d : V^2 \rightarrow \mathbb{R}^{|G^c| + 2\,|G^p|}\) is the concatenation of displacement functions, i.e. \(d(v,v&#39;) = \left( d^c_1(v,v&#39;), d^c_2(v,v&#39;), \dots, d^p_1(v,v&#39;), d^p_2(v,v&#39;), \dots \right) \).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Articulated Gaussian Processes</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/20/reference"/>
   <updated>2015-01-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/20/reference</id>
   <content type="html">&lt;p&gt;Consider an arbitrary undirected graph embedded in \(\mathbb{R}^2\), for example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-20-articulated_graph_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another example is the skeletonization of a binary image, in which each skeleton pixel is a vertex, and adjacent skeleton pixels have an edge between them.  Below is an example of a skeletonized neuron image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-20-neuron_skeleton.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We want to generalize the Branching gaussian process to handle graphs with loops.  The basic idea is to model linear chains using the traditional 1D covariance based on curve-distance, and model loops using a 2D covariance based on euclidean position (e.g. 2D squared exponential covariance).  The result is an articulated set of plate-like subgraphs connected by  chain-like subgraphs.  We could call this an &quot;articulated Gaussian process&quot; as a generalization to &quot;branching Gaussian processes.&quot;&lt;/p&gt;

&lt;p&gt;First, we&#39;ll show how to separate the graph into chain-like and plate-like regions.  Then we&#39;ll show how to embed the graph in a high-dimensional Euclidean space such that traditional covariance functions over this space have nice properties, like conditional independence and piecewise smooth regions.&lt;/p&gt;

&lt;h2&gt;Partitioning into &quot;Plates&quot; and &quot;Chains&quot;&lt;/h2&gt;

&lt;p&gt;We first partition the graph into subgraphs we call &quot;chains&quot; and &quot;plates.&quot;  First, find &lt;a href=&quot;http://en.wikipedia.org/wiki/Biconnected_component&quot;&gt;biconnected components&lt;/a&gt; in the graph using Tarjan&#39;s algorithm.  Biconnected components of size greater than 2 become &quot;plates.&quot;  Biconnected components of size two are chain-links; maximal subgraphs of connected chain links are &quot;chains.&quot;  Any vertex shared by two subgraphs is an &quot;articulation point.&quot;  Let \(G^c = \{G^c_i\}\) be the set of chain subgraphs, \(G^p = \{G^p_j\}\) be the set of plate subgraphs.&lt;/p&gt;

&lt;p&gt;Below is such a partition, with two trivial chains and two plates identified:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-20-biconnected_components_reprise.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Constructing a Gaussian process&lt;/h2&gt;

&lt;p&gt;Let \(Z = \{z_i\}\) be the 2D position of vertices \(V = \{v_i\}\) embedded in the Euclidean plane. We seek a Gaussian process over \(Z\) that satisfies three properties:  (a) the covariance between points on a chain must be a function of their geodesic position (i.e. distance along the chain), (b) the covariance between points on a plate must be a function of their Euclidean positions, and (c) points in different subgraphs must be independent conditioned on any articulation point on the path connecting them.  Naturally, we require that the covariance function be positive definite.&lt;/p&gt;

&lt;p&gt;To guarantee positive definiteness, we will embed the graph in a high-dimensional Hilbert space and then use a standard covariance function on this space.  This allows our model to be agnostic to choice of covariance function.  In what follows, we descibe how to construct such a space, which we call the graph&#39;s &lt;em&gt;index space&lt;/em&gt;.  Briefly, we satisfy constraints (a) and (b) by embedding vertices such that relative positions within subgraphs are preserved.  To satisfy (c), subgraphs will be embedded in mutually orthogonal hyperplanes, connected only at articulation points.&lt;/p&gt;

&lt;h1&gt;Constructing the index-space&lt;/h1&gt;

&lt;p&gt;For each plate-type subgraph \(G^p_i \in G^p \), we define a local displacement function \(d^p_i : V^2 \rightarrow \mathbb{R}^2 \):&lt;/p&gt;

&lt;div&gt;\[
d^p_i(v,v&#39;) = \begin{cases}
    z - z&#39; &amp; \text{ if } v,v&#39; \in G^p_i \\
      0 &amp; \text{ otherwise,}
      \end{cases}
\]
&lt;/div&gt;


&lt;p&gt;where \((z,z&#39;)\) are the 2D Euclidean position of vertices \((v,v&#39;)\). Similarly, for each chain-type subgraph \(G^c_i \in G^c\), we define a local displacement \(d^c_i : V^2 \rightarrow \mathbb{R} \).  For any two vertices \(v,v&#39;\) connected by path \(\mathcal{P}\), we define the local displacement by the geodesic distance between the points:&lt;/p&gt;

&lt;div&gt;\[
  d^c_i(v,v&#39;) = \begin{cases}
    \sum_{(j,k) \in \mathcal{P}} ||z_j - z_k|| &amp; \text{ if } v,v&#39; \in G^c_i \\
      0 &amp; \text{ otherwise}
      \end{cases}
  \]
  &lt;/div&gt;


&lt;p&gt;Note that for chains, exactly one such path exists, making the above expression well-defined.&lt;/p&gt;

&lt;p&gt;We define the full displacement \(d : V^2 \rightarrow \mathbb{R}^{|G^c| + 2|G^p|}\) as the concatenated outputs of all local displacements, i.e. \(d(v,v&#39;) = (d^c_1(v,v&#39;), d^c_2(v,v&#39;), \dots, d^p_1(v,v&#39;), d^p_2(v,v&#39;), \dots)\).  The full displacement will be central to defining the index-space.&lt;/p&gt;

&lt;p&gt;We arbitrarilly pick a vertex \(v_0\) to be the graph&#39;s root and use depth-first search to impose a tree topology over the other vertices.  Because the subgraphs are biconnected, this also defines a tree-topology over subgraphs.  We define the &quot;local origin&quot; of a subgraph as the vertex first encountered in a depth-first search.    For all non-root vertices \(v_i\), we introduce the concept of a &quot;predecessor&quot; vertex \(v_{\pi(i)}\).  The predecessor of a local origin is the the parent subgraph&#39;s local origin; the predecessor of all other vertices is the local origin of the subgraph that contains it.&lt;/p&gt;

&lt;p&gt;For each vertex \(v_i \in V\), we assign an index \(x_i \in \mathbb{R}^{|G^c| + 2\,|G^p|} \).  Let the root vertex \(v_0\) have index \(x_0 = \mathbf{0}\).  For all other vertices, we define the index recursively:&lt;/p&gt;

&lt;div&gt;
\[
  x_i = x_{\pi(i)} + d(v_i, v_{\pi(i)})
\]
&lt;/div&gt;


&lt;p&gt;Because vertices within a subgraph differ only by their local displacement, each subgraph lies on an axis-aligned hyperplane (2D for plates, 1D for chains).  All such hyperplanes are mutually orthogonal and touch only at articulation points.  Also, within hyperplanes corresponding to plates (resp. chains), relative Euclidean (geodesic) position is preserved from the original 2D embedding.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - fitting progress</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/18/work-log"/>
   <updated>2015-01-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/18/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Implemented a image-pyramid-based likelihood and results are much better!  Also relaxed the prior, which appears to be overfit.&lt;/p&gt;

&lt;h2&gt;Run 1:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;parameterization&lt;/em&gt;: eigenspace, scaled by sqrt of eigenvalues&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Eigenvectors:&lt;/em&gt; 30&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Prior scaling:&lt;/em&gt; 36&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Algorithm&lt;/em&gt;: pattern-search in two passes
&lt;strong&gt; &lt;em&gt;termination criteria&lt;/em&gt;: mesh size &amp;lt; 0.5 (roughly 1/2 pixel change)
&lt;/strong&gt; &lt;em&gt;First pass&lt;/em&gt;: Ran until convergence at pyramid level 3 (i.e. 1/4th original size)
&lt;em&gt;&lt;em&gt; &lt;/em&gt;Second pass&lt;/em&gt;: Ran until convergence at pyramid level 1&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;After first pass, the global maximum seemed to have been found.  The second pass quickly refined it.&lt;/p&gt;

&lt;p&gt;This wasn&#39;t happening with the prior I trained, which suggests overfitting.&lt;/p&gt;

&lt;h2&gt;Possible changes to consider&lt;/h2&gt;

&lt;p&gt;Try using fewer eigenvectors.&lt;/p&gt;

&lt;p&gt;Use likelihood mixture rather than product.  Should be smoother and allow more non-optimal results.&lt;/p&gt;

&lt;p&gt;Consider scaling by inverse of eigenvector dynamic range, so moving by 1 guarantees at least one pixel changing.&lt;/p&gt;

&lt;p&gt;Consider using hyperpriors during training.&lt;/p&gt;

&lt;p&gt;Train cubic spline covariance model.&lt;/p&gt;

&lt;h2&gt;Next steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Extend the track
&lt;strong&gt; Pass 1: reproject to initialize sampler, run
&lt;/strong&gt; Pass 2: Use reprojected model as prior&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Get full dataset results
** run all pairwise on all datasets&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run on detected data&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Prior, visualized</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/14/reference"/>
   <updated>2015-01-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/14/reference</id>
   <content type="html">&lt;p&gt;Input tree.  (Mean of the local prior)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-input_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training tree.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-training_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sample from the locality prior only:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-local_prior_sample.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-local_sample_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-smooth_sampled_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Samples from full prior (including epipolar constraints)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-12-full_prior_samples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-full_prior_samples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how all corresponding points lie on the same epipolar line.&lt;/p&gt;

&lt;p&gt;The plot of eigenvalues of the prior covariance matrix suggest a very low-dimensional embedding:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-full_prior_eigs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Plotting log-sqrt eigenvalues compresses the dynamic range, making it easier to read:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-log_sqrt_prior_eigs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/13/work-log"/>
   <updated>2015-01-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/13/work-log</id>
   <content type="html">&lt;p&gt;Spent morning considering covariance over loopy graphs.  Implemented and example of &quot;shortest geodesic distance&quot;, modified to be minimum sum of squared distances between junctions over all paths.  This is a generalization of our metric over tree-structured graphs.  As anticipated, with loopy graphs, this results in non-positive-definite covariance matrices.&lt;/p&gt;

&lt;p&gt;Consider this example, with two paths between nodes (1) and (4), one twice as long as the other.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-13-exmple_graph.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The shortest squared-distance matrix is&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 0     1     4     9     4    13
 1     0     1     4     5     8
 4     1     0     1     8     5
 9     4     1     0    13     4
 4     5     8    13     0     4
13     8     5     4     4     0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code for this is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d1 = 0:3; % route 1 distances
d2 = 0:2:6; % route 2 distances
D1 = pdist2(d1&#39;, d1&#39;, &#39;euclidean&#39;).^2;
D2 = pdist2(d2&#39;, d2&#39;, &#39;euclidean&#39;).^2;
G  = sparse(size(D));
% top-left block is simply a straight path
G(1:length(d1),1:length(d1)) = D1;
% for bottom-left, compute shortest path between nodes through
% junctions
for i = 1:(length(d2)-2)
  i_ = length(d1)+i;
  for j = 1:length(D)
    j_ = j;
    if i_ == j_, G(i_,j_) = 0; G(j_,i_) = 0; continue, end
    G2 = sparse(4,4);
    G2(1,2) = D(i_, 1);
    G2(1,3) = D(i_, length(d1));
    G2(2,3) = D(1,length(d1));
    G2(1,4) = D(i_, j_);
    G2(2,4) = D(1,j_);
    G2(3,4) = D(length(d1),j_);
    d = graphshortestpath(G2&#39;, 1,4, &#39;directed&#39;, false);
    G(i_, j_) = d;
    G(j_, i_) = d;
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At large enough scales, the squared-exponential covariance matrix has negative eigenvalues:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scale = 10;
min(eig(exp(-full(G)/(scale^2*2))))

    ans =

       -0.0134
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, this covariance function is not positive definite.&lt;/p&gt;

&lt;h2&gt;A loopy graph strategy&lt;/h2&gt;

&lt;p&gt;We need to be able to embed the graph into a Euclidean space that preserves most of the interesting properties.&lt;/p&gt;

&lt;p&gt;Here&#39;s a simple approach: construct a graph with junctions as vertices and geodesic distance as edge weight.  Find the graph&#39;s minimum spanning tree, and transform the tree into curve-tree coordinates.  For unclaimed curve segments, linearly interpolate between the coordinates of the endpoints.&lt;/p&gt;

&lt;p&gt;This is nice, but if an unclaimed curve segment&#39;s length is perticularly long, it should act differently than a short (straight) line between the points.  To handle this, we introduce a new dimension for each unclaimed curve segment, and allow the transformed segment to have longer length by dipping into this dimension to some extent (with a limit of none for straight lines).&lt;/p&gt;

&lt;p&gt;Ideally, all pairs of points on the transformed segment will have the same distance as their geodesic distance in untransformed space.  However, this is not possible, because it would contradict the fact that the endpoint distance is defined by the graph distance in the minimum spanning tree.  At least we can try to preserve local distances, and a parabolic arc is a reasonable choice for this.   How to derive a parabola equation of an appropriate length?&lt;/p&gt;

&lt;p&gt;Given the distance between the end-points and the length of the desired curve, we seek a symmetric parabola with an appropriate arc length.   Given arc length and end-points, we can compute the focus the parabola (f), and from that we can compute the parabolic equation y = x&lt;sup&gt;2&lt;/sup&gt;/(4f).&lt;/p&gt;

&lt;h2&gt;Training foreground likelihood&lt;/h2&gt;

&lt;p&gt;First, reproject curve into second image.&lt;/p&gt;

&lt;p&gt;Draw as medial axis w/ width.&lt;/p&gt;

&lt;p&gt;Compute inverse medial axis.&lt;/p&gt;

&lt;p&gt;Grab all foreground/background pixels in probability map.&lt;/p&gt;

&lt;p&gt;create histogram.  Try smoothing and/or annealing.&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;p&gt;Foreground model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-lik_fg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Background model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-14-lik_bg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/12/work-log"/>
   <updated>2015-01-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/12/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Training epipolar prior.&lt;/p&gt;

&lt;p&gt;Fixing local prior using parameters learned yesterday.  The computing the epipolar and full prior is more computationally intensive, since it involves twice as many dimensions.  To mitigate, I eliminated every other point from the data.&lt;/p&gt;

&lt;p&gt;Without the epipolar prior, the data likelihood is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood (baseline) =  -2471.12 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fixing euclidean scale and variance to zero, I trained the epipolar variance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood =  -2395.05
training_result = 

    epipolar_variance: 4.39
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By allowing some euclidean-correlated variance, the iid variance should be able to drop a bit.  Fixing euclidean scale to 1/100&lt;sup&gt;2&lt;/sup&gt;, I trained epipolar_variance and euclidean_variance jointly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood = -2394.88
training_result =

    epipolar_variance: 4.3214
      euclidean_scale: 1.0000e-04
   euclidean_variance: 0.0335
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now training all three:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood = -2394.57
training_result = 

     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now training all 10 parameters together:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood = -1294.59
training_result = 

     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
        noise_variance: 0.0821
     geodesic_variance: 193.9141
        geodesic_scale: 0.0031
branch_linear_variance: 0.0800
 branch_const_variance: 6.1725
       linear_variance: 0.1561
        const_variance: 685.7670
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;...but we actually want noise_variance to be fixed to 1.  Repeating previous run with noise_variance=1 results in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Log-likelihood = -2381.29
training_result = 

     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
        noise_variance: 1
     geodesic_variance: 33.6972
        geodesic_scale: 0.0011
branch_linear_variance: 0.1028
 branch_const_variance: 6.9943
       linear_variance: 0.0737
        const_variance: 1.8641e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;running for 30 more iterations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ll = 2376.76
     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
        noise_variance: 1
     geodesic_variance: 160.7604
        geodesic_scale: 5.3712e-04
branch_linear_variance: 0.1028
 branch_const_variance: 6.9943
       linear_variance: 0.0835
        const_variance: 2.1123e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;The fullly trained model is then:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     epipolar_variance: 4.2877
    euclidean_variance: 0.0458
       euclidean_scale: 2.1581e-06
        noise_variance: 1
     geodesic_variance: 15.9174
        geodesic_scale: 0.0019
branch_linear_variance: 0.0485
 branch_const_variance: 3.7438
       linear_variance: 0.0348
        const_variance: 685.7670
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The samples from this prior look very nice:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-12-full_prior_samples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Motion is restricted to be near the epipolar lines, and the resulting curves remain smooth.&lt;/p&gt;

&lt;p&gt;Matlab says only 314 out of 3766 are nonnegligible.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/11/work-log"/>
   <updated>2015-01-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/11/work-log</id>
   <content type="html">&lt;p&gt;Finished training.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;training_result = 

         final_const_variance: 1.1669e-04
  final_branch_const_variance: 1.7636e+03
        final_linear_variance: 1.4425e+03
      final_geodesic_variance: 195.4912
         final_geodesic_scale: 0.0065
                  final_noise: 0.0776

training_result.initial
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  ans =&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;         const_variance: 831.7886
  branch_const_variance: 0.8552
        linear_variance: 0.1183
     euclidean_variance: 1.7367
        euclidean_scale: 1.0000
      geodesic_variance: 225.8247
         geodesic_scale: 1.0000
      epipolar_variance: 1.7367
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These just don&#39;t make sense.  It allows for almost zero total translation, but lots of pert-curve translation (const_variance vs. branch_const_variance).  Linear variance allows for way too much scaling.  Geodesic variance seems sensible, but the (inverse-squared) geodesic scale is way too low... Basically this becomes a stand-in for const_variance.  Perhaps this is just a local minimum, we could try fixing const_variance to some large number and forcing geodesic variance to serve its intended purpose.&lt;/p&gt;

&lt;p&gt;But the strangest thing is that&lt;/p&gt;

&lt;p&gt;I sampled some curves from this prior, and they are about what I&#39;d expect: rotated and scaled with very little nonrigid deformation.   Below is a sample, along with the mean tree barely visible in the center:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-tree_sample.png&quot; alt=&quot;tree sample&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Branches remain connected to tree, despite large per-curve offset variance.&lt;/li&gt;
&lt;li&gt;covariance matrix is singular&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Ruled-out causes of error:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;badly implemented logmvnpdf.  Tested against reference implementation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Training observations&lt;/h2&gt;

&lt;p&gt;The simulated annealing algorithm is very sensitive to our local minima.  It can sometimes find a good result in less than 500 iterations, but other times it gets stuck after less than 100 and never improves.  I can probably tweak the annealing schedule to increase temperature earlier(is &quot;reannealing&quot; the right value to tweak?&quot;).&lt;/p&gt;

&lt;p&gt;The patternsearch algorithm beats simulated annealing after only 16 iterations (vs. 5k or more).   Annealing really struggles to get out of that minimum.&lt;/p&gt;

&lt;p&gt;Surprisingly, after adding a seventh parameter (for branch-specific affine deformation), patternsearch totally fails to find a good minimum starting from default parameters.  Initializing it with the result of the six-parameter model gives much better results.  This suggests a coarse-to-fine strategy for training might work well in general.  However, this is a very specific case:  the six-parameter model was flexible enough to explain the data quite well, but was overly permissive -- it allowed too many nonsensical solutions.  Adding the seventh parameter introduced an alternative explanation of the data, one requiring less nonrigid deformation.&lt;/p&gt;

&lt;p&gt;To try: genetic algorithm;  particle swarm; simplex method; start pattern search from SA result; globalsearch; grid search with multistart&lt;/p&gt;

&lt;p&gt;Globalsearch &amp;amp; multistart w/
  simplex
  fminunc&lt;/p&gt;

&lt;h2&gt;Misc results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-tree_sample2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likleihood = -3.1787e+03
training_result = 

         final_const_variance: 6.0197e+06
  final_branch_const_variance: 1.5774e-09
        final_linear_variance: 1.6250
      final_geodesic_variance: 238.6891
         final_geodesic_scale: 0.0039
         final_noise_variance: 0.1487
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not bad.  Lots of geodesic variance to account for distortion.  Why is const variance so high?  Linear variance seems a bit high, but reasonable.&lt;/p&gt;

&lt;p&gt;It might be a good idea to allow each branch to have a linear distortion relative to its initial point.&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.5318e+03
training_result = 

       final_const_variance: 2.3156e+09
final_branch_const_variance: 2.4988
      final_linear_variance: 33.8221
    final_geodesic_variance: 1.7165e+03
       final_geodesic_scale: 0.0029
       final_noise_variance: 0.0880
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much smaller noise variance, much higher for all other variances.  const variance is astronomical... is the optimizer exploiting numerical error arising from a poorly-conditioned matrix?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-tree_sample6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.4848e+03
training_result =

       final_const_variance: 0.1383
final_branch_const_variance: 1.7466
      final_linear_variance: 5.4218e-04
    final_geodesic_variance: 378.6715
       final_geodesic_scale: 0.0048
       final_noise_variance: 0.0832
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is close to what I would hope for -- small constant and linear variance, most variance coming from deformation.  Still, the scale is fairly short (~14 pixels), allowing for quite a bit of deformation.&lt;/p&gt;

&lt;p&gt;Ran for several thousand more iterations.  Seem to be converging.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.4799e+03
training_result =

       const_variance: 0.0717
branch_const_variance: 3.2826
      linear_variance: 0.0028
    geodesic_variance: 401.4291
       geodesic_scale: 0.0047
       noise_variance: 0.0831
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Running from scratch with the &quot;pattern_search&quot; algorithm, I got excellent results very quickly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.4256e+03
training_result =

         const_variance: 261.7339
branch_const_variance: 3.0796
      linear_variance: 0.0035
    geodesic_variance: 165.2169
       geodesic_scale: 0.0063
       noise_variance: 0.0821
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are the most sensible results yet!  Moderately large constant variance , small but nonnegligible branch constant variance.  A touch of linear variance to rotations.  Smaller deformation variance in response to larger const variance.  Yes!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Adding a new parameter assigning independent affine deformation variance to each branch.  This should allow less variance to be allotted to geodesic_variance, which should improve marginal-likelihood.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-sampled_tree9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Initial training pass failed to find the old local minimum.  Restarting it using the previous optimum resulted in very nice parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.3107e+03
training_result = 

        const_variance: 291.9853
 branch_const_variance: 2.0196
       linear_variance: 0.0366
branch_linear_variance: 0.0516
     geodesic_variance: 12.7402
        geodesic_scale: 0.0136
        noise_variance: 0.0796
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the dramatic drop in geodesic variance and increase in marginal likelihood.  On the other hand, the (corrected) geodesic scale dropped from(~12 to ~8), meaning slightly higher effective dimension.  The resulting tree looks much closer to the original shape than previous models.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Previous model had a problem: allowing each curve to affine-transform independently results in parents drifting away from children, which in reality isn&#39;t possible.  We need each child to inherit the covariance of the parent, which we accomplish by simply taking the dot-product of the &quot;geodesic&quot; index.&lt;/p&gt;

&lt;p&gt;The pattern_search algorithm is having a hard time finding a better set of&lt;/p&gt;

&lt;p&gt;Starting from the same position as previous starting point, results in a worse log-likelihood:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.3131e+03
training_result = 

        const_variance: 3.1017e-25
 branch_const_variance: 2.9157
       linear_variance: 0.0326
branch_linear_variance: 0.0575
     geodesic_variance: 12.5426
        geodesic_scale: 0.0137
        noise_variance: 0.0796
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Zero const variance is troubling.  Other parameters are mostly unchanged, except slightly higher branch_const_variance.  It seems the translation component was mostly used to align curves affected by parallax.&lt;/p&gt;

&lt;p&gt;Actually, zero const variance isn&#39;t so surprising, because our now model alls branches to pivot around their starting point, and the root of the tree hardly moves between views.  What movement there is could be solved by pivoting the whole tree around it&#39;s center, using the global affine transformation.&lt;/p&gt;

&lt;p&gt;It seems we have a redundant representation here.  The branch affine transformations subsume the global affine transformation, unless there&#39;s evidence that a multi-level type of model is reasonable (which it might be).&lt;/p&gt;

&lt;p&gt;Starting from the result of the previous run gives slightly better results, but still not better than the &quot;bad&quot; model used previously.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2.3122e+03
training_result = 

        const_variance: 498.3842
 branch_const_variance: 2.9623
       linear_variance: 0.0400
branch_linear_variance: 0.0523
     geodesic_variance: 12.0562
        geodesic_scale: 0.0138
        noise_variance: 0.0799
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Now optimization approach, adding one dimension at a time and reoptimizing. Results are almost identical to second-to-last run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood:  -2.3131e+03
training_result = 

        noise_variance: 0.0800
     geodesic_variance: 12.3965
        geodesic_scale: 0.0136
branch_linear_variance: 0.0576
 branch_const_variance: 2.9157
       linear_variance: 0.0327
        const_variance: 8.3205e-09
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, low const_variance, but we manually can do 1D optimization to improve it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-ml_vs_const_variance.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;training_result = 

        noise_variance: 0.0800
     geodesic_variance: 12.3965
        geodesic_scale: 0.0136
branch_linear_variance: 0.0576
 branch_const_variance: 2.9157
       linear_variance: 0.0327
        const_variance: 435.8995
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Some plots showed that at low const_variance, the terrain become rocky, making search difficult.&lt;/h2&gt;

&lt;p&gt;Forcing noise variance to be 1.0 results in much smoother priors:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2015-01-11-smooth_sampled_tree.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;training_result = 

        noise_variance: 1
     geodesic_variance: 15.9174
        geodesic_scale: 0.0019
branch_linear_variance: 0.0485
 branch_const_variance: 3.7438
       linear_variance: 0.0348
        const_variance: 685.7670
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This kind of makes sense, since our point-correspondence algorithm has mean error of 0.5, even if correspondences are perfect.  In many cases, correspondences aren&#39;t perfect, because the triangulation metric used to find correspondences isn&#39;t sufficiently informative to infer the true correspondence.  So forcing i.i.d. noise variance to some minimum value prevents the prior from taking up that noise.&lt;/p&gt;

&lt;h2&gt;misc notes&lt;/h2&gt;

&lt;p&gt;Try different deformation model - cubic spline?&lt;/p&gt;

&lt;p&gt;euclidean affine transform&lt;/p&gt;

&lt;p&gt;next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;train epipolar likelihood&lt;/li&gt;
&lt;li&gt;MCMC

&lt;ul&gt;
&lt;li&gt;from sample to raster&lt;/li&gt;
&lt;li&gt;train bernoulli likelihood&lt;/li&gt;
&lt;li&gt;adaptive MH (or test with just simulated annealing)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Dangling tasks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;train on multiple different view-pairs&lt;/li&gt;
&lt;li&gt;ground-truth and train on neighboring views (rather than four-separated)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Trained prior parameters</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/11/reference"/>
   <updated>2015-01-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/11/reference</id>
   <content type="html">&lt;p&gt;Training only local prior w/ noise_variance fixed at 1.0:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        noise_variance: 1
     geodesic_variance: 15.9174
        geodesic_scale: 0.0019
branch_linear_variance: 0.0485
 branch_const_variance: 3.7438
       linear_variance: 0.0348
        const_variance: 685.7670
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Training full model (local prior and epipolar prior) w/ noise variance fixed at 1.0&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log-likelihood = -2368.62

     epipolar_variance: 4.5465
    euclidean_variance: 0.0422
       euclidean_scale: 4.2001e-07
        noise_variance: 1
     geodesic_variance: 233.9050
        geodesic_scale: 4.3327e-04
branch_linear_variance: 0.1079
 branch_const_variance: 7.0217
       linear_variance: 0.1016
        const_variance: 2.4123e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  Notice significantly larger geodesic variance, and much longer scale length (48 vs 22 pixels).  All other variances increased too.  This is probably because we&#39;ve treated the two priors as independent, but they aren&#39;t, so multiplying them results in too little overall variance.&lt;/p&gt;

&lt;p&gt;By comparison, below is a nearby local minimum, obtained by training all 9 parameters from scratch.&lt;/p&gt;

&lt;p&gt;  log-likelihood:  -2370.05&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       epipolar_variance: 4.6435
      euclidean_variance: 3.1113e-35
         euclidean_scale: 7.4415e+152
          noise_variance: 1
       geodesic_variance: 283.5387
          geodesic_scale: 3.9692e-04
  branch_linear_variance: 0.1236
   branch_const_variance: 6.5436
         linear_variance: 0.0746
          const_variance: 5.4990e-109
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This model moves all epipolar prior variance into the iid epipolar_variance variables. Also the local prior&#39;s offset variance has been moved into the deformation variance, geodesic_variance, while shortening the scale.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Projection of point onto line using distance from two reference points</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/09/reference"/>
   <updated>2015-01-09T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/09/reference</id>
   <content type="html">&lt;p&gt;Consider two known points A, and B, an unknown point C. If we know the distances between C and the other two points, we can recover \(\pi_{AB}(C)\), the projection of C onto the line AB.&lt;/p&gt;

&lt;p&gt;The distance between \(\pi_{AB}(C)\) and A is&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
d(A, \pi_{AB}(C)) = d(A,B) (1+g) / 2 \text{, where} \\
g &amp;= \left ( d(A,C)^2 - d(B,C)^2 \right ) / d(A,B)^2
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;This can be derived using the pythagorean theorem and fact triangles AC\pi(C) and BC\pi(C) share a side.&lt;/p&gt;

&lt;p&gt;This can be used to derive an expression for \(\pi_{AB}(C)\) using a weighted sum of A and B:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\pi_{AB}(C) &amp;= (A(1-g) + B(1+g) ) / 2
\end{align}
\]
&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>Geodesic distance kernel and BGP kernel -- simplified representation</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/08/reference"/>
   <updated>2015-01-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/08/reference</id>
   <content type="html">&lt;p&gt;With a nod to Mercer&#39;s theorem, we can represent gaussian processes over graph-structured points in a very simple way.  We may transform our input space over graph nodes into a new eucliean space.&lt;/p&gt;

&lt;p&gt;For trees with N branches, we denote a point a distance x along the i-th branch by the ordered pair \(i, x\).  For each branch i, its parent is denoted by p(i), and its branch point is denoted by b(i).  If branch i has not parent, we define p(i) = b(i) = 0.  We can define a transformation \(\phi : \mathbb{N}\times\mathbb{R} \rightarrow \mathbb{R}^N\), whose k-th dimension is:&lt;/p&gt;

&lt;p&gt;  &lt;div&gt;
  [
    \phi_k(i,x) = \begin{cases}
        x &amp;amp; \text{ if } i == k \
        0 &amp;amp; \text{ if } i == 0 \
        \phi_k(p(i), b(i)) &amp;amp; \text{ otherwise.}
        \end{cases}
  ]
  &lt;/div&gt;.&lt;/p&gt;

&lt;p&gt;If branch k is the root of a subtree containing (i,x), \(\phi_k\) is the branch position of sub-subtree containing (i,x).&lt;/p&gt;

&lt;p&gt;This formulation makes it easier to define covariance functions over tree structures.  O&lt;/p&gt;

&lt;p&gt;It is often useful to model each tree branch as a squared-exponential curve.  This is sometimes falsely implemented as squared exponential covariance function over geodesic distance instead of Euclidean distance.  However, this covariance function isn&#39;t positive definite (need illustration).  Instead, if we define a distance function \(d(i,x,i&#39;,x&#39;)\) as the sum of squared geodesic distances between adjacent junctions along the path between (i,x) and (i&#39;,x&#39;).  Using this distance metric with a squared-exponential covariance function results in exactly the model we seek.&lt;/p&gt;

&lt;div&gt;
  k(i,x,i&#39;,x&#39;) = \exp\{-d(i,x,i&#39;,x&#39;)\}
&lt;/div&gt;


&lt;p&gt;We can use our transformation above to represent this more succinctly:&lt;/p&gt;

&lt;div&gt;
  k(\phi, \phi&#39;) = \exp\{- \| \phi - \phi&#39; \|^2\}
&lt;/div&gt;


&lt;p&gt;Since each dimension in phi corresponds to exactly one branch, \( \phi - \phi \) is a vector of distances between branch points on the each curve corresponding to (i,x) and (i&#39;,x&#39;).  The squared l2 norm of this expression is equivalent to d(i,x,i&#39;,x&#39;).  Note that the fact that we can represent this covariance function using a standard covariance function with transformed inputs shows that it is positive definite.&lt;/p&gt;

&lt;p&gt;A second useful tree model is the one I introduced in my dissertation proposal -- the branching cubic spline model.  The covariance function for this model involved a recursive function that was complicated and wasn&#39;t previously proven to be positive definite.  Using our input transformation, this simplifies to:&lt;/p&gt;

&lt;div&gt;
\[
  k(\phi, \phi) = \sum_i k_c(\phi_i, \phi&#39;_i)
  \]
&lt;/div&gt;


&lt;p&gt;where k_c() is the cubic spline covariance function.  The recursive nature of the definition of \(\phi\) ensures that each curve inherits the covariance of its parent curve.  And because \(k_c(0,x) == 0\), points on different subtrees only receive covariance from their shared ancestors.  Again, this formulation suffices to show that our original covariance function is positive definite.&lt;/p&gt;

&lt;h2&gt;Loopy graphs&lt;/h2&gt;

&lt;p&gt;Non-tree structured graphs are more difficult, but I have a few possible approaches.&lt;/p&gt;

&lt;p&gt;One way of interpreting a loop is as two separate branches (one at each junction) that gradually blend into one another.  Implementing a transformation \(\phi\) is easy under this interpretation -- just linearly interpolate between the two independent lines in \(\phi\).  Unfortunately, the resulting embedding doesn&#39;t preserve distances between nearby points. I tried a similar approach with cubic Hermite splines instead of linear interpolation, but again, distances aren&#39;t preserved.  This could be solved if we could (a) guarantee that the bridging curve has exactly the right length and (b) the mapping between graph points and the bridging curve in \(\phi\) preserves distance.  Both of these result in equations that can&#39;t be solved analytically, but numerical techniques could work if we cared enough.&lt;/p&gt;

&lt;p&gt;We could try something similar, but use a spherical arc to connect the two branch points in \(\phi\).  This would preserve total length and adjacent distances, but it violates the property that the curve&#39;s endpoint is orthogonal to its parent.&lt;/p&gt;

&lt;p&gt;Both of these bridging techniques also violate the elegant property of the tree-based covariance, namely that the L1 distance between points is equal to thier geodesic distance.&lt;/p&gt;

&lt;p&gt;A third possibility is to simply use the distance function \(d(i,x,i&#39;,x&#39;)\) we introduced before.  The down side of this is I con&#39;t think of a proof for its positive-definiteness.  But if it is PD, it should have the properties we want, and is relatively easy to implement.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/04/work-log"/>
   <updated>2015-01-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/04/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>Skeleton deformation fitting</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/02/work-log"/>
   <updated>2015-01-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/02/work-log</id>
   <content type="html">&lt;p&gt;Since the skeletons are very different between views, the old graph-matching approach probably won&#39;t work.  I believe the graph deformation idea is still a good one, but the likelihood involving correspondences won&#39;t work because it doesn&#39;t handle false positive or false negative parts of the graph.&lt;/p&gt;

&lt;p&gt;A better idea is to go back to our old approach: use a pixel-based likelihood to evaluate deformed graphs.  Use MCMC to explore the space and use the GP prior covariance for proposals or search directions.  After computing the cholesky decomposition, this should proceed relatively quickly, even for large point sets.&lt;/p&gt;

&lt;p&gt;The model is the medial axis transform of the original image, plus GP-distributed deformation, which includes an epipolar constraint.  To render into the second image, we perform an inverse medial axis transform.&lt;/p&gt;

&lt;p&gt;Because of the epipolar constraint, the number of effective dimensions is nearly halved.  The correlation between nearby points should reduce dimension by an even more significant factor.&lt;/p&gt;

&lt;h2&gt;Initial tasks&lt;/h2&gt;

&lt;p&gt;Train GP on ground truth data.
  (try adding runners between almost-bridged points)
Train Bernoulli distribution over foreground, background&lt;/p&gt;

&lt;p&gt;Algorithms
    binary image to graph
    All pairs shortest path.
    polyline w/ interpolated intensity
  x medial axis transform
  x Inverse medial axis transform
    specialized mcmc
        adaptive metropolis hastings
        hit and run sampling?&lt;/p&gt;

&lt;h2&gt;Possible extensions:&lt;/h2&gt;

&lt;p&gt;  use a weaker distance function for geodesic distance (huber function?)
  make geodesic distance greater at junctions&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2015/01/01/work-log"/>
   <updated>2015-01-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2015/01/01/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;  Some improvements to the approach outlined at the end of previous post...&lt;/p&gt;

&lt;p&gt;  Tweak epipolar likelihood to add a small affine and nonlinear distortion component
  (e.g. due to lens distortion).  this allows epipolar variance to be smaller;
  forces deviations to be spatially correlated.&lt;/p&gt;

&lt;p&gt;  Find some good correspondences; fix camera distortion
      approach 1: distort image using GP and known correspondences.
      approach 2: optimize camera distortion parameters under GP and known correspondences&lt;/p&gt;

&lt;p&gt;  Use unambiguous correspondences first.&lt;/p&gt;

&lt;p&gt;  I fear that graph topology may be too noisy and fluxuating to be a reliable source
  of information.  Is keypoint matching with epipolar constraint enough to solve this?
  Maybe with added spatial distortion?&lt;/p&gt;

&lt;p&gt;  How can adding correspondences reduce uncertainty in remaining points?  The approach
  above can reduce uncertainty to almost lying on the epipolar line.   If we can recover
  small subgraphs, those can help too.  But will false joints break things? (this would
  be a good experiment)  Maybe just linear sections with interior keypoint nodes.
  This should at least improve fine-scale matching later.&lt;/p&gt;

&lt;p&gt;  2D gives us lateral branch candidates, but are ambiguous.  Can resolve them in 3D?&lt;/p&gt;

&lt;p&gt;  Idea: branch points have higher geodesic distance?&lt;/p&gt;

&lt;p&gt;  Simpler problem: Camera repair for volumetric reconstruction.&lt;/p&gt;

&lt;p&gt;  Sift keypoint matching
    nonlinear distortion fix
    ransac matching keypoints + distortion correction&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/31/work-log"/>
   <updated>2014-12-31T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/31/work-log</id>
   <content type="html">&lt;p&gt;Working with KernelBoost code from &lt;a href=&quot;http://cvlab.epfl.ch/page-108936-en.html&quot;&gt;http://cvlab.epfl.ch/page-108936-en.html&lt;/a&gt;.  Running training example, should take about 9 hours.  Working on preparing our data to train with.&lt;/p&gt;

&lt;p&gt;Prepared files for Structure from Silhouette Probability Maps and sent to Dr. Tabb to try reconstruction.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Graph matching with epipolar constraints</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/29/reference"/>
   <updated>2014-12-29T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/29/reference</id>
   <content type="html">

&lt;div&gt;\[
  \def\CV#1{\boldsymbol{#1}}
  \def\cross{\times}
  \def\ocross{\otimes}
  \]&lt;/div&gt;


&lt;p&gt;What follows is a variation on the work of &lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CCUQFjAA&amp;amp;url=http%3A%2F%2Fcvlab.epfl.ch%2Ffiles%2Fcontent%2Fsites%2Fcvlab2%2Ffiles%2Fpublications%2Fpublications%2F2012%2FSerradellGKMF12.pdf&amp;amp;ei=YDWjVMXYCpWwogSmj4HYDQ&amp;amp;usg=AFQjCNFCQfO_wNWnnFZa_35HQtW9xA3ogA&amp;amp;bvm=bv.82001339,d.cGU&quot;&gt;Serradell et al. (CVPR 2012)&lt;/a&gt;, which we modify to introduce an epipolar constraint and use a covariance function based on geodesic distance.  The epipolar constraint requires significant rework of the original derivation, which assumes the x and y dimension are independent.  The use of geodesic distance is necessary to handle parallax motion not present in Serradell&#39;s data.&lt;/p&gt;

&lt;p&gt;See also &lt;a href=&quot;http://www.iaeng.org/publication/WCE2007/WCE2007_pp265-268.pdf&quot;&gt;(Yu, Tian, Liu, 2007)&lt;/a&gt; for an earlier application of GP for point matching, including expressions for marginal likelihood derivatives for training.&lt;/p&gt;

&lt;p&gt;Consider a 3D tree structure observed in two views as 2D trees.  We seek a correspondence between points in the 2D trees.  We treat this as a problem of matching graphs embedded in \(\mathbb{R}^2\), which we solve using nonlinear Gaussian process regression, using epipolar constraints to ensure the result is consistent with the known camera configuration.&lt;/p&gt;

&lt;p&gt;Consider a embedded graph in two views, A and B, with vertex positions \(X_A = \{x^A_1, \dots, x^A_{M_A}\}\) and \(X_B = \{x^B_1, \dots, x^B_{M_B}\}\).  We model the graph in B as arising from an affine transform of the graph in A plus a nonlinear deformation.  The goal is to find a correspondence between the graph vertices that minimizes the distortion between them.  Because the graph structures may be noisy, we seek an approach that is robust to graphs with minor differences in topology.&lt;/p&gt;

&lt;h2&gt;Epipolar constraints&lt;/h2&gt;

&lt;p&gt;We can use the known relationship between two cameras to constrain we expect points \(X_A\) to appear in view B.  The fundamental matrix \(F\) between view A and view B is a 3x3 matrix that constrains points in \(X_A\) to lie on their epipolar line in view B.  Because we expect both the points and the fundemental matrix to include some error, we relax this constraint and assume each point lies near its epipolar line with variance \(\sigma^2_e\).&lt;/p&gt;

&lt;p&gt;For any point \(\boldsymbol{x}\) in view A, the epipolar line in view B is given by \(\boldsymbol{l} = F^T [\boldsymbol{x}^\top 1]^\top \).  The normal vector perpendicular to the epipolar line is \(\hat {\boldsymbol{n}} = [l_1 l_2]^\top / \| [l_1 l_2] \|\).  The penalized epipolar constraint can be expressed as a Gaussian distribution \(\mathcal{N}(\boldsymbol{e}&#39;, \Sigma_e)\), where \(\boldsymbol{e&#39;}\) is the epipole in view B, and \(\Sigma_e^{-1} = (\hat{\boldsymbol{n}} \hat{\boldsymbol{n}}^\top) / \sigma_e^2 \).  Note that the precision matrix \(\Sigma_e^{-1}\) is rank-deficient, representing infinite variance along the epipolar line.&lt;/p&gt;

&lt;p&gt;The joint likelihood of all points \(X^A = \{\boldsymbol{x}^A_1, \dots, \boldsymbol{x}^A_M\}\) with epipolar normals \(\{\boldsymbol{\hat{n}}_1, \dots, \boldsymbol{\hat{n}}_M\}\) is \(\mathcal{N}(\mu_E, \Sigma_E)\), where&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\mu_E &amp;= [\boldsymbol{e}&#39;^\top \boldsymbol{e}&#39;^\top \dots]^\top \\
\Sigma^{-1}_E &amp;= S S^\top \\
S &amp;= (1/\sigma_e) \left ( \begin{array}{cccc} 
    \boldsymbol{\hat{n}}_{1} &amp; 0 &amp; \dots &amp; 0 \\
    0 &amp; \boldsymbol{\hat{n}}_{2} &amp; \dots &amp; 0 \\
    \vdots &amp; \vdots &amp; &amp; \vdots \\ 
    0 &amp; 0 &amp; \dots &amp; \boldsymbol{\hat{n}}_{M}
    \end{array}
    \right ).
\end{align}
\]
&lt;/div&gt;


&lt;h2&gt;Nonrigid deformation prior &lt;/h2&gt;

&lt;p&gt;We assume the motion of points from view A to view B arise as an affine transformation plus some nonrigid deformation.  The main cause of nonrigid deformation is parallax motion, but other less predictable factors like camera lens distortion of small scene movements are possible too.&lt;/p&gt;

&lt;p&gt;Assuming motion in the x and y direction are independent, single-dimensional prior covariance between the \(i\)th and \(j\)th points is&lt;/p&gt;

&lt;div&gt;
\[
  k(i,j) = \theta_0 + \theta_1 x_i^T x_j + \theta_2 \exp \left \{ -\frac{\theta_3}{2} \| x_i - x_j  \|^2 \right \} + \theta_4 \exp \left \{ -\frac{\theta_5}{2} d( v_i, v_j) \right \}.  \;\;(1)
\]
&lt;/div&gt;


&lt;p&gt;Here, \(d(v_i, v_j)\) is the geodesic graph distance, i.e. the closest path between vertices \(v_i\) and \(v_j\).  The first three terms are the same as Serradell et al. -- the first two terms define an affine transformation and the third is a nonlinear distortion term that penalizes local geometry in A being distorted in B.   However, we recognize that points with dissimilar 2D position may appear similar in 2D due to projection.  The significant parallax shift between such points violates the locality assumption of the third covariance term.  The fourth term in (1) introduces deformation based on geodesic distance.  We observe that relative geodesic distances are better preserved under projection than relative Euclidean distances.   This term provides a better explanation for parallax shift between points at similar Euclidean positions but dissimilar positions in the graph.&lt;/p&gt;

&lt;p&gt;Equation (1) applies to one-dimensional points; we need an expression relating two-dimensional points.  The x and y dimensions are independent under the prior, so the covariance matrix over 2D points is simply a block-diagonal matrix of two covariance matrices over 1D points.  We permute this matrix so dimensions are ordered first by increasing point index, then spatial dimension.  Formally, let \(X_I\) and \(X_J\) be the vertical concatenation of 2D points with indices \(I = \{i_1, \dots, i_m\}\) and \(J = \{j_1, \dots, j_n\}\).  Define \(k(I,J)\) as the gram matrix of covariances \(k(i,j)\) for each pair \((i,j) \in I \cross J\).  We define the prior covariance between point sets \(X_I\) and \(X_J\) as&lt;/p&gt;

&lt;div&gt;
\[
K(I,J) = k(I,J) \ocross I_2, 
\]
&lt;/div&gt;


&lt;p&gt;where \(\ocross\) denotes Kronecker product and \(I_2\) is a 2x2 identity matrix.  The result is a \(2 \vert I \vert \times 2 \vert J \vert\) covariance matrix over a sequence of stacked 2D variables with independent dimensions.&lt;/p&gt;

&lt;h2&gt;Correspondence matching&lt;/h2&gt;

&lt;p&gt;To estimate correspondences between \(X^A\) and \(X^B\), we follow the coarse-to-fine matching strategy in Serradell et al. (2012).  We first find a coarse set of correspondences between branch points and tips using the method below.  A fine-grained correspondence is then found between the densely spaced remaining points, conditioned on the coarse correspondences.&lt;/p&gt;

&lt;p&gt;Given a (possibly empty) set of \(N\) correspondences, we can derive a posterior distribution over possible locations for points \(X^A\) in view B.  For a given correspondence of size N, let \(X^B_N\) be the points in graph B corresponding to points in A with indices \(J_N = \{j_{1}, \dots, j_{N}\}\).  Let \(J = \{1, \dots, M\}\) be the set of all vertex indices in graph A.  The marginal posterior for point \(x^A_{i_*}\) is&lt;/p&gt;

&lt;div&gt;
\[
  \begin{align}
  \mu_N(i_*) &amp;=  K_*^T C^{-1}_N \left ( \begin{array}{c} X_N^B \\ \mu_E \end{array} \right ) \\
  \sigma^2_N(i_*) &amp;= K(i_*, i_*) + \sigma_n^2 I_2 - K_*^T C_N^{-1} K_*  \text{, where} \\
  C_N^{-1} &amp;= 
  \left (
  \begin{array}{cc} 
  K(J_N, J_N) + \sigma_n^2 I &amp; K(J, J_N) \\
  K(J_N, J) &amp; K(J,J) + \Sigma_E
  \end{array}
  \right )^{-1}
  \end{align} \\
  K_* = K(J_N, i_*)
\]
&lt;/div&gt;


&lt;p&gt;However, because \(\Sigma_E^{-1} \) is rank-deficient, we must rewrite \(C_N^{-1}\)  as&lt;/p&gt;

&lt;div&gt;
\[
  \begin{align}
  C_N^{-1} &amp;= 
  \left (
  \begin{array}{cc} 
      I &amp; 0 \\
      0 &amp; S
  \end{array}
  \right ) 
  
  \left (
  \begin{array}{cc} 
  K(J_N, J_N) + \sigma_n^2 I &amp; S K(J, J_N) \\
  K(J_N, J) S &amp; S^\top K(J,J) S + I
  \end{array}
  \right )^{-1}

  \left (
  \begin{array}{cc} 
      I &amp; 0 \\
      0 &amp; S^\top
  \end{array}
  \right ) 
  \end{align}
\]
&lt;/div&gt;


&lt;p&gt;Note that despite \(\Sigma_E^{-1}\) being rank-deficient, the expression above is non-singular.&lt;/p&gt;

&lt;p&gt;Using the expression above, we can compute the z-score of each point in B against the image of each point in A.  Starting with an empty correspondence set, we construct a set of correspondence candidates whose z-score is below a threshold.  We use z-score as weights to sample a correspondence from that set, and use that correspondence to update the z-scores of all remaining candidates by conditioning on the new correspondence set.  We repeat until no valid candidates exist.  Unclaimed vertices are treated as outliers.  We repeat this process several times and keep the best fit.  To avoid spending time on bad matches, we quit early if the relative geodesic distances between corresponding graphs differ too greatly.&lt;/p&gt;

&lt;p&gt;Fine-grained correspondences between remaining points can be found using the Hungarian algorithm using pairwise z-score as a cost metric.&lt;/p&gt;

&lt;h2&gt;3D reconstruction&lt;/h2&gt;

&lt;p&gt;The simplest approach to 3D reconstruction is to triangulate the corresponding points in two views.  Multiple views can be chained together from pairwise correspondence using transitivity.  A better approach would be to use my BGP model to find an optimal registration between sequences of recovered 3D trees.  I don&#39;t know if I could fit both the above and that into a single conference paper, because the temporal BGP model is a bit complicated.  I have a simpler model in mind that might work better, and might be easier to squeeze into the end of a paper.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;None yet, working on it.&lt;/p&gt;

&lt;p&gt;Still unclear how best to do evaluation.  The original paper evaluated against synthetic data generated by &lt;a href=&quot;http://vascusynth.cs.sfu.ca/Software.html&quot;&gt;VacuSynth&lt;/a&gt;, which we might be able to use too.  They used mean squared error between estimated and true position, which I&#39;m not thrilled with, because it doesn&#39;t explicitly capture whether the topology is matched correctly.&lt;/p&gt;

&lt;p&gt;If Dr. Tabb&#39;s software works reasonably well, we could measure reconstruction quality and compare our reconstruction against hers.  Ground truth would be the triangulated 2D ground truth, smoothed with hand-tuned BGP.&lt;/p&gt;

&lt;p&gt;We could develop&lt;/p&gt;

&lt;p&gt;Precision/recall against ground-truth curves.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>GP Graph-matching for 3d reconstruction</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/23/work-log"/>
   <updated>2014-12-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/23/work-log</id>
   <content type="html">&lt;p&gt;Based on &lt;a href=&quot;http://cvlab.epfl.ch/files/content/sites/cvlab2/files/publications/publications/2012/SerradellGKMF12.pdf&quot;&gt;this paper&lt;/a&gt; on graph matching for registration of biomed data.  We can adapt the approach to our multi-view data using knowledge of (a) the cameras and (b) small motion between views.  Our graphs will deform differently than theirs, because parallax doesn&#39;t maintain relative euclidean distances.  However, geodesic distances should be roughly maintained, so we can use geodesic distance in our nonlinear deformation GP.   The coarse-to-fine strategy should make inference more tractible than our previous appraoches; fitting branch points first, interior points later.  This approach will require better foreground-background segmentation than we&#39;ve used in the past, which we can achieve by training a pixel classifier with the Weka segmenter in Fiji.&lt;/p&gt;

&lt;p&gt;This appraoch answers the question of how to project an embedded graph from one view to another using a small number of known correspondences.   In the original paper, the GP evidence consisted of the graph vertex positions in the original view, which are allowed to deform in the second view using a squared-expoential covariance function.  In our approach, we will also use the original vertex positions in order to encourage minimal motion, under the assumption of minimal camera motion.  However, we will also introduce evidence from the epipolar lines, using known cameras.  To accomplish this, we will backproject the points to a default depth, and reproject them into the second image.  The likelihood covariance will have infinite variance in the direction of the epipolar line.  For any particular point, the product of (a) the &quot;zero motion&quot; likelihood function and (b) the &quot;epipolar constraint&quot; likelihood function should be their product -- an enlongated eliptical gaussian with mean lying on (or very near) the eipolar line nearest to the original point location.&lt;/p&gt;

&lt;p&gt;The prior covariance between graph points will be given by a covariance function similar to that in the original paper, but using geodesic distance between points instead of euclidean distance.  This should allow greater movement in the tips of plant.&lt;/p&gt;

&lt;p&gt;We can train this model on ground truth curve skeletons.&lt;/p&gt;

&lt;p&gt;The coarse graph matching proceeds by proposing correspondence between two pairs of points, using point covariance to choose good candidates.  We then update to covariance of remaining points and propose a new correspondence from the pair with fewest matches.  We resume until a threshold of matches is met, and we evaluate the result.  We then back up the decision tree and try the other correspondence candidate (i.e. depth first search over all candidates).  Note that at each level of the decision tree, the number of candidates decreases, due to less uncertainty in point positions, which mitigates the combinitoric explosion in correspondences.  We terminate early if the relative geodesic distance between pairs of corresponding nodes differs significantly.  We may also use triangulation error as a stoping criterion, but this is probably already handled by the epipolar constraint likelihood.&lt;/p&gt;

&lt;p&gt;In some cases, the graph changes topology between views, due to overlapping stems.  Our approach should be robust somewhat to different topologies, but it is unclear how much.  We may need to convert our graphs to trees somehow, maybe by sampling.  Other possible heuristics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;use X-junctions as candidates for overlapping edges&lt;/li&gt;
&lt;li&gt;assume bottom of image is graph root; prefer nodes closer to the root according to breadth-first-search depth when breaking loops.&lt;/li&gt;
&lt;li&gt;eliminate short segments, merging their nodes&lt;/li&gt;
&lt;li&gt;prune short edges, or edges with fast-varying width. (e.g. at flower tips.)&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Mean / Covariance&lt;/h2&gt;

&lt;div&gt;
\[
\begin{align}
  \mu_N(\mathbf{x}^A) &amp;= \mathbf(k)^T \mathbf{C}_N^{-1} \mathbf{Y}_N^B \\
  \sigma^2_N(\mathbf{x}^A) &amp;= k(\mathbf{x}^A, \mathbf{x}^A) + \beta^{-1} - \mathbf{k}^T \mathbf{C}_N^{-1}\mathbf{k}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
  \mathbf{C}_N = diag(K,3) + K_y
  K_y = (\sigma_o^{-1} I + \Sigma_e^{-1})^{-1}
  Y^B_N = K_y (\sigma_o^{-1} I X^B_N + \Sigma_e^{-1} e
  e is the epipole
  \pi^K(X) is the epipiolar projection of the points in X into view K (see explanation)
  \Sigma^A_e = diag([\Sigma^A_{e,1}, ..., \Sigma^A_{e,N}])
  \Sigma^{-A}_{e,i} = 1/\sigma_e^2 R(\Theta(x_i)) [1 0; 0 0] R^T(\Theta(x_i))
  \Theta(x_i) = atan2(-F_1 x_i, F_2 x_i)
  K_ij = k(x_i^A, k_j^A)
\end{align}
\]
&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/21/work-log"/>
   <updated>2014-12-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/21/work-log</id>
   <content type="html">&lt;p&gt;Generate synthetic vascular data with &quot;Vacusynth&quot;, like in &lt;a href=&quot;http://cvlab.epfl.ch/files/content/sites/cvlab2/files/publications/publications/2012/SerradellGKMF12.pdf&quot;&gt;this paper&lt;/a&gt; on graph matching.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>GP with two constraints</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/18/reference"/>
   <updated>2014-12-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/18/reference</id>
   <content type="html">&lt;p&gt;Consider a GP \(f(x)\) with covariance \k(x,y)\).  We can construct a new GP \f&#39;(x)\) by constraining two points in \(f(x)\), which we&#39;ll call &quot;endpoints&quot;.  Without loss of generality, assume the constraints occur at \(x = 0 \) and \(x = 1\).&lt;/p&gt;

&lt;p&gt;We can derive the expression for covariance of the doubly-constrained function, \(k&#39;&#39;(x,y)\).  Let \(k_{a,b})\) denote \(k(a,b)\).  Recall our &lt;a href=&quot;/ksimek/research/2014/12/07/reference/&quot;&gt;earlier result&lt;/a&gt; for the singly-constrained GP at \(x = 0\):&lt;/p&gt;

&lt;div&gt;
\[
  k&#39;(x,y) = k_{xy} - k_{x0} k_{y0} / k_{00}
\]
&lt;/div&gt;


&lt;p&gt;We can apply this tranformation twice to obtain \(k&#39;&#39;(x,y)\):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
  k&#39;&#39;(x,y) &amp;= k&#39;_{xy} - k&#39;_{x0} k&#39;_{y0} / k&#39;_{00} \\
           &amp;= 
           (k_{xy} - k_{x0} k_{y0} / k_{00}) - 
              (k_{x1} - k_{x0} k_{10} / k_{00}) 
              (k_{1y} - k_{10} k_{y0} / k_{00})
              (k_{11} - k_{10} k_{10} / k_{00})^{-1}
           &amp;= 
          \frac{1}{(k_{11} - k_{10} k_{10} / k_{00})^{-1}}
          \left [
            (k_{xy} - k_{x0} k_{y0} / k_{00})
            (k_{11} - k_{10} k_{10} / k_{00}) - 
            (k_{x1} - k_{x0} k_{10} / k_{00}) 
            (k_{1y} - k_{10} k_{y0} / k_{00})
          \right ] \\
          &amp;= 
          \frac{1}{(k_{11} - k_{10} k_{10} / k_{00})}
          \left [
            (k_{xy} - k_{x0} k_{y0} / k_{00})
            (k_{11} - k_{10} k_{10} / k_{00})  - 
            (k_{x1} - k_{x0} k_{10} / k_{00}) 
            (k_{1y} - k_{10} k_{y0} / k_{00})
          \right ] \\
          &amp;= 
          \frac{k_{00}}{k_{11}k_{00} - k_{10}^2}
          \left [
             k_{00} k_{11} k_{xy} 
            -k_{00} k_{11} k_{x0} k_{y0} /k_{00} 
            -k_{00} k_{xy} k_{10}^2/k_{00} 
            +k_{00} k_{x0} k_{y0} k_{10}^2/k_{00}^2
            -k_{00} k_{x1} k_{y1} 
            +k_{00} k_{x1} k_{y0} k_{10} /k_{00} 
            -k_{00} k_{x0} k_{y0} k_{10}^2/k_{00}^2
            +k_{00} k_{x0} k_{y1} k_{10} /k_{00} 
          \right ] \\
          &amp;= 
          \frac{1}{k_{11}k_{00} - k_{10}^2}
          \left [
            k_{00} k_{11} k_{xy} 
           -k_{xy} k_{10} ^2
           +k_{x0} k_{y1} k_{10} 
           +k_{x1} k_{y0} k_{10} 
           -k_{x0} k_{y0} k_{11} 
           -k_{x1} k_{y1} k_{00} 
          \right ]
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;It&#39;s important to note that this expression is symmetric w.r.t {0,1}. Also note that no pair of values appear more than once, making further simplification difficult.&lt;/p&gt;

&lt;p&gt;We&#39;ll denote the covariance for a constrained GP by k(x,y ; a, b), where a,b are the constraint  indices.&lt;/p&gt;

&lt;h2&gt;Curve trees with runners&lt;/h2&gt;

&lt;p&gt;We can modify the curve tree model to introduce &quot;runners&quot; -- special curves that are connected on both ends to curve tree curves.  The introduction of runner curves transforms a curve tree into a curve DAG.  In general, it isn&#39;t possible to unambiguously identify runners by inspecting a DAG, because there are always at least two ways to break a loop.  Thus, we need to augment our directed graph with labels to indicate runners explicitly.&lt;/p&gt;

&lt;p&gt;A runner differs from a normal curve in two ways.  First, that it always has two parents, which define its endpoints.  Second, its gaussian process differs from a standard curve, due to it being constrained on both ends.  The runner curve process is a linear process connecting its two endpoints, plus a smooth curve process that allows deviations from the linear curve.  The latter process must be constrained on both ends to pass through the endpoints, using the derivation above.&lt;/p&gt;

&lt;h3&gt;Deriving the runner covariance&lt;/h3&gt;

&lt;p&gt;Let \(A\) and \(B\) be random variables representing endpoints of a runner curve, lying somewhere on its parent curves.  A straight line between those curves is given by&lt;/p&gt;

&lt;div&gt;
\[
  f(t) = t A + (1-t) B
\]
&lt;/div&gt;


&lt;p&gt;Let us derive the covariance of two points \(X = f(s) \) and \(Y = f(t)\) on the line.&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
  cov (X,Y) &amp;= E[XY] - E[X]E[Y] \\
           &amp;= E[XY] - 0 \text{(assiming zero mean curves)} \\
           &amp;= E[ \right ( tA + (1-s) B \left) \right ( tA + (1-t) B \left) ] \\
           &amp;= E[ ts A^2] + E[t(1-s)BA] + E[s(1-t)BA] + E[(1-t)(1-s)B^2 ]
           &amp;= ts var(A) + cov(B,A) [t (1-s) + s (1-t)] + (1-t)(1-s)var(B)
\begin{end}
\]
&lt;/div&gt;


&lt;p&gt;The GP above assumes endpoints occur at 0 and 1. We can relax this to end at \(T_\mathrm{max}\), giving us the &quot;linear interpolation gaussian process:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
  k_l(s,t; T_\mathrm{max}) &amp;= 1/T_\mathrm{max}^2 \right ( ts var(A) + cov(B,A) [t (T_\mathrm{max}-s) + s (T_\mathrm{max}-t)] + (T_\mathrm{max}-t)(T_\mathrm{max} - s)var(B) \left )
\begin{end}
\]
&lt;/div&gt;


&lt;p&gt;This gaussian process will form the backbone of the runner curv.   To allow deviations from the linear path, we add a smooth curve GP with covariance \(k_c\).  This GP must be constrained to pass through zero at \(0\) and \(T_\mathrm{max}\) using the derivation above.  For any two points on the runner, the covariance is given by:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
  k(x,y) &amp;=  \right( k_l(x,y ; T_\mathrm{max}) + k_c(x,y ; 0,T_\mathrm{max})  \left ) + 
\begin{end}
\]
&lt;/div&gt;


&lt;p&gt;If \(x\) lies on the runner and \(y\) lies elsewhere on the curve DAG, the covariance is:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
  cov(X,Y) &amp;= E[XY] \\
           &amp;= E[(sA + (1-s)B) Y] \\
           &amp;= E[sAY] + E[(1-s)BY ] \\
           &amp;= s \mathrm{cov}(AY) + (1-s)\mathrm{cov}(BY) \\
\begin{end}
\]
&lt;/div&gt;


&lt;p&gt;i.e. the linear intpolation of its endpoint covariances.&lt;/p&gt;

&lt;p&gt;One notable property of the runner curve is that it introduces no additional attachment between its parent curves.  For this reason, we can first construct the curve tree covariance matrix without runners, and then add runner blocks in a second pass.  This is nice, because the runner equation doesn&#39;t fit nicely into the recursive equation for BGP covariance.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/11/work-log"/>
   <updated>2014-12-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/11/work-log</id>
   <content type="html">&lt;p&gt;Reached out to Amy Tabb with the possibility of using her voxel reconstruction method to bootstrap the branching gaussian process.  She&#39;s offered to try it on our data, and if it works, perhaps there&#39;s a collaboration possible.&lt;/p&gt;

&lt;p&gt;Continuing to work on managing the Diadem dataset.  It is still downloading; meanwhile, I&#39;m unraring the parts I have and resizing the ones I&#39;ve unrared for easy viewing.  Some datasets need levels adjusted to see anything; imagemagick&#39;s -auto-level flag does a good enough job to get the gist of the dataset.&lt;/p&gt;

&lt;p&gt;Working on incorporating new FIRE immunity data.  Some inconsistencies between the old and new data.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Some dates are inconsistent (e.g. mistaking July for June or vice-versa).&lt;/li&gt;
&lt;li&gt;I noticed that my old spreadsheet lost some decimal places, due to a combination of Excel formatting and unjudicious copy/pasting.  Needs fixing.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;NEXT: R coding / mixed effects model&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Diadem dataset investigation</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/10/work-log"/>
   <updated>2014-12-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/10/work-log</id>
   <content type="html">&lt;h1&gt;Software&lt;/h1&gt;

&lt;h2&gt;ImageJ &lt;/h2&gt;

&lt;p&gt;ImageJ has great image stack tools (slicing, manipulation, analysis).  Has the &quot;tubeness&quot; measure from the diadem challenge.&lt;/p&gt;

&lt;h2&gt;ImageJ Cookbook&lt;/h2&gt;

&lt;p&gt;The &quot;&lt;a href=&quot;http://fiji.sc/Cookbook&quot;&gt;cookbook&lt;/a&gt;&quot; plugin for microscopy seems to be very useful in particular.&lt;/p&gt;

&lt;h2&gt;MBF Plugin Collection&lt;/h2&gt;

&lt;p&gt;The original software associated with the &quot;ImageJ for Microscopy&quot; reference (see below).  It seems defunct now, and is supplanted by the ImageJ Cookbook plugin suite.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://fiji.sc/MBF_Plugin_Collection&quot;&gt;link&lt;/a&gt;;&lt;/p&gt;

&lt;h2&gt;Neuromantic&lt;/h2&gt;

&lt;p&gt;Need to look into this tool.  Was mentioned in Diadem data documentation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.reading.ac.uk/neuromantic/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://journal.frontiersin.org/Journal/10.3389/fninf.2012.00004/abstract&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;count: 3&lt;/p&gt;

&lt;h2&gt;Xvoxtrace&lt;/h2&gt;

&lt;p&gt;Manual segmentation tool&lt;/p&gt;

&lt;h2&gt;Synu&lt;/h2&gt;

&lt;p&gt;Surface reconstruct tool (used with Xvoxtrace?)&lt;/p&gt;

&lt;h2&gt;Neurolucida&lt;/h2&gt;

&lt;p&gt;Segmentation tool.  Provides nice single-line segmentations, rather than the volumetric ones from other tools.&lt;/p&gt;

&lt;p&gt;Extension: *.asc (ascii)&lt;/p&gt;

&lt;p&gt;Count: 5&lt;/p&gt;

&lt;h2&gt;Analyze (7.5)&lt;/h2&gt;

&lt;p&gt;Image stack tool?  Compressed image stacks&lt;/p&gt;

&lt;p&gt;extension: &lt;em&gt;.img, &lt;/em&gt;.hdr&lt;/p&gt;

&lt;h2&gt;Amira&lt;/h2&gt;

&lt;p&gt;Semi-automatic segmentation&lt;/p&gt;

&lt;h2&gt;BioRadPIC&lt;/h2&gt;

&lt;p&gt;microscopy image data format&lt;/p&gt;

&lt;p&gt;extension: *.PIC&lt;/p&gt;

&lt;h2&gt;flNeuronTool&lt;/h2&gt;

&lt;p&gt;Neuron Tracing tool (C++)&lt;/p&gt;

&lt;h2&gt;cvapp - Duke / Southampton Morphology editor&lt;/h2&gt;

&lt;p&gt;Neuron tracing, labelling.  Can read neurolucida files.&lt;/p&gt;

&lt;h2&gt;Vaa3D &lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://vaa3d.org&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;NeuronStudio&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://research.mssm.edu/cnic/tools-ns.html&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Rayburst&lt;/h2&gt;

&lt;p&gt;Algorithm used in at least one tracing algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://research.mssm.edu/cnic/tools-rayburst.html&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Neurozoom&lt;/h2&gt;

&lt;p&gt;TBD&lt;/p&gt;

&lt;h2&gt;NeuronML&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.neuroml.org/tool_support&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;open format for representing neurons, neuron networks, and others.&lt;/p&gt;

&lt;h2&gt;NeuronLand &lt;/h2&gt;

&lt;p&gt;convert between over 20 difference neuron morphology formats&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://neuronland.org/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Other&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;autoneuron&lt;/li&gt;
&lt;li&gt;neuronj&lt;/li&gt;
&lt;li&gt;neuronmetrics&lt;/li&gt;
&lt;li&gt;Raveler&lt;/li&gt;
&lt;li&gt;tablet&lt;/li&gt;
&lt;li&gt;TRAKA&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;References&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://medicine.osu.edu/neuroscience/Documents/Biotechniques%20ImageJ%20overview%202013.pdf&quot;&gt;ImageJ for Microscopy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sezgin &amp;amp; Sankur. 2004. Survey over image thresholding techniques and quantitative performance evaluation. Journal of Electronic Imaging, 2004&lt;/li&gt;
&lt;li&gt;For reconstruction and analysis of dendritic spines: Rodriguez A., Ehlenberger D.B., Dickstein D.L., Hof P.R., Wearne S.L. Automated Three-Dimensional Detection and Shape Classification of Dendritic Spines from Fluorescence Microscopy Images. PLoS ONE 3(4): e1997&lt;/li&gt;
&lt;li&gt;For reconstruction and analysis of dendritic arbors: Wearne, S.L., Rodriguez, A., Ehlenberger, D.B., Rocher, A.B., Hendersion, S.C., and Hof, P.R. New Techniques for imaging, digitization and analysis of three-dimensional neural morphology on multiple scales.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Neuron Datasets&lt;/h1&gt;

&lt;h2&gt;NeuroMorpho&lt;/h2&gt;

&lt;p&gt;Seems to be an open neuron reconstruction dataset.  They don&#39;t appear to have the original images, unfortunatley.  I could try to contact the labs&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://neuromorpho.org/neuroMorpho/index.jsp&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Duke Southampton Archive of Neuronal Morphology&lt;/h2&gt;

&lt;h2&gt;Cell-centered database&lt;/h2&gt;

&lt;p&gt;General database of cell images.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ccdb.ucsd.edu/CCDBWebSite/index.html&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;Useful search terms:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Neuron (obviously)&lt;/li&gt;
&lt;li&gt;Dendritic tree&lt;/li&gt;
&lt;li&gt;Spiny Dendrite&lt;/li&gt;
&lt;li&gt;Purkinje&lt;/li&gt;
&lt;li&gt;neurolucida (ground truth tool)&lt;/li&gt;
&lt;li&gt;vascular?&lt;/li&gt;
&lt;li&gt;protoplasmic astrocyte&lt;/li&gt;
&lt;li&gt;cerebellar basket cell&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Examined datasets&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://ccdb.ucsd.edu/sand/main?mpid=1&amp;amp;event=displaySum&quot;&gt;dendritic tree&lt;/a&gt; - could be good for &quot;tracking&quot; through slices.  Ground truth segmentation isn&#39;t clear&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ccdb.ucsd.edu/sand/main?mpid=12&amp;amp;event=displaySum&quot;&gt;spiny dendrite&lt;/a&gt; -&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ccdb.ucsd.edu/sand/main?mpid=2&amp;amp;event=displaySum&quot;&gt;Purkinje neuron (dendritic tree)&lt;/a&gt; - very complex branching structure, with ground truth&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ccdb.ucsd.edu/sand/main?mpid=3494&amp;amp;event=displayRecon&quot;&gt;Purkinje neuron (spiny dendrite)&lt;/a&gt; - nice electron microscopy image&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ccdb.ucsd.edu/sand/main?mpid=3687&amp;amp;event=displaySum&quot;&gt;Purkinje neuron (multiphoton)&lt;/a&gt;  Nice but no ground truth&lt;/li&gt;
&lt;li&gt;(http://ccdb.ucsd.edu/sand/main?mpid=3380&amp;amp;event=displaySeg) - confocal data seems in the seetspot between clean and noisy.  Nice 3d ground truth&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ccdb.ucsd.edu/sand/main?mpid=3693&amp;amp;event=displaySeg&quot;&gt;medium spiny neuron (transmitted light&lt;/a&gt; - has a nice looking ground-truth&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ccdb.ucsd.edu/sand/main?mpid=8244&amp;amp;event=displayRecon&quot;&gt;neocortex pyramidal neuron&lt;/a&gt; - different-looking neuron, different lab, could be an easier dataset&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Datasets to look into&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Maryann Martone - at least 4 very complex dendritic tree stacks w/ ground truth.&lt;/li&gt;
&lt;li&gt;Mark Ellisman, Stephen Larson - at least 1 full neuron w/ semi-auto segmentation&lt;/li&gt;
&lt;/ul&gt;


&lt;h4&gt;Data types&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;confocal&lt;/li&gt;
&lt;li&gt;IVEM / UHVEM - {intermediate, ultra high} voltage electron microscopy)&lt;/li&gt;
&lt;li&gt;transmitted light&lt;/li&gt;
&lt;li&gt;multiphoton - very clean images&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Miscellaneous&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Lucifer yellow - cell stain&lt;/li&gt;
&lt;li&gt;Golgi - staining method&lt;/li&gt;
&lt;li&gt;genesis modelling package?&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;To Try&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Decovolution with synthetic and emperical point spread functions.&lt;/li&gt;
&lt;li&gt;using duke/southampton format - swc&lt;/li&gt;
&lt;li&gt;use neuromorpho for training
** experimental condition: &quot;Control&quot;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/08/work-log"/>
   <updated>2014-12-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/08/work-log</id>
   <content type="html">&lt;p&gt;Finished implementing and debugging neuron skeleton-image-to-graph code.  It was deceptively difficult; there are lots of corner cases when tracing pixels that lead to unexpected behavior.  I had to totally redesign how junctions were calculated three times.  I also twice refactored how previously-visited pixels were recorded, and how chains are split at junctions.&lt;/p&gt;

&lt;p&gt;We define the &quot;neighbor set&quot; of a pixel as the set of all nonzero 8-connected neighbors with the caveat that diagonal neighbors are not &quot;blocked&quot; by a horizontal neighbor.  The idea here is that we want all paths through the graph to be unique, and &quot;blocked&quot; diagonal neighbors have two paths: one in which the diagonal neighbor is adjacent to the current pixel, and one in which it is adjacent to the blocking pixel.  Omitting it from the neighbor set solves the problem by eliminating the extra path, while guaranteeing a path still exists that passes through the offending neighbor pixel.  In practice, this prevents chains from sneaking around previously visited pixels by visiting diagonal neighbors.  It also is central to the definition of a &quot;junction&quot; below.&lt;/p&gt;

&lt;p&gt;We use a modified floodfill algorithm to explore the entire graph.  First, we add one or more seed pixels to the queue.  While the queue is not empty, we dequeue a pixel, add it to the current pixel chain, and enqueue its neighbors.  We also check if it is a junction pixel; if so, we add the current pixel-chain to the chain set, and begin a new empty chain.  We define a junction as any pixel with a neighbor set of size three or more.  If during any iteration, no pixels are added to the queue, the chain has reeached its end; we add it to the chain set and begin a new empty chain.&lt;/p&gt;

&lt;p&gt;Several additional details are important.&lt;/p&gt;

&lt;p&gt;Because we allow several seed pixels, our depth-first search may encounter the seed pixels while they still exist deeper in the queue.  The side effect is that the seed pixels may be added to two different chains.  We resolve this by marking pixels when they are added to a chain, and if a pixel is dequeued that is already added to a chain, we discard it.&lt;/p&gt;

&lt;p&gt;After finding all chains, at most one endpoint is associated with a junction.  We require that the junction point appear in every pixel-chain that enters the junction, so some chains need their second junction point added.  Recall that a chain only terminates at junctions or if it has no unclaimed neighbors.  For any non-junction endpoints, any neighbors (not counting the chain&#39;s antecedent pixel) must be junctions.  To prove there is at most one junction neighbor, assume it had two junction endpoints.  Then the pixel would have three neighbors: two endpoints and its antecedent pixel. This implies the pixel is a junction itself -- a contradition.  Thus, adding a junction to a dangling endpoint amounts to finding an unclaimed neighbor.&lt;/p&gt;

&lt;p&gt;Two adjacent junction pixels must be handled separately.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Generalizing the brownian bridge</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/07/reference"/>
   <updated>2014-12-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/07/reference</id>
   <content type="html">&lt;p&gt;the Brownian bridge is a modified Weiner process a constrained that f(0)=0 and f(1)=0.  According to the &lt;a href=&quot;http://en.wikipedia.org/wiki/Brownian_bridge&quot;&gt;Wikipedia page&lt;/a&gt;, its covariance is \(k(x,y) = min(x,y) (1-max(x,y))\).&lt;/p&gt;

&lt;p&gt;What if we want something like a Brownian bridge, but with a different base distribution?  Or a more general question: how can we derive a covariance function for a Gaussian process with a constained point?  Consider a general covariance function, \(k_0(x,y)\), and a constraint, \(f(x_c) = z\).  After conditioning on \(f(x_c)\), the new covariance function is \(k = k_0(x,y) - k(x, x_c) k(x_c, y) / k(x_c, x_c)\).  Notice that applying this to thw Weiner process covariance, \(k_0(x,y) = min(x,y)\) results in the Brownian bridge covariance: \(k(x,y) = min(x,y) - min(x,1) min(1,y) / min(1,1) = min(x,y) - x y = min(x,y) (1 - max(x,y))\. QED.  Deriving the mean curve in closed form is more difficult, and is likely impossible if the GP has infinite basis.  In the special case where the original mean function is zero and the constraint value is zero, the conditoinal mean is also zero.&lt;/p&gt;

&lt;p&gt;If we want to condition both ends of the distribution to pass through constraints, we can apply the above formula twice.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Neuron work planning</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/05/reference-neurons"/>
   <updated>2014-12-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/05/reference-neurons</id>
   <content type="html">&lt;p&gt;Input data: Skeleton graph
* nodes: junctions (data: none)
* edges: curves (data: curve points + indices)&lt;/p&gt;

&lt;p&gt;Ouptut data: curve graph
* nodes: curves (data: curve points + indices)
* edges: branches (data: branch point and index offset)&lt;/p&gt;

&lt;p&gt;Converting from skeleton graph to curve graph requires a &quot;resolution&quot; for each junction
  4 possible resolutions for three-way graph { (ab,c) (a,bc) (ac,b) (a,b,c) }
  6 possible resolutions for 4-way graph&lt;/p&gt;

&lt;p&gt;Special case: loops&lt;/p&gt;

&lt;p&gt;  a &lt;-&gt; b &lt;-&gt; c &lt;-&gt; d &lt;-&gt; a
        ^     ^     ^   &lt;br/&gt;
        |     |     |
        V     v     v
        e     f     g&lt;/p&gt;

&lt;p&gt;  above, subgraph {a,b,c,d} constitutes a loop.
  requires &quot;loop resolution&quot;: pick two endpoints and interpolate (estimating the index changebetween them). the remaining junction nodes become lateral branches (which require their offsets).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct loop_resolution
{
  int endpoint_1;
  int endpoint_2;
  double index_delta;
  double lateral_branch_index[n];
  double lateral_index_deltas[n];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Inference&lt;/h2&gt;

&lt;p&gt;preprocess: compute putative junction resolution branch poionts
preprocess: compute putative loop resolution branch points and index offsets&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ideal method: Gibbs sample over junction/loop resolutions.  Metropolis sample model parameters.&lt;/li&gt;
&lt;li&gt;Metropolis method 1: To avoid computing full BGP marginal likelihood at each sample, compute local improvement at each junction/loop. Occasionally evaluate full BGP and accept/reject based on difference between local improvement and global improvement.&lt;/li&gt;
&lt;li&gt;Metropolis method 2: After each global improvement, extend local curves using junction resolutions.  New junction resolutions can use this information to improve local estimates.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;dependency flow:&lt;/h2&gt;

&lt;p&gt;skeleton_graph plus junction resolutions implies curve graph
Curve graph plus BGP parameters implies BGP matrix
BGP matrix plus BGP mean implies marginal likelihood&lt;/p&gt;

&lt;h2&gt;Model:&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;Curve covariance: smooth central curve plus mean-reverting random-walk plus small iid noise (for quantization error)
    smooth central curve: either cubic spline GP or squared-exponential GP w/ initial constraints
    random walk curve: Ornstein-Uhlenbeck GP
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Training:&lt;/h2&gt;

&lt;p&gt;manually specify all junction and loop resolutions
per image:
    argmax smoothness covariance , random walk covariance,
        compute BGP covariance matrix
fix a distribution over smoothness covariance and random walk covariance&lt;/p&gt;

&lt;h2&gt;Straw man:&lt;/h2&gt;

&lt;ol type=&quot;a&quot;&gt;
&lt;li&gt;Resolve using best linear fit and threshold&lt;/li&gt;
&lt;li&gt;resolve using trained SVM&lt;/li&gt;
&lt;li&gt;resolve using sampling with manually-set parameters (aka &quot;first pass&quot; below)&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;First pass: no training&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;don&#39;t train.  set covariances somewhat randomly (overestimate; use matlab generate samples and eyeball )
use full gibbs sampling and run for several days.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Second pass: training, don&#39;t sample model parameters.&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;gather ground truth on all 8 datasets (imagej? c++?)
train parameters for all 8 datasets
fit parameter model to 7 datasets; inference on 8th.
For each parameter covariance, use largest over all training 7 datasets (no gibbs sampling)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Third pass: full model (training and sampling model parameters&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;self explanatory
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Fourth pass: full model  w/ metropolis method 1&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;(see &quot;metropolis method 1&quot; above, under &quot;inference&quot;)
Use local marginal likelihood during Gibbs sampling; periodically accept/reject new model using metropolis method on full marginal likelihood.
don&#39;t update local curves after accepting 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Fifth pass: full model  w/ metropolis method 2&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;(see &quot;metropolis method 2&quot; above, under &quot;inference&quot;)
same as previous, but update local curves after accepting.
Should result in better acceptance, faster convergence.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Evaluation:&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;Percent of correctly classified junction resolutions
    (Ignore &quot;don&#39;t know&quot; junction resolutions in ground truth annotation)
compare straw man, various &quot;Nth pass&quot; method above.
Compare energy vs. time on various &quot;Nth pass&quot; method above
Visual: show smoothed skeleton
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Extensions&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;allow junction breaking.&lt;/li&gt;
&lt;li&gt;allow creating of junctions by filling gaps.

&lt;ul&gt;
&lt;li&gt;(requires additional annotation code)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;imageJ plugin for ground-truthing&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Miscellanous observations&lt;/h2&gt;

&lt;p&gt;Loop collapsing shouldn&#39;t occur if interpolated curve or lateral curves pass through block region (non-neuron).  Prefer breaking loop in this case.&lt;/p&gt;

&lt;p&gt;When breaking loops, prefer curves that grow narrower that farther then get from the tree.  Each edge can be assigned a &quot;direction&quot; posterior probability based on its tendancy to grow thinner.  Could use an OU process; given initial point, compute probability of remaining.  Since OU prefers reverting to the mean (zero in this case), the forward direction should be preferred.&lt;/p&gt;

&lt;p&gt;When breaking, prefer keeping thick branch points, rather than thin ones.&lt;/p&gt;

&lt;p&gt;When resolving junctions, make sure &quot;parent&quot; curve doesn&#39;t wind up in a child configuration.  In other words, in a y junction, one curve will be closer to the tree root than the other two.  The other two cannot end up being connected, because that would imply the parent curve is a lateral branch of the child.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Meeting notes (R code for Mixed Effects models)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/03/work-log"/>
   <updated>2014-12-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/03/work-log</id>
   <content type="html">&lt;p&gt;Notes from today&#39;s meeting with Emily&lt;/p&gt;

&lt;p&gt;  Modelling repeated measure data for diads (e.g coupled osc)&lt;/p&gt;

&lt;p&gt;  standard: multi-level model
      papers that introduced it?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  most doable in sass and R, 
      one piece in sass but not R
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  Weighted sum of predictors
  multi-level - fixed effects model
      global average plus individual average&lt;/p&gt;

&lt;p&gt;  What happens to residuals around lines.&lt;/p&gt;

&lt;p&gt;  eg. my result is grand slope plus my difference, plus residual&lt;/p&gt;

&lt;p&gt;  &quot;what happens to residuals?&quot;  what does that mean
      model residuals
      auto-correlation or partner covariation
      want both
      hack in R:  put them in as fixed effects.  lag-1 &lt;-  don&#39;t totally understand this
      R package - NLME  nonlinear mixed effects models
          -&gt; lme() function is most used
              -&gt; correlation structure
              -&gt; Type A: &quot;variance functions&quot;
                  -&gt; NLME defines a default set of variance functions
              -&gt; Type B: &quot;correlation structures&quot;
                  -&gt; NLME defines a default set of CorrStruct classes
                     We may need to define our own
  SASS version
      - Emily will send the url
      - a &quot;repeated statemnt&quot;
          sets up structure on residuals (the &quot;R matrix&quot;)
      - &quot;type equals&quot;
          the type we want in SASS-speak  un@ar1  &quot;direct product AR 1&quot;&lt;/p&gt;

&lt;p&gt;  S or S+ are &quot;paid versions&quot; of R.&lt;/p&gt;

&lt;h2&gt;  Questions&lt;/h2&gt;

&lt;p&gt;  Where can I get sass?
      runs in VMWare
      runs through the internet through UA
      Emily has it running&lt;/p&gt;

&lt;h2&gt;  Need:&lt;/h2&gt;

&lt;p&gt;  example data
  example results in SASS&lt;/p&gt;

&lt;p&gt;  book: mixed effects models in S or S+ .  pinhiero bates
  niell: longitudinal data in R&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Brainstorm: Neuron-tracing</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/12/01/work-log"/>
   <updated>2014-12-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/12/01/work-log</id>
   <content type="html">&lt;p&gt;  threshold
  distance transform
  skeleton
  identify chains, endpoints, and intersections
  identify endpoint merge candidates
  identify t-junction merge candidates&lt;/p&gt;

&lt;p&gt;  identify ambiguous cases
    ambiguous intersection (which one is the lateral?)
    ambiguous merge (endpoint vs t-junction?&lt;/p&gt;

&lt;p&gt;  for each skeleton point,
    find direction and distance to two matching points
    get width&lt;/p&gt;

&lt;p&gt;  identify soma as point with largest distance&lt;/p&gt;

&lt;p&gt;  filter bad skeleton points
    average touch-point velocity is much less than skeleton point velocity.
    with changes almost as fast as skeleton point moves&lt;/p&gt;

&lt;p&gt;  
  GROUND TRUTH
  &lt;/p&gt;

&lt;p&gt;  w/ known skeleton
  
  identify skeleton points that are good/bad
  manually resolve junctions
  manually perform merges&lt;/p&gt;

&lt;p&gt;  general
  
  Trace out general topology w/ bezier curves.&lt;/p&gt;

&lt;p&gt;  
  LEARNING
  &lt;/p&gt;

&lt;p&gt;  quantities of interest:&lt;/p&gt;

&lt;p&gt;  GP parameters
    position changes
    width changes&lt;/p&gt;

&lt;p&gt;  soma shape/size?&lt;/p&gt;

&lt;p&gt;  GP MODELS&lt;/p&gt;

&lt;p&gt;  smooth curve plus mean-reverting random walk?
    smooth curve resolves ambiguous branch using curvature&lt;/p&gt;

&lt;p&gt;  
  LIKELIHOOD
  &lt;/p&gt;

&lt;p&gt;  Allow greater uncertainty along cross-section, because width doesnt change symmetrically.&lt;/p&gt;

&lt;p&gt;  Evidence is weaker where skeleton points are weak (i.e. width changes quickly)&lt;/p&gt;

&lt;p&gt;  
  INFERENCE
  &lt;/p&gt;

&lt;p&gt;  Questions to answer:&lt;/p&gt;

&lt;p&gt;  which skeleton sections are for actual dendrites?&lt;/p&gt;

&lt;p&gt;  
  STRAW MEN
  &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt; Use all curves; no merging, use heuristics to resolve junctions&lt;/li&gt;
&lt;li&gt; same as 1. but use weak curve heuristic to trim bad cases
train an svm to decide which curves are good?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;  
  Evaluation
  &lt;/p&gt;

&lt;p&gt;  general, weak
  
  number of curves
  number of lateral branches
  histogram of curve lengths&lt;/p&gt;

&lt;p&gt;  general, strong
  
  topology evaluation / mot metrics - match to ground truth and count identity swaps, etc&lt;/p&gt;

&lt;p&gt;  w/ GT against known skeleton
  
  misclassified junctions
  false positive/false negative merges&lt;/p&gt;

&lt;p&gt;  
  misc
  
  Is there any benefit to reconstructing the smoothed neuron?  probably not. but the latent model is still useful for classifying junctions.&lt;/p&gt;

&lt;p&gt;  How do deal with un-ground-truthable junctions?&lt;/p&gt;

&lt;p&gt;  how to deal with regions poorly modelled by a skeleton?&lt;/p&gt;

&lt;p&gt;  I should look into neuron-tracing prior art.&lt;/p&gt;

&lt;p&gt;  threshold issue:
    try multiple thresholds and somehow merge skeletons?&lt;/p&gt;

&lt;h2&gt;matlab code&lt;/h2&gt;

&lt;p&gt;  cd /Volumes/offload/Downloads/JT106_Ex33_Original
  img = imread(&#39;JT106_Ex33_61.tif&#39;);
  level = graythresh(img)
  I = img &gt; (level * max(img(:)));
  D = bwdist(~I);
  skel = bwmorph(D,&#39;skel&#39;,Inf);
  imagesc(skel)
  skel2 = bwmorph(D,&#39;skel&#39;,5);&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/11/05/work-log"/>
   <updated>2014-11-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/11/05/work-log</id>
   <content type="html">&lt;p&gt;Met with Kobus and Andrew regarding the seed counting task.  Andrew summarized the current state of the project.  We have foreground-background segmentation in octave, and it seems to work well in the easy cases.  If I understand correctly he has some code to match possible exemplars with an example exemplar, using an affine transform to try to align them.  Given this set of aligned exemplars, a shape model could be learned for more sophisticated analysis.&lt;/p&gt;

&lt;p&gt;Kobus outline a strategy for generating a &quot;heatmap&quot; for data-driven proposals:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;find edgelets&lt;/li&gt;
&lt;li&gt;for each edgelet, try matching to the contour of an exemplar.  If it matches well, add a &quot;vote&quot; to the pixel where that exemplar wouid appear (or rather it&#39;s centroid).  Could give partial votes for partial matches.  Could give weaker votes to shorter edges.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Possible things to show for 10m talk:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;pipeline: thresholding, interactive edge extraction tool.&lt;/li&gt;
&lt;li&gt;a sketch of the algorithm&lt;/li&gt;
&lt;li&gt;preliminary results&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Talk outline:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;outlining the problem, show example image&lt;/li&gt;
&lt;li&gt;its difficulties, show example image,&lt;/li&gt;
&lt;li&gt;what has been done, show thresholding,&lt;/li&gt;
&lt;li&gt;motivate planned work: exemplar model implies a large search space&lt;/li&gt;
&lt;li&gt;what is planned to be done, and motivation for that plan.&lt;/li&gt;
&lt;li&gt;current progress&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Summarizing the &quot;motivation&quot; step: searching the entire image over the entire space shape (including the number of shapes) is an exponentially growing problem.  Need a set of good candidates.  Our &quot;exemplar map&quot; could be a good way to reduce the search space by only considering locations where the seed is likely to appear.&lt;/p&gt;

&lt;p&gt;Andrew provided some resources that could help&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;slides from earlier presentation are in: /space/predoehl/iplant-demo-2012oct17.tar.bz2&lt;/li&gt;
&lt;li&gt;octave code for thresholding is in /data/predoehl/seed-counting/.../...&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To do before CVPR is over:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;run edgelet extraction on all images&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;After CVPR:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;implement voting scheme, run on example image`&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Other thoughts&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Could my edge-based likelihood be useful here?
:w&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/10/28/work-log"/>
   <updated>2014-10-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/10/28/work-log</id>
   <content type="html">&lt;p&gt;Trying to address cholesky issues arising new dynamic covariance.  Eschewing clique tree implementation in favor of direct method for obtaining MAP curves.&lt;/p&gt;

&lt;p&gt;gdb on OSX is losing stack frame at exception.  Running in vagrant...&lt;/p&gt;

&lt;p&gt;g++ is segfaulting on my vagrant ubuntu&lt;/p&gt;

&lt;p&gt;gdb on v11 is crashing due to missing libpython (probaby lost during upgrade to ubuntu 14.04).&lt;/p&gt;

&lt;p&gt;trying my ubuntu 12.04 desktop server.  Taking a realllly long time to check out (15 minutes and counting)&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;OK, finally rolling on Vll.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Getting singular posterior covariance matrix.  why?  ... Singular prior matrix.  why? ...  New dynamics covariance is singular.  But this shouldn&#39;t affect joint covariance, right?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;bug: sqexp parameters not being set when computing prior
bug: sqexp scale parameter not set when reading params
bug: conditional sqexp math error&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;AHA!  Reviewing Rasmussen and Williams, I see now that Koller and Friedman&#39;s &quot;Canonical form&quot; is exactly wrong for the types of tasks we want to perform.  When the prior is close to singular (or is singular), we can still do inference, we just have to be careful not to invert the prior covariance matrix.  W&amp;amp;R show how to do this in the simple case.  It remains to extend it to inference in varable blocks organized in markov chains.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>New dynamic models</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/10/27/work-log"/>
   <updated>2014-10-27T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/10/27/work-log</id>
   <content type="html">&lt;p&gt;The current mode for dynamics is a cubic spline spatial process modulated by a Weiner temporal process.  This is allowing excessive drift toward the cameras when curves are long.  A stationary covariance function should avoid this, but out model requires zero covariance at the start of a curve.&lt;/p&gt;

&lt;p&gt;To resolve this, we can condition the process to start at the origin.&lt;/p&gt;

&lt;p&gt;K - K:0 1/K0 K:0&#39;&lt;/p&gt;

&lt;p&gt;The equivalent covariance function is:&lt;/p&gt;

&lt;p&gt;k_{x|x_0}(x,x&#39;) = k(x,x&#39;) - k(x, 0) 1/k(0,0) K(0,x&#39;)&lt;/p&gt;

&lt;p&gt;When k(x,x&#39;) is a squared exponential, the conditional covariance function becomes&lt;/p&gt;

&lt;p&gt;k_{x|x_0}(x,x&#39;) = k(x,x&#39;) \left (1 - exp{(-x_i x_j)/s}\right)&lt;/p&gt;

&lt;p&gt;This causes the covariance to start at zero, and smoothly transition to the standard kernel, over a time scale (s).&lt;/p&gt;

&lt;p&gt;Adding this as an option to the covariance function...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Now getting cholesky failures during reconstruction.&lt;/p&gt;

&lt;h2&gt;OSX building issues&lt;/h2&gt;

&lt;p&gt;Still struggling with building and debugging on OSX.  Got c++11 with init_Compile working on OSX.  Can&#39;t run dbg without it segfaulting.  I&#39;m trying &lt;code&gt;brew update&lt;/code&gt; to rebuild it and all its dependencies.  Meanwhile, i&#39;m setting up and ubuntu virtual machine through Vagrant to help with debugging if I can&#39;t get gdb going.&lt;/p&gt;

&lt;p&gt;The fortran spline library I imported is segfaulting in OSX.  Can&#39;t debug it, because GDB is segfaulting.  HOW DOES NAYONE DEVELOP ON THIS SYSTEM???   I&#39;ll dig into it with LLDB while my virtual machine is setting up. Also running valgrind to find out where the illegal access is happening.&lt;/p&gt;

&lt;h2&gt;Misc notes&lt;/h2&gt;

&lt;p&gt;When computing ML, must try curves both forward and backward, since the covariance for dynamics is assymetrical.  Root motion is much more penalized than tip motion.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reconstruction problems</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/10/26/work-log"/>
   <updated>2014-10-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/10/26/work-log</id>
   <content type="html">&lt;p&gt;Visualizing 3d reconstruction of match candidates, and the results are problematic:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;No obvious good matches.&lt;/li&gt;
&lt;li&gt;Significant drift toward the camera.&lt;/li&gt;
&lt;li&gt;Marginal likelihood is promoting the worst reconstructions.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Issue 3 is possibly because there is a preference for placing structure near the camera center.  If perturbation model is loose enough, each view can pull curves twoard its camera, and the likelihood reward is high.&lt;/p&gt;

&lt;p&gt;How does it compare to simple triangulation?&lt;/p&gt;

&lt;p&gt;Very poorly!  The good triangulation candidates fit basically perfectly -- no perturbation necessary.  We should reduce the perturbation variances, but this is causing the clique tree algorithm to become numerically unstable.  Is there a better solution for piecewise reconstruction, that avoids taking inverse/cholesky of near-singular matrices?&lt;/p&gt;

&lt;p&gt;Open questions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Is the current GP implementation working?&lt;/li&gt;
&lt;li&gt;can adjusting GP parameters give reasonabley flexible dynamics without significiatn drift toward camera?&lt;/li&gt;
&lt;li&gt;Do we need to linearize the camera model?&lt;/li&gt;
&lt;li&gt;Is ML promoting the best matches?  Should be roughly in-line with triangulation error&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Actually, we have no real evidence that GP smoothing is even working.  Need some unit tests:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Test GP smoothing on single curve.  is it smoother?  is RMS error only minimally affected?&lt;/li&gt;
&lt;li&gt;TEST GP on two views of a curve.  Can we recover the original curves?  Does ML match Matlab implementation?&lt;/li&gt;
&lt;li&gt;Test piecewise marginal likelihood -- does it match full-matrix marginal likelihood?&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/10/21/work-log"/>
   <updated>2014-10-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/10/21/work-log</id>
   <content type="html">&lt;p&gt;Current task: implementing clique tree algorithm when noise distribution is improper.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Why do we need this now?  Didn&#39;t this work in the Matlab version of the code?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;No, in matlab, we did reconstruction by inverting a matrix containing all points in all views.  This worked when correspondences are known, and reconstruction was the only goal, but it&#39;s hugely inefficient when sampling over correspondences.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Why do we need a reconstruction at each MCM iteration?&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;we need to estimate indices after each merge, which requires projecting the current estimate&lt;/li&gt;
&lt;li&gt;projecting is a primary method of finding good candidates.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;em&gt;What&#39;s the current difficulty?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The main difficulty is arising when constructing the conditional posterior distribution $p(x_2 | x_1, y_2)$, wher $x_i$ and $y_i$ are the curve in view $i$ and its observations.  In implementation, the covariance of the matrix is not positive definite, suggesting there is a bug somewhere, or we&#39;re suffering from numerical instability.  In either case, the issue need to be addressed or the inference will fail.&lt;/p&gt;

&lt;p&gt;I wrote up a unit test for this, and the issue isn&#39;t arising in the test.  But in practice, some match proposals encounter this problem (currently the 5th such match out of ~300 is suffering this issue, so it seems prevalent).&lt;/p&gt;

&lt;p&gt;I also coded up two different implementations to construct the conditional posterior, one that performs the matrix operations directly, and another less efficient version that uses the functionality from the Canonical_gaussian class.  The latter is much less likely to contain math errors, as it&#39;s well tested.   Both implementations exhibit this problem, suggesting the issue is with the matrices themselves.  I&#39;ve exported all of the matrices involved and imported them into matlab.  The same issue is arising there.&lt;/p&gt;

&lt;p&gt;Should I test the prior matrices to make sure they are PD?  Maybe they are almost singular because indices are nearly overlapping?  I think I have an old blog where I derive an alternative implementation for this situation...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/09/26/work-log"/>
   <updated>2014-09-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/09/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;TODO: troubleshoot triangulation width penalty in DTW matching&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Camera caliberation, revisited</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/08/01/work-log"/>
   <updated>2014-08-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/08/01/work-log</id>
   <content type="html">&lt;p&gt;Revisiting camera calibration -- can it be done automatically with opencv?&lt;/p&gt;

&lt;p&gt;Can get a ballpark camera from a single pass of OpenCV: (1) extract chessboards, (2) calibrate.&lt;/p&gt;

&lt;p&gt;pass 2: mask out previous chessboard and re-run
pass n: repeat until no new chessboards&lt;/p&gt;

&lt;p&gt;Stage 2: find circular path&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;For each estimated pose,
    1. get camera center and add to list
    1. get camera rotation and add to list
    2. Assume pattern is rotated 180-degrees, find camera center, add to list
    3. Repeat 1 and 2 five more times, once for each face of cube.

Find a strong plane in cameras list (ransac)
Find a circle in planar points (ransac)
    (there will be four points per camera on this circle, due to cube
     ambiguity.  use known rotation angle to prune extra points)
    For each camera pose, bring to canonical reference frame (pointing toward circle center).  Find average deviation (geodesic mean of quaternions).

Estimate missing cameras by interpolation.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stage 3: Dense point collection&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;re-run opencv, this time masking each face using roughly known face position
    (avoids interference from mutliple faces)
    (use white mask to simulate white margin that opencv hope for)
    (Hopefully we get points on each face now)
    (points now correspond to same faces, so better calibration)
    (! possibly rectify top chessboard, to improve capture rate)
    (! conservatively mask to remove other faces, then mask everything but good face, leaving a large margin to allow for imperfactions)
Re-run pairwise calibration in opencv
Undistort images
Run bundle adjustment in ceres solver

This calibration occurs without assumptions about cube, which makes it robust to imperfect construction.

POSSIBLE EXTENSIONS
    add some ceres solver constraints to ensure reconstruction is cube-like
        face points are equally spaced
        face points are in orthogonal grid
        faces are orthogonal
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Thoughts on a New approach</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/07/23/work-log"/>
   <updated>2014-07-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/07/23/work-log</id>
   <content type="html">&lt;ol&gt;
&lt;li&gt;Improve calibration: distortion, bundle adjustment&lt;/li&gt;
&lt;li&gt;MRF-based per-view depth estimates&lt;/li&gt;
&lt;li&gt;merging or killing structure by Maximum marginal likelihood&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;0. feature extraction hacks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Use ELSD for segments&lt;/li&gt;
&lt;li&gt;use background subtraction to ignore spurious curves&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;1. Improve calibration&lt;/h2&gt;

&lt;p&gt;There&#39;s no reason not to model lens distortion and run bundle adjustment.  This will allow some form of multi-view stereo to become feasible.&lt;/p&gt;

&lt;p&gt;bundle adjustment will let us correct for errors in calibration target (finally!)&lt;/p&gt;

&lt;h2&gt;2. Initial dept estimate for all curve points&lt;/h2&gt;

&lt;p&gt;It should be relatively easy to create a simple MRF for depth estimation. Likelihood: nearest reprojected distance.  prior: first or second order markove in 1-dimension.  Look into MRF solvers (is kolmogorov&#39;s code available? TRW?).&lt;/p&gt;

&lt;p&gt;alternative: keypoint matching and gap-filling.  think pmvs, but with 2 dof instead of 3&lt;/p&gt;

&lt;p&gt;alternative:&lt;/p&gt;

&lt;h2&gt;3. merging, branching, and pruning using maximum marginal likleihood&lt;/h2&gt;

&lt;p&gt;greedily pick pairs to merge, using DTW to match points and branching gaussian process to perform marginal likleihood.    compare against mcmc&lt;/p&gt;

&lt;p&gt;future papers could add branching and background rejection.&lt;/p&gt;

&lt;h2&gt;4. reconstruct&lt;/h2&gt;

&lt;p&gt;maximum posterior&lt;/p&gt;

&lt;h2&gt;5. evaluate&lt;/h2&gt;

&lt;p&gt;compare against hand-picked correspondences&lt;/p&gt;

&lt;p&gt;compare against bundler + pmvs.&lt;/p&gt;

&lt;h2&gt;other ideas&lt;/h2&gt;

&lt;p&gt;**Alternarive initial depth estimate: sift keypoint matching for seeding multi-view stereo.   use background model to mask-out background keypoints.&lt;/p&gt;

&lt;p&gt;ceres-solver for nonlinear least squares task.&lt;/p&gt;

&lt;p&gt;evaluate sensitivity to poor calibration.  which methods deteriorate fastest?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE - Sampling strategy</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/27/work-log"/>
   <updated>2014-05-27T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/27/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Tuning&lt;/h2&gt;

&lt;p&gt;On the issue of HMC step size, I wondered if anyone had published on the relationship between ideal step size and the log-posterior&#39;s second derivative.  I found an answer in section 4.2 of &lt;a href=&quot;http://arxiv.org/pdf/1206.1901.pdf&quot;&gt;Neal 2011&lt;/a&gt;[pdf], which poses the question in the context of a Gaussian-shaped posterior.  Using eigenvalue analysis, he shows that a step size larger than 2 standard deviations results in unstable dynamcis, and the state will diverge to infinity.  From Neal (2011):&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For low-dimensional problems, using a value for  that is just a bit below the stability limit
is sucient to produce a good acceptance rate. For high-dimensional problems, however,
the stepsize may need to be reduced further than this to keep the error in H to a level that
produces a good acceptance probability.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we can estimate the Hessian of the log-posterior (perhaps diagonally), we can use this to choose the step-size as some fraction of that (user settable).  Thus, our tuning run will perform Laplace approxization:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;perform local minimizations of negative log posterior.&lt;/li&gt;
&lt;li&gt;estimate diagonal Hessian at miminimum.&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Sampling the clustering model&lt;/h2&gt;

&lt;p&gt;I&#39;ve devised the strategy below for sampling the clustering model.  I have determined that all variables can be Gibbs sampled except the latent piecewise linear model.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Gibbs sample new membership values&lt;/li&gt;
&lt;li&gt;update each cluster&#39;s piecewise linear model using HMC&lt;/li&gt;
&lt;li&gt;Gibbs sample observation model using known latent values&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Step 3 needs some explanation.&lt;/p&gt;

&lt;h2&gt;Bayesian multiple linear regression&lt;/h2&gt;

&lt;p&gt;Let \(x\) be the column vector of latent immune activity values at each time, (t).  These are provided by the (fixed) piecewise linear model.  Recall that the observation model is:&lt;/p&gt;

&lt;div&gt;
\[
    y = A x + B + \epsilon
\]

&lt;br /&gt;

where \(\epsilon\) is a normally distributed random variable with variance \(\sigma^2\).
&lt;/div&gt;


&lt;p&gt;Thus, the error is given by&lt;/p&gt;

&lt;div&gt;
\[
    e = \left ( (A B)\begin{array}{c} x^\top \\ 1 \end{array} - y^\top \right ) / \sigma
\]

&lt;br /&gt;

Let \(\beta = (vec(A)^\top vec(B)^\top)^\top \) be the concatenated vectorization of A and B,  and \(X = (x 1)^\top\).  Following the [derivation provided by wikipedia](http://en.wikipedia.org/wiki/Bayesian_multivariate_linear_regression), the resulting log-likelihood can be written as a guassian w.r.t. \(\beta\):

\[
    -(\beta - \hat \beta)^\top (\Sigma_\epsilon^{-1} \ocross X X^\top) (\beta - \hat \beta)
\]
&lt;/div&gt;


&lt;p&gt;where \(\ocross\) is the kronecker product in our case, the observation noise variance \(\Sigma_\epsilon\) is simply \(I \sigma&lt;sup&gt;2&lt;/sup&gt; \).&lt;/p&gt;

&lt;p&gt;If we assume a uniform prior over \(A\) and \(B\), then this Gaussian becomes our conditional posterior distribution, which we can easilly sample from with Gibbs.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE - improving fitting</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/26/work-log"/>
   <updated>2014-05-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Run: Refactoring kmeans, fixed bug&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Refactored k-means to make replicates easier to run.  Also fixed a bug in how collapsed clusters are handled.
&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BEFORE REPOPULATION FIX

    10 itns
    ----------
    training error: 3.61348
    test error: 4.60147

    15 itns
    --------
    training error: 3.60094
    test error: 4.58815

    20 itns
    ----------
    training error: 3.60094
    test error: 4.58815

AFTER REPOPULATION FIX

    10 itns
    ---------
    training error: 3.5833
    test error: 4.57327

    15 itns
    ---------
    training error: 3.58389
    test error: 4.57315

    20 itns
    ---------
    training error: 3.58411
    test error: 4.57321
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Neither of these results match what I was getting on Friday.  Testing error in particular is worse.  Did I break something?&lt;/p&gt;

&lt;p&gt;Found it:  error in evaluation code arising from bad copy/paste.&lt;/p&gt;

&lt;p&gt;Another issue:  should be using centered_data.txt, not data.txt.&lt;/p&gt;

&lt;h2&gt;Run: multiple repetitions&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Run kmeans with 10 repetitions.&lt;br/&gt;
&lt;strong&gt;Issues&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Found assert failure - fixing empty clusters sometimes fails.&lt;/li&gt;
&lt;li&gt;only fixed empty clusters after first iteration.  fixed&lt;/li&gt;
&lt;li&gt;fails when same cluster is picked twice.  fixed&lt;/li&gt;
&lt;li&gt;cluster weights was computed wrongly.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;trivial model error: 3.55057

10 itns
-------
training error: 3.54682
test error: 4.51326

20 itns
------------
training error: 3.53998
test error: 4.52958
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Training and teesting error improve over the single-initialization version.  Test error is slightly worse for 20-iteration run; possibly due to overfitting.&lt;/p&gt;

&lt;h2&gt;Run: New baseline - use centered data&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Perform 10 replicates of k-means using centered log-transformed data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Trivial model error: 1.40502

10 itns
-----------
training error: 1.10196
test error: 1.11679

15 itns
----------
training error: 1.09979
test error: 1.09903

20 itns
--------------
training error: 1.09756
test error: 1.09137
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Run: compare against null model&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Do we do better or worse with a constant model? (slope zero, intercept zero)
&lt;strong&gt;Results&lt;/strong&gt;: see previous runs; &quot;trivial model&quot; results have been added.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;br/&gt;
It is interesting that the trivial model performs better on raw data than the cluster model.  With rescaled data, the cluster model performs better.&lt;/p&gt;

&lt;h2&gt;Run: continuous model (aborted)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Re-run using the continuous model.
&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Found bug in line-fitting corner case.  Fails if all observations occur at same time.&lt;/li&gt;
&lt;li&gt;found huge bug in preprocessing -- all values are identical!  Was an indexing error introduced when we added per-plate centering.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Will need to re-run all experiments.&lt;/p&gt;

&lt;h2&gt;Run: baseline (rerun)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Re-run baseline fitting of centered data using discontinuous model.  10 Repetitions&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Trivial model error: 1.58913
Single cluster error: 1.58365
training error: 1.61625
test error: 1.64005
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: trivial model outperforms clustered model.  This shouldn&#39;t be happening, need to investigate.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE - Refactoring, k-folds cross validation, baseline evaluation</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/23/work-log"/>
   <updated>2014-05-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/23/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Refactored code base to better support model evaluation and k-folds cross validation.&lt;/p&gt;

&lt;h2&gt;Run: Cross validation (baseline)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Run 10-folds cross validation on the non-sampling (analytical) model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error values are mean per-dimension negative log-likelihood.

3 kmeans iterations:
------------
training error: 3.94522
test error: 4.9166

5 kmeans iterations:
---------------
training error: 3.97775
test error: 4.9501

10 kmeans iterations:
------------------
training error: 4.01519
test error: 5.0074

20 kmeans iterations:
-----------
training error: 4.01538
test error: 5.0068
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;br/&gt;
Increasing iterations also increases training error, which seems counterintuitive, especially because the algorithm debugging messages show the total log likelihood increasing.&lt;/p&gt;

&lt;p&gt;But the log likelihood uses known membership, so values will naturally be higher.  Model evaluation marginalizes over clusters, and since we have fixed the cluster weights to be equal, the dominating cluster is under-promoted.  I would guess setting cluster weights from training membership proportions will reverse this trend, so more iterations will improve training error.&lt;/p&gt;

&lt;h2&gt;Run: Re-run with uneven cluster wieghts&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;:  Set cluster weights proportional to the training membership values.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;5 itns
--------
training error: 3.62931
test error: 4.61624

10 itns
---------
training error: 3.61348
test error: 4.60147

15 itns
-------
training error: 3.60094
test error: 4.58815

20 itns
--------
training error: 3.60094
test error: 4.58815
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: Success! Error dropped as kmeans converged further&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE - data prep, analyzing clustering</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/20/work-log"/>
   <updated>2014-05-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/20/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Appending radiation data to full data table&lt;/h2&gt;

&lt;p&gt;Ran into some issues needing refactoring when trying to add radiation date columns to fire_all.csv.  The work was slow-going, because the parsing code is so incredibly slow (2-3 minutes to read and write all the data).   I managed to speed it up slightly by skipping the missing data check when no missing keys are specified.&lt;/p&gt;

&lt;p&gt;Some of the bugs fixed:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Enumeration columns weren&#39;t catching misparse errors, thus, some misparsed columns weren&#39;t being caught.&lt;/li&gt;
&lt;li&gt;Enums fail to write correctly.&lt;/li&gt;
&lt;li&gt;Refactored to allow writing of enumation columns as text.&lt;/li&gt;
&lt;li&gt;Found and fixed bugs in handling missing data and case sensitivity.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Finally (!) merged and committed radiation columns into fire_all.csv.  Committed and wrote a summary for Warren.&lt;/p&gt;

&lt;h2&gt;Analyzing cluster membership&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Are the membership values output by our clustering code related to treatment type?&lt;br/&gt;
&lt;strong&gt;Method&lt;/strong&gt;: Modify preprocessing code to output a matlab struct instead of a text file.  Modify clustering code to output memberships as a text file instead of a color image.  Visualize both cluster membership and ground truth treatment type and compare.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;table class=&quot;data&quot;&gt;
&lt;tr&gt;&lt;th&gt;Treatment&lt;/th&gt;&lt;th&gt;p(cluster == 1 | treatment)&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Overall&lt;/td&gt;&lt;td&gt;74.7%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;None&lt;/td&gt;&lt;td&gt;95.5%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Radiation only&lt;/td&gt;&lt;td&gt;44.0%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Chemotherapy only&lt;/td&gt;&lt;td&gt;73.9%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Both&lt;/td&gt;&lt;td&gt;85.2%&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;


&lt;p&gt;So, cluster 3 strongly corresponds to &quot;some treatment&quot; and most likely implies radiation only.&lt;/p&gt;

&lt;h2&gt;Next steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Improve clustering and modeling results.

&lt;ul&gt;
&lt;li&gt;Compute fitting and testing error.&lt;/li&gt;
&lt;li&gt;Repeated runs of kmeans (trivial to try)&lt;/li&gt;
&lt;li&gt;Sampling observation parameters&lt;/li&gt;
&lt;li&gt;Sampling of membership (should be easy)&lt;/li&gt;
&lt;li&gt;Sampling of linear model parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/19/work-log"/>
   <updated>2014-05-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Memory leak in kjb_write_image&lt;/h2&gt;

&lt;p&gt;Found a memory leak in kjb_write_image.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;convert_image_file_from_raster
-&gt; kjb_system
-&gt; create_system_command_process
-&gt; kjb_fork&lt;/p&gt;

&lt;h2&gt;Clustering with centered data&lt;/h2&gt;

&lt;p&gt;Modified &lt;code&gt;inprogres/process_treatment_dates.m&lt;/code&gt; to log-transforms and normalize data. Re-ran clustering. Some observations:&lt;/p&gt;

&lt;p&gt;First, noise standard deviation dropped from 25 to 17.  The change is surprisingly subtle, considering we significantly reduced the dynamic range of the data.  I should probably investigate this more...&lt;/p&gt;

&lt;p&gt;Second, observation offset was on the order of 1e5.  Should be near zero.  Definitely investigate this more.&lt;/p&gt;

&lt;p&gt;Centering did seem to fix cluster collapse, and the memberships were a bit more evenly distributed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-19-clusters_centered.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Investigating large observation offset.&lt;/h2&gt;

&lt;p&gt;Run 1: force offset to zero.  does noise variance decrease?&lt;br/&gt;
Result: RMS error increased from ~17 to ~19.&lt;/p&gt;

&lt;p&gt;looks  like a bug in preprocessing&lt;/p&gt;

&lt;p&gt;Found bugs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;was normalizing on all data, not on rows.&lt;/li&gt;
&lt;li&gt;was dividing by mean (i.e. 0) not standard deviation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;preprocessed data looks much better now.&lt;/p&gt;

&lt;p&gt;New bug: not enabling missing data. Fixing and rerunning...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;RMS error is down to 0.4 for global regression model.  Results for line-fitting in three regions is shown below (blue=before, green=during, red=after)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-19-single_cluster_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This seems in the ballpark.  Full results below&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  0.00000000e+00  0.00000000e+00
  7.22085911e-01  2.02331116e+00  1.04881185e+00
  7.22916954e+01 -2.79730868e+02  1.41688548e+02
 -2.56235739e-04  4.05002380e-04  1.79111799e-03  6.25535845e-04 -6.55416611e-04  8.34133235e-05  1.67634110e-03
  2.49784115e-01  6.06155787e-01 -1.52977199e+00  1.15511031e+00  6.25982470e-01  4.20970189e-01 -5.78777462e-01
0.473467
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that observation offset (5th line) is nonzero, even though we centered it.  That&#39;s because this is the offset for the regression model, which is able to reveal more structure by taking time into account.  This explains the lower-than-one standard deviation (6th line).  The observation scaling (4th line) is interesting -- IFN-g and IL-8 are negataed, and TNF-a is tiny.  Largest apparent activity in IL-1B and IL-2.  I&#39;m guessing this could change significantly once we start sampling the observation parameters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clustering results&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;K-means converges after 6 iterations.&lt;/p&gt;

&lt;p&gt;Cluster memberships:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-19-clusters_centered_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Only two strong clusters, despite using a three-cluster model.&lt;/p&gt;

&lt;p&gt;Full results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Observation (global):
A: -2.56235739e-04  4.05002380e-04  1.79111799e-03  6.25535845e-04 -6.55416611e-04  8.34133235e-05  1.67634110e-03
B:   2.49784115e-01  6.06155787e-01 -1.52977199e+00  1.15511031e+00  6.25982470e-01  4.20970189e-01 -5.78777462e-01
eps: 0.473467

cluster #1
m:  7.72956909e-01  2.23224762e+00  8.57568833e-01
b:  4.86565183e+01 -3.35063295e+02  3.15151426e+02

cluster #2
m:  9.18374118e+00  0.00000000e+00  0.00000000e+00
b: -5.45940758e+02  0.00000000e+00  0.00000000e+00

cluster #3
m: -3.69070988e+00 -3.02755277e+00  1.64214653e+00
b:  2.82195009e+02  3.96896877e+02 -2.56559679e+02
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Cluster 2 (the trivial cluster) seems to only have ata for the first region, which explains why it&#39;s ideosyncratic.&lt;/p&gt;

&lt;p&gt;Clusters 1 and 3  differ dramatically in slope; cluster three drops significantly in the first two regions.&lt;/p&gt;

&lt;p&gt;Do these clusters align closely with treatment type?  (do tomorrow)&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Are cluster memberships linked to treatment type?

&lt;ul&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;matlab preprocess into struct, not file.  then save struct to file&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;use struct to visualize treatment type&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;save cluster membership to file, visualize in matlab next to (2.)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;repeat k-means multiple times with random initializations.&lt;/li&gt;
&lt;li&gt;handle different treatment types.&lt;/li&gt;
&lt;li&gt;do full sampling.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>FIRE - immunity data transformations</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/18/reference"/>
   <updated>2014-05-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/18/reference</id>
   <content type="html">&lt;p&gt;this is a follow-up to [this earlier post], in which immunity data is plotted as histograms for each &quot;plate&quot;.&lt;/p&gt;

&lt;p&gt;After log-transforming the variables the extreme outliers now look more sensible, and the distributions are far more symmetric, with the exception of a few TNF-\(alpha\) plates.&lt;/p&gt;

&lt;script&gt;
    $(function() {
        $( &quot;#plate_hist_tabs&quot; ).tabs();
    });
&lt;/script&gt;


&lt;div id=&quot;plate_hist_tabs&quot;&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#ifng_plates&quot;&gt;IFN-g&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#tnfa_plates&quot;&gt;TNF-\(\alpha\)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#IL1B_plates&quot;&gt;IL-1b&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il2_plates&quot;&gt;IL-2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il6_plates&quot;&gt;IL-6&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il8_plates&quot;&gt;IL-8&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il10_plates&quot;&gt;IL-10&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;div id=&quot;ifng_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-18-ifng_plates.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il10_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-18-il10_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL1B_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-18-il1b_plates.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il6_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-18-il6_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il8_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-18-il8_plates.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;tnfa_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-18-tnfa_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il2_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-18-il2_plates.png&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>FIRE - cleanup</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/16/work-log"/>
   <updated>2014-05-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/16/work-log</id>
   <content type="html">&lt;h2&gt;Skype w/ Kobus&lt;/h2&gt;

&lt;p&gt;Reflecting on &lt;a href=&quot;/ksimek/research/2014/05/15/reference&quot;&gt;immunity histograms&lt;/a&gt; I created late last night.  In discussion with Kobus, we&#39;ve decided a few things could help: (a) log-transforming, (b) introducing a scaling factor, and (c) using robust likelihood (e.g. truncated squared error).&lt;/p&gt;

&lt;p&gt;Will Skype with Warren on Monday afternoon to hand off the self-report clustering analysis.  Will spend this afternoon cleaning up the data and code to make it easier for him to pick it up.&lt;/p&gt;

&lt;p&gt;Kobus will likely need my input in the coming weeks to incorporate branching Gaussian processes into an upcoming grant proposal.&lt;/p&gt;

&lt;h2&gt;Self-report clustering cleanup&lt;/h2&gt;

&lt;p&gt;Add documentation and READMES for clustering, experiment directory, output files.&lt;/p&gt;

&lt;p&gt;Add matlab script to visualize clustering results.&lt;/p&gt;

&lt;p&gt;Created &lt;a href=&quot;/ksimek/research/2014/05/16/reference/&quot;&gt;reference page&lt;/a&gt; for clustering results.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE - Self-report clustering results.</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/16/reference"/>
   <updated>2014-05-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/16/reference</id>
   <content type="html">&lt;p&gt;Results of initial clustering of self-report data.  (I showed these a few weeks ago, but never recorded them here).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;: Five survey values, recorded once per each of nine visits, for a total of 45 dimensions per cluster.  Missed visits are treated as missing data and ignored during inference.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model, Inference&lt;/strong&gt;: Multivariate gaussians with diagonal covariance were fit to each cluster.  Cluster memberships and cluster parameters were fit with EM, with 100 different initializations to avoid local minima.  Number of clusters was varied between 2 and 50, and the optimal choice of three clusters was determined using BIC.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Means&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-16-cluster_means.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Standard Deviations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-16-cluster_std_devs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Disucssion&lt;/h2&gt;

&lt;p&gt;PSS is in the 0.9-1.0 range for all clusters.&lt;/p&gt;

&lt;p&gt;Note that FACT data has high standard deviation, making its curves harder to interpret.&lt;/p&gt;

&lt;p&gt;Also note that unlike other variables, high FACT scores correspond to &lt;em&gt;negative&lt;/em&gt; outcomes (e.g. high pain, high nausea, etc)&lt;/p&gt;

&lt;p&gt;Third cluster is the &quot;always good&quot; cluster.&lt;/p&gt;

&lt;p&gt;Second cluster is the &quot;high emotional acceptance cluster&quot;.  Notable that DAS and SRI start low, but increase over time.  This suggests a hyptothesis that high emotional acceptance can result in improved outcomes over time.&lt;/p&gt;

&lt;p&gt;First cluster is the &quot;low emotional acceptance cluster&quot;.  SRI and DAS start low and never improve.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE - first clustering test</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/15/work-log"/>
   <updated>2014-05-15T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/15/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;16806&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Run #1:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Run initial model estimation on real data for the first time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Single cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0.00000000e+00  0.00000000e+00
8.73210306e-03  6.75372609e-02  3.38471516e-02
1.88790755e+00 -1.88387427e+01 -9.88494611e+00
-8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
-5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Multiple cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;num_clusters:3
log weights: 0 0 0
cluster #1
  0.00000000e+00  0.00000000e+00
 -2.68876589e-01 -7.04390256e-02  1.45450526e-02
  6.16258430e+03  6.13067965e+03  6.12930057e+03
 -1.08771413e-02  5.76005556e-04 -1.04761899e-02 -1.69093096e-03  9.36991863e-04 -3.78348959e-04  1.27422332e-03
 -4.89152880e+01 -9.21454922e+00 -1.77681544e+01  2.47259001e+00  6.28886806e+00  8.63120843e+00 -2.43004449e+01
26.2893
cluster #2
  0.00000000e+00  0.00000000e+00
  1.20215477e-01  0.00000000e+00  2.24383835e-01
  6.11014579e+03  0.00000000e+00  5.96700128e+03
 -1.08771413e-02  5.76005556e-04 -1.04761899e-02 -1.69093096e-03  9.36991863e-04 -3.78348959e-04  1.27422332e-03
 -4.89152880e+01 -9.21454922e+00 -1.77681544e+01  2.47259001e+00  6.28886806e+00  8.63120843e+00 -2.43004449e+01
26.2893
cluster #3
  0.00000000e+00  0.00000000e+00
 -1.52115349e-01  2.77516102e+00  1.82492585e+00
 -3.84569849e+00 -7.18131761e+02 -1.63056790e+02
 -1.08771413e-02  5.76005556e-04 -1.04761899e-02 -1.69093096e-03  9.36991863e-04 -3.78348959e-04  1.27422332e-03
 -4.89152880e+01 -9.21454922e+00 -1.77681544e+01  2.47259001e+00  6.28886806e+00  8.63120843e+00 -2.43004449e+01
26.2893
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Surprisingly high epsilon (~26).  This is far beyond the dynamic range of the data, suggesting either (a) a bug, (b) a terrible model, or (c) failure of the analytical estimation method to find a good result.  Option (a) seems more likely, since a flat line give a lower error variance than this.  Perhaps our observation basis A was poorly estimated.  Lets re-run with PCA method.&lt;/p&gt;

&lt;h2&gt;Run #2: PCA method&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Re-run but using PCA instead of regression to estimate observation transformation, A.  (i.e. change constant &lt;code&gt;use_regression_method&lt;/code&gt; to false).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Single cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  0.00000000e+00  0.00000000e+00
  8.73210306e-03  6.75372609e-02  3.38471516e-02
  1.88790755e+00 -1.88387427e+01 -9.88494611e+00
 -8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
 -5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Multiple Cluster&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;num_clusters:3
log weights: 0 0 0
cluster #1
  0.00000000e+00  0.00000000e+00
  1.13041314e-02  6.75372609e-02  3.41203573e-02
  3.56421036e-01 -1.88387427e+01 -1.11922319e+01
 -8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
 -5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497
cluster #2
  0.00000000e+00  0.00000000e+00
  1.05708618e-03  0.00000000e+00  0.00000000e+00
  5.35534897e+01  0.00000000e+00  0.00000000e+00
 -8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
 -5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497
cluster #3
  0.00000000e+00  0.00000000e+00
  0.00000000e+00  0.00000000e+00  3.06188063e-02
  1.18920520e+02  0.00000000e+00  7.71079754e+01
 -8.09450747e-01 -2.56388154e-01 -3.61218358e-01 -4.23137123e-02  1.64628674e-02 -1.02962552e-02 -3.82633520e-01
 -5.26954930e+01 -9.01436620e+00 -2.14090141e+01  1.88492958e+00  6.61450704e+00  8.49971831e+00 -2.38576056e+01
24.6497
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;No noticable improvement.  During K-means, cluster collapse was frequent, which didn&#39;t occur in previous run.&lt;/p&gt;

&lt;h2&gt;Immunity data inspection&lt;/h2&gt;

&lt;p&gt;Below are histograms of raw immunity readings for each marker. TNF-\(\alpha\), IL-2, and IL-6 have sensible distributions.&lt;/p&gt;

&lt;p&gt;IL-1b and IL-10 have weirdly peaked distribution with heavy tails.  Perhaps outliers are isolated to specific plates (investigated next).&lt;/p&gt;

&lt;p&gt;IFN and IL-8 are borderline; peaked with heavy tails but not as bad as IL-1b and IL-10.&lt;/p&gt;

&lt;script&gt;
    $(function() {
        $( &quot;#hist_tabs&quot; ).tabs();
    });
&lt;/script&gt;




&lt;div id=&quot;hist_tabs&quot;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#IFN&quot;&gt;IFN-g&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#TNFa&quot;&gt;TNF-\(\alpha\)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL1B&quot;&gt;IL-1b&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL2&quot;&gt;IL-2&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL6&quot;&gt;IL-6&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL8&quot;&gt;IL-8&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL10&quot;&gt;IL-10&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
    &lt;div id=&quot;TNFa&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-tnfa.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL2&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL2.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL8&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL8.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL6&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL6.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL1B&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL1B.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IFN&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IFN.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL10&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL10.png&quot; /&gt;
    &lt;/div&gt;
  &lt;/div&gt;


&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;h2&gt;Per-plate distributions.&lt;/h2&gt;

&lt;script&gt;
    $(function() {
        $( &quot;#plate_hist_tabs&quot; ).tabs();
    });
&lt;/script&gt;


&lt;div id=&quot;plate_hist_tabs&quot;&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#ifng_plates&quot;&gt;IFN-g&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#tnfa_plates&quot;&gt;TNF-\(\alpha\)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#IL1B_plates&quot;&gt;IL-1b&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il2_plates&quot;&gt;IL-2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il6_plates&quot;&gt;IL-6&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il8_plates&quot;&gt;IL-8&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il10_plates&quot;&gt;IL-10&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;div id=&quot;ifng_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-ifng_plates.png&quot; /&gt;
    &lt;br /&gt;
    One of the worse plates in terms of missing/out-of-range data (331/710).  Irregular distributions, and plate 9 has no radings within range.
    &lt;/div&gt;
    &lt;div id=&quot;il10_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il10_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
        Another reasonably strong variable in terms of missing data (637/710).  Plots seem somewhat irregular, but possibly due to excessible outliers in plates 10, 7, 8, 4.  This could be good support for a clustering model if it would explain the heavy tails in these plates
    &lt;/p&gt;

    &lt;/div&gt;
    &lt;div id=&quot;IL1B_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL1B_plates.png&quot; /&gt;
    &lt;p&gt;
    In terms of missing data, this variable is borderline (550/710).  Plates seem to split between (a) seemingly exponential-distrubted data (2,3,4,7,8, 10), and (b) irregular data (plates 1,5,6,9).  I&#39;m doubtful that this will be useful for inference.
    &lt;/p&gt;
    &lt;p&gt;
    Note &lt;em&gt;massive&lt;em&gt; variation in support between plates.  e.g. plate 8 maxes out at 80, while plate 1 stops at 1.3.
    &lt;/p&gt;
    &lt;/div&gt;

    &lt;div id=&quot;il6_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il6_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
    Very little missing data in this dataset (699/710).  Seems much more consistent than most other datasets. Scale and shape vary a bit, but not glaring inconsistencies, aside from a few outliers (e.g. plate 9).
    &lt;/p&gt;
    &lt;p&gt;
    Lots of between-plate variability (plate 7 vs. 10).
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il8_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il8_plates.png&quot; /&gt;
    &lt;/div&gt;
    &lt;p&gt;
        Seems to be Gamma-distributed or log-normal distributed but with lots of big outliers.  Plates 5 and 10 have extreme outliers
    &lt;/p&gt;
    &lt;div id=&quot;tnfa_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-tnfa_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
    The best plate in terms of missing data (710/710), TNF-\(\alpha\) seems very regular within-plates.  Note the strong gaussian shapes here, compared to the full-dataset histogram earlier.  
    &lt;/p&gt;
    &lt;p&gt;
    Variation between plates is notable (see plate 5 vs. plate 8).  Supports a regularization approach.
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il2_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il2-plates.png&quot; /&gt;
    &lt;p&gt;
    This is one of the stronger plates in terms of missing data (710/710 present).  Several plates have decent gaussian distributions, albeit with heavy tails (1, 7, 9), but several are irregular (4, 5, 8, 9), and others look more exponential (2, 3, 10).
    &lt;/p&gt;
    &lt;/div&gt;
&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>FIRE immunity plots</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/15/reference"/>
   <updated>2014-05-15T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/15/reference</id>
   <content type="html">&lt;p&gt;Below are histograms of raw immunity readings for each marker.&lt;/p&gt;

&lt;p&gt;Aside from TNF-\(\alpha\), IL-2, and IL-6, distributions are very peaked and heavy-tailed. Specific-plate histograms (shown later) reveal more detail.&lt;/p&gt;

&lt;script&gt;
    $(function() {
        $( &quot;#hist_tabs&quot; ).tabs();
    });
&lt;/script&gt;




&lt;div id=&quot;hist_tabs&quot;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#IFN&quot;&gt;IFN-g&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#TNFa&quot;&gt;TNF-\(\alpha\)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL1B&quot;&gt;IL-1b&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL2&quot;&gt;IL-2&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL6&quot;&gt;IL-6&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL8&quot;&gt;IL-8&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#IL10&quot;&gt;IL-10&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
    &lt;div id=&quot;TNFa&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-tnfa.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL2&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL2.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL8&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL8.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL6&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL6.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL1B&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL1B.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IFN&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IFN.png&quot; /&gt;
    &lt;/div&gt;
    &lt;div id=&quot;IL10&quot;&gt;
        &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL10.png&quot; /&gt;
    &lt;/div&gt;
  &lt;/div&gt;


&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;h2&gt;Per-plate distributions.&lt;/h2&gt;

&lt;p&gt;Below are per-plate histograms of immunity readings, broken into plates.  Compared to the full-popuation histograms above, signfiicant regularity emerges in these plots, esp. in TNF-\(\alpha\), IL-8, IL-10.&lt;/p&gt;

&lt;script&gt;
    $(function() {
        $( &quot;#plate_hist_tabs&quot; ).tabs();
    });
&lt;/script&gt;


&lt;div id=&quot;plate_hist_tabs&quot;&gt;
    &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#ifng_plates&quot;&gt;IFN-g&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#tnfa_plates&quot;&gt;TNF-\(\alpha\)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#IL1B_plates&quot;&gt;IL-1b&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il2_plates&quot;&gt;IL-2&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il6_plates&quot;&gt;IL-6&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il8_plates&quot;&gt;IL-8&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#il10_plates&quot;&gt;IL-10&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;div id=&quot;ifng_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-ifng_plates.png&quot; /&gt;
    &lt;br /&gt;
    One of the worse plates in terms of missing/unreadable data (331/710).  Irregular between and within plates.
    &lt;/div&gt;
    &lt;div id=&quot;il10_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il10_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
        Another reasonably strong variable in terms of missing data (637/710).  Plots seem somewhat irregular, but possibly due to excessible outliers in plates 10, 7, 8, 4.  This could be a good argument for a clustering/mixture model, as it might explain the heavy tails in the aforementioned plates.
    &lt;/p&gt;

    &lt;/div&gt;
    &lt;div id=&quot;IL1B_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-IL1B_plates.png&quot; /&gt;
    &lt;p&gt;
    In terms of missing data, this variable is borderline (550/710).  Plates seem to split between (a) seemingly exponential-distrubted data (2,3,4,7,8, 10), and (b) irregular data (plates 1,5,6,9).  This could pose problems for our inference efforts.
    &lt;/p&gt;
    &lt;p&gt;
    Note &lt;em&gt;massive&lt;/em&gt; variation between plates.  e.g. plate 8 has a maximum at 60, while plate 1 stops at 1.3.
    &lt;/p&gt;
    &lt;/div&gt;

    &lt;div id=&quot;il6_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il6_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
    Very little missing data in this variable (699/710 observed).  Seems much more consistent than most other datasets. Scale and shape vary a bit, but no glaring inconsistencies, aside from a few outliers (e.g. plate 9).
    &lt;/p&gt;
    &lt;p&gt;
    Lots of between-plate variability (e.g. plate 7 vs. 10).
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il8_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il8_plates.png&quot; /&gt;
    &lt;p&gt;
        An excellent variable in terms of missing data (710 of 710 readings within range). The results seem very regular compared to other variables.  Seems to be Gamma-distributed or log-normal distributed but some big outliers.  Plates 5 and 10 have extreme outliers
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;tnfa_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-tnfa_plates.png&quot; /&gt;&lt;br /&gt;
    &lt;p&gt;
    The best plate in terms of missing data (710/710), TNF-\(\alpha\) seems very regular within-plates.  Note the strong gaussian shapes here, compared to the full-dataset histogram earlier.  
    &lt;/p&gt;
    &lt;p&gt;
    Variation between plates is notable (see plate 5 vs. plate 8).  Supports a regularization approach.
    &lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=&quot;il2_plates&quot;&gt;
    &lt;img src=&quot;/ksimek/research/img/2014-05-15-il2-plates.png&quot; /&gt;
    &lt;p&gt;
    This is one of the strongest plates in terms of missing data (710 of 710 observed).  Several plates have decent gaussian distributions, albeit with heavy tails (1, 7, 9), but several are irregular (4, 5, 8, 9), and others look more exponential (2, 3, 10). 
    &lt;/p&gt;
    &lt;/div&gt;
&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>FIRE - cluster w/ missing data</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/13/work-log"/>
   <updated>2014-05-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/13/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Testing missing data in cluster model&lt;/p&gt;

&lt;h2&gt;Run #1 - enable missing data&lt;/h2&gt;

&lt;p&gt;Segfault resulting from empty cluster.  Writing routine to create a cluster from worst point.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Still getting weird results.  Clusters are collapsing constantly.&lt;/p&gt;

&lt;p&gt;Even a &lt;em&gt;single&lt;/em&gt; missing value screws up results.  There must be a bug in my initial estimate script&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;BUG: true/false swap when determining whether to use missing-data-enabled line fitting&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Several bugs related to computing epsilon.  Fixed after several hours :-/&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;It seems we can continue to increase the missing percentage indefinitely, without the clustering suffering (or at least until an entire observation becomes missing, which isn&#39;t handled).&lt;/p&gt;

&lt;p&gt;Likely the small amount of noise is helping us a lot here.  We&#39;ll see how it works on real data.&lt;/p&gt;

&lt;h2&gt;Real FIRE data&lt;/h2&gt;

&lt;p&gt;High-level Tasks&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;merge radation data from Laura (into demograph dataset?)&lt;/li&gt;
&lt;li&gt;for each subject,
 first chemo
 last chemo
 first rad
 last rad&lt;/li&gt;
&lt;li&gt;write results in FIRE data format&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Reading and merging radiation data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;construct out_db cols: subject_ID, had_radiation&lt;/li&gt;
&lt;li&gt;if row has start and end date,

&lt;ul&gt;
&lt;li&gt;if out_db already has start or end date, record error&lt;/li&gt;
&lt;li&gt;else record start and end date&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;assert all &quot;radiation=yes&quot; have start and end date

&lt;ul&gt;
&lt;li&gt;can already see 57533 fails this test&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Do same for chemo dates.&lt;/p&gt;

&lt;p&gt;Merge chemo and rad.&lt;/p&gt;

&lt;p&gt;compute &quot;type&quot;&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;All implemented in &lt;code&gt;in_progress/process_treatment_dates.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Data consistency issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;some subjects surgeries occur after treatment

&lt;ul&gt;
&lt;li&gt;57527, 57563, 575139, 575145&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Only 97 of 136 subjects have immune data&lt;/li&gt;
&lt;li&gt;some subjects disagree about treatment type

&lt;ul&gt;
&lt;li&gt;57517 - no histo dates, but hist = 70&lt;/li&gt;
&lt;li&gt;575146, 575156 - demo:rad = yes, but no dates in rad_supl&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>FIRE - streamlining; missing data</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/12/work-log"/>
   <updated>2014-05-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/12/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;16784&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Refactored kjb::Matrix::resize to be re-use allocated space under more circumstances.  This is currently a bottleneck in inference, and signficantly improves runtime.&lt;/p&gt;

&lt;p&gt;Implemented Missing data handling:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Likelihood ignores missing values&lt;/li&gt;
&lt;li&gt;initial model estimation handles gracefully handles missing data&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;TODO:  Test missing data.  Randomly convert observations to missing, see if cluster parameters and memberships are still correctly estimated.  Is there a &quot;critical point&quot; above which missing data ruins results?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE Debugging cluster model</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/09/work-log"/>
   <updated>2014-05-09T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/09/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Initial cluster estimation, something like k-means.&lt;/p&gt;

&lt;p&gt;likelihood isn&#39;t monotonically increasing over time.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;we aren&#39;t updating cluster weights&lt;/li&gt;
&lt;li&gt;we&#39;re doing soft assignment&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;found bug:&lt;/p&gt;

&lt;h2&gt;Run #1: initialize by K-means (part 1)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: do k-means to initialize cluster assignments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Estimate A, B: no
estimate epsilon: no
shared observation model: yes
continuity constraints: yes
num clusters: 3
observed dimensions: 7
num observations: 150
A estimation method: regression
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Revision&lt;/strong&gt;: 16767&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: Build first cluster from all data.  Iteratively choose &quot;bad&quot; points under current model as prototypes for new cluster.  Then run k-means.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;:  Converges after 15 iterations.&lt;/p&gt;

&lt;p&gt;In the 2x150 image below, first row is ground truth clustering, second row is experimental results.  Specific coloring is irrelevant, grouping is.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-12-cluster_results_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Not great results.  Hopefully noise is just too high to find any useful structure without more observations.&lt;/p&gt;

&lt;p&gt;Interesting that the g.t. clustering is so uneven.  May want to increase Dirichlet distribtuion&#39;s alpha parameter.&lt;/p&gt;

&lt;h2&gt;Run 2:  Rerun with small noise&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;:  Re-run with noise lowered to 0.01.
&lt;strong&gt;Revision&lt;/strong&gt;:  16773
&lt;strong&gt;Results&lt;/strong&gt;:  Converges after 3 iterations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-12-cluster_results.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Much better!  We don&#39;t correctly find the &lt;em&gt;tiny&lt;/em&gt; middle cluster, which we can hopefully find during HMC.&lt;/p&gt;

&lt;p&gt;TODO: Return to this model when testing HMC&lt;/p&gt;

&lt;h2&gt;Aside: speeding up Matrix::resize(); adding Matrix::realloc()&lt;/h2&gt;

&lt;p&gt;The documentation for Matrix::resize claims it reuses allocated space, which is actually untrue.  Spent some time playing with reimplementations before realizing any good solution will require reworking the C library, which would require more precision and care than I have the time for at the moment.&lt;/p&gt;

&lt;p&gt;Instead, made two halfway measures:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;added spacial case; space is reused iff new_cols == num_cols &amp;amp;&amp;amp; new_rows &amp;lt;= num_rows.&lt;/li&gt;
&lt;li&gt;Added new function: Matrix::realloc, which is like resize, but always preserves allocated storage if possible, and doesn&#39;t guarantee data is preserved afterward.&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Run 3: Rerun with unknown observation model&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: same&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE = Cluster model</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/08/work-log"/>
   <updated>2014-05-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Implemented cluster model in &lt;code&gt;cluster_model.{cpp,h}&lt;/code&gt;.  Implemented synthetic data generation in &lt;code&gt;synthetic.{cpp,h}&lt;/code&gt;.  Working on initial model estimation using k-means.&lt;/p&gt;

&lt;h2&gt;Run #1:  Adding estimation of epsilon.&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: After estimating &#39;A&#39;, also estimate the noise scale, &lt;code&gt;epsilon&lt;/code&gt; (assumed 1.0 until now).&lt;br/&gt;
&lt;strong&gt;Method&lt;/strong&gt;: Project data onto &#39;A&#39; in data space, take mean projection error (see kjb_c::project_rows_onto_basis).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Using known epsilon: 0.5
-------------------
Training error: -45.3955
Prediction error: -46.5

Estimating epsilon: 0.46192
----------------------
Training error: -45.7958
Prediction error: -47.1319
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: in the ballpark!&lt;/p&gt;

&lt;h2&gt;Run #2: initial cluster estimation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Iteratively estimate three clusters and assign memebership.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issues&lt;/strong&gt;: third cluster initialization is identical to second.  Guess: it&#39;s picking the same bad point over and over.&lt;/p&gt;

&lt;h2&gt;Run #3: initial cluster estimation, per-cluster obs. model&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Like Run #2, but each cluster has an individual observation model&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE - continuous model, clustering</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/07/work-log"/>
   <updated>2014-05-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/07/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;unknown (see text)&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Today I finished run #10 from yesterday, in which I tried whitening the data before doing inference.  The results were inconclusive but not encouraging.&lt;/p&gt;

&lt;p&gt;I need to clean up the code which has gained some cruft from yesterday&#39;s tests.  I&#39;m removing the whitening option, since it doesn&#39;t help with synthetic data, but I&#39;ll keep the whtening function for testing on real data later.&lt;/p&gt;

&lt;h2&gt;Run 1: contiguous model&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Introduce piecewise continuity constriants.   Re-run and evaluate prediction error.&lt;br/&gt;
&lt;strong&gt;Method&lt;/strong&gt;:  add a special case to Piecewise_linear_model.  If b.size() == 1, assume contiguous model and infer the other b&#39;s on the fly.  Initial model estimate uses the same code as the non-continouous model, and simply resizes b.x.b to 1 afterward.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Ground Truth
--------------
Training error:   88.6756
Prediction error: 90.3623

Initial Model
------------
Training error:   88.6303
Prediction error: 90.4582

Best Model
----------
Training error:   88.6268
Prediction error: 90.4576
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The prediction error of the estimated model is extremely close to that of ground truth.  This is a simpler model, so that probably explains improved prediction accuracy.&lt;/p&gt;

&lt;p&gt;It&#39;s notable that the best model is now different than the initial estimate.  That&#39;s because the continuity constraints make the problem not solvable analytically.&lt;/p&gt;

&lt;p&gt;Also notable is that training error for the estimated models is better than that of the ground truth.&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>Fire: 10 inference experiments</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/06/work-log"/>
   <updated>2014-05-06T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/06/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/fire.html&quot;&gt;FIRE&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Piecewise Linear Clustering (tests)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;fire/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;piecewise_linear/&amp;#8203;test&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Ran 10,000 iterations, keeping observation model fixed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: The three plots below are incorrect.  corrected plot follows.
Ground truth model:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_gt_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Initial model:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_initial_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Final model:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_final_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: There was a bug in my plotting code for the plots above. Here is the corrected plot, which is more sensible:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run1_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initial estimate is bad, esp y-offset.  Should review this code.&lt;/li&gt;
&lt;li&gt;Recovered results are noticably better.  Not perfect, but this may be because we&#39;re seeing the &lt;em&gt;final&lt;/em&gt; model, not the best one.&lt;/li&gt;
&lt;li&gt;Keeping observation model fixed seems to improve speed by two orders of magnitude (why???)&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Run 2:  Keep best model&lt;/h2&gt;

&lt;p&gt;Added a &#39;best_sample_recorder&#39; to keep track of the best model we&#39;ve seen.&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Observations&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Best model is pretty good&lt;/li&gt;
&lt;li&gt;Why is initial model so different from that in the previous run?  same model, same random seed.&lt;/li&gt;
&lt;li&gt;Keep in mind, on average only 1/3 of the [0,1] domain is represented in the dataset, because it&#39;s divided into &quot;before&quot;, &quot;during&quot; and &quot;after&quot; regions.&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;Discussion&lt;/h3&gt;

&lt;p&gt;Initial model uses a different observation model, so it&#39;s hard to say whether it&#39;s good or not.  I get the feeling that there&#39;s a lot of slack in this model, meaning several different combinations of observed and latent parameters can give nearly equal results.&lt;/p&gt;

&lt;p&gt;Also, initial model estimation doesn&#39;t correctly handle x-offsets for &quot;during&quot; and &quot;after&quot; regions.  TODO: fix this.&lt;/p&gt;

&lt;p&gt;Note that the noise epsilon is very large relative to the model&#39;s dynamic range.  It&#39;s hard to visualize, since the plots above are sent through an observation model before noise is added. But the observation model&#39;s scaling is around 0.5.&lt;/p&gt;

&lt;p&gt;Variability in the latent variable is around 0.3.  Observation model scales that to 0.15, then adds noise on the order of 1.0.  So signal-to-noise ratio is around 0.15 -- not great.  Luckilly we have lots of observations (150 people x 9 time points x 7 observed dimensions), but in our non-synthetic model, I hope our noise is smaller.&lt;/p&gt;

&lt;h2&gt;Run 3: Improved initial model estimate&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;SVN REVISION&lt;/em&gt;: 16742&lt;br/&gt;
&lt;em&gt;Description&lt;/em&gt;: Fixed initial model estimate by centering each region at x=0.  See &lt;code&gt;model.cpp:partition_observations()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;# iterations: 10,000 iterations&lt;br/&gt;
Running time: 0:18.53&lt;br/&gt;
Results:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Summary:&lt;br/&gt;
Initial model has changed, but initial offset is still way off.  Other models (final and best) are the same.&lt;/p&gt;

&lt;p&gt;Discussion:&lt;br/&gt;
Need to dig more into the initial estimation code.  Is boost&#39;s RNG seeded with current time?&lt;/p&gt;

&lt;h2&gt;Run 4: fixed observation parameters&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;SVN REVISION&lt;/em&gt;: 16743&lt;br/&gt;
Goal: See if fixing observation parameters (A, B) improve line-fits.  If so, issue is in observation parameters.  If not, issue is with line-fitting.&lt;br/&gt;
Running time: 0:18.26&lt;/p&gt;

&lt;p&gt;Results:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Discussion:&lt;br/&gt;
Bad results; issue is probably in line-fitting, not estimation of observation model.&lt;/p&gt;

&lt;h2&gt;Run 5: fixed observation parameters (take 2)&lt;/h2&gt;

&lt;p&gt;Description: Found a bug when using fixed offset B -- wasn&#39;t subtracting offset before doing PCA to find A.&lt;br/&gt;
Revision: 16744&lt;br/&gt;
Results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Discussion:
No change.  In retrospect, this change only applies when not using fixed observation parameters, so no change is to be expected.  But a good bug to fix for later on!&lt;/p&gt;

&lt;h2&gt;Run 6:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;:&lt;br/&gt;
Found another bug: when projecting points onto pincipal component, if the direction vector \(d\) isn&#39;t normalized to 1, the projected points are off by a factor of \(|d|^2\).   The observation equation is given by:&lt;/p&gt;

&lt;div&gt;
\[
y = Ax + B
\]

The goal is to solve for \(x\), which means we need the Moore-Penrose pseudoinverse, \(A^+ = (A^\top A)^{-1} A \).  When \(A\) is a column vector, the term in perentheses on the right-hand size is the squared magnitude of \(A\), which was ommitted in the original equation.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Revision&lt;/strong&gt;:  16745&lt;br/&gt;
&lt;strong&gt;Invocation&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# On bayes01
cd ~ksimek/src/fire/src/piecewise_linear/test
./test_inference &amp;gt; /dev/null
cat results.txt | \
    awk &#39;{row=NR%7; if(row == 3 || row == 4) print;}&#39; \
    &amp;gt; ~/tmp/lin.txt

# on local machine
rsync -avz v01:tmp/lin*.txt ~/tmp/

# in matlab on local machine
cd ~ksimek/work/src/fire/src/matlab/in_progress
figure
lin = load(&#39;~/tmp/lin7_1.txt&#39;)&#39;
lin3 = reshape(lin, [3, 2, 4])
visualize_pl_result(lin3, ...
    {&#39;ground truth&#39;, ...
    &#39;initial model&#39;, ...
    &#39;final model&#39;, ...
    &#39;best model&#39;})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;br/&gt;
&lt;img src=&quot;/ksimek/research/img/2014-05-06-run6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion:&lt;/strong&gt;&lt;br/&gt;
Finally getting good initial estimates.  In fact, initial estimate is the &lt;strong&gt;optimal estimate&lt;/strong&gt; when A and B are known.  The best model doesn&#39;t look perfect, especially in the red curve, but there seems to be significant variance in the red curve estimator, as shorn in &quot;final model&quot;.&lt;/p&gt;

&lt;h2&gt;Run 7: Initial estimate of A&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Time to add initial estimation of A.  Sampling will still only estimate latent parameters.    I&#39;m curious how close this will be to optimal.  I&#39;m forcing the ground-truth A to have unit-length so the experimental results are comparable to the ground truth.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Invocation&lt;/strong&gt;: see previous&lt;br/&gt;
&lt;strong&gt;Revision&lt;/strong&gt;: 16746&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Baseline&lt;/strong&gt;:  The results below are estimates of the linear model when A is known.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run7_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Below are results of sampling, using an estimate of A from the noisy data&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run7_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion:&lt;/strong&gt;&lt;br/&gt;
Initial estimate is slightly worse in the green and blue curves, but still in the ballpark, as we would hope.  As observation noise decreases, this estimate should improve.&lt;/p&gt;

&lt;p&gt;It&#39;s notable that HMC still can&#39;t find a better model.  This is good news for the quality of our estimate.&lt;/p&gt;

&lt;h2&gt;Run 8: initial estimate of A and B&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;:  Adding initial estimation of B to the test.&lt;br/&gt;
&lt;strong&gt;Invocation&lt;/strong&gt;: see previous&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-05-06-run_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;br/&gt;
Much worse.  This could be caused by overestimating the magitude of (B), while understimating the magnitude of (b) (which can result in identical models due to our model being overdefined).  Since (b) currently adds positive offset to all observations, estimating (B) as the mean over observations could is likely to capture some of (b).&lt;/p&gt;

&lt;p&gt;Probably the best way to evaluate is to measure prediction error.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt; run 9 shows that this model predicts quite well, despite having different model parameters.&lt;/p&gt;

&lt;h2&gt;Discussion - Scaling and offset&lt;/h2&gt;

&lt;p&gt;I&#39;m starting to think we should consider preprocessing the immunity data bafore passing it through this model, for two reasons&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. poorly scaled data; correlated noise&lt;/strong&gt;&lt;br/&gt;
It seems too easy that we can solve for &#39;A&#39; using simple PCA.  Said differently, dynamics are the interesting part of our model, but A is determined irrespective of dynamics.&lt;/p&gt;

&lt;p&gt;The observation transformation A has an intuitive interpretation as the direction of greatest variation in the data, after taking dynamics into account.  The problem with this is that if data is badly scaled, the direction of greatest variation will come from the noise, not the dynamics.   If possible, wed prefer to separate variation due to noise from variation due to dynamics.  That way, A can capture the dynamic relationships between variables (e.g. IL-6 and IL-5 both start low and end high), not just correlation (IL-6 and IL-5 co-vary, but no temporal information).&lt;/p&gt;

&lt;p&gt;For this reason, it might help to &quot;whiten&quot; the data by PCA, so the inferred direction of A is more likely to capture dynamics only.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Variability in immunity measurements&lt;/strong&gt;&lt;br/&gt;
Second, Ive been thinking on the fact that individuals immunity values differ significantly in scale and offset.  From what I understand, much of this variation occurs within plate, i.e. its a byproduct of laboratory conditions, not immunity dynamics.   This could affect our clustering, as the latent slope and y-intercept parameters depend directly on the datas scale and offset.  As a result, I fear our learned clusters will reflect different groups of laboratory conditions, rather than different groups of immunity dynamics.&lt;/p&gt;

&lt;p&gt;We could replace readings with z-score within each person, but this has two problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;readings that are relatively constant but legitimately high would seem more typical than they are.&lt;/li&gt;
&lt;li&gt;extremely steep changes over time would tend to flatten. similarly, relatively low slopes would tend to look higher after transforming.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;No obvious answer; may need to try several solutions.&lt;/p&gt;

&lt;h2&gt;Run 9: Held-out error &lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: Since our model is overdefined, comparing model parameters is no longer a good indicator of model quality.  Instead, we should held-out data to evaluate the predictive power of the model&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: hold out 20% of data, measure log-likelihood for (a) ground truth model, (b) initial model estimate.  Two cases: 1. using global mean for B, PCA for A, 2. using ground truth A and B.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Training error:
------------

    Ground truth:              -13301.3
    Ana. Soln (Known A, B):    -13295.2
    Ana. Soln (Inferred A, B): -13318.3

Testing error:
-----------

    Ground Truth:               -1886.17
    Ana. Soln (Known A, B):     -1887.3
    Ana. Soln (Inferred A, B):  -1887.06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Training error is &lt;em&gt;better&lt;/em&gt; for analytical solution with known A and B, which what we&#39;d expect if the analytical solution was the true optimum.  When A and B are inferred, training error is worse, since PCA doesn&#39;t account for dynamics.&lt;/p&gt;

&lt;p&gt;Prediction error is worse than ground truth for all models, which is a good sanity check.  But they&#39;re all in a very tight ballpark (less than 1.0), which suggests that we aren&#39;t overfitting.  A better test would be to compare against one or more simpler models, but these results are good enough to proceed.&lt;/p&gt;

&lt;p&gt;One good conclusion is that even though inferring A and B results in vastly different latent models (see run #8), the prediction is just as good.&lt;/p&gt;

&lt;h2&gt;Run 10: Whitening before inference&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;: transforming observations so they follow a standard m.v. Gaussian may help in infering.  Test prediction error and compare against run #9 errors (should be nearly identical).
&lt;strong&gt;Revision&lt;/strong&gt;: 16756&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: transform data by PCA.  Using linear regression to infer initial direction (since PCA is out).  Force B to be zero.  Fit model.  Evaluate held-out error after transforming back to data space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(&quot;Error&quot; is average negative log likelihood over 150 points)

Ground truth
------------
    Training error: 88.6756
    Prediction error: 90.3623

Estimated Model
--------------
    Raw data; A,B from PCA
    ---------
    Training error: 88.7887
    Prediction error: 90.5986

    Raw data; A,B from Regression
    ------------
    Training error: 88.6285
    Prediction error: 90.4405

    Whiteened Data; A,B from Regression
    --------------
    Training error: 93.8502
    Prediction error: 95.7252
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;:&lt;br/&gt;
Training and prediction error is worse for whitened data, which is contrary to my expectation.  In retrospect, we&#39;re fitting to different data, so it&#39;s not too surprising that it predicts worse.  The story may be different if the data is truely poorly scaled.  Our example data was already well scaled, so its not a great test.&lt;/p&gt;

&lt;p&gt;But in the end, its defnitely clear that whitening isn&#39;t innocuous, and possibly harmful.&lt;/p&gt;

&lt;h2&gt;Mid-term TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Tuning HMC&lt;/li&gt;
&lt;li&gt;refactor model

&lt;ul&gt;
&lt;li&gt;Start and end times out of model&lt;/li&gt;
&lt;li&gt;epsilon per-dimension&lt;/li&gt;
&lt;li&gt;missing data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add offset constraints (continuous model)&lt;/li&gt;
&lt;li&gt;clustering&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Meta-notes on experiments&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;always test on synthetic data first&lt;/li&gt;
&lt;li&gt;always test on a simpler model first (fix some parameters)&lt;/li&gt;
&lt;li&gt;always have a visualization in mind when running a test.&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>FIRE - piecewise linear inference</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/05/work-log"/>
   <updated>2014-05-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/05/work-log</id>
   <content type="html">&lt;p&gt;Finished compiling inference test with synthetic data.&lt;/p&gt;

&lt;p&gt;Gradient is taking incredibly long, especially for a 7-dimensional model.  PErhaps 100 data points is too large, but I&#39;m guessing the cost of allocating vectors for each function evaluation is the botteneck (GDB seems to agree).&lt;/p&gt;

&lt;p&gt;&lt;div&gt;
&lt;strong&gt;Time to run 1 iteration&lt;/strong&gt;&lt;br/&gt;
&lt;em&gt;Single threaded, on bayes01&lt;/em&gt;
&lt;table border=&quot;1&quot;&gt;
&lt;tr&gt;
&lt;td&gt;Baseline &lt;br/&gt;
(Debugging mode, allocation checking enabled)&lt;/td&gt;
&lt;td&gt;
1:28.94
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Heap checking disabled
&lt;/td&gt;
&lt;td&gt;
0:21.06 (-1:07.88, 4.22x)
&lt;/td&gt;
&lt;tr&gt;
&lt;td&gt;Heap &amp;amp; initialization checking disabled
&lt;/td&gt;
&lt;td&gt;
0:11.19 (-9.87 1.88x)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PRODUCTION=1 (with -O2)
&lt;/td&gt;
&lt;td&gt;
0:07.24 (-3.95 1.55x)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-O3
&lt;/td&gt;
&lt;td&gt;
0:07.86 (+0.62 0.92x)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;Used gprof with grpof2dot.py to get the following diagram:
&lt;a href=&quot;/img/2014-05-05-gprof_1.pdf&quot;&gt;gprof.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;iso_mvn_lpdf is getting hit hard:&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;c&quot;&gt;    &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;iso_mvn_lpdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M_PI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;It&#39;s already pretty lean (no alloc, all c-style).  But we can move the divide-by-epsilon out of the loop for an easy 1.8x speedup.&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;c&quot;&gt;    &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;iso_mvn_lpdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M_PI&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&lt;/em&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Now the bottleneck is all the allocation, copying and freeing of kjb::Vector temporaries.&lt;/p&gt;

&lt;p&gt;I tweaked the code for evaluating a piecewise linear function to avoid creating kjb::Vector temporaries, and running time dropped dramatically &lt;strong&gt;from 11.4s to 1.26s.&lt;/strong&gt;  This is in production mode, so its surprising that more temporaries aren&#39;t optimized out.&lt;/p&gt;

&lt;p&gt;GProf after eliminating temporaries: &lt;a href=&quot;/img/2014-05-05-gprof_2.pdf&quot;&gt;gprof.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since 1 iterations takes about 1.2s, bumping up to 10 iterations.&lt;/p&gt;

&lt;p&gt;Remaining speed-up opportuntiies:  exploit gradient independence, parallel gradient&lt;/p&gt;

&lt;h2&gt;Parallel gradient&lt;/h2&gt;

&lt;p&gt;Enabled 8-way parallel gradient evaluation, and got &lt;strong&gt;worse&lt;/strong&gt; performance!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;single threaded:  0:09.63
multi threaded: 0:15.48
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This despote &lt;code&gt;top&lt;/code&gt; displaying 550% CPU utilization.&lt;/p&gt;

&lt;p&gt;Maybe gnuprof is affecting performance&lt;/p&gt;

&lt;h2&gt;Tuning&lt;/h2&gt;

&lt;p&gt;step size
gradient size&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/05/01/work-log"/>
   <updated>2014-05-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/05/01/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/30/work-log"/>
   <updated>2014-04-30T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/30/work-log</id>
   <content type="html">&lt;p&gt;Tucson day.&lt;/p&gt;

&lt;p&gt;Attended Anh&#39;s dissertation defense.&lt;/p&gt;

&lt;p&gt;Sat in on Jinyan&#39;s practice Talk.&lt;/p&gt;

&lt;p&gt;Finished coding model, data, and inference parts of FIRE piecewise linear model.  Started driver implementation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/29/work-log"/>
   <updated>2014-04-29T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/29/work-log</id>
   <content type="html">&lt;p&gt;Submitted &lt;a href=&quot;https://code.google.com/p/yamlmatlab/issues/detail?id=14&amp;amp;thanks=14&amp;amp;ts=1398783433&quot;&gt;bug report and patch&lt;/a&gt; to YAMLMatlab, which fixes an issue where row vectors weren&#39;t being read correctly.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Started working on piecewise linear model proposed by Kobus.  Wrote out initial equations, started initial coding.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ellipse and Line Segment Detector ; Experiment Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/28/work-log"/>
   <updated>2014-04-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/28/work-log</id>
   <content type="html">&lt;p&gt;Got &lt;a href=&quot;http://ubee.enseeiht.fr/vision/ELSD/&quot;&gt;ELSD&lt;/a&gt; compiling and running on OS X.  Details &lt;a href=&quot;http://ksimek.github.io/2014/04/28/compiling-elsd-on-osx/&quot;&gt;here&lt;/a&gt;.  &lt;a href=&quot;http://ksimek.github.io/misc/elsd_results.html&quot;&gt;Results were very nice&lt;/a&gt;; will try to use them for ECCV.&lt;/p&gt;

&lt;p&gt;Working on &lt;code&gt;publish_results&lt;/code&gt; function in matlab to store results in Experiment Log.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/27/work-log"/>
   <updated>2014-04-27T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/27/work-log</id>
   <content type="html">&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Auto SVN commit and record in run script&lt;/li&gt;
&lt;li&gt;m-file to build a run after the fact with comments&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/20/work-log"/>
   <updated>2014-04-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/20/work-log</id>
   <content type="html">&lt;p&gt;display visualization thumbnails
    how is visualization YAML laid out?
        2D array.  How is 2D array represented in YAML?
        Running on two datasets to see
            Added dataset 10 to list
            Dataset 10 has many missing views (why?  didn&#39;t I fix this?)
            Fixing unsuppressed output in process_gt2&lt;/p&gt;

&lt;h2&gt;TODO:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;investigate missing views in dataset 10&lt;/li&gt;
&lt;li&gt;test end-to-end with multiple datasets&lt;/li&gt;
&lt;li&gt;get visualization thumnails displaying&lt;/li&gt;
&lt;li&gt;implement visualization details page&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>TULIPS testing framework</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/16/work-log"/>
   <updated>2014-04-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/16/work-log</id>
   <content type="html">&lt;p&gt;Building calling framework.&lt;/p&gt;

&lt;p&gt;create experiment
run/rerun experiment&lt;/p&gt;

&lt;p&gt;questions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;which params to run first&quot;&lt;/li&gt;
&lt;li&gt;what do smoothness and continuity thresholds mean?&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Test suite</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/15/work-log"/>
   <updated>2014-04-15T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/15/work-log</id>
   <content type="html">&lt;p&gt;Building evaluations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;index spacing w.r.t. reconstruction spacing&lt;/li&gt;
&lt;li&gt;index discontinuity&lt;/li&gt;
&lt;li&gt;index smoothness&lt;/li&gt;
&lt;li&gt;&quot;accuracy&quot; w.r.t. triangulation&lt;/li&gt;
&lt;li&gt;point motion&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Building experiment engine: &lt;code&gt;experiments/exp_2014_04_14_gt_reconstruct.m&lt;/code&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/14/work-log"/>
   <updated>2014-04-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/14/work-log</id>
   <content type="html">&lt;p&gt;Added pomodoro reporting.&lt;/p&gt;

&lt;h2&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Email re: Friday seminar&lt;/li&gt;
&lt;li&gt;Fire report on clustering&lt;/li&gt;
&lt;li&gt;build tulips test suite
&lt;strong&gt; run wacv
&lt;/strong&gt; run tests on results
&lt;strong&gt;&lt;em&gt; index discontinuity
&lt;/em&gt;&lt;/strong&gt; index stretching
&lt;strong&gt;&lt;em&gt; distance from triangulation
&lt;/em&gt;&lt;/strong&gt; maximum motion
&lt;strong&gt; aggregate over all datasets
&lt;/strong&gt; export to HTML (summary and drill-down)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;code&gt;corr_to_likelihood_3.m&lt;/code&gt; is ripe for revisiting&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE meeting notes</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/11/work-log"/>
   <updated>2014-04-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/11/work-log</id>
   <content type="html">&lt;p&gt;The following is an unedited set of notes from Friday&#39;s meeting.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;multiplexes&lt;/p&gt;

&lt;p&gt;il-6 and Il-1 correlated with XXX.  Highest confidence&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;TNF-\alpha bad correlation with XXX  - will it stand up?  lower confidence&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Thad - harder to correlated with mind/body stimuli&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IFN \gamma - mostly out of range (low levels)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plates:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;some variability between plates.&lt;/li&gt;
&lt;li&gt;same person always had same plate.&lt;/li&gt;
&lt;li&gt;for each plate, we know what was the smallest readable value.&lt;/li&gt;
&lt;li&gt;should we replace OOR&amp;lt; with plate minimum?&lt;/li&gt;
&lt;li&gt;could we model it as a bound?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;properly report how immune markers correlate in our data?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rebecca has ICC values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Karen&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;IL-6 tends to increase as disease progresses&lt;/li&gt;
&lt;li&gt;shouldn&#39;t occur in our subjects, because disease isn&#39;t progressing.&lt;/li&gt;
&lt;li&gt;Some people had cancer return (12 died), so it might apply there.&lt;/li&gt;
&lt;li&gt;IL-6 and TNF-alpha seem to be linked to mortality in breast cancer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Absolute inflamatory markers aren&#39;t clinically calibrated, but covariation may be explanatory.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Th2 Th2?  Immune activation. ratio of Th1 and Th2 molecules.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;see early part of karen&#39;s paper.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Thad: Women being treated for health cancer are relatively healthy for 50&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;most studies are in the context of extreme stress and .&lt;/li&gt;
&lt;li&gt;radion and chemo may be

&lt;ul&gt;
&lt;li&gt;we see IL-6, etc firing in the first or second visit after in these events.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hypothesis: high levels of markers could maybe lead to psychological issues later.

&lt;ul&gt;
&lt;li&gt;fatigue / depression relationship&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;turning on inflamatory stuff doesn&#39;t really help things.  Sometimes cancer thrives on it.&lt;/li&gt;
&lt;li&gt;turning down inflamation before treatment can result in more effective radiotion treatment (karen: this is questionable)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can I get info on chemo treatment?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What variables do we care most about?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;time&lt;/li&gt;
&lt;li&gt;treatments&lt;/li&gt;
&lt;li&gt;other biological?&lt;/li&gt;
&lt;li&gt;have a partner?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CENTER ON TREATMENT&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TNF alpha, IL-1, and IL-6 (in that order) you&#39;d expect to see during cancer treatment&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;how long before observation did most recent treatment occur?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Radiation duration: 1 month  (sampling point could come before or during)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;have chemo dates, but not radiation dates.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;use &quot;days since treatment&quot; for chemo.&lt;/li&gt;
&lt;li&gt;Karen: we should have radiation dates too, double check.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>FIRE date analysis</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/10/work-log"/>
   <updated>2014-04-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/10/work-log</id>
   <content type="html">&lt;ol&gt;
&lt;li&gt;awk script to replace blanks with -99&lt;/li&gt;
&lt;li&gt;awk script to replace dates with epoch numbers&lt;/li&gt;
&lt;li&gt;decide which columns to use and how to pool&lt;/li&gt;
&lt;li&gt;clustering&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Something is wrong with the scripts that generate the database -- some values are missing from the goldStandardDateCRF key field. It&#39;s because some records exist solely to hold immunity data, which doesn&#39;t have an official gold standard date column.  Can probably ignore for now.&lt;/p&gt;

&lt;h2&gt;Clustering&lt;/h2&gt;

&lt;p&gt;For now, use pooled scores for each of 5 databses:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sri3.csv sritot
das4.csv dastot
ea4.csv  easumtot
fact4.csv facttot
pss4.csv psstot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The datasets were originally &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2014/03/06/work-log/&quot;&gt;named here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read data, isolated columns of interest, and concatenated each subjects&#39; visits into long rows, then write to csv file for processing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fire_data_path = &#39;../../data/&#39;
[db, db_meta] = fire_build_self_report_database(fire_data_path);
[cluster_db, cluster_db_meta] = fire_gather_columns(&#39;../../experiments/2014_04_10_cluster_3/columns.txt&#39;, db_meta, db);
wide_db = e_2014_03_merge_visits(cluster_db)
fire_write_csv(wide_db(:,2:end), [], &#39;test.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;EXPERIMENT PATH&lt;/strong&gt;: experiments/2014_04_10_cluster_3&lt;/p&gt;

&lt;p&gt;README file in experiment path describe how to prepare data and run experiment.&lt;/p&gt;

&lt;p&gt;Result: 3-cluster model, with apparently very good separation. reulsts are in $EXPERIMENT_PATH/out/cluster_results.txt&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/08/work-log"/>
   <updated>2014-04-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/08/work-log</id>
   <content type="html">&lt;p&gt;Rewrote training procedure from scratch and now getting much more sensible results.  The overall goal is to isolate parameters to try to make them as independent as possible; and then use one or two-dimensional optimization to estimate them.&lt;/p&gt;

&lt;p&gt;First, I hard-coded several variables.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rate variance is set to its theoretical value of 0.333 &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log/&quot;&gt;as described here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;perturb_rate_variance is now zero -- it seems to be causing weird results probably due to overfitting.  Results are very sensitive to small changes of rate perturbatyion.  Could re-introduce later.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Second, I estimated perturb_position_variance directly, by measuring the variance of all groups of initial points.&lt;/p&gt;

&lt;p&gt;Third, estimated noise and smoothness variance under the assumption of near-infinite position and rate variance.  This follows from the theory of cubic-spline models, which places no penalty over initial position and rate.  Used gradient-free minimizer, &lt;code&gt;fminsearch&lt;/code&gt;, which works well for two-dimensional problems.&lt;/p&gt;

&lt;p&gt;Fourth, estimated perturb smoothing and perturb scale, using noise and smoothness variance.&lt;/p&gt;

&lt;p&gt;Fifth, estimated position mean and variance directly.&lt;/p&gt;

&lt;p&gt;Note: all this is done using heuristic indexing, not the marginal likelihood optimization.  This is another key change from earlier training.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt;: Much more sensible perturb smoothing parameter (~1e-5 vs 1e-4) and perturb scale (2.06 vs 0.9).  End-to-end reconstruction is much more sensible; curve is centered among triangulated points.&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;train on run dataset with significant motion (wacv dataset #10; i.e. arab_6)&lt;/li&gt;
&lt;li&gt;iteratively update noise/smoothness and perturb smoothness&lt;/li&gt;
&lt;li&gt;iteratively update indexing after setting params&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;PERTURB_POSITION_VARIANCE MAY BE THE OVERFITTER!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/07/work-log"/>
   <updated>2014-04-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/07/work-log</id>
   <content type="html">&lt;p&gt;I&#39;ve been experimenting with alternative methods for inferring the index set and have several good options.  When equipped with a good index set, it seems most of the problems we&#39;ve been seeing disappear.  As sad as it seems, its seeming like index optimization should be tossed out, despite all the time I spent deriving the equations.&lt;/p&gt;

&lt;p&gt;I&#39;ve observed that too high of a perturb smoothing variance causes the mean curve to drift toward the linear-initial model.  Manually lowering perturb smoothing variance and raising smoothing variance causes better reconstruction and much higher marginal likelihood.&lt;/p&gt;

&lt;p&gt;Centering matters -- too low of a position_variance causes rate variance to take over, which causes weird results in some cases.&lt;/p&gt;

&lt;p&gt;Should refact tr_train to use new index estimation scheme; goal: getting better higher smoothness variance and lower perturb_smoothness variance&lt;/p&gt;

&lt;p&gt;Open question: is camera linearization beneficial in training?&lt;/p&gt;

&lt;p&gt;Refactored proces_tracks to recieve index estimation method.&lt;/p&gt;

&lt;p&gt;Refactored run_wacv_5 to use iterative index estimation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/04/work-log"/>
   <updated>2014-04-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/04/work-log</id>
   <content type="html">&lt;p&gt;Question: is curve stretching only present in last point?  If so, can we hack it away?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;detect index stretching (threshold on diff)&lt;/li&gt;
&lt;li&gt;for each, visualize with test_optim&lt;/li&gt;
&lt;li&gt;fun on full dataset, count (1) how many stretching occur and (2) how many are the last point?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Example&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[data, parents] = load_wacv_data(5, [397 530]&#39;, params, 1:4:36); %&#39;
[mu, Tracks, params] = get_wacv_result(5, params.model_type, &#39;deleteme_new4&#39;);
out = test_for_wandering_index(Tracks,20);
[wander_is_last, wander_view_num] = exp_2014_04_04_test_wandering_index(Tracks, data, params, 20)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Answer: Usually, wandering is present in last point, but not always.  Curve 1 in the example has 15 internal wandering events.  All other curves are last point.&lt;/p&gt;

&lt;p&gt;Question: Can we correct the bad indices in curve 1 by using a numerical hessian?&lt;/p&gt;

&lt;p&gt;Question: could we avoid internal wandering by preventing endpoint wandering?  e.g. is internal wandering occurring because endpoint wandering is so extreme?&lt;/p&gt;

&lt;p&gt;Question: does stretched point have higher likelihood than hand-picked index?  If so, what adjustments to params reverses this case?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Crash recovery; Index estimation - the saga continues</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/03/work-log"/>
   <updated>2014-04-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/03/work-log</id>
   <content type="html">&lt;p&gt;Machine crashed; need to get matlab environment back to where it was yesterday&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;load save tracks files&lt;/li&gt;
&lt;li&gt;re-run training (compare against any recorded values)&lt;/li&gt;
&lt;li&gt;re-run wacv with hessian disabled&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Done.  Process saved in &lt;code&gt;setups/setup_workspace_2014_04.m&lt;/code&gt; for future reference.&lt;/p&gt;

&lt;h2&gt;Index estimation&lt;/h2&gt;

&lt;p&gt;Using &lt;code&gt;wacv-2012/test_optim.m&lt;/code&gt; to visualize the correspondence between triangulated points and reconstructed curve.  This is a rough proxy for visualizing curve indices -- bad indices result in a bad reconstruction, and the matching will look bizarre.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;After much debugging, found that test_optim.m had a regressed, and was displaying a bad reconstruction&lt;/p&gt;

&lt;p&gt;I learned an excellent approach for visually assessing indices: run &lt;code&gt;test_optim&lt;/code&gt; with centered_mode=true and split_views=true.  The resulting plot will connect triangulated points to posterior points, WITH an arrow indicating the backprojection direction.  For each curve, rotate the plot so the backprojection arrow disappears (looking in the backprojection direction) -- the connecting line for that point should be nearly perpendicular to the reconstructed curve if the reconstruction is good.&lt;/p&gt;

&lt;p&gt;A second key observation: although minimizing the marginal likelihood does result in index shrinkage, the ratio of distances between adjacent points is equal to the ratio of differences between adjacent indices.  Thus, the scale component&lt;/p&gt;

&lt;p&gt;Third:  When bad index estimates cause terrible recponstructions, the per-view reconstructions actually look not bad (i.e. not marginalizing over offset perturbations).  Position of per-view reconstructed curves is suspect.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>TULIPS: index issue</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/02/work-log"/>
   <updated>2014-04-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/02/work-log</id>
   <content type="html">&lt;p&gt;Tested gradient and hessian -- is it correct with new model?  ... Yes.&lt;/p&gt;

&lt;p&gt;Is index optimization working?  Quick test:  ran this code twice, once using analytical hessian, once with hessian-free algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[mu_, Tracks_] = get_wacv_result(5, params_trained.model_type, &#39;deleteme_new2&#39;);
out = optimize_ml_wrt_indices(Tracks_(14), [], data, params_trained);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Analytical method halts early -- can&#39;t improve.  Final gradient is noticibly non-zero&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-04-02-grad_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hessian-free method seems very effective:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-04-02-grad_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Perhaps the optimization is failing; let&#39;s run end-to-end with hessian-free algorithm.  It will be super slow, but if it gives better results, we may have our solution.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[mu_, Tracks_] = get_wacv_result(5, params_trained.model_type, &#39;deleteme_new3&#39;);
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE: Verifying date bug, export-to-csv</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/04/01/work-log"/>
   <updated>2014-04-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/04/01/work-log</id>
   <content type="html">&lt;p&gt;Continuing the process of resolving the &quot;rotated dates&quot; issue, in which the last gold-standard date was shifted to the beginning of each visit-set.&lt;/p&gt;

&lt;p&gt;Laura has corrected the issue on her end, but we&#39;ve already made several changes to our copies of the databases, so I wrote a script to make the correction
on our end.  But there was some confusion about whether the original or updated &quot;Visit&quot; values are correct.  Laura has sent me her databases so I can
confirm that we&#39;re working with the same data, and hopefully I can determine if our visit values are correct and consistent with hers.&lt;/p&gt;

&lt;p&gt;Data is in MS access, so will need to install it on my virtual machine.&lt;/p&gt;

&lt;h2&gt;Export to csv&lt;/h2&gt;

&lt;p&gt;Finished merging records, confirming, and exporting to CSV.  Formalized the process into two files under &lt;code&gt;fire/src/matlab/scripts&lt;/code&gt;, which perform end-to-end converstion.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Fire: mergin immunity data</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/31/work-log"/>
   <updated>2014-03-31T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/31/work-log</id>
   <content type="html">&lt;p&gt;Having written code for guessing visit-number from dates in the Immunity dataset, I ran it and found several records with significant error.  Spent a long time inspecting each one in detail, and found lots of data-entry errors (wrong year, wrong month, transposed digits, miskeyed subject id&#39;s, etc).  In one case, the visit number was off-by-one in the self-report data, which was only apparent when the immunity data was introduced and an extra &quot;halways-between&quot; visit was present.   Was able to correct about 30 of these errors confidently; in the end only about 10 out of 700 records remained error greater than 10 days.&lt;/p&gt;

&lt;p&gt;Wrote code to merge new datasets into the full database; used it to merge newly-cleaned immunity data into the full database.&lt;/p&gt;

&lt;p&gt;Next step: write export-to-CSV routine&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Debugging Index optimization</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/27/work-log"/>
   <updated>2014-03-27T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/27/work-log</id>
   <content type="html">&lt;p&gt;Still getting terrible stretching due to extreme index drift during optimization.&lt;/p&gt;

&lt;h3&gt;Experiment 1&lt;/h3&gt;

&lt;p&gt;Theory: camera linearization is messing with our 3D point localization.  Propose solving by increasing noise covariance for noisy.&lt;/p&gt;

&lt;p&gt;Assuming an effective depth uncertainty of (h), and in-plane uncertainty of (w), the in-plane uncertainty of the adjusted covariance matrix (w&#39;) is given by&lt;/p&gt;

&lt;p&gt;[
w&#39; = w \mathrm{cos}^2 \theta + h \mathrm{sin}^2 \theta
]&lt;/p&gt;

&lt;p&gt;This is illustrated roughly below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-27-covariance_adjustment.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Implemented this using hand-set depth uncertainty (h = 3)mm.  Didn&#39;t resolve the issue for extremely stretched curves, but the weak stretching seems improved.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Discussion&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Noise variance is CRAZY low.  Small deviation from true curve is causing huge contortions to resolve.  Why is it trained this low if contradictions result?  This smells like overfitting, except we&#39;re testing and training on the same date at the moment so that seems to be ruled out.&lt;/p&gt;

&lt;p&gt;Loosen and retrain?&lt;/p&gt;

&lt;p&gt;We apparently need to multiply standard deviation by 4 to recover; according to &lt;a href=&quot;/ksimek/research/2014/03/25/work-log/&quot;&gt;yesterday&#39;s experiment&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Experiment #2&lt;/h3&gt;

&lt;p&gt;Plot likelihood (reference implementation) against noise variance.&lt;/p&gt;

&lt;p&gt;Apparently it does want a higher noise variance, but only 2.5x more, not 4x. is this enough to improve reconstruction?&lt;/p&gt;

&lt;p&gt;Nope.&lt;/p&gt;

&lt;h3&gt;Experiment #3&lt;/h3&gt;

&lt;p&gt;Repeat yesterday&#39;s &quot;Overextension&quot; experiment #1.  Multiplying variance by 16x seemed to fix issues, but now I suspect that we just got lucky.  Try a few values near 16x.&lt;/p&gt;

&lt;p&gt;With linearization enabled:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;15x terrible&lt;/li&gt;
&lt;li&gt;15.5x resolved&lt;/li&gt;
&lt;li&gt;16x resolved&lt;/li&gt;
&lt;li&gt;16.5x terrible&lt;/li&gt;
&lt;li&gt;50x terrible&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Without linearization:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;15x resolved&lt;/li&gt;
&lt;li&gt;15.5x resolved&lt;/li&gt;
&lt;li&gt;16x terrible&lt;/li&gt;
&lt;li&gt;16.5x  resolved&lt;/li&gt;
&lt;li&gt;50x resolved&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Yes, tiny tweaks to variance cause us to revert to bad reconstructions.  We probably just got lucky when we observed 16x working well yesterday.&lt;/p&gt;

&lt;p&gt;Now we&#39;re back to square one.  noise variance doesn&#39;t seem to be connected to this issue of extended&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Discussion&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It seems linearization is a bane to index refinement.  We introduced it to address reconstruction issues; perhaps we should only apply it in that case.&lt;/p&gt;

&lt;h1&gt;FIRE&lt;/h1&gt;

&lt;p&gt;Re-merging and rechecking new datasets&lt;/p&gt;

&lt;p&gt;Only one error, fixed and sent changes to Rebecca.&lt;/p&gt;

&lt;p&gt;Regularizing immunity CSV; adding new column type: &quot;numeric_missing:XXX&quot;, where missing data is represented by one of a few possible strings.  Split the &quot;date(other date)&quot; column into &quot;date&quot; column and &quot;other date&quot; column.  Parsed in matlab.  comparing immunity dates to self-report dates for each subject; interpolating as needed.   Only 9 notable anomalies out of 710.&lt;/p&gt;

&lt;p&gt;Next: discretize, measure and visualize discretization errors, export to csv&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Debugging reconstruction anomalies</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/25/work-log"/>
   <updated>2014-03-25T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/25/work-log</id>
   <content type="html">&lt;p&gt;Continuing from yesterday.  Issue: strange reconstruction results after retraining.&lt;/p&gt;

&lt;h2&gt;Issue: Reconstruction anomalies&lt;/h2&gt;

&lt;h3&gt;Experiment #1&lt;/h3&gt;

&lt;p&gt;Add a small amount to the diagonal of the covariance and re-run reconstruction.&lt;/p&gt;

&lt;p&gt;Result:  Adding a moderate amount of covariance to the diagonal seems to fix the results but probably due to additional smoothing.&lt;/p&gt;

&lt;p&gt;Adding 1.5 - makes results worse, bizarre jutting sections. This counterintuitive if the phenomenon is due to increasing noise variance.
Adding 2 - makes reconstruction look good.
Adding 10000 - reconstruction becomes straight sticks.&lt;/p&gt;

&lt;p&gt;Discussion: analytically this is equivalent to multiplying the noise variance by a constant.  We already know that increasing noise variance solves the issue.  The question remains: why did our training algorithm prefer this? Also, why does decreasing noise variance force the reconstruction &lt;em&gt;away&lt;/em&gt; from the data?  Strong contradiction between data and&lt;/p&gt;

&lt;h3&gt;Experiment #2&lt;/h3&gt;

&lt;p&gt;Add a small amount to the diagonal of the prior covariance matrix&lt;/p&gt;

&lt;p&gt;Result: no obvious improvement (inconclusive)&lt;/p&gt;

&lt;h3&gt;Experiment #4&lt;/h3&gt;

&lt;p&gt;Disable attachment&lt;/p&gt;

&lt;p&gt;Result:  &lt;em&gt;significant improvement&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Discussion:  This was totally unexpected.  Almost all issues seem to be caused by bad attachment.  The affect of bad attachment is probably exacerbated by small noise variance -- the hardness of the constraints lead to contradictions that are resolved by anomalous reconstructions.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Bad attachment guess, combined with nonrobust choice of nosie variance causes contradiction in posterior and strange reconstructions.  Need to develop a better approach to infer attachment.  Issue there is partly that reconstructed individual curves have tails that extend past the attachment point, likely due to bad index estimatization.  If we fix index estimation, we&#39;ll be closer to a good branch estimation procedure.&lt;/p&gt;

&lt;h2&gt;Misstep - index optimization &quot;bug&quot;&lt;/h2&gt;

&lt;p&gt;Noticed camera linearization was occuring &lt;em&gt;after&lt;/em&gt; index optimization, which is likely causing the looping.&lt;/p&gt;

&lt;p&gt;Added optional camera linearization to &lt;code&gt;process_tracks.m&lt;/code&gt;.  Issues seemed to get worse, but in fact, linearization wasn&#39;t running -- forget to set flag to &#39;true&#39;.&lt;/p&gt;

&lt;h2&gt;Overextension&lt;/h2&gt;

&lt;h3&gt;Experiment #1&lt;/h3&gt;

&lt;p&gt;Hypothesis: overrestrictive noise variance results in contradictions requiring contortion to resolve.&lt;/p&gt;

&lt;p&gt;Approach: relax noise&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2x - much worse&lt;/li&gt;
&lt;li&gt;4x - different, still bad&lt;/li&gt;
&lt;li&gt;8x - still bad&lt;/li&gt;
&lt;li&gt;16x - mostly resolved; weak tails&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Is linearization causing it?  is model 5 causing it?  Is linearization trsutworthy?&lt;/p&gt;

&lt;p&gt;Linearization seems to make it a little worse.&lt;/p&gt;

&lt;p&gt;Should we take linearization into account when optimizing depth?&lt;/p&gt;

&lt;p&gt;scenario: curve points toward camera, high uncertainty as to index, because BP line is in direction of curve.  After linearization, BP lines change; no longer in direction of curve, index optimization can&#39;t fix bad 3d position.&lt;/p&gt;

&lt;h2&gt;Miscellaneous thoughts&lt;/h2&gt;

&lt;p&gt;It kind of makes sense that iid noise would be zero, since the data we&#39;re drawing from is so smooth to begin with, and basically noiseless. Any errors arise from mistracing and are strongly correlated due to the smoothness of the Bezier curves.&lt;/p&gt;

&lt;p&gt;If the issue is near-singular matrices, the traditional solution to this is adding a tiny amount of covariance to the diagonal of the prior matrix.&lt;/p&gt;

&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;p&gt;Index optimization resulting in weird loops at endpoints&lt;/p&gt;

&lt;p&gt;Camera linearization reuslts in extensive stretching and looping during index optimization&lt;/p&gt;

&lt;p&gt;T&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>TULIPS - Debugging training</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/24/work-log"/>
   <updated>2014-03-24T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/24/work-log</id>
   <content type="html">&lt;p&gt;Issue: after training, still getting a very large perturb_smoothing_variance:  1e-4 instead of more reasonable 1e-6.&lt;/p&gt;

&lt;p&gt;Need to update tracks to reflect new noise parameter.&lt;/p&gt;

&lt;h2&gt;Cleanup: building data pipeline&lt;/h2&gt;

&lt;p&gt;Most of the processing pipeline is currently implemented in &lt;code&gt;wacv_2014/run_wacv_4.m&lt;/code&gt;.  Moving the track-processing part into &lt;code&gt;process_tracks.m&lt;/code&gt;.  Using the &#39;7-stage&#39; pipeline [as documented here].&lt;/p&gt;

&lt;h2&gt;Retraining&lt;/h2&gt;

&lt;p&gt;updated tracks, retrained.  New trained parameters look reasonable, but the reconstruction is terrible:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-24-retrain_reconstruction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Maybe index optimization is failing, because the model #5 gradient has a bug?  Or hitting the iteration limit?  Running reconstruction with model #3 (whose gradient is proven).&lt;/p&gt;

&lt;p&gt;Nope, it seems to be the new parameters:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-24-retrain_reconstruction_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It&#39;s weird, because the noise variance dropped signficantly, but we&#39;re seeing greater drift away from the data.&lt;/p&gt;

&lt;p&gt;Lets try relaxing the iteration limit anyway...&lt;/p&gt;

&lt;p&gt;More iterations makes it more crazy.&lt;/p&gt;

&lt;p&gt;Sanity check time.  Roll back to old parameters and reconstruct.&lt;/p&gt;

&lt;p&gt;Check.  Old parameters give a sensible reconstruction.&lt;/p&gt;

&lt;p&gt;Differences: (1) much lower noise variance, and (2) much lower perturb smoothing variance.&lt;/p&gt;

&lt;p&gt;The noise variance seems to be the issue here -- raising it to pre-training levels returns us to sensible reconstructions.&lt;/p&gt;

&lt;p&gt;Running out of steam -- time to take a break.  Next steps -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;manual test noise variance; hypothesis: large variance gives higher likelihood.&lt;/li&gt;
&lt;li&gt;ensure we aren&#39;t suffering from near-singular matrix during reconstruction. (piece-wise reconstruction?)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Debugging training</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/23/work-log"/>
   <updated>2014-03-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/23/work-log</id>
   <content type="html">&lt;p&gt;There must be something wrong with the training marginal likelihood function, because the normal ML function pulls &lt;code&gt;perturb_smoothing_variance&lt;/code&gt; lower, but training ML does not.&lt;/p&gt;

&lt;p&gt;Can we refactor-out the training ML to test it independently of the training process?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Found a problem&lt;/strong&gt; - perturbation scale was artificially constrained to be in (0,5).  Training set perturb scale to 4.99996, unsurprisingly.  I can&#39;t remember why I constrained it in this way, or why I chose 5 as the max, but it makex sense to relax it now.  Setting to 50, with intent to remove constraint altogether.  Still doesn&#39;t explain why perturb_smoothing_variance remains so high.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test #1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Approach&lt;/em&gt;: plot tr_curves_ml vs. perturb_smoothing_variance.  Compare against curve_tree_ml vs. perturb_smooth_variance&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Results&lt;/em&gt;: Confirmed curves have different shape (ignoring offset).  training ML plot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-23-train_ml_vs_perturb_smoothing_variance.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Reference ML plot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-23-reference_ml_vs_perturb_smoothing_variance.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Is attachment the issue?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Approach:&lt;/em&gt; re-run test #1 after detaching curves&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Result:&lt;/em&gt; no qualitative change&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Observations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prior covariances seem equivalent.
Likelihood covariances differ.  Investigating...  turns out noise variance was being handled wrongly in &lt;strong&gt;two places&lt;/strong&gt;.  Was dividing when I should have multiplied (cognitive issue: variance vs. precision); was using value instead of sqrt (cognitive issue: variance vs. standard deviation).&lt;/p&gt;

&lt;p&gt;Bingo!  Results are within a factor of 1.6e-5 of constant offset (for model #3)&lt;/p&gt;

&lt;p&gt;Still getting differences with model #5.&lt;/p&gt;

&lt;p&gt;prior is different in the perturb offset component -- differences are unstructured but large (on the order of +-5e-1).  Numerical instability?  Why here and not with model 3?&lt;/p&gt;

&lt;p&gt;HUGE BUG in model #5: should have used &#39;independent&#39; temporal model, not constant model.&lt;/p&gt;

&lt;p&gt;Fixed, model #5 now agrees in both implementations (training and refernce).&lt;/p&gt;

&lt;h2&gt;Retraining&lt;/h2&gt;

&lt;p&gt;Running training now that we&#39;ve worked out the apparent bugs.&lt;/p&gt;

&lt;p&gt;perturb_scale is exploding to 49.999 (near max).  unclear why.  lets cap it lower for now, investigate later.&lt;/p&gt;

&lt;p&gt;Capped scale to 10; it seems to settle lower, so we were probably just seeing a transient before.  Possibly just return NaN in this case and it should take a smaller step.&lt;/p&gt;

&lt;h2&gt;Adding a new model&lt;/h2&gt;

&lt;p&gt;get_model_kernel
get_model_kernel_derive
get_base_covariance
tr_curve_ml&lt;/p&gt;

&lt;p&gt;Changes:
* relaxed scale maximum
* fixed bug in training marginal likelihood
* fixed bug in model 5&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Experiment - Full-camera linearization</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/18/work-log"/>
   <updated>2014-03-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/18/work-log</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;Write function to make point covariances parallel&lt;/li&gt;
&lt;li&gt;re-run training - did values improve?

&lt;ul&gt;
&lt;li&gt;reduced perturbation variance.&lt;/li&gt;
&lt;li&gt;noise variance change?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;re-run reconstruction - are pathologies present?

&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Okay done.  Initial pass at linearization &lt;strong&gt;fails to affect&lt;/strong&gt; training or reconstruciton results.  Basically a no-op.  Are these results legit?  Let&#39;s visualize the linearized model&#39;s direction vectors to see if we&#39;ve resolve convergence issues.  Is it possible the linearization is being overridden somewhere?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;
&lt;code&gt;in_progress/visualize_bp_lines.m&lt;/code&gt; - quiver plot of backprojection lines, as defined by the smallest eigenvector of the precision matrix.&lt;/p&gt;

&lt;p&gt;After plotting, it&#39;s clear the mean direction isn&#39;t right.  Before:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-18-bp_lines.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After running &quot;linearize_cameras.m&quot;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-18-bp_lines_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how the lines shift upward without explanation.&lt;/p&gt;

&lt;p&gt;Found bug: didn&#39;t align direction vectors before taking mean.  Added dot-product check.&lt;/p&gt;

&lt;p&gt;BP lines now look good:  parallel versions of originals, minimal shifting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Re-running reconstruction...  no noticible change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nothing seems to change the reconstruction.  I suspect a bug that is nullifying all our changes.  Strategy: make a dramatic change and see if reconstruction changes.  if not, there&#39;s a bug somewhere.&lt;/p&gt;

&lt;p&gt;Approach:  Change bp-direction eigenvalue from 0 to be the same as the others.&lt;/p&gt;

&lt;p&gt;Expected outcome: Drifting in reconstruction should be nearly eliminated.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Outcome:  Expected change was observed - drift mostly eliminated.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next steps: trace the construction of GP posterior covariance, end-to-end.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exploration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Rolling back changes from last experiment bit-by-bit until desired reconstruction vanishes.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Weird, now everything works as expected -- no drift using camera linearization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Observations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Tried running linearization per-curve instead of per-camera, and the former shows more drifting than the latter but less drifting than per-point.  Basically as expected.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Run on datasets 7-11.&lt;/p&gt;

&lt;p&gt;Outcome: Results seem legitimate.&lt;/p&gt;

&lt;h2&gt;Cleanup&lt;/h2&gt;

&lt;p&gt;We have tried several things to fix this drifting issue, all of which mostly failed until now.  Now that we&#39;ve found a cause of drifting and a fix, need to roll back each of the earlier changes one by one.&lt;/p&gt;

&lt;h3&gt;Re-add index optimization&lt;/h3&gt;

&lt;p&gt;(Disabled smooth index metaprior, because it is likely to have a bug.)&lt;/p&gt;

&lt;p&gt;Seems to help some places, hurt others.  Some &quot;binding&quot; (?), causing bulging away from data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-18-binding.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looks red curve has a start index of -2, which is probably causing the bulge.  issue with attachment inference code, not index estimation.&lt;/p&gt;

&lt;p&gt;In other places, originally over-extended curves are properly trimmed after index optimization. Before:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-18-trimming_after.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-18-trimming_before.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Training&lt;/h2&gt;

&lt;p&gt;Re-ran training using linearized cameras, and no notable change in results.  This was unexpected; we expected far less perturbation variance, since the linearized cameras don&#39;t want to drift anymore.&lt;/p&gt;

&lt;p&gt;Ran a manual test, plotting marginal likelihood vs. perturbation variance between the trained value (2.3023e-04) and our anticipated ideal value (~1e-6).  Indeed, the optimal marginal likelihood is achieved at lower variances, suggesting the training routine has a bug.&lt;/p&gt;

&lt;p&gt;The training routine uses a different routine for computing the likelihood, which may be flawed (possibly due to the new model we&#39;re using).  Work on it tomorrow.&lt;/p&gt;

&lt;p&gt;BUG: training routine assumes precisions are based on noise variance of 1.0.  Fixed; no affect on perturb_smoothing_variance.&lt;/p&gt;

&lt;h2&gt;Training hypothesis - Index compression&lt;/h2&gt;

&lt;p&gt;Idea:  possibly the indices are compressed, requiring deformation (stretching) to fit the data.  Thus, training would want the deformation variance to be higher.&lt;/p&gt;

&lt;p&gt;What is the best way to get unit-rate spacing of indices?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;oracle reconstruct, chord-length parameterize&lt;/li&gt;
&lt;li&gt;chicken-egg: reconstruct, chord-length parameterize, repeat&lt;/li&gt;
&lt;li&gt;chicken egg w/ independent curves&lt;/li&gt;
&lt;li&gt;heuristic reconstruct w/ independent curves&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Fix training routine.  Goal: perturb_smoothing_variance ~= 1e-6&lt;/li&gt;
&lt;li&gt;compare model #5 vs. model #3 under the parallel camera model&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;p&gt;How best to compute marginal likleihood during model selection?  (point-wise linearized or camera-wise?)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Debugging WACV errors</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/17/work-log"/>
   <updated>2014-03-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/17/work-log</id>
   <content type="html">&lt;p&gt;Troubleshooting reconstruction anomalies.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;significant translational offset&lt;/li&gt;
&lt;li&gt;Why is reconstructed base point moving?  Initial points should be 100% corellated.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Observations during debugging:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Observation indices start at negative values&lt;/li&gt;
&lt;li&gt;Initial point in view 2 is 5mm past initial point in view 9&lt;/li&gt;
&lt;li&gt;translational offset was due to auto-centering feature of &lt;code&gt;view_all_wacv.m&lt;/code&gt;.  Changed default to &quot;no auto centering&quot;&lt;/li&gt;
&lt;li&gt;Solved: Base point movement was due to auto-centering.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Okay, now on to bigger issues:  why is tip moving so much?&lt;/p&gt;

&lt;p&gt;Theory: converging center-lines without converging uncertainty causes drift toward camera and is untenable.&lt;/p&gt;

&lt;p&gt;Could re-shaping the uncertainty cylinders help?&lt;/p&gt;

&lt;p&gt;Also strange:  perturb variance is huge - probably to allow curves to shift toward the camera, exploiting the &quot;singular region&quot; phenomenon &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2013/08/06/work-log/&quot;&gt;studied earlier&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Singular regions&lt;/h2&gt;

&lt;p&gt;These are becoming the main struggling point.    It is likely that the &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2013/12/05/work-log/&quot;&gt;endpoint drift&lt;/a&gt; is caused by this too. We could use fully linear cameras, which would avoid converging of uncertainty cones, but we lose some of the novelty of point-wise linearization.&lt;/p&gt;

&lt;p&gt;Had the idea of iteratively reconstructing and re-estimating the uncertainty cylinders, but this is not guaranteed to be monotonic -- in fact it seems likely to reduce the solution&#39;s true likelihood at each iteration.  First it would drift toward the camera; then we shrink the covariance cylinders, which reduces the likelihood; repeat.&lt;/p&gt;

&lt;p&gt;Perhaps the reason point-wise linearization is &quot;novel&quot; is because everyone knows it doesn&#39;t work, but no one has published on that fact.  Or I haven&#39;t reviewed that early literature enough.&lt;/p&gt;

&lt;p&gt;Maybe PWL (point-wise linearization) is bad for reconstruction but still okay for marginalization?  Or maybe it&#39;s just causing our learning to fail, but once we learn reasonable parameters, the pathological behavior disappears?  Assuming reasonable values on perturbation variance, we shouldn&#39;t see significant drift toward cameras (how much?).&lt;/p&gt;

&lt;h2&gt;Full-camera Linearization&lt;/h2&gt;

&lt;p&gt;Should try full-camera linearization.  Can we implement this quickly without changing much infrastructure?  Probably.  Find the mean direction and replace all data covariances with the one in that direction.  New matrix -- First eigenvector: mean direction;  second and third: doesn&#39;t matter (Gram-Schmidt from old eigenvectors?).  If original second and third eigenvalues aren&#39;t equal, use their mean for new ones.&lt;/p&gt;

&lt;p&gt;This is kind of a hybrid of point-wise linearization and full-camera linearization, because the global direction is camera-wise, but each point&#39;s variance is different.  Equivalently, this is full-camera linearization with nonuniform point weights (far points have lower weight, higher variance).&lt;/p&gt;

&lt;p&gt;As long as we&#39;re abandoning some novelty, could go with traditional full-camera linearization, which might yield a more efficient index optimization scheme?  Should compare the different approaches.&lt;/p&gt;

&lt;p&gt;Diagonal data covaraince means we can use the technique from &lt;a href=&quot;http://papers.nips.cc/paper/4281-efficient-inference-in-matrix-variate-gaussian-models-with-iid-observation-noise&quot;&gt;this 2011 NIPS paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implement&lt;/li&gt;
&lt;li&gt;re-train, re-run reconstruction, check for&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/13/work-log"/>
   <updated>2014-03-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/13/work-log</id>
   <content type="html">&lt;p&gt;Finished cleanup and committing of code for GMM EM with missing data.&lt;/p&gt;

&lt;p&gt;Reviewing where we left off after WACV, hopefully looking at this indexing issue with fresh eyes should help.&lt;/p&gt;

&lt;p&gt;As it stands, we never put a nail in the coffin of the drifting index.  The marginal likelihood should discourage dramatic stretching of endpoint indices like we see in the image below.&lt;/p&gt;

&lt;p&gt;Recalling the problem history.  Was getting weird loops in reconstruction; bad indices were causing the curve to bind and twist to fit the spacing.  Point-wise correspondence matching (the DTW part of the pipeline) assumes (naively) no motion in the scene.  In some geometric configurations, this causes drifting offsets in the matching &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2013/11/11/work-log/&quot;&gt;as described here&lt;/a&gt;.  The hacks to improve indices post-hoc (described &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2013/08/15/work-log&quot;&gt;here&lt;/a&gt;) also assumes no motion, so doesn&#39;t help.  Needed an optimization scheme that accounts for motion, and the natural choice is to maximize the marginal likelihood.&lt;/p&gt;

&lt;p&gt;After adding likelihood maximization, started getting a new problem: the final point&#39;s likelihood is extremely far from the previous point&#39;s, resulting in &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2013/12/05/work-log/&quot;&gt;results described on this page&lt;/a&gt;, example below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-drift_ds5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tried several heuristic energy functions to the optimization &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2013/12/07/work-log/&quot;&gt;here&lt;/a&gt; (and several entried following it), including (a) keeping the index spacing equal to the distance between points, (b) keeping the index spacing between points smooth.  The latter seemed to help, but reflecting on the code, it seems there may be a bug in the gradient computation, and I question the validity of the results.  Needs more investigation.&lt;/p&gt;

&lt;p&gt;Also issue of mean curve drifing far from maximum likelihood curve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-13-reconstruction_offset.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;According to &lt;a href=&quot;http://vision.sista.arizona.edu/ksimek/research/2013/12/27/work-log/&quot;&gt;this entry&lt;/a&gt;, increasing the temporal correlation between views improved it.  I also recall decreasing the perturb_position_variance helps too.  It&#39;s disappointing that the trained values for these parameters are causing these problems.  It&#39;s possible our model is them problem:  camera perturbations are independent, while scene perturbations are correlated, but we&#39;re lumping both sources of noise into the scene perturbation model.&lt;/p&gt;

&lt;p&gt;Separate perturbation process into two processes: scene perturbation (with perturb_smoothing_variance) and camera noise (with perturb_position_variance and perturb_rate_variance).  This should allow the scene perturbation to have longer temporal correlations, because the i.i.d. camera noise is separated out from it.&lt;/p&gt;

&lt;p&gt;It is interesting that the perturb_position_variance is so low (0.64) but we get such variation.  Is this because the likelihood is so much stronger?  Or a side-effect of the marginal likelihood preferring data points to bunch together, so pulling toward the camera is common.&lt;/p&gt;

&lt;p&gt;Training didn&#39;t use good indices.  need to optimize indices jointly with parameters&lt;/p&gt;

&lt;h2&gt;Re-running training&lt;/h2&gt;

&lt;p&gt;Files in the &lt;code&gt;train/&lt;/code&gt; subdirectory is really old and use out-of-date data structures. Working on getting it running again.&lt;/p&gt;

&lt;p&gt;Done.  Added new model &quot;#5&quot; that separates camera perturbation into a separate process.&lt;/p&gt;

&lt;p&gt;Retrained on new model.  Changes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;perturbation scale doubled&lt;/li&gt;
&lt;li&gt;perturb smoothing variance increased by two orders of magnitude&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Offsetting isn&#39;t resolved, and reconstructed scene moves around like crazy!&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Separate out camera perturbation process&lt;/li&gt;
&lt;li&gt;We&#39;re due for a retraining
&lt;strong&gt; Fix training to use better index optimization
&lt;/strong&gt;&lt;em&gt; start with linear index spacing, learn, re-fit indices using ML, learn again
&lt;/em&gt;** use new model w/ camera perturbation process&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Future steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;adapt camera parameters per-dataset?&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Debugging log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/12/work-log"/>
   <updated>2014-03-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/12/work-log</id>
   <content type="html">&lt;p&gt;Debugging new implementation of EM GMM fitting with handling of missing data.&lt;/p&gt;

&lt;p&gt;After first iteration, likelihood weights (in p_sum_mp) are grossly uneven.  Likely issue is in the E step.&lt;/p&gt;

&lt;p&gt;Variances in (var_mp) are suspect in the E step.  So are means (in u_mp).  Preceeding M step now looks suspect.  How are mean and variance computed?&lt;/p&gt;

&lt;p&gt;Was resetting accumulators for x and x&lt;sup&gt;2&lt;/sup&gt; after each point.  Part of mis-guided refactoring attempt yesterday.  Rebuilding and re-running; are mean, variance, and likelihood weights reasonable after first iteration?&lt;/p&gt;

&lt;p&gt;Okay, results are identical to reference implementation when no missing data.  Adding missing data to the dataset...&lt;/p&gt;

&lt;p&gt;Results look good at 50% missing, but occasionally getting local optima (~10% of the time).  In those cases, the final likleihood is lower, so no evidence of bug.&lt;/p&gt;

&lt;p&gt;Time for clean up.  A bit of clean up to the test, adding run-time flags for missing, held-out, etc.&lt;/p&gt;

&lt;p&gt;Did signfiicant rework of the GMM EM test suite.  Now can specify which of five tests to run, and non-interactive mode tests all five tests.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE - notes: joint meeting</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/07/work-log"/>
   <updated>2014-03-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/07/work-log</id>
   <content type="html">&lt;h3&gt;Q: how to align data?&lt;/h3&gt;

&lt;p&gt;David - align by first treatment&lt;/p&gt;

&lt;p&gt;Emily - end point is also interesting - release point&lt;/p&gt;

&lt;p&gt;Emily - Three most interesting time-syncs:  line up at first,  line up at last, line up since diagnosis&lt;/p&gt;

&lt;p&gt;Rebecca - Surgery date? (diagnosis date could be a proxy for this)&lt;/p&gt;

&lt;p&gt;Question for karen:  if surgery, when did it happen?&lt;/p&gt;

&lt;p&gt;David:  what is being studied?  adjustment to diagnonis, or adjustment to treatment?&lt;/p&gt;

&lt;h3&gt;Rebecca plots&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;refacttot over time by treatment group&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Ambivalence&lt;/h3&gt;

&lt;p&gt;This idea of ambivalence seems to be an interesting avenue of study.  The main idea is having high &quot;helpfulness&quot; and high &quot;upsetting&quot; indicates an &quot;ambivalent&quot; relationship;  there seems to be much variability between how these two variables covary, may cluster well.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Dataset: SRI&lt;/li&gt;
&lt;li&gt;we see evidence of this in our data&lt;/li&gt;
&lt;li&gt;how do these dimensions track over time?  (this would be novel)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Different dynamics of interest:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;healthy: float low vs flat high&lt;/li&gt;
&lt;li&gt;healthy: one doesnt affect the other much.&lt;/li&gt;
&lt;li&gt;uncertain:  okay but wobbly, and connected&lt;/li&gt;
&lt;li&gt;unhealthy:  high &quot;upsetting&quot; but low &quot;helpfulness&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt;: can ambivalence (sri) model predict &quot;health&quot;?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;health could be measured by physical functioning (fact), or stress (pss)&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Investigae ambivalence as a two-dimensional model.  2D dynamical states-space model?  Cluster on a few different state space models?&lt;/p&gt;

&lt;p&gt;but continue with exploratory.  Remember to split up SRI dataset into two columns.&lt;/p&gt;

&lt;p&gt;Miscellanous

Might be interesting to fit a 5-dim first order auto-regressive model on the 5 datasets of interest, with iid observations.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE discussion</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/06/work-log"/>
   <updated>2014-03-06T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/06/work-log</id>
   <content type="html">&lt;h2&gt;Skype w/ Kobus&lt;/h2&gt;

&lt;p&gt;Below are some random notes from the discussion.&lt;/p&gt;

&lt;p&gt;Generally speaking, the eventual goal is growth curve that indicates whether subject is getting better or worse over time.  This might split into three time-regions with different dynamics:  pre-treatment, treatment, recovery.&lt;/p&gt;

&lt;p&gt;The population could be split into three groups: radiation, chemo, untreated;  start by picking one and modeling it (probably chemo is a good start).  For now, untreated participant will probably be left out until we have a better idea of what we&#39;re modeling.&lt;/p&gt;

&lt;p&gt;Absolute elapsed time may be less interesting than relative progress through treatment (but need to investigate).&lt;/p&gt;

&lt;p&gt;Will probably want to synchronize sequential data on an event (e.g. first treatment), since the first visit holds no strong meaning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Immediate taks:&lt;/strong&gt;  Cluster trajectories using GMM with missing data. Use visit-number as time index, syncing on (x \in) {first-treatment, final treatment}.  Split into chemo and radiation groups and handle separately. Be smart about choosing dataset to minimize missing data.  Visualize by plotting single dimension, one curve per cluster.&lt;/p&gt;

&lt;h2&gt;Misc. TODO:&lt;/h2&gt;

&lt;p&gt;Send kobus MARRS paper, Hinton tech report
Read relevant chapters of murphy&lt;/p&gt;

&lt;h2&gt;Gathering data for clustering&lt;/h2&gt;

&lt;p&gt;Emily says to use these datasets specifically for the first approach:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; [&#39;sri3.csv&#39;, &#39;das4.csv&#39;, &#39;ea4.csv&#39;, &#39;fact4.csv&#39;, &#39;pss4.csv&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;SRI&lt;/h3&gt;

&lt;p&gt;Several redundant columns here.  Questions 4 and 5 ( &quot;PartnerUpsetting4&quot; and  &quot;Conflicted5&quot;) are recoded as &quot;repartnerupsetting4&quot; and &quot;reconflicted5&quot;, and then all columns are cloned into XXXn_conv, for n in {1,...,5}.  Thus, the relevant columns are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;partnerimp1_conv
partnerpredictable2_conv
partnerhelpful3_conv    
repartnerupsetting4_conv
reconflicted5_conv
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;DAS&lt;/h3&gt;

&lt;p&gt;Relevant columns:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Affection1 
Sex2
Kiss3
TooTired4
ShowLove5
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;EA&lt;/h3&gt;

&lt;p&gt;Relevant columns:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FigureOut1
Comfortable2
GladPleased3
Worthwhile4
Cherish5
AttendTo6
Enjoy7
Appreciate8
LetGo9
TakeCareOf10
EnergyFigureOut11
Like12
InTouch13
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;FACT&lt;/h3&gt;

&lt;p&gt;Relevant columns (all should be reversed)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;LackEnergy1
Nausea2
TrblMeetNeedsFam3
HavePain4
SideEffectsBother5
FeelSick6
TimeInBed7
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;PSS&lt;/h3&gt;

&lt;p&gt;Like SRI, we used the recoded &lt;code&gt;*_conv&lt;/code&gt; fields:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;upsetunexpectantly1_conv
unabletocontrol2_conv
stressed3_conv
redealtwithhassles4_conv
ineffectivecoping5_conv
reconfidenthandleprob6_conv
regoingyourway7_conv
notcopewithall8_conv
recontrolirritations9_conv
reontopofthings10_conv
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Summary&lt;/h3&gt;

&lt;p&gt;Forty total dimensions.&lt;/p&gt;

&lt;p&gt;These datasets apply to everyone, and should have minimal missing data (30 total):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PSS
FACT
EA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These datasets apply only to those in a relationship (10 total):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DAS
SRI
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take three approaches:&lt;/p&gt;

&lt;p&gt;(1) look for nontemporal relationships among these dimensions (PCA).  Several of these dimensions may collapse into one.
(2) concatenate all visits into a vector and cluster (40 * 9 = 360 dimensional points)
(3) repeat (2) but re-center based on treatment date&lt;/p&gt;

&lt;h2&gt;Misc&lt;/h2&gt;

&lt;p&gt;999 nonempty records in the full 40-dimensional model.&lt;/p&gt;

&lt;p&gt;651 records are missing-data-free in the full 40-dimensional model.&lt;/p&gt;

&lt;p&gt;863 records are missing-data-free in the 30-dimensional relationship-free model.&lt;/p&gt;

&lt;p&gt;TODO:  concatenate for approach (2) above, and re-evaluate coverage.&lt;/p&gt;

&lt;h2&gt;PCA&lt;/h2&gt;

&lt;p&gt;We didn&#39;t really expect PCA to give interesting results, but I ran it anyway.  Definitely no obvious clusters.&lt;/p&gt;

&lt;p&gt;About 20 of the 40 dimensions seem relevant?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>FIRE - initial analysis</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/05/work-log"/>
   <updated>2014-03-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/05/work-log</id>
   <content type="html">&lt;h2&gt;Merging datasets into database &lt;/h2&gt;

&lt;p&gt;To perform interesting queries across datasets, we&#39;ll need to merge them into a consistent format.  The &quot;structure array&quot; dataset doesn&#39;t seem to be suited to complex queries, so we&#39;ll use a cell array with rows corresponding to records and columns corresponding to fields.  We will merge records from different datasets using (Subject_ID, Visit) as a unique key.  We&#39;ll also merge the metadata structures of all datasets.&lt;/p&gt;

&lt;p&gt;New files:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data/fire_read_all.m&lt;/code&gt;  - calls &lt;code&gt;fire_read_csv&lt;/code&gt; on all .csv files in path&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data/merge_datasets.m&lt;/code&gt; converts output of &lt;code&gt;fire_read_all.m&lt;/code&gt; into a NxM cell array and a metadata structure describing each columns.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data/fire_classify_data_columns.m&lt;/code&gt; - infers the subtype of each numeric fields (bool, int, float).  Also computes coverage. Stores in column metadata.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Can now do rudimentary table queries using matlab operators.  For example, the code below loads the database and queries, &quot;how many bool fields have coverage above 95%?&quot;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /Users/ksimek/work/src/fire/src
% read data
[data, meta] = fire_read_all(&#39;../data&#39;);
% merge into single &quot;database&quot; table
[db, db_meta] = merge_datasets(data, meta);
% idenitfy column types and measure coverage
db_meta_2 = fire_classify_data_columns(db, db_meta);
% perform query: number of bool fields with 95% or greater coverage
I = ([db_meta_2.coverage] &amp;gt; 95); 
sum(cellfun(@(x) strcmp(x,&#39;bool&#39;), {db_meta_2(I).subtype}))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Miscellaneous thoughts&lt;/h2&gt;

&lt;p&gt;If doing dynamical analysis, consider re-aligning time, using one of the following as origin:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;first treatment&lt;/li&gt;
&lt;li&gt;first chemo treament&lt;/li&gt;
&lt;li&gt;first rad treament&lt;/li&gt;
&lt;li&gt;inflamation threshold&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>FIRE - background reading, thinking and planning</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/03/04/work-log"/>
   <updated>2014-03-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/03/04/work-log</id>
   <content type="html">&lt;h2&gt;Structure Learning&lt;/h2&gt;

&lt;p&gt;The fact that we have a 500-dimensional dynamical system offers opportunities to learn high-dimensional relationships.  I spent some time looking into structure learning.&lt;br/&gt;
&lt;strong&gt;Variational Bayes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mlg.eng.cam.ac.uk/zoubin&quot;&gt;Zoubin Ghahramani slides on variational bayes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Variational EM could learn LDS with unknown structure or switching state space model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dirichlet Process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Video: &lt;a href=&quot;http://videolectures.net/mlss2010_griffiths_isfd/&quot;&gt;Inferring structure from data&lt;/a&gt;, Tom Griffiths, Cognitive Science and Machine Learning Summer School, 2010&lt;/p&gt;

&lt;p&gt;Maybe could be used to learn LDS structure. Griffiths shows a noisy-or model with unknown dimension, using a Dirichlet process prior.  At its center is a rank-defficient matrix constructed from product of NxM and MxN matrices, where M &amp;lt; N.  Could use something similar to learn sparse dynamics matrix, but we&#39;re dealing with continuous-valued data rather than binary-valued.  If this can be done, there must be existing literature on it, will dig further.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parameter Learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This tech report from Ghahramani and Hinton derives the EM for learning LDS parameters and latent states jointly.  For the M step, they derive expressions for the full transition matrix, plus the system and observation noise covariance, and initial state.  For the E step, they use Kalman for forward estimation followed by a backward recursion.  We&#39;re more interested in a sparse representation of dynamics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.learning.eng.cam.ac.uk/zoubin/papers/tr-96-2.pdf&quot;&gt;Parameter Estimation for Linear Dynamical Systems&lt;/a&gt;[pdf]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MARRS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://journal.r-project.org/archive/2012-1/RJournal_2012-1_Holmes~et~al.pdf&quot;&gt;Multiple Auto-regressive State-Space models for Analyzing Time-series Data&lt;/a&gt; by Elizabeth E. Holmes, Eric J. Ward, Kellie Wills&lt;/p&gt;

&lt;p&gt;This &lt;a href=&quot;http://cran.r-project.org/web/packages/MARSS/index.html&quot;&gt;R package&lt;/a&gt; handles learning and inference in general state-space models, with unspecified or semi-specified model structure.  It uses EM to infer the model and latent state jointly.  On the plus side, it handles missing data, and the model is very flexible.  On the down side, it doesn&#39;t handle i.i.d. data in an obvious way (although could handle a small number by repeating matrix blocks).  Could probably implement some form of the TIES model, sans prior.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other R Packages&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.jstatsoft.org/v36/i12/&quot;&gt;dlm - An R Package for Dynamic Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/KFAS/index.html&quot;&gt;KFAS -  Kalman Filter and Smoother for Exponential Family State Space Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cran.r-project.org/web/packages/FKF/FKF.pdf&quot;&gt;FKF: Fast Kalman Filter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Modeling&lt;/h2&gt;

&lt;p&gt;Kobus pointed out that the short time-frame means second-order dynamics likely won&#39;t emerge.&lt;/p&gt;

&lt;h2&gt;Matlab CSV File parsing&lt;/h2&gt;

&lt;p&gt;Matlab only handles CSV files with numeric-valued fields.  We have dates and strings (and possibly others), so we need to write our own CSV parser.&lt;/p&gt;

&lt;p&gt;Creating a &lt;em&gt;.meta file for each &lt;/em&gt;.csv file, which describes column names and datatypes.  Currently only three ways to parse a column: &quot;numeric&quot;, &quot;mm/dd/yy/ date&quot;, and &quot;ignore&quot;.  Possibly more in the future.&lt;/p&gt;

&lt;p&gt;Wrote code for parsing meta files and csv files (all code is in projects/fire/trunk/src):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;io/fire_read_meta.m&lt;/code&gt; - Reads a .meta file into a 1xN structure array.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;io/fire_read_csv.m&lt;/code&gt; - Reads a .csv file, using the corresponding .meta file to determine names and datatypes.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Misc notes about data&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Most datasets seem to be uniquely indexed on the pair (Subject_ID, Visit).&lt;/li&gt;
&lt;li&gt;There are 136 subjects and 9 visits&lt;/li&gt;
&lt;li&gt;Most datasets have 998 entries, except
&lt;strong&gt; fcsfq5.csv - missing first visit
&lt;/strong&gt; poms4.csv - missing 12 subjects
&lt;strong&gt; sri3.csv - 8 missing records (misc.)
&lt;/strong&gt; fcsfq5.csv - missing 3 subjects
&lt;strong&gt; demograph.csv - Extra records (investigating...)
&lt;/strong&gt;* Several missing Visit ID&#39;s&lt;/li&gt;
&lt;li&gt;Total of 1009 (Subject_Id, Visit) pairs (some non-overlapping occurrances)&lt;/li&gt;
&lt;li&gt;Subject_ID scheme: 575 + a 2 or 3 digit number&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Coverage&lt;/h3&gt;

&lt;p&gt;The following applies only to parsable fields only&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;161 (numeric or date) fields w/ full coverage (ignoring the index fields)
** Unclear if all time-points are covered by all subjects&lt;/li&gt;
&lt;li&gt;Median coverage per record is &lt;strong&gt;163/547&lt;/strong&gt; fields&lt;/li&gt;
&lt;li&gt;Median coverage per fields is 97.0%.&lt;/li&gt;
&lt;li&gt;303 / 547 fields have over 95% coverage.
&lt;strong&gt; 273 numeric fields
&lt;/strong&gt;&lt;em&gt; 193 int
&lt;/em&gt;&lt;strong&gt; 10 double
&lt;/strong&gt;* 70 bool&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Coverage map is below (rows are fields, columns are records)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2014-03-05-fire_coverage.png&quot; alt=&quot;Data coverage&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Datatypes statistics&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;32 float fields&lt;/li&gt;
&lt;li&gt;160 bool fields&lt;/li&gt;
&lt;li&gt;323 int fields&lt;/li&gt;
&lt;li&gt;30 date fields&lt;/li&gt;
&lt;li&gt;The rest are text, inconsistent, or unknown format&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;How many fully covered fields have more than two values?&lt;/p&gt;

&lt;p&gt;Next pass:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;merge all columns into a large cell array&lt;/li&gt;
&lt;li&gt;for each column, store pointer back to column metadata
** i.e. dataset name, column name &amp;amp; type&lt;/li&gt;
&lt;li&gt;infer numeric type (boolean, int, float)&lt;/li&gt;
&lt;li&gt;Example query: get all int/float columns, excluding ID and visit
&lt;strong&gt; Try clustering them (how to visualize?)
&lt;/strong&gt; Try PCA on them - do visible clusters emerge? can we name them?)&lt;/li&gt;
&lt;li&gt;Example Query: get all numerical columns vs. time
&lt;strong&gt; Dynamic factor analysis?
&lt;/strong&gt; split into subjects and visit indices&lt;/li&gt;
&lt;li&gt;consider boolean fields&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Should probably read the unreadable fields as text.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Proposal Practice Post-mortem</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/02/08/work-log"/>
   <updated>2014-02-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/02/08/work-log</id>
   <content type="html">&lt;h2&gt;General Advice&lt;/h2&gt;

&lt;p&gt;Take a more top-down approach; split into &quot;Modeling&quot; and &quot;Inference&quot; sections.  give details on request.&lt;/p&gt;

&lt;p&gt;Tailor more toward committee audience and less to a general audience.&lt;/p&gt;

&lt;p&gt;Questions to ask yourself:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is the purpose of this talk to this audience?&lt;/li&gt;
&lt;li&gt;What do you want the audience to remember?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Structure the talk and paper more tree, with details on the leafs.&lt;/p&gt;

&lt;p&gt;Get the the Arabidopsis application faster.&lt;/p&gt;

&lt;p&gt;Have a &quot;map&quot; of the various research parts; show how they connect and where we are currently.    &lt;strong&gt;Need a stronger big picture.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Focus on the key pieces of research, ditch the rest (e.g. drop DTW focus).&lt;/p&gt;

&lt;h2&gt;Probably okay?  (not comments)&lt;/h2&gt;

&lt;p&gt;Related work (SfM and SfS).  I was worried I left out too many recent developments, but no one commented on it (but it&#39;s in the paper).&lt;/p&gt;

&lt;h2&gt;Issues&lt;/h2&gt;

&lt;p&gt;Need stronger transitions between topics.&lt;/p&gt;

&lt;p&gt;Unclear how much left there is to do.&lt;/p&gt;

&lt;p&gt;Unclear what the key contributions are, and what is just &quot;other stuff we did&quot;.&lt;/p&gt;

&lt;p&gt;Too much time talking about alternative approaches.  (I can probably cut quite a bit, time-wise by taking this advice).&lt;/p&gt;

&lt;p&gt;Graphics: backprojection lines too light.&lt;/p&gt;

&lt;h2&gt;Too Much:&lt;/h2&gt;

&lt;p&gt;Phenotyping&lt;/p&gt;

&lt;p&gt;projective geometry (a few good pictures would be better)&lt;/p&gt;

&lt;p&gt;Linear approximation details (e.g. emphasis on running time).&lt;/p&gt;

&lt;h2&gt;Confusion&lt;/h2&gt;

&lt;p&gt;Temporal component of model is not for camera motion, but for trackingnonrigid deformation.&lt;/p&gt;

&lt;p&gt;3-tuple slide: add a ground plane to images to convey 3D, maybe eliminate hard black outlines what falsely convey 2D image-plane context.&lt;/p&gt;

&lt;p&gt;How is DTW section connected to the rest?&lt;/p&gt;

&lt;p&gt;When I say parts are &quot;given&quot; or &quot;assumed&quot;, what does that mean?  i.e. how are they given?  (This should be clearer when I re-orgainze top-down.  Natually handled in inference)&lt;/p&gt;

&lt;p&gt;What does it mean that CS model is more &quot;expressive&quot; than squared exponential model?&lt;/p&gt;

&lt;p&gt;Likelihood linearization was confusing, esp. the justification.  Since local linearization isn&#39;t novel, could just say we linearize, and exploit conditional independence of points to do it in linear running time.&lt;/p&gt;

&lt;h2&gt;Misc TODO:&lt;/h2&gt;

&lt;p&gt;In paper, mention that index estimation is currently slow and ucnlear if fit for MCMCDA approach.&lt;/p&gt;

&lt;h2&gt;Questions&lt;/h2&gt;

&lt;p&gt;What constraints are there on data collection?  Are cameras required to be on a turntable?&lt;/p&gt;

&lt;p&gt;What is the connection between the covariance function and the species?  Do you think the cubic spline covariance function this is generally applicable?&lt;/p&gt;

&lt;p&gt;What is the connection to the science?  What can we learn about species from this?  (I could probably talk about two aspects - (a) can build models using data recovered from 3D reconstruction and (b) learning population models in heirarchical BGP (phenotypes as parameters))&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>GPLVM & GPDM notes</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/01/29/reference"/>
   <updated>2014-01-29T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/01/29/reference</id>
   <content type="html">&lt;p&gt;[1] N. Lawrence, Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models, The Journal of Machine Learning Research, vol. 6, Dec. 2005.&lt;/p&gt;

&lt;p&gt;Beautifully illustrates the relationship between GPLVM and kernel PCA with a unifying objective function.  PPCA marginalizes the input values and maximizes the linear parameters.  GPLVM marginalizes the &quot;linear parameters&quot; (more generally, the kernel that generalizes the linear parameters), and maximizes the inputs.&lt;/p&gt;

&lt;p&gt;Paper provides the derivative of the marginal likelihood w.r.t. K, which greatly simplifies our derivations in &lt;a href=&quot;/ksimek/research/2013/11/25/reference/&quot;&gt;this post&lt;/a&gt; and this one(/ksimek/research/2013/11/10/reference/).  We used inside-out derivation, Lawrences uses forward method, which is clearer.&lt;/p&gt;

&lt;p&gt;[2] R. Urtasun, D. J. Fleet, A. Hertzmann, and P. Fua, Priors for people tracking from small training sets, presented at the Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, 2005, vol. 1, pp. 403410.&lt;/p&gt;

&lt;p&gt;Application: golf swing, walk&lt;/p&gt;

&lt;p&gt;Learn inputs, (\mathbf{x}), with independent prior, (exp(-\mathbf{x}^\top \mathbf{x})/2).&lt;/p&gt;

&lt;p&gt;\partial L / \partial K = K^{-1} Y Y^\yop K^{-1} - D K^{-1}&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Planning - Spring Interdisciplanry Computational Intelligence Seminar</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/01/17/work-log"/>
   <updated>2014-01-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/01/17/work-log</id>
   <content type="html">&lt;h2&gt;Spring Topics of Interest&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Active learning / Deep Learning&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Active Learning of an Action Detector from Untrimmed Videos - Grauman&lt;/li&gt;
&lt;li&gt;A Convex Optimization Framework for Active Learning?&lt;/li&gt;
&lt;li&gt;ImageNet Classification with Deep Convolutional Neural Networks (hinton)&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Optimization (not mcmc)&lt;/h2&gt;

&lt;p&gt;e.g. &quot;relaxation&quot; methods&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Estimating the 3D Layout of Indoor Scenes and its Clutter from Depth Sensors&lt;/li&gt;
&lt;li&gt;Holistic Scene Understanding for 3D Object Detection with RGBD cameras&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Scenes and captions&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;From Large Scale Image Categorization to Entry-Level Categories - &quot;best paper, iccv 2013&quot;&lt;/li&gt;
&lt;li&gt;NEIL: Extracting Visual Knowledge from Web Data&lt;/li&gt;
&lt;li&gt;A Sentence is Worth a Thousand Pixels&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Detection&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;HOGgles: Visualizing Object Detection Features&lt;/li&gt;
&lt;li&gt;Finding the Best from the Second Bests -- Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Miscellaneous&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Latent Data Association: Bayesian Model Selection for Multi-target Tracking - ian reid  (possibly of interest to Ernesto?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Relative Attributes For Large-scale Abandoned Object Detection (Quanfu and Prasad&#39;s paper) &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Fan_Relative_Attributes_for_2013_ICCV_paper.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Inferring Dark Matter and Dark Energy from Videos&lt;/p&gt;

&lt;h2&gt;Schedule &lt;/h2&gt;

&lt;p&gt;Rough schedule for first 7 weeks&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Practice Talk - Colin&lt;/li&gt;
&lt;li&gt;Practice Talk - Kyle&lt;/li&gt;
&lt;li&gt;Practice talk - Andrew&lt;/li&gt;
&lt;li&gt;Learning TBD  (Deep learning or active learning) - Kobus&lt;/li&gt;
&lt;li&gt;Learning TBD - Qiyam&lt;/li&gt;
&lt;li&gt;Fast object segmentation in unconstrained video - Kate?&lt;/li&gt;
&lt;li&gt;(ECCV, no session?)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Remaining order TBD&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models - Colin&lt;/li&gt;
&lt;li&gt;Tree Shape Priors with Connectivity Constraints using Convex Relaxation on General Graphs - kyle?&lt;/li&gt;
&lt;li&gt;MCMC Sampling - Jinyan&lt;/li&gt;
&lt;li&gt;TBD - Ernesto&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>HPC meeting</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/01/17/hpc-meeting"/>
   <updated>2014-01-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/01/17/hpc-meeting</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://sites.google.com/a/case.edu/hpc-upgraded-cluster/cluster-faq#TOC-What-are-modules-&quot;&gt;Modules&lt;/a&gt; automatically set path and library paths for specific libraries.&lt;/li&gt;
&lt;li&gt;HPC has a nice &lt;a href=&quot;&quot;&gt;web interface&lt;/a&gt; for subitting and managing jobs.  ATM, the only way to get stdout and stderr outputs.&lt;/li&gt;
&lt;li&gt;Intel compiler can use openMP to automatically leverage coprocessor cards.  Can use same code&lt;/li&gt;
&lt;li&gt;Thrust already installed (&lt;code&gt;module cuda&lt;/code&gt;); boost is header-only.&lt;/li&gt;
&lt;li&gt;Apparently, the copropocessor cards run linux, and I can ssh into them!  Still unsure what the host address is.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Dis Prop (ctd); new HPC </title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/01/15/work-log"/>
   <updated>2014-01-15T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/01/15/work-log</id>
   <content type="html">&lt;p&gt;Continue writing dissertation proposal.  Worked on SfM, MVS literature review.  Spent quite some time trying to understand self-calibration.  The issue is that with a general perspective camera, SfM can only perform reconstruction up to a projective ambiguity (most of the SfM literature of late is apparently reticent about this?).  It turns out, if we fix at least one intrinsic parameter (how about skew = 0? or aspect ration = 1?) then we get a metric reconstruction (which maybe explains why it&#39;s no longer covered in SfM papers).&lt;/p&gt;

&lt;p&gt;This shows that bundle adjustment can give a metric reconstruction, but the question remains how to initialize it.  Pollefeys et. al (1998) propose an analytical self-calibration solution.  Snavely et al. (2007) apparently just use bundle adjustment anyway, presumably using a simplified pinhole camera (no skew or ppo) with a reasonable default for &lt;em&gt;f&lt;/em&gt;, or EXIF tags when possible.  Brown and Lowe (2005) initialize each new camera with the old camera&#39;s intrinsics, no word on how the first pair is initialized (maybe obvious but I missed it?).&lt;/p&gt;

&lt;h2&gt;New HPC &lt;/h2&gt;

&lt;p&gt;UA just installed their new 80-node HPC cluster and by a stroke of luck, Ive gotten early access to it!  It looks like all machines have 16 CPU cores and 256 GB RAM; 60 nodes have multiple GPU cards while the other 20 have Intel PHI general purpose compute cards.  It&#39;s amazing to query the cluster and see the entire thing at 0% utilization... that won&#39;t last long!  Having fun poking around it and reading the IBM LSF manuals &lt;a href=&quot;http://publibfp.dhe.ibm.com/epubs/pdf/c2253460.pdf&quot;&gt;2&lt;/a&gt; (the job queueing system).&lt;/p&gt;

&lt;p&gt;I found the following resource to be very useful, even though it&#39;s from University of Miami&#39;s personal LSF installation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.ccs.miami.edu/hpc/lsf/9.1.1/&quot;&gt;unofficial LSF 9.1.1 Documentation site&lt;/a&gt; (University of maimi)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.ccs.miami.edu/hpc/lsf/9.1.1/print/lsf_foundations.pdf&quot;&gt;Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.ccs.miami.edu/hpc/lsf/9.1.1/print/lsf_admin.pdf&quot;&gt;Jobs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The hew official documents I could find were less useful:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Command Reference [&lt;a href=&quot;http://publibfp.dhe.ibm.com/epubs/pdf/c2753051.pdf&quot;&gt;pdf&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Administration Reference [&lt;a href=&quot;http://publibfp.dhe.ibm.com/epubs/pdf/c2253460.pdf&quot;&gt;pdf&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Configuration Reference[&lt;a href=&quot;http://publibfp.dhe.ibm.com/epubs/pdf/c2753061.pdf&quot;&gt;pdf&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Notes&lt;/h3&gt;

&lt;p&gt;Okay great!  Got a general idea of how to use the system from the &lt;a href=&quot;http://www.ccs.miami.edu/hpc/lsf/9.1.1/print/lsf_users_guide.pdf&quot;&gt;user guide&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bsub&lt;/code&gt; - submit default job queue&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bsub -q &amp;lt;queue-name&amp;gt;&lt;/code&gt; - submit specific job queue&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bsub -I&lt;/code&gt; - interactive job&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bjobs&lt;/code&gt; - view submitted jobs&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bqueues&lt;/code&gt; - list info for all queues&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bparams&lt;/code&gt; - details for default queue&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lsload&lt;/code&gt; - list load on hosts&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lshosts&lt;/code&gt; - list hosts w/ configuration&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bhosts&lt;/code&gt; - view batch server hosts (what&#39;s a batch server?)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lsid&lt;/code&gt; - cluster info (name and master host)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lsrun&lt;/code&gt; - run a command on a free host&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lsgrun&lt;/code&gt; - run a command on a group of free hosts&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lsltakss&lt;/code&gt; - view and add local tasks (huh?)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lsrtasks&lt;/code&gt; - view and add remote tasks (huh?)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bkill 1234&lt;/code&gt; -  kill job 1234&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bkill -r&lt;/code&gt; - force removal of job (if hung after kill?)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bstop&lt;/code&gt; - suspend job&lt;/li&gt;
&lt;li&gt;`bresume - resume job&lt;/li&gt;
&lt;li&gt;&lt;code&gt;btop 1234&lt;/code&gt; - move job 1234 to top of queue&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bbot 1234&lt;/code&gt; - move job 1234 to bottom of queue&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;On gpu* machines&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nvidia-smi&lt;/code&gt; list details for all gpu cards (and running processes?)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;On phi* machines&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;micinfo&lt;/code&gt; lists details for all coprocessor cards&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Also MPI is supported&lt;/p&gt;

&lt;p&gt;Discovered two large gsfs disks (192 TB and 43 TB, resp.).  Apparently GSFS is a GPU-enabled encrypted disk.  No write permissions ATM&lt;/p&gt;

&lt;h3&gt;Questions&lt;/h3&gt;

&lt;p&gt;Some random thoughts while using the system&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;can/should we get Matlab?  Octave?&lt;/li&gt;
&lt;li&gt;Subversion?&lt;/li&gt;
&lt;li&gt;write permissions on GSFS disks?&lt;/li&gt;
&lt;li&gt;No job report email after calling &lt;code&gt;bsub -u ksimek@email.arizona.edu&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Talk w/ Kobus Re: Dissertation Proposal</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/01/13/work-log"/>
   <updated>2014-01-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/01/13/work-log</id>
   <content type="html">&lt;p&gt;Met w/ Kobus today to discuss the proposal for the first time.&lt;/p&gt;

&lt;h4&gt;Organization&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Key idea&lt;/strong&gt;: Branching GP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Close 2nd&lt;/strong&gt;: Model selection&lt;/p&gt;

&lt;p&gt;Other stuff like high-throughput phenotyping, biological modeling, multi-view stereo, etc. take a back seat (but should be mentioned)&lt;/p&gt;

&lt;h4&gt;Key sections&lt;/h4&gt;

&lt;p&gt;The structure isn&#39;t rigid, but the three main sections that need to be strong are: (1) Literature review, (2) current progress, (3) future work.&lt;/p&gt;

&lt;p&gt;Didn&#39;t ask about &quot;Assumptions and Limitations&quot;.  Kobus mentioned that things like that can protect you, but they don&#39;t come up very often.  Seems not critical, use best judgement.&lt;/p&gt;

&lt;p&gt;&quot;Broader impacts&quot; or &quot;Importance of Topic&quot; section could be a heading to what would otherwise just fall into the introduction.&lt;/p&gt;

&lt;p&gt;It seems the &quot;Outcomes&quot; section is probably best weaved into the &quot;proposed work&quot;.&lt;/p&gt;

&lt;p&gt;It seems listing the dissertation chapters isn&#39;t strictly necessarilly, either.&lt;/p&gt;

&lt;h4&gt;Future work&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Full automatic system
Automatic camera detection
Apply to another domain (can be vague; can change later)

Less likely:
    leafs, flowers, etc.  Stick to GP&#39;s
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;Miscellaneous&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;Don&#39;t stress about correctness.  This is a proposal, i.e. *pre* investigation.  
Let&#39;s wait until the end of the week to decide if we need to reschedule.
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/01/07/work-log"/>
   <updated>2014-01-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/01/07/work-log</id>
   <content type="html">&lt;p&gt;Layout organization, pick a section, start writing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ohio.edu/eecs/graduate/documents/upload/PhDDissertationProposalOutline2-06.pdf&quot;&gt;Proposal guide, CS department, Ohio University&lt;/a&gt; [pdf]&lt;/p&gt;

&lt;p&gt;In researching multiple-view plant datasets, I came across the &lt;a href=&quot;http://www.imageclef.org/node/179&quot;&gt;LifeCLEF 2014 challenge&lt;/a&gt;.  The goal is plant-species identification from single images.  Images are of several different organs (leafs, fruit, stems, branches) or entire structures (branches, entire plant).  Most images are in the wild, but some (leafs) are from a flatbed scanner.  All images are given a one-to-five rating based on their quality.  Even this isn&#39;t the type of task we&#39;re focusing on, it&#39;s valuable to note what types of tasks the community is focusing on.&lt;/p&gt;

&lt;p&gt;Also available is a fish classification task from videos.  It might be interesting to try to track GP curves in an image and classify fish based on their GP parameters (temporal and spatial).  The cubic spline model is already scale-invariant, so scale issues might not be a big deal.  GP-LVM model might be even more interesting.&lt;/p&gt;

&lt;p&gt;TODO - look into CVPR 2013 &quot;thin branch&quot; paper.  Existing datasets?  Their dataset?  Literature reivew?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Roughing out Proposal</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/01/07/proposal_rough"/>
   <updated>2014-01-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/01/07/proposal_rough</id>
   <content type="html">&lt;p&gt;Simultaneous stereo and tracking of nonrigid structure for semantic reconstruction&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;h2&gt;Problem statement&lt;/h2&gt;

&lt;p&gt;We have collected a dataset consisting of multiple specimens of Arabidopsis Thaliana, imaged at several angles.  Specimens come from two different genetic strains, and imaging occurred at various stages of development.  The elapsed time between the first and last image is several minutes, and in many cases, plants exhibit noticible motion between the first and last image.  Our primary goal is to recover the full 3D branching structure of each specimen, but we also seek to recover the motion between frames as it may contain useful phenotype information.  Our reconstrution will be composed of geometric primitives, from which plant scientists may extract valuable phenotype traits like branch angle and curvature.&lt;/p&gt;

&lt;p&gt;In a sense, our problem encompasses two complementary areas in computer vision, tracking and multi-view stereo.  In classical tracking, the camera is fixed and the goal is to recover motion; in stereo, the camera is moving and the goal is to recover the fixed structure.  In our problem, both structure and camera undergo motion, with each object&#39;s motion confounding inference of the other.  Is an object&#39;s 2D motion best explained by parallax or by motion in 3D?   Either of these conclusions (or both) could the case; underconstrained problems like these require additional assumptions to be solved uniquely.  We a Bayesian approach to the problem, encoding these assumptions in the form of a prior distribution over structure and motion, and using Bayesian inference to recover both the optimal solution and a distribution over alternative solutions.&lt;/p&gt;

&lt;p&gt;Phenotype traits:
* branch depth,
* branch angles,
* stem curvature,
* torsion,
* phototropism,
* interbranch distances,
* biomass,
* etc.&lt;/p&gt;

&lt;p&gt;In modelling branching stem structures, we propose a representation that is simultaneously expressive and tractible.&lt;/p&gt;

&lt;p&gt;(The following might be best posed in related work, after describing SfM and SfS backgroun)&lt;/p&gt;

&lt;p&gt;In addition to the difficulties interent to simultaneous inference of structure and motion, the nature of plant structure poses specific challenges to the reconstruction task.  Our primary structures of interest are plant stems, which are nearly absent of texture features.  Most structure-from-motion algorithms use so-called keypoints to find matches between images, and high quality keypoints cannot be found without strong texture features.   The thin geometry of plant stems also poses difficulties, especially with popular shape-from-silhouette algorithms.  The thin backprojection cones that arise from stems often fail to intersect, because cameras are imperfectly calibrated and stems undergo motion between views.  Algorithms exist to improve camera pose estimates by minimizing reconstruction error, but these algorithms assume structure is stationary.&lt;/p&gt;

&lt;p&gt;Plant structure also poses challenges on the on the tracking side,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  * nonrigid/nonparametric structure
  * nonrigid motion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modelling challenges
      * unknown topology, cardinality&lt;/p&gt;

&lt;p&gt;the primary difficulty is that the object and camera move simultaneously.  Since our goal is 3D tracking,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;By jointly modeling camera error, 3D spatial structure, and temporal motion, we seek to improve upon existing approaches that ignore one or more of these aspects, while providing a rich description of the 4D scene.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We propose a Bayesian&lt;/p&gt;

&lt;p&gt;Model camera, structure, motion directly&lt;/p&gt;

&lt;p&gt;The proposed work will&lt;/p&gt;

&lt;p&gt;The primary contribution of this research will be a novel method for robust reconstruction of thin, textureless structure from multiple views, a specific case of multiple view reconstruction that is rarely considered.  To this end, our work will make novel contributions in several areas, including&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A probabilistic generative model for branching structure in 3D that is applicable across many domains and problem settings.&lt;/li&gt;
&lt;li&gt;A Bayesian formulation of multiple-view 4D reconstruction of thin branching structure that is robust to camera miscalibration and nonrigid motion.&lt;/li&gt;
&lt;li&gt;Several novel edge-based likelihood functions, which are robust to noise and missing data and can be evaluated efficiently on GPU hardware.&lt;/li&gt;
&lt;li&gt;A novel formulation of multiple-view reconstruction as a Bayesian data-association problem, and an approach for appoximate inference.&lt;/li&gt;
&lt;li&gt;An efficient dynamic programming algorithm for dense multiple-view point correspondences of linear structure.&lt;/li&gt;
&lt;li&gt;A novel, principled approach to the problem of Bayesian model selection in the presence of nonlinear likelihood terms.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Importance of Topic&lt;/h2&gt;

&lt;h3&gt;Plant biology, food source&lt;/h3&gt;

&lt;p&gt;Researchers estimate that global food production must double by 2050 to meet the expected growth in demand \cite{tilman2011}.  Advances in high-throughput genotyping have yielded great advancements in the understanding of plant biology at the micro- level, but without efficient methods for macro-level analysis, the relationship between genetic variation and practical measures like crop yields or drought resistance remains difficult to study.  High-throughput phenotyping provides a way to bridge this micro-/macro- gap by automatically quantifying observable traits (e.g morphology, behavior), allowing macro-level study of plants from different genetic strains at a large scale.&lt;/p&gt;

&lt;p&gt;Existing high-throughput systems&lt;/p&gt;

&lt;p&gt;and demand for simpler, lower-cost systems has increased in recent years&lt;/p&gt;

&lt;p&gt;The earliest high throughput phenotyping platforms (HTPPs) used expensive sensing equipment, proving the effectiveness of HTPPs but illustrating a need for lower-cost solutions \cite{TODO}.  TODO:  Make an argument about the lack of 3D analysis of full branching architecture.  Use \cite{fiorani2013} to survey existing methods,  use \cite{biskup2007} as example similar to ours.&lt;/p&gt;

&lt;p&gt;High-throughput phenotyping
    understanding mutatations
    phenotyping is a limiting factor in breeding
    high-throughput genotyping has left a gap in the phenotyping
    relating genes to biological traits (e.g. crop yields, drought resistance, root system efficiency)
    Understand the effects of genetic variation in terms of practical measures like crop yields or drought resistance.
    understand connections between genetics and macro structure&lt;/p&gt;

&lt;h3&gt;Reconstruction&lt;/h3&gt;

&lt;p&gt;Multiple view 3D reconstruction is a widely-studied area in computer vision.  While this area has seen great success in recent years, most research has focused on reconstructing large structures with considerable surface area \cite{TODO}.  Most of these approaches fail in the presence of thin, textureless structure like those exhibited by plant stems.  The applications for multiple-view reconstruction are&lt;/p&gt;

&lt;h2&gt;#&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;By separating the concerns of modelling and inference, our approach should generalize well to other research areas that involve branching linear systems, like neuron tracing \cite{TODO}, vascular segmentation \cite{TODO}, and root structure architecture \cite{TODO}.  We intend to 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Prior Research&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Biological application&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;RSA, Hypotrace, etc.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Multi-view Reconstruction&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;PMVS first attempt&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The well-known patch-based multiple view stereo algorithm (PMVS) uses full-color images to reconstruct a 3D mesh \cite{furukawa2010}, making them effective when a silhouette is unavailable.  For each image patch, a similar image patch is sought in other views such that both appearance and depth are roughly consistent between matches.  However, due to ambiguities that arise in flat image regionts, the algorithm starts by matching special keypoints whose surface exhibit distinct texture, and filling-in patches in-between.  In our situation, our surfaces are extremely thin (often three pixels or fewer in width) and exhibit no notable texture features, causing patch-based reconstructions to fail.  Further, PMVS assumes rigid structure and perfectly calibrated cameras; our plant&#39;s slight movements and our miscalibrated cameras would likely cause the algorithm&#39;s consistency checks to fail.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;PMVS Second attempt&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The well known asdfjl;kj uses appearance consistency to estimate the depth of image patches.  It relies heavilly on texture data to find inital matches, which makes it ineffective at reconstructing textureless structure like plant stems.  Futhermore, like the visual-hull based methods, our miscalibrated cameras and slightly moving plant structure would cause the consistency checks in this algorithm to fail.
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Tracking&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bayesian Model Choice&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Curve prior model&lt;/p&gt;

&lt;p&gt;  Methods for modeling smooth curves and surfaces have been widely studied since the early days of computer vision.  The popular active countour or &quot;snakes&quot; algorithm models smoothness by an energy functional penalizing discontinuities in position or higher-order derivatives \cite{asdf}.  When the energy function penalizes the second-derivative, the model becomes equivalent to Euler&#39;s &quot;elastica&quot; model of an ideal elastic rod, which also mimicks the classic mechanical splines used in traditional drafting and shipbuilding \cite{levien2008}.  Other spline models have been developed for different applications, including visual modelling, data interpolation (e.g. cubic splines), data smoothing (cubic smoothing splines), and visual modelling (B-splines).  In computer vision, splines provide a convenient representation for nonrigid structure and motion.  Thin-plate splines \cite{wood2003} have been applied to reconstruction of deformable surfaces \cite{schmid} \cite{mcinerney1993} \cite{perriollat2011}.&lt;/p&gt;

&lt;p&gt;  Gaussian process (GP) theory provides an elegant probabilistic representation of continous curves and surfaces\cite{williams2006}.  As shown in \cite{wahba1990}, classic cubic smoothing splines can be reformulated as the maximum posterior curve under a particular GP prior and i.i.d. Gaussian noise.  Other GP models can acheive different types of smoothness, like simple continuity (white noise process), or (C^\inf) smoothness (squared-exponential covariance process).  By formulating the curve model probabilisitically, GP&#39;s can represent and propagate uncertainty in continuous curves in a principled and tractible manner .&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Curve models&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Edge-based likelihoods / energy functions&lt;/li&gt;
&lt;li&gt;Thin plate splines&lt;/li&gt;
&lt;li&gt;Deformable models&lt;/li&gt;
&lt;li&gt;Sampling-based Structure From Motion&lt;/li&gt;
&lt;li&gt;Misc Structure from Motion&lt;/li&gt;
&lt;li&gt;Model-based Reconstruction&lt;/li&gt;
&lt;li&gt;Evaluation
  Middlebury metric.
  Evaluation of&lt;/li&gt;
&lt;li&gt;Gaussian Process&lt;/li&gt;
&lt;li&gt;Miscellaneous&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Limitations or Key Assumptions&lt;/h2&gt;

&lt;p&gt;foliage doesn&#39;t significantly obscure branching structure.&lt;/p&gt;

&lt;h2&gt;Potential Outcomes, Contributions to Knowledge&lt;/h2&gt;

&lt;h2&gt;Proposed Chapters&lt;/h2&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;We model stems as a set of points along the medial axis of a tube with circular cross-sections.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Dissertation Proposal - Preparation, Organization</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/01/06/work-log"/>
   <updated>2014-01-06T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/01/06/work-log</id>
   <content type="html">&lt;p&gt;Spent morning and early afternoon doing preparation for my dissertation proposal by doing some background reading and laying out a rough sketch of what I&#39;d like to cover.&lt;/p&gt;

&lt;p&gt;Read Ernesto&#39;s dissertation proposal.  The bulk of Ernesto&#39;s proposal seemed to be his two published papers, with an extended literature review and a brief description of remaining work to be done.  This isn&#39;t too surprising, since he was so far along in his research when he wrote the proposal.  Unfortunately, I have no such publications to draw on, but I may be able to use part of the tracking paper, which I was third author on.  But on the whole, I&#39;ll need to lean more in the &quot;proposal&quot; direction, and less in the &quot;dissertation&quot; direction than Ernesto was able to.  Time to dig for more representative examples of the proposal I&#39;ll be writing...&lt;/p&gt;

&lt;p&gt;Read &lt;a href=&quot;http://www.gwr.arizona.edu/writingproposal1.htm&quot;&gt;this article&lt;/a&gt;, describing a dissertation proposal in science.  The second page has a nice organization of possible sections for my proposal.&lt;/p&gt;

&lt;p&gt;According to the aforementioned article, length is 10-40 pages.  This is supported by Ernesto&#39;s proposal (25 pages w/ references), and &lt;a href=&quot;http://www.cs.unc.edu/~cssa/guides/proposals/&quot;&gt;these example proposals from computer science&lt;/a&gt;, which fall between 9 and 20 pages, with references.  The number of references is between 14 and 85, with a median around 20.&lt;/p&gt;

&lt;p&gt;I was surprised to see the level of brevity and abstractness in most of the example proposals above.  I&#39;m assuming these were written early in the research phase, shortly after the end of coursework;  it is encouraging to see that this document need not be a &lt;em&gt;tour de force&lt;/em&gt;.  Overall, I&#39;m thinking that since I&#39;ve developed my research so extensively, my proposal will probably be heavier on references and detail, and likely longer than the average.  However, I need to avoid falling into the trap of trying to write my dissertation instead of a proposal.&lt;/p&gt;

&lt;p&gt;I&#39;m feeling more confident now that I can finish this by the deadline I set for myself of January 31, and a reschedule hopefully won&#39;t be needed.  My initial investigation suggested that writing the proposal should take three to nine months, a surprise that knocked the wind out of me.   I realize now that much of that time is spent investigating topics, doing initial research, and reviewing literature, not specifically writing.  Since I&#39;ve completed those steps (extensively!) it should be reasonable to assume I can write the full document in 2.5 weeks.  I&#39;d like to get Kobus a first draft within the week, with at least the rough structure layed out, so he can correct my course if I&#39;m way off.&lt;/p&gt;

&lt;p&gt;I briefly reviewed the computer science department&#39;s graduate program policy for comp exams (something I haven&#39;t looked it in probably far too long!) and realized I should have scheduled my dissertation proposal in Fall 2011!!  It should be no surprise the department was urging me to complete this right away!&lt;/p&gt;

&lt;h2&gt;Organizational Notes&lt;/h2&gt;

&lt;p&gt;Below are notes I jotted down while reading Ernesto&#39;s proposal.&lt;/p&gt;

&lt;h3&gt;Parts&lt;/h3&gt;

&lt;p&gt;Research items worth covering in the proposal (or dedicating chapters to in the dissertation proper)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Edge extraction, Stroke-width transform skeleton.&lt;/li&gt;
&lt;li&gt;Bayesian Model

&lt;ul&gt;
&lt;li&gt;GPU-enabled edge-based likelihood

&lt;ul&gt;
&lt;li&gt;Blurred-difference likelihood&lt;/li&gt;
&lt;li&gt;Chamfer likelihood&lt;/li&gt;
&lt;li&gt;GMM likelihood&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Prior: &quot;Branching Gaussian Process&quot;

&lt;ul&gt;
&lt;li&gt;Modification for multimodal likelihoods (Importance sampling)&lt;/li&gt;
&lt;li&gt;novel covariance function&lt;/li&gt;
&lt;li&gt;Temporal modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Marginalization: Laplace Approximation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Initial estimate: Dynamic programming algorithm for multi-view triangulation&lt;/li&gt;
&lt;li&gt;Inference: MCMCDA&lt;/li&gt;
&lt;li&gt;Index estimation

&lt;ul&gt;
&lt;li&gt;Analytical gradients derived&lt;/li&gt;
&lt;li&gt;Dimensionality reduction? (for pixel likelihood)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Background / Related work&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Multi-view Reconstruction

&lt;ul&gt;
&lt;li&gt;Visual Hull&lt;/li&gt;
&lt;li&gt;Voxel-based (Hough transform)&lt;/li&gt;
&lt;li&gt;Space carving (Hough + photoconsistency)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Biological application

&lt;ul&gt;
&lt;li&gt;Root structure Architecture papers&lt;/li&gt;
&lt;li&gt;Neuron Tracing&lt;/li&gt;
&lt;li&gt;Vascular segmentation and modeling&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CCwQFjAA&amp;amp;url=http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2013%2Fpapers%2FTabb_Shape_from_Silhouette_2013_CVPR_paper.pdf&amp;amp;ei=SELLUs63G4e9rgHwuYHYAw&amp;amp;usg=AFQjCNHAaFQi-2H2zTQUOSm67WI2p87odw&amp;amp;sig2=L0V4Txw-fqypkloITh-dlA&amp;amp;bvm=bv.58187178,d.aWM&quot;&gt;Tree branches - Amy Tabb, CVPR-2013&lt;/a&gt; [pdf]&lt;/li&gt;
&lt;li&gt;L-systems - &lt;a href=&quot;http://vladlen.info/publications/metropolis-procedural-modeling/&quot;&gt;Metropolis Procedural Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;S-C Zhu plant paper - Bayesian reconstruction of 3d shapes and scenes from a single image&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tracking

&lt;ul&gt;
&lt;li&gt;Oh et al.&lt;/li&gt;
&lt;li&gt;Ernesto&lt;/li&gt;
&lt;li&gt;Tracklets? (TODO)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bayesian Model Choice

&lt;ul&gt;
&lt;li&gt;P.J. Green, RJMCMC&lt;/li&gt;
&lt;li&gt;Laplace approximation&lt;/li&gt;
&lt;li&gt;Candidates estimator&lt;/li&gt;
&lt;li&gt;Examples without model choice?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Curve models

&lt;ul&gt;
&lt;li&gt;Song-Chun Zhu - Parsing Images into Regions, Curves, and Curve Groups&lt;/li&gt;
&lt;li&gt;snakes&lt;/li&gt;
&lt;li&gt;Curve indicator random field&lt;/li&gt;
&lt;li&gt;Implicit definitions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Edge-based likelihoods / energy functions

&lt;ul&gt;
&lt;li&gt;Curve indicator random field&lt;/li&gt;
&lt;li&gt;Joe Schlecht&#39;s papers&lt;/li&gt;
&lt;li&gt;Chamfer matching

&lt;ul&gt;
&lt;li&gt;Shotton, PAMI 2007 - Multi-Scale Categorical Object Recognition Using Contour Fragments&lt;/li&gt;
&lt;li&gt;Shotton, ICCV 2005 - Contour-Based Learning for Object Detection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Poon &amp;amp; Fleet 2002 - Hybrid Monte Carlo Filtering: Edge-Based People Tracking&lt;/li&gt;
&lt;li&gt;TODO: More examples of edges in bayesian inference&lt;/li&gt;
&lt;li&gt;Gradient Vector Flow&lt;/li&gt;
&lt;li&gt;Blurred Gaussian&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Thin plate splines

&lt;ul&gt;
&lt;li&gt;Ferarri - Accurate Object Detection with Deformable Shape Models Learnt from Images&lt;/li&gt;
&lt;li&gt;TODO: others&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deformable models

&lt;ul&gt;
&lt;li&gt;Fua, Uratsen (TODO)&lt;/li&gt;
&lt;li&gt;Monocular Template-based Reconstruction of Inextensible Surfaces Perriollat, Hartley, and Bartoli&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sampling-based Structure From Motion

&lt;ul&gt;
&lt;li&gt;Dellaert - SfM Without Correspondence (and other?)&lt;/li&gt;
&lt;li&gt;Forsyth - Bayesian Structure From Motion; Joy of Sampling&lt;/li&gt;
&lt;li&gt;TODO: dig here&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Misc Structure from Motion

&lt;ul&gt;
&lt;li&gt;Semantic Structure From Motion?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model-based Reconstruction

&lt;ul&gt;
&lt;li&gt;Joe, Luca&#39;s work&lt;/li&gt;
&lt;li&gt;Savarese, Fei-Fei - 3D generic object categorization, localization and pose estimation&lt;/li&gt;
&lt;li&gt;TODO: dig here&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Evaluation

&lt;ul&gt;
&lt;li&gt;Diadem Challenge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gaussian Process

&lt;ul&gt;
&lt;li&gt;Basic: Williams and Rasmussen&lt;/li&gt;
&lt;li&gt;GP-LVM&lt;/li&gt;
&lt;li&gt;Urstein Ulenbeck process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Miscellaneous

&lt;ul&gt;
&lt;li&gt;Stroke width Transform&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Layout Ideas&lt;/h3&gt;

&lt;p&gt;An initial strategy for laying out the proposal is below.  I may want to rethink this after reading &lt;a href=&quot;http://www.gwr.arizona.edu/writingproposal1.htm&quot;&gt;this guide&lt;/a&gt;, which suggests a much higher-level strategy.  However, the level of detail in the strategy that follows may be justified by the late stage I&#39;m at with my research; could contribute to a stronger argument.  Should sleep on it.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Intro.&lt;/em&gt; describe problem, motivate and give background.
&lt;em&gt;Body.&lt;/em&gt; One Seciton per part, with one or more of the following parts&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Background and related work (show it&#39;s sensible and proven, but also novel in this case)&lt;/li&gt;
&lt;li&gt;Progress so far (include derived equations, intermediate results, etc.)&lt;/li&gt;
&lt;li&gt;Work to be done (defend why it&#39;s promising).&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Future Work&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Leafs, flowers?&lt;/li&gt;
&lt;li&gt;Root structure architecture?&lt;/li&gt;
&lt;li&gt;Neurons?&lt;/li&gt;
&lt;li&gt;Vascular modeling&lt;/li&gt;
&lt;li&gt;Likelihood improvements - color, patch-based?&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Diagrams&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Camera setup

&lt;ul&gt;
&lt;li&gt;One camera, multiple angles&lt;/li&gt;
&lt;li&gt;Multiple cameras, many angles&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Degenerate likelihood

&lt;ul&gt;
&lt;li&gt;(rough diagram in notes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data / edges&lt;/li&gt;
&lt;li&gt;Likelihoods

&lt;ul&gt;
&lt;li&gt;BD Likelihood

&lt;ul&gt;
&lt;li&gt;GMM per-pixel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Equations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Two-phase likelhood&lt;/li&gt;
&lt;li&gt;Curve Covariance - 1. smooth, 2. Rate and offset, 3. temporal&lt;/li&gt;
&lt;li&gt;Branching covariance&lt;/li&gt;
&lt;li&gt;Laplace approximation&lt;/li&gt;
&lt;li&gt;Marginal likelihood approx w/ Laplace&lt;/li&gt;
&lt;li&gt;Mean approx w/ Laplace&lt;/li&gt;
&lt;li&gt;Index gradients&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Miscellaneous Thoughts&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Think more on diagrams; what do I have?  what do I need?&lt;/li&gt;
&lt;li&gt;Width as depth-cue; single-view reconstruction&lt;/li&gt;
&lt;li&gt;GPU-based gradient?  Can render index w/ each edge. tell GPU how index moves to avoid re-rendering full model.&lt;/li&gt;
&lt;li&gt;Is mean-offset really a problem?  If there&#39;s no size/shape distortion (pure translation), maybe not in many applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use chicken/egg for index optimization in the wild&lt;/strong&gt;. As opposed to energy minimization we use for ground-truth.  The data will drive the fitting well enough so clever index fitting isn&#39;t necessary.&lt;/li&gt;
&lt;li&gt;Remember to draw from the blog (TODO)&lt;/li&gt;
&lt;li&gt;Add MRF smoothness to pixel Likelihood? (to address non-dependence)&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Proposal TODO:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Contact Dr. Zhang or Dr. Gniady about joining the committee.&lt;/li&gt;
&lt;li&gt;Complete form in UAccess.&lt;/li&gt;
&lt;li&gt;Look for quality blog posts to draw content/equations&lt;/li&gt;
&lt;li&gt;Lit Review

&lt;ul&gt;
&lt;li&gt;Look into tracklets; MCMCDA?  Ernesto has a referencek&lt;/li&gt;
&lt;li&gt;examples ignoring model choice problem?&lt;/li&gt;
&lt;li&gt;More edge-based likelihood applications

&lt;ul&gt;
&lt;li&gt;Ferrari papers?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Misc TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Dig into (Vladlen Koltun&#39;s literature)[http://vladlen.info/] on reconstruction.&lt;/li&gt;
&lt;li&gt;Dig into Ferrari edge/contour literature.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Reading: Semantic SLAM w/ GPLVM shape priors; FIRE reading</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2014/01/03/work-log"/>
   <updated>2014-01-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2014/01/03/work-log</id>
   <content type="html">&lt;p&gt;Read two papers.  The first was David Sbarra&#39;s paper for the FIRE project, modeling sleep disturbances vs. depression symptoms.  The second is a dense SLAM paper from CVPR 2013.&lt;/p&gt;

&lt;p&gt;Decomposing&lt;/p&gt;

&lt;h2&gt;Decomposing Depression: On the Prospective and Reciprocal Dynamics of Mood and Sleep Disturbances&lt;/h2&gt;

&lt;p&gt;by David A. Sbarra and John J. B. Allen&lt;/p&gt;

&lt;p&gt;David&#39;s paper, which will likely play a role in the FIRE project this Spring semester.  Uses time-series data of ~100 people at 5 time points describing two variables: mood and sleep disruptions.  A dynamical model (first order diff-eq) is developed to model the data, which captures drift, self-regulation, and inter-variable coupling.&lt;/p&gt;

&lt;p&gt;Since the model is trained and evaluated on the same data, it isn&#39;t clear how predictive this model would be on held-out data.  Chi-squared test is used to determine which parameters are meaningful; I&#39;m not familiar enough with this method to comment on it, but the Bayesian literatures constant struggle with model selection suggests that the classical (i.e. frequentist) methods like this may need scruitiny.&lt;/p&gt;

&lt;p&gt;Synthetic data is drawn from the resulting fit for 5 subjects, using observations for initial values. I couldn&#39;t compare it to the true data or maybe I misunderstood the plot -- perhaps David can elaborate on this.&lt;/p&gt;

&lt;p&gt;A vector-field plot nicely illustrates the flow of the dynamical system.&lt;/p&gt;

&lt;p&gt;Can 6 parameters really model this data well?  Need to get familiar with the data.  Perhaps a good candidate nonparametric modeling?&lt;/p&gt;

&lt;p&gt;Ideas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Make Bayesian by using LDS similar to Jinyan&#39;s work.&lt;/li&gt;
&lt;li&gt;Model the direction at each grid-point independently, using GP to enforce smoothness and avoid overfitting.&lt;/li&gt;
&lt;li&gt;Consider higher-order diff-eq.&lt;/li&gt;
&lt;li&gt;Use a GP-LVM model to handle non-linear dynamics.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Dense Reconstruction Using 3D Ojbect Shape Priors&lt;/h2&gt;

&lt;p&gt;Uses GP-LVM-based shape priors (cars) to improve existing dense SLAM implementation.  Manages to heavilly leverage parallelism in CPU and GPU to achieve (arguably) real-time performance.  Use Feltzenswalb car detector to find consistent detections in two views and perform rough pose estimation.  Uses color-based (3D histogram) and depth-based energy functions to refine pose and shape; authors derive the gradient of these energy functions w.r.t. rigid transformations and GP-LVM latent variables.  Tractibility of GP-LVM addressed by taking lowest N frequencies from a DCT; reduced 128x128x128 to 25x25x25.&lt;/p&gt;

&lt;p&gt;How is the GP-LVM trained?  Where is the training data coming from?&lt;/p&gt;

&lt;p&gt;Related papers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;ol type=&quot;a&quot;&gt;
&lt;li&gt;&lt;ol type=&quot;a&quot;&gt;
&lt;li&gt;Newcombe, A. J. Davison, S. Izadi, P. Kohli, O. Hilliges, J. Shotton, D. Molyneaux, S. Hodges, D. Kim, and A. Fitzgibbon, KinectFusion: Real-time dense surface mapping and tracking, Mixed and Augmented Reality (ISMAR), 2011 10th IEEE International Symposium on, pp. 127136, 2011.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The implicit level-set representation and the merging of depth images are the same as those from KinectFusion.  Also, KinectFusion&#39;s stereo results are used to evaluate their monocular results.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;ol type=&quot;a&quot;&gt;
&lt;li&gt;&lt;ol type=&quot;a&quot;&gt;
&lt;li&gt;Bao and S. Savarese, Semantic structure from motion, presented at the Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, 2011, pp. 20252032.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Spiritual predecessor of this paper -- use detectors to improve structure from motion.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/27/work-log"/>
   <updated>2013-12-27T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/27/work-log</id>
   <content type="html">&lt;h2&gt;Debugging&lt;/h2&gt;

&lt;p&gt;New energy function causes much worse index ordering.  Ordering before optimization is terrible; new energy function seems to introduce new local minima that we can&#39;t overcome.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running old version to recall problems.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Significant offset from LL reconstruction.  &lt;strong&gt;Increasing perturb_scale improves significantly&lt;/strong&gt; (from 2.5 to 81).  When influsence of wide-baseline views is small, pulling toward camera is significant; increasing perturb_scale increasing that influence.&lt;/p&gt;

&lt;p&gt;Side effect of increasing perturb_scale is massive increase of tip extension in the direction of the camera.  This is because the posterior is more peaked, decreasing the size of the global optimum.&lt;/p&gt;

&lt;p&gt;Adding index smoothness metaprior seems to help, but I&#39;m not confident in the implementation, since there&#39;s no unit test for it.  Also getting a small amount of index extension, possibly because end-caps arent a full unit away from previous point.  can this be fixed?  Can&#39;t really use chord-length in 2D, because this could contain significant noise.  Although using ordinal isn&#39;t much different.  Could store fraction of last point.  Also need to acknowledge that different sub-sampling periods cause GP parameters to change meaning.  Should re-number ordinals based on resampling.  Store in data?&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;unit test index metaprior&lt;/li&gt;
&lt;li&gt;experiment with constant length energy function?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;dataset 6: overextension in teal top curve, red mid curve
dataset 7: overextension in dk green top curve
dataset 8: extreme overextension in teal top curve; bad ll localization
dataset 9: overextension in red  top curve&lt;/p&gt;

&lt;p&gt;and others.  Always when ll point is in direction of previous curve.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Testing CL energy</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/24/work-log"/>
   <updated>2013-12-24T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/24/work-log</id>
   <content type="html">&lt;p&gt;Testing constant-length energy function.  First test: \(\eta\) and it&#39;s Jacobian, implemented in &lt;code&gt;test/test_eta_deriv.m&lt;/code&gt;.  Test passes.  Interesting observation: jacobian is nearly bidiagonal.  Hopefully the hessian will have similar form, so ignoring the off-diagonal terms won&#39;t be too detrimental.&lt;/p&gt;

&lt;p&gt;Need to implement end-to-end test for \(E\) and its gradient/hessian.  Compare against analytical Hessian estimate and the crude \(J&#39;J\) Hessian approximator.&lt;/p&gt;

&lt;p&gt;need to update likelihood means?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/23/work-log"/>
   <updated>2013-12-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/23/work-log</id>
   <content type="html">&lt;p&gt;Lost matlab workspace during reboot, because I accidentally saved the current figure instead of the workspace.  Working on reloading them.&lt;/p&gt;

&lt;p&gt;Realized I never documented why I abandoned the hyperprior GP (which models index smoothness) is failing.  Looking through the matlab logs to recreate the failing test.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Done; added notes to previous entries to better illustrate the story as it originally developed.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Implemented new constant-length energy function in &lt;code&gt;wacv-2012/cl_energy.m&lt;/code&gt;.  Need to test.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Constant-length energy function - Hessian</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/23/reference"/>
   <updated>2013-12-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/23/reference</id>
   <content type="html">&lt;p&gt;Recall energy function&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    E &amp;= \frac{1}{2} x^\top D^\top D x + \frac 12 \mu^\top D^\top D \mu  - x^\top D^\top \eta \\

\end{align}
\]
&lt;/div&gt;


&lt;p&gt;and its gradient,&lt;/p&gt;

&lt;div&gt;
\[
    E&#39; = \frac{\partial E}{\partial x} = x^\top D^\top D + \mu^\top D^\top D J_\mu - \eta^\top D - x^\top D^\top J_\eta
\]
&lt;/div&gt;


&lt;p&gt;The derivative w.r.t. t_i of E&#39; (i.e. the i-th row of the Hessian) is given by:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \frac{\partial E&#39;}{\partial t_i} &amp;=
        (D^\top D)_{:i} + 
        (J_\mu^\top D^\top D J_\mu)_{:i} + 
        \frac{\partial J_\mu^\top}{\partial t_i} D^\top D \mu - 
        (D^\top J_\eta)_{:i} -  
        (J_\eta^\top D^\top)_{:i} - 
        \left \{ x^\top D^\top \frac{\partial J_\eta}{\partial t_i} \right \}^\top
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Of the six terms, the third and sixth are particularly problematic when generalizing to the full hessian, because they involve the Jacobian of a Jacobian, which is a 3D tensor.&lt;/p&gt;

&lt;p&gt;To keep running time to \(O(n&lt;sup&gt;3&lt;/sup&gt;)\), we&#39;ll use diagonal approximations for those terms.&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \frac{\partial^2 \mu}{(\partial t_i)^2} &amp;= \frac{\partial}{\partial t_i} (J_\mu)_{:i} \\

                                            &amp;= \frac{\partial}{\partial t_i} \left [\operatorname{diag_{3x1}}(\Delta_{3x3} z) + \left( \Delta_{1x3} \right)^\top \odot \operatorname{repmat}(z_3 , N/3, 1) + K_* J_z \right ]_{:i} \\
                                            &amp;= \frac{\partial}{\partial t_i} \left( 
                                                    \begin{array}{c}
                                                    0 \\ 0 \\ \vdots \\ \delta^\top_{i3x3} z \\ \vdots \\ 0 \\ 0
                                                    \end{array} \right )
                                                    + \delta_{i,3x1} \odot \operatorname{repmat}(z^{(3)}_i, N/3, 1) 
    + K_* (J_z )_{:i} \\
                                            &amp;= \left( 
                                                    \begin{array}{c}
                                                    0 \\ 0 \\ \vdots \\ C^\top_{i3x3} z \\ \vdots \\ 0 \\ 0
                                                    \end{array} \right ) + 
                                                \left (
                                                    \begin{array}{c}
                                                    0 \\ 0 \\ \vdots \\ \delta^\top_{i3x3} z&#39;_i \\ \vdots \\ 0 \\ 0
                                                    \end{array} \right )
                                                    + C_{i,3x1} \odot \operatorname{repmat}(z^{(3)}_i, N/3, 1) 
                                                    + \delta_{i,3x1} \odot \operatorname{repmat}(z&#39;^{(3)}_i, N/3, 1) 
                                                        + K_* (J&#39;_z )_{:i} \\
                                                        + K&#39;_* (J_z )_{:i} \\
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Started deriving \(J&#39;_z\), but was stymied by the complexity.  For now, we&#39;ll resort to ignoring these terms and see how the optimization goes.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/19/work-log"/>
   <updated>2013-12-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting Jacobians of z and mu.&lt;/p&gt;

&lt;p&gt;Analytical and numerical versions of z&#39;s Jacobian differ significantly in some entries.  To troubleshoot, stepping backward through the derivations of J_z outlined in &lt;a href=&quot;/ksimek/research/2013/12/12/reference/&quot;&gt;the reference post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I will refer to the final equation from that post and count backward from the final line, describing the result of implementing each line.  E.g. line 0 is the final result, the one used to compute z_ana; line 1 is the one labelled &quot;(3D version)&quot; .&lt;/p&gt;

&lt;p&gt;Setup:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Track = detach(trash_Tracks_11(end), params_test);
K = get_K(Track); 
S = Track.ll_S;
U = speye(size(K)) + S * K * S&#39;;
Ui = inv(U);
kern = get_model_kernel_derivative(params_test);
ind = get_curve_indices(Track);
views = Track.ll_views_flat;
Delta = eval_kernel(kern, ind, ind, views, views);
Delta3 = one_d_to_three_d(Delta);

i = 1;   
i3 = 3*(i-1)+1;
I = i3:i3+2;

delta_i = Delta(i,:)&#39;;
delta_i3 = one_d_to_three_d(delta_i);

dK = zeros(size(K));
dK(I,:) = dK(I,:) + Delta3(I,:);
dK(:,I) = dK(:,I) + Delta3(I,:)&#39;;

A = A = S&#39; * Ui * S;
z = z = A * y;
N = length(K);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Implementing line 1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line_1 = -A(:,I) *delta_i3&#39; * z - A * delta_i3 * z(i3:i3+2);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It actually looks pretty good.  So why is line_0 wrong?&lt;/p&gt;

&lt;p&gt;Let&#39;s isolate the first term.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line_1_a = -A(:,I) *delta_i3&#39; * z;
line_0_a = -sum_1xN(A .* repmat((Delta3 * z)&#39;, N,1), 3);
plot(xx, line_1_a - line_0_a(:,i))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Error within 1e-12.  Great!  Error must be in the second term...&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;line_1_b = A * delta_i3 * z(i3:i3+2);
line_0_b =  - A * (Delta3(1:3:end, :)&#39; .* repmat(reshape(z,3,[]), N/3, 1));
plot(xx, line_1_b - line_0_b(:,i))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep.  it&#39;s a mess.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Found the bug.  Was incorrectly trying to implement Delta&lt;em&gt;{1x3} by taking Delta&lt;/em&gt;{3x3}(1:3:end, :), which is totally wrong.  Also, was mis-associating the matrix multiplication and elementwise multiplication.  That is, I was taking (A * Delta_1x3) .&lt;em&gt; XXX  instead of A * (Delta_1x3 .&lt;/em&gt; XXX).&lt;/p&gt;

&lt;p&gt;Those two fixes, and the dZ test now passes.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still getting bad results for dmu.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Same bug as for dZ.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Developed new constant-width energy function, and derived its gradient.  Writeup &lt;a href=&quot;/ksimek/research/2013/12/19/reference&quot;&gt;is here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Derive Hessian of energy function&lt;/li&gt;
&lt;li&gt;implement and test gradient and hessian.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Constant-length energy function -- revisited</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/19/reference"/>
   <updated>2013-12-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/19/reference</id>
   <content type="html">&lt;p&gt;My &lt;a href=&quot;/ksimek/research/2013/12/12/reference/&quot;&gt;earlier derivation&lt;/a&gt; of the constant-length energy function was flawed, because it pooled the individual lengths before comparing them to the pooled index-spacing.  Thus, this energy function enforces the &lt;em&gt;sum&lt;/em&gt; of squared lengths but not the individual lengths.  Chalk it up to trying to be too clever in avoiding a square root.&lt;/p&gt;

&lt;p&gt;In what follows, I derive a new energy function and its gradient.  The Jacobians of \(\mu\) and \(z\) are re-used from the earlier treatment.&lt;/p&gt;

&lt;div&gt;
&lt;p&gt;
  Let \(\mu\) be the maximum posterior curve, given an index set, \(\mathbf{x}\), arranged as a single column in &quot;xyzxyz&quot; format.  Let \(\mu^{(3)}\) be the 3 by N/3 matrix obtained by rearranging the points of \(\mu\) into column vectors.  That is, the i-th column \(\mu^{(3)}_i\) is the i-th reconstructed point.  
  &lt;/p&gt;

&lt;p&gt;
  Let \(\eta\) be the vector of absolute distances between adjacent points in \(mu^{3}\). Formally,
  
\[
    \eta_i = \| \mu^{(3)}_i - \mu^{(3)}_{i-1} \|.
\]

Note that \(\eta^\top \eta = \|\eta\|^2 = \mu^\top D^\top D \mu\), where \(D\) is the adjacent differences matrix, adapted to operate on column vectors in the &quot;xyzxyz&quot; format.

&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;The constant width energy function is defined as&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    E &amp;= \frac{1}{2} \left ( Dx  - \eta \right ) ^2 \\
      &amp;= \frac{1}{2} x^\top D^\top D x + \frac 12 \eta^\top \eta  - x^\top D^\top \eta \\
      &amp;= \frac{1}{2} x^\top D^\top D x + \frac 12 \mu^\top D^\top D \mu  - x^\top D^\top \eta \\

\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Gradient is given by&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial E}{\partial x} = x^\top D^\top D + \mu^\top D^\top D J_\mu - \eta^\top D - x^\top D^\top J_\eta
\]
&lt;/div&gt;


&lt;p&gt;where \(J_z\) is the Jacobian of \(\mathbf{z}\) w.r.t. \(\mathbf{x}\).&lt;/p&gt;

&lt;p&gt;The Jacobian of \(z\) and \(\mu\) are derived in &lt;a href=&quot;/ksimek/research/2013/12/12/reference/&quot;&gt;this earlier post&lt;/a&gt;.  It remains to find the Jacobian of \(\eta\).&lt;/p&gt;

&lt;div&gt;
Note the identity \( \eta_i^2 = \| \mu^{(3)}_i - \mu^{(3)}_{i-1} \|^2 \) can be rewritten in terms of the full vector \(\eta\) as 

\[
\eta \odot \eta = \operatorname{sum_{3x1}}(D \mu \odot D \mu)
\]

where \(\operatorname{sum_{kx1}}\) implements k-way blockwise summation over columns of a matrix.  Formally, it is the function \(f : \mathbb{R}^{NxM} \rightarrow R^{(N/k)xM}\) (for any N divisible by k), such that 

\[
    (f(A))_{ij} = \sum_{s = (i-1)\times k+1}^{i \times k} A_{sj}
\]
&lt;/div&gt;


&lt;p&gt;The Jacobian of \(\mathbf{\eta}\) can then be given by&lt;/p&gt;

&lt;div&gt;
\[

\begin{align}
\frac{\partial \eta}{\partial x_i} &amp;= \frac{\partial}{\partial x_i} \left ( \eta \odot \eta \right)^{\circ\frac12} \\
                                 &amp;= \frac{\partial}{\partial x_i} \left ( \operatorname{sum_{3x1}}(D \mu \odot D \mu) \right)^{\circ\frac12} \\
                                &amp;= \frac12 \left ( \eta \odot \eta \right)^{\circ \frac{-1}{2}} \odot \frac{\partial}{\partial x_i} 
                                    \left ( \operatorname{sum_{3x1}}(D \mu \odot D \mu )\right) \\
                                &amp;= \frac12 \eta^{\circ (-1)}  \odot
                                     \left ( \operatorname{sum_{3x1}}(\frac{\partial}{\partial x_i} D \mu \odot D \mu )\right) \\
                        J_\mu &amp;= \frac12 \eta^{\circ (-1)}  \odot
                                     \left ( \operatorname{sum_{3x1}}( 2 D \mu \odot D J_\mu )\right) \\
                                &amp;= \eta^{\circ (-1)} \odot \left ( \operatorname{sum_{3x1}}( D \mu \odot D J_\mu )\right) \\

\end{align}
\]

where \(x^{\circ(-1)} = \left( \frac{1}{x_{ij}} \right)\) is the Hadamard (i.e. element-wise) inverse, and \(x^{\circ \frac12}\) is the Hadamard root.
&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>Constant-length energy function</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/12/reference"/>
   <updated>2013-12-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/12/reference</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Update: What follows is an early working of the constant-length energy function and much of which I learned to be invalid.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Energy function and its gradient are:&lt;/p&gt;

&lt;div&gt;
\[
E = 0.5 (0.5 \mu^\top D^\top D \mu - 0.5 t^\top D^\top D t)^2 \\
\frac{\partial E}{\partial t_i} = 2E \left [ 0.5 \mu^\top D^\top D J_\mu - t^\top D^\top D  \right ]
\]
&lt;/div&gt;


&lt;p&gt;Mu and it&#39;s jacobian are:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\mu &amp;= K_* S^\top \left( S K S^\top + I\right)^{-1} S y \\
    &amp;= K_* S^\top U^{-1} S y \\
    &amp;= K_* z \\
\frac{\partial \mu}{\partial t_i} 
    &amp;= K&#39;_* z + K_* z&#39; \\
J_\mu &amp;= \operatorname{diag_{3x1}}(\Delta_{3x3} z) + \left( \Delta_{1x3} \right)^\top \odot \operatorname{repmat}(z_3 N/3, 1) + K_* J_z 
\end{align}
\]

where \(J_z\) is the Jacobian of \(z\), and \(z_3\) is the re-arrangement of \(z\) into columns of xyz vectors.  \(\Delta_{3x3}\) is the conversion of \(\Delta\) to 3D by block-diagonalizing three copies of \(\Delta\) and permuting rows and columns so each (x,y,z) is grouped together.  \(\Delta_{1x3}\) repeats \(\Delta\) over three columns and permuting columns.  \(\operatorname{diag_{3x1}}\) is a modified diagonalization operator where x is split into 3x1 matrices, which are arranged into block-diagonal form.
&lt;/div&gt;




&lt;div&gt;
Let \(A = S^\top U^{-1} S \), so \(z = Ay \).
\[
\begin{align}
\frac{\partial z}{\partial t_i} &amp;= \frac{\partial }{\partial t_i} D^\top U^{-1} S y \\
                              &amp;= - S^\top U^{-1} \frac{\partial U}{\partial t_i} U^{-1} S y \\
                              &amp;= - S^\top U^{-1} S \frac{\partial K}{\partial t_i} S^\top U^{-1} S y \\
                              &amp;= - A \frac{\partial K}{\partial t_i} z \\
                              &amp;= - A \left \{
                                      \left (
                                      \begin{array}{c} 
                                          \mathbf{0}             \\
                                          \vdots        \\
                                          \delta_i^\top \\
                                          \vdots        \\
                                          \mathbf{0}             \\
                                      \end{array} \right ) + 
                                      \left (
                                          \begin{array}{c}
                                          \mathbf{0} &amp; \cdots &amp; \delta_i &amp; \cdots &amp; \mathbf{0}
                                          \end{array} 
                                      \right) 
                                    \right \} z \\
         &amp;= - A_i (\delta_i^\top z) - A \delta_i z_i \\
         &amp;= - A_{3i:3i+2} (\delta_{i,3x3}^\top z) - A \delta_{i,3x3} z_{3i:3i+1} &amp; \text{(3D version)} \\
J_z &amp;= - \operatorname{sum_{1x3}}\left(A \odot \left( \Delta_{3x3} z \right)^\top \right) - A \left [ \left( \Delta_{1x3}  \right )^\top \odot \operatorname{repmat}(z_3, N/3, 1) \right ]
\end{align}
\]
&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>Optimizing indices: Length constraints</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/10/work-log"/>
   <updated>2013-12-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/10/work-log</id>
   <content type="html">&lt;p&gt;Continuing along the inspiration from the yesterday&#39;s Pascal Fua paper investigating length constraint terms for index optimization.&lt;/p&gt;

&lt;p&gt;It&#39;s difficult, because length implies known structure, which in turn implies known indices.  So constraining indices based on length feels like a circular argument.  This has been a sticking point in my thinking for a long time.  But we can formulate this in terms of expectations, namely, given an index set, the &lt;em&gt;expected segment length&lt;/em&gt; should be equal to difference of their indices.&lt;/p&gt;

&lt;p&gt;Let \( L_i = |x_i - x_{i-1}| \) be the distance between points with adjacent indices.&lt;/p&gt;

&lt;p&gt;Expected length given and index set, \(t\) is:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\mathrm{E}[L_i^2] &amp;= \mathrm{E}[\| x_i - x_{i-1}\|^2] \\
                &amp;= \mathrm{E}[x_i^2 - 2 x_i x_{i-1} + x_{i-1}^2 ] \\
                &amp;= \mathrm{E}[x_i^2] - 2 \mathrm{E}[x_i x_{i-1}] + \mathrm{E}[x_{i-1}^2 ] \\
                &amp;= \left( \mathrm{Cov}[x_i]  + \left( \mathrm{E}[x_i] \right) \right) - 2 \left( \mathrm{Cov}[x_i, x_{i-1}] + \mathrm{E}[x_i]\mathrm{E}[x_{i-1}] \right) + \left( \mathrm{Cov}[x_{i-1}]  + \left( \mathrm{E}[x_{i-1}] \right) \right)\\
                &amp;= \left(k(x_i,x_i) + k(x_{i-1}, x_{i-1} - 2 k(x_i, x_{i-1}) \right) + \left( \mu_i^2 + \mu_{i-1}^2 - 2 \mu_i \mu_{i-1} \right)\\
                &amp;= D K_i D^\top + (D \mu_{i,i-1})^\top D \mu_{i,i-1}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Where D is the differencing matrix, \(D = (1, -1)\).&lt;/p&gt;

&lt;p&gt;Observe that each dimension is independent in the squared distance equation, so we can treat each dimension separately in the equation above.&lt;/p&gt;

&lt;p&gt;For now, we&#39;ll aproximate by ignoring the first term, since nearby indices should be nearly 100% corellated.  So all that remains is the squared difference of the reconstructed points.&lt;/p&gt;

&lt;p&gt;We&#39;ll consider two different energy functions.  The first is exactly what we&#39;d like to minimize, but it needs an approximation.&lt;/p&gt;

&lt;div&gt;
\[

E_i = \left( \mathrm{E}[L_i] - \Delta_{t_i} \right)^2
    \approx \left( \sqrt{\mathrm{E}[L_i^2]} - \Delta_{t_i} \right)^2

\]
&lt;/div&gt;


&lt;p&gt;We can&#39;t compute \(\mathrm{E}[L_i] \) directly, so we approximate it by taking the square root of the expected square length.  The square root function is concave, so Jensen&#39;s inequality tells us that this approximation will never under-estimate the expected length, and since our main objective is to prevent index shrinkage, overestimating is preferred to underestimating.    The result is \(D \mu \), the adjacent differences of the posterior mean.&lt;/p&gt;

&lt;p&gt;TODO: writeup derivation for gradient for \(E\) and \(\mu\)&lt;/p&gt;

&lt;p&gt;Implemented analytical Jacobian for \(\mu\) in &lt;code&gt;posterior_mu_gradient.m&lt;/code&gt;.  Some error in results, according to &lt;code&gt;test/test_mu_deriv.m&lt;/code&gt;, but passes the inspection test.  Overall, diagonal term looks okay, so error is probably in derivation of dZ.  Particularly damning is that the quality metric isn&#39;t sensitive to the delta step size.&lt;/p&gt;

&lt;p&gt;Should probably test Jacobian dZ.  It&#39;s undergone some changes today.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Yep, subtle but significant error.  Inspection suggests &lt;code&gt;term2&lt;/code&gt; is the culprit.  Can we focus on the individual faulty elements and check the partial derivatives?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Hyperprior</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/09/work-log"/>
   <updated>2013-12-09T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/09/work-log</id>
   <content type="html">&lt;p&gt;Training hyperprior over indices vs. observation ordinal.  Implemented in &lt;code&gt;wacv_2012/wacv_train_index_prior&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Training is apparently very sensitive to initial point.  Is the hessian wrong?  Implemented &lt;code&gt;test_mu_deriv()&lt;/code&gt;  to test hessian using finite differences.&lt;/p&gt;

&lt;p&gt;Struggling with modeling offset and rate.  Mean is usually zero, but not in this case.  Don&#39;t really care about initial conditions for now (and struggling with learning it anyway), so just learn smoothing sigma and set offset and rate covariance to large.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Index GP hyperprior doesn&#39;t fix index shrinkage issue, which is the root of several problems.  Tried another hack that adds an energy term to keep the total length constant.&lt;/p&gt;

&lt;p&gt;Still getting unexplained extension of curves.  Time to review the literature for ideas.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Reading more on GP Latent Variable Models.&lt;/p&gt;

&lt;p&gt;[1] N. D. Lawrence and A. J. Moore, Hierarchical Gaussian process latent variable models, presented at the ICML &#39;07: Proceedings of the 24th international conference on Machine learning, 2007.&lt;/p&gt;

&lt;p&gt;At once similar and different from what we&#39;re doing.  In some sense, the index is a latent variable, but we aren&#39;t using latent variables as a dimension reduction technique, as is the crux of the GPLVM approach.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Reading up on existing literature dealing with reconstructing inextensible structures.&lt;/p&gt;

&lt;p&gt;[1] M. Salzmann and P. Fua, Linear Local Models for Monocular Reconstruction of Deformable Surfaces, IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 5, pp. 931944.
[2] M. Salzmann, R. Urtasun, and P. Fua, Local deformation models for monocular 3D shape recovery, presented at the Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 2008, pp. 18.
[3] M. Perriollat, R. Hartley, and A. Bartoli, Monocular Template-based Reconstruction of Inextensible Surfaces, Int J Comput Vision, vol. 95, no. 2, Nov. 2011.
[4] J. Taylor, A. D. Jepson, and K. N. Kutulakos, Non-rigid structure from locally-rigid motion, presented at the Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, 2010, pp. 27612768.&lt;/p&gt;

&lt;p&gt;All these papers assume we can match keypoints of some patches, which isn&#39;t the case in our problem since our curves don&#39;t have distinctive texture.  However, I can draw inspiration from [1], in which several interesting energy functions are constructed as homogeneous linear equations and with linear constraints.  In one formulation, the normal posterior energy is supplemented with extra terms that prevent edge-lengths from changing.  In another formulation, the rigidity constraint is replaced with an inequality that prevents extension but disallows contraction, which permits sharp folds.&lt;/p&gt;

&lt;p&gt;There are significant differences in our case.  First, we don&#39;t have a reference structure to compare the model against, so we don&#39;t know the reference length of each segment.  Second, this assumes known correspondences, which we don&#39;t have.  But we can use the idea of adding an energy term based on length.&lt;/p&gt;

&lt;p&gt;Reference [2] uses GPLVM (Gaussian process latent variable model) to model local deformations.  The latent variable formulation allows for nonlinear deformations (e.g. kinks, creases).  On the downside, it&#39;s nonconvex, so is best suited for tracking only.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Struggling with index offset and shrinkage</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/07/work-log"/>
   <updated>2013-12-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/07/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;running with 5mm in-plane variance.  Dataset 4 and 5&#39;s issues seem to be resolved.  Interesting, since we still have isotropoic offset veriance in-place, too.&lt;/p&gt;

&lt;p&gt;Can we get old issue to return by setting in-plane variance to zero?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Yes.  And setting in-plane to nonzero and isotropic to zero?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Yes.  But shifting is very significant, and motion is large and random.&lt;/p&gt;

&lt;h2&gt;The offset mystery&lt;/h2&gt;

&lt;p&gt;There&#39;s obviously some part of the model thats resulting in these offsets, its just a matter of zeroing out parameters until the phenomenon goes away.&lt;/p&gt;

&lt;p&gt;Position perturbation variance is the root of the problem; both iso and nonisotropic version fail in this way.&lt;/p&gt;

&lt;p&gt;Some modification I made is causing indices to collapse...  probably insufficient flexibility since I disabled rate and smoothness perturbations.&lt;/p&gt;

&lt;p&gt;weird, rate and position variance seem tied to disappearing curves (index collape?).  Seems that increasing rate variance a lot is causing it.&lt;/p&gt;

&lt;p&gt;Increasing rate variance seems to facilitate perturb_rate_variance having more influcence.  Possibly because we&#39;re getting index shrinkage, so small fluctuations in rate result in large fluctuations in structure.&lt;/p&gt;

&lt;h2&gt;Preventing index shrinkage&lt;/h2&gt;

&lt;p&gt;My current hypothesis is that index shrinkage is resulting in the highly non-intuitive results we&#39;re seeing.  Intuition assumes index corresponds to distance, but when shrinkage occurs, this is no longer the case, so intuition becomes cloudy or fails entirely.&lt;/p&gt;

&lt;p&gt;We really need to constrain indices somehow, without preventing corrections.&lt;/p&gt;

&lt;p&gt;First attempt: force length to remain constant.  added a squared penalty over total change in index length.  It&#39;s tricky because of our view-grouped transformation; only one group contributes to the size of the maximum end-point, and only one index contributes to the minimum start-point.  Implemented, but didn&#39;t get desired result.  Instead, indices shifted wildly to comply with the length constraint.  In some cases, overall shrinkage occurred, combined with occasional spikes to keep the length up.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Observation: plot of index vs. observation number is &lt;em&gt;smooth&lt;/em&gt;.  Add a GP prior over index using observation ordinal as index.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Implemented in &lt;code&gt;optimize_ml_wrt_indices_2.m&lt;/code&gt;.  Initial tests seem promising, but need to train on parameters.  Zeroth and first derivitives w.r.t. parameters:&lt;/p&gt;

&lt;div&gt;
\[

\begin{align}
g(x) = \log p(x | t) &amp;= -0.5 x^\top \left( \sigma_1^2 K_1 + \sigma_2^2 K_2 + \sigma_3^2 K_3 \right)^{-1} x - 0.5 \log \left | \sigma_1^2 K_1 + \sigma_2^2 K_2 + \sigma_3^2 K_3 \right | - C \\
                &amp;= -0.5 x^\top U^{-1} x - 0.5 \log | U | - C \\

\frac{\partial g(x)}{\partial \sigma_i} &amp;= \sigma_i \left(z&#39; K_i z - \text{Tr}\left[U^{-1} K_i\right] \right)
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;where \(U = \sigma_1 K_1 + ... \) and \(z = U^{-1} x \).&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;get curve indices from reconstruction&lt;/li&gt;
&lt;li&gt;group by view; get corresponding ordinal; add to set&lt;/li&gt;
&lt;li&gt;construct energy function over set of curves using functions above&lt;/li&gt;
&lt;li&gt;initial fit of rate and offset using direct means;&lt;/li&gt;
&lt;li&gt;refine all using Newton-Raphson&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Troubleshooting excessive index drift in endpoints; fixing Hessian under variable transformation.</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/05/work-log"/>
   <updated>2013-12-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/05/work-log</id>
   <content type="html">&lt;p&gt;Getting bizarre spikes in indices during optimization.  Confirmed that removing the spikes will improve ML, but there&#39;s a steep well between the two local minima as we reduce the index value:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-ml_vs_index.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The index starts at a fairly reasonable initial value, so my only guess is that the Hessian is suggesting a large step which happens to step over the well.&lt;/p&gt;

&lt;p&gt;I&#39;m wondering if the hessian is screwy; or maybe it&#39;s just the transformation we&#39;re using.  Optimizing raw indices doesn&#39;t exhibit this problem, but it is a problem in our our current approach of working with log differences to prevent re-ordering.&lt;/p&gt;

&lt;p&gt;A prior over index spacing should probably prevent this; I&#39;m hesitatnt to add the extra complexity at this point, considering the additional training and troubleshooting it would entail.&lt;/p&gt;

&lt;p&gt;Should unit test the transformation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Gradient looks okay.&lt;/p&gt;

&lt;p&gt;On-diagonal elements have significant error!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-hessian_error.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Actually, significant off-diagonal error, but on-diagonal dominates.&lt;/p&gt;

&lt;p&gt;Since gradient is fine, and it uses the same jacobian, I&#39;m guessing the problem isn&#39;t the jacobian transformation, but the hessian itself.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Confirmed.  diagonal is (mostly) too high, error on order of 1e-3.  Gradient error is around 1e-9.&lt;/p&gt;

&lt;p&gt;Detective work.  Look at diagonal of each Hessian term, compare to residual of diagonal, look for patterns.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Ugh, worst afternoon ever.  Spent hours trying every trick in the book to track down the source of the error, including lots of time looking at the raw Hessian (it wasn&#39;t the raw Hessian).  Finally found the bug: I formula I used for the chain rule for Hessians was wrong.  In particular, it was missing a second term that (in my problem) corresponded to adding the transformed gradient to the diagonal.   See &lt;a href=&quot;http://en.wikipedia.org/wiki/Chain_rule#Higher_derivatives_of_multivariable_functions&quot;&gt;Faa di Bruno&#39;s formula&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Total error is much reduced now, but not zero.  around 0.1, instead of 20 before, new results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-hessian_error_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The norm-check is around 1e-4; very nice.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Re-running dataset 11 with hopes that we don&#39;t lose the global optimum.  Interesting observation: optimization requires more iterations than before to converge.  It seems a more conservative hessian results in smaller steps and is less likely   Looks better:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-reconst_hess_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice we&#39;re still getting offset, but at least the reconstruction is qualitatively better that before. However, now we&#39;re getting a small loop at the top:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-weird_loop.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It seems to be caused by changing of index order between views. Needs some thought to best address.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Re-running on all datasets.  Hopefully excessive index drift won&#39;t be too big an issue.  Possibly an extra term to prevent drift far from initial points would be sensible.&lt;/p&gt;

&lt;p&gt;Datasets 4,5  still drifts:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-05-drift_ds4.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/ksimek/research/img/2013-12-05-drift_ds5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Datasets 7,9  have detached curves&lt;/p&gt;

&lt;p&gt;Dataset 10, curve 2 (?) appears to have failed to prune&lt;/p&gt;

&lt;h2&gt;endpoint drift&lt;/h2&gt;

&lt;p&gt;It looks like interior points are confined by their neighbor points from drifting too far, but end points have no such constraint.  After a small amount of drift, they&#39;re able to loop back on themselves and land directly on the backprojection line.  It&#39;s surprising that the extra flexibility afforded by large spacing between indices doesn&#39;t cause marginal likelihood to suffer, since most of the new configurations are bad ones.&lt;/p&gt;

&lt;p&gt;Considerations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;in-plone offset perturbation&lt;/li&gt;
&lt;li&gt;penalize excessive drift&lt;/li&gt;
&lt;li&gt;penalize excessive gaps (a-la latent GP model).&lt;/li&gt;
&lt;li&gt;penalize shrinkage.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Constraining offset perturbation&lt;/h2&gt;

&lt;p&gt;Constructing the offset perturbation variance so perturbations can only occur parallel to the image plane.  Code is pretty straightforward:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cam_variance = blkdiag(repmat(sigma,N,N), repmat(sigma,N,N), zero(N,N));
camvariance(I,I) = cam_variance;
world_variance = R&#39; * cam_variance * R
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where R is the rotation matrix from world to camera coordinates (i.e. from the extrinsic matrix).&lt;/p&gt;

&lt;p&gt;The main difficulty here is logistical: throughout the codebase we store the prior variance in 1D format, and assume it will be expanded to 3D isotropically.  Now we have a non-isotropic part of the prior, and we need to make sure it&#39;s included wherever the prior is needed.&lt;/p&gt;

&lt;p&gt;Time for a real refactor.  Need a &#39;get_K&#39; function, instead of just taking the prior_K, adding the branch variance, and then adding the nonisotropic offset variance each time.  Throw error if prior_K or branch_variance isn&#39;t ready.  refactor strategy: search files for prior_K; those that call one_d_to_three_d shortly thereafter are refactor candidates, others will need further recursion to find the regactor point.  But it all starts with prior_K.  There shouldn&#39;t be that many leaf-nodes that use prior_K -- ML, gradient, reconstruction, WACV-spectific stuff...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Saturday&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Refactoring to use nonisotropic perturb position covariance.&lt;/p&gt;

&lt;p&gt;Refactoring construct_attachment_covariance will be difficult, because it works in 1D matrix format, but the new covariance must operate in 3D format.  Will involve refactoring everything to 3D, re-index using block indexing, and double-check that noting relies on an independence assumption.&lt;/p&gt;

&lt;p&gt;Better idea: apply perturb covariance after the fact.  All connected curves will get the same covariance offset, so none of the special logic in &lt;code&gt;construct_attachment_covariance&lt;/code&gt; is needed.&lt;/p&gt;

&lt;p&gt;IDEA: use image position as an alternative index, and define a within-plane scaling covariance over it to account for poor calibration.  Don&#39;t need to know any tree structure; shouldn&#39;t distort reconstruction positions since it&#39;s in-plane.&lt;/p&gt;

&lt;p&gt;in-plane covariance adds to branch_index &lt;em&gt;only&lt;/em&gt;.  Thus, the only change needs to be in att_set_branch_index and/or detach.  ...  But what if we want index-specific in-plane covariance (e.g. scaling).  Best to drop it into get_K, which will trickle into att_set_branch_index&lt;/p&gt;

&lt;p&gt;Question: should each curve be allowed to shift in the image, or should the entire image be shifted as one?  The former gives more flexibility, but possibly undeserved.  One concern is that attaching two curves will eliminate that freedom, unfairly penalizing attachment.  On the other hand, of all curves tend to shift in the same direction, the ML will increase after attaching them, promoting attachment.  Will use per-curve shift for now.&lt;/p&gt;

&lt;p&gt;Question: should shift be correlated between views?  The theory is shift arises due to camera miscalibration.&lt;/p&gt;

&lt;p&gt;Issue: sometimes the covariance matrix is gotten in other ways, e.g. when computing branch index, the parent&#39;s covariance is computed in-line.&lt;/p&gt;

&lt;p&gt;TODO: recurse on
* &lt;code&gt;construct_attachment_covariance&lt;/code&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/04/work-log"/>
   <updated>2013-12-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/04/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Implemented re-ordering of indices.  Some are improved, some are worse.&lt;/p&gt;

&lt;p&gt;Found bug in re-ordering -- the gradient and Hessian&#39;s transformation Jacobian wasn&#39;t updated properly.  After a few false starts, got it right.  Results seem much better.  Still getting signficant positive index offset for three curves, all others start at zero.  Post-procssing to force them to zero seems to fix; more investigation is warranted in the future.&lt;/p&gt;

&lt;p&gt;Offset may be due to insufficient perturb variance?  Positive index offset only increases the variance of the first point; the conditional variance of future points is roughly the same for any index offset, due to conditional independence.  Possibly the perturb position variance is too low?  Carefull setting it too high, we&#39;ll get drift toward the camera.  Quick test: increase perturb position variance from 5&lt;sup&gt;2&lt;/sup&gt; to 20&lt;sup&gt;2&lt;/sup&gt;.  The offset phenomenon is exagerated.  When position perturbations are left in, the attached model moves all over the place, over 100 mm at times. Well beyond the perturb variance.&lt;/p&gt;

&lt;p&gt;Remember that increasing the perturb variance increases the marginal within-view variance, too.&lt;/p&gt;

&lt;h2&gt;Reintroducing connections.  All bad behaviors in dataset 8 appear to be fixed!&lt;/h2&gt;

&lt;p&gt;Running on dataset 9.  Crashed.  missing views of some curves.  Going into ground truth tool to fix...&lt;/p&gt;

&lt;p&gt;Still crashing.  bug in process_gt2 -- new minimum points code. fixed.&lt;/p&gt;

&lt;p&gt;Running okay, not much movement even though dataset has significant movement.&lt;/p&gt;

&lt;p&gt;We were removing perturbations coming from the rate kernel.  new getting more movement, still not the right kind.&lt;/p&gt;

&lt;p&gt;plotting triangulated data over reconstructed data.  Significant offset, no explanation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-04-ds9_reconst_and_triangulation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Intrestingly, one curve wasn&#39;t properly connected to the parent; that curve shows no offset.  Re-running with no connections to test this observation.  Offset seems gone, ecept for the middle part of the root curve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-04-ds9_reconst_and_riangulation_no_connection.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Observation: main curve shifts a lot between views; other curves have almost no shift. Possible index shrinkage problem?&lt;/p&gt;

&lt;p&gt;measure index shrinkage: index vs. distance&lt;/p&gt;

&lt;p&gt;If position variance is too low, we might be getting a pull toward the origin.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Off-center may be caused by pulling toward camera in per-view perturbation reconstructions.  The overall reconstruction is basically the mean of the per-view reconstructions, so if the camera-pull isn&#39;t symmetric between views, this can lead to significant shift in the mean.&lt;/p&gt;

&lt;p&gt;Indeed, lowering the perturb position variance reduces reconstruction offset.  Recall that camera-pulling occurs because likelihood cylinders converge as they approach the camera.  See the &lt;a href=&quot;/ksimek/research/2013/08/06/work-log/&quot;&gt;original discussion of this phenomenon back in August&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Non-isotropic perturbation offset variance&lt;/h2&gt;

&lt;p&gt;Idea: can we limit offset perturbation to only occur in the directions of the imaging plane?  It seem possible -- just have constant offset in xy, zero offset in z, and rotate to match camera.  Can even add this to S, so it&#39;s part of that noise model.  This should take care of camera-pulling, at least to the extent that it&#39;s caused by the offset model.  Can even scale offset variance based on the distance from the camera, effectively using pixel units instead of world units.&lt;/p&gt;

&lt;h2&gt;closing&lt;/h2&gt;

&lt;p&gt;Running current version on all datasets.&lt;/p&gt;

&lt;p&gt;Program crashes if curves are missing.  Regression in code that constructs Track assoc -- emtpy curves weren&#39;t being removed. Fixed.&lt;/p&gt;

&lt;p&gt;Will inspect results in the morning.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/03/work-log"/>
   <updated>2013-12-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15864&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Fixed change-of-variable transformation for Hessian.  OPtimization now finishes with fewer iterations and with higher likelihood solution.&lt;/p&gt;

&lt;p&gt;Still getting weird overshoot curves:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-new_recons_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It seems that our optimization occasionally causes large DC offset compared to the unoptimized indices.  See below, blue is original, green is optimized.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-updated_indices_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the other hand, some indices are much improved.  Here&#39;s our always problematic curve #10, see how the large gap after the first two indices is removed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-updated_indices_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The DC offset issue raises two questions: (1) why isn&#39;t offset of zero preferred, and (2) why does DC offset result in overshooting?&lt;/p&gt;

&lt;p&gt;I think I know the answer to (2): we always start reconstructions at index zero.  So if the evidence starts at index 10, the reconstrution will always draw the segment between index 0 and 10, even if there&#39;s no evidence.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Cached prior K is used in gradient computation; cache isn&#39;t updated when indices change.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;On second look, I was wrong, cached K is updated in a call to &lt;code&gt;att_set_start_index_2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I&#39;ll probably need to update children branch indices when parent indices change.  Or only do index optimization in isolation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Why is DC offset preferred?  It&#39;s not an inference problem; confirmed that zero-offset indeed has a lower ML (not by much though).&lt;/p&gt;

&lt;p&gt;It&#39;s clear that the curve is beneficting from extra freedom gained by starting at a higher index.  The question is which parameter is encouraging this (smoothing, offset, rate)?&lt;/p&gt;

&lt;p&gt;Lowering position variance below the optimal point reverses the trend, surprisingly.  Increasing perturb position variance helps, but doesn&#39;t reverse.&lt;/p&gt;

&lt;p&gt;Increasing rate variance past the optimal point reverses the trend, but the effect is not dramatic.&lt;/p&gt;

&lt;p&gt;Increasing smoothing variance far past optimal shows a dramatic trend reversal.&lt;/p&gt;

&lt;p&gt;Increasing rate variance makes position variance behave naturally -- zero offset is better for all values of position variance, with peak difference near the optimal.  Correction: at very low position variance ( &amp;lt;= 5) the positive-offset model looks better, as we&#39;d expect, since offset makes the model more forgiving.&lt;/p&gt;

&lt;h2&gt;Correction: increasing position variance is required for some curves.&lt;/h2&gt;

&lt;p&gt;Let&#39;s re-run with larger rate variance.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;After increasing rate variance, indicies grow without bound!  Eventually they get into the range of 4e6.  Its our old friend, curve #10.  This is very surprising, because the marginal likelihood should strongly discourage such large indices, since the number of poor configufations grows with index magnitude.&lt;/p&gt;

&lt;p&gt;Could add a regulating factor, but it really seems like this shouldn&#39;t be necessary.&lt;/p&gt;

&lt;p&gt;Fixed by simply returning -inf on failure.  It seems this phenomenon is simply the result of an aggressive step in the wrong direction.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I&#39;m starting to realize the allowing the indices to move freely can result in all kinds of subtle issues.  For example, nothing prevents the spacing between indices from shrinking simultaneously, because the rate kernel is still happy (in fact, happier), the curve just appears to be moving more quickly.  If the smoothing variance was set too high, this &quot;quickening&quot; makes the marginal likelihood happier, because points have less freedom to move in space -- the prior&#39;s slack is pulled out.&lt;/p&gt;

&lt;p&gt;Something similar might be happening to cause indices to drift upward simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; Whatever the dynamics, index optimization might be a good idea to get a decent first-pass MAP reconstruction, but indices should be re-mapped using chord-length parameterization when computing ML&#39;s and final reconstructions.&lt;/p&gt;

&lt;p&gt;For now, just subtract the excess, so first index is zero.&lt;/p&gt;

&lt;p&gt;Reconstructions look better, but curve 7 still overshoots at the beginning:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-new_recons_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Possibly relaxing ordering constraints between views will fix this.&lt;/p&gt;

&lt;p&gt;Relaxing smoothness variance a small amount makes it worse.  Arg...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;OMG, plotting the 2D curve over the images shows significant offset!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-orrset_error_lo_res.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;hi resolution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-03-offset_error_hi_res.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice curve #7 (dark green) is waaaay off on it&#39;s first point.  What&#39;s going on here?  I&#39;ve re-traced that curve twice, is there some offset being added in opengl?  Why haven&#39;t we seen the evidence of this before?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;check other views for offset error&lt;/li&gt;
&lt;li&gt;check other datasets for offset error&lt;/li&gt;
&lt;li&gt;quantify offset error (1 pixel?)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Looks like curves are shifted left by 0.5 pixels and down by 1.5 pixels.  Part of the culprit is the flip_y function, which erroneously added 1 after flipping (presumably to account for the pixel grid).  This brings it to within 0.5 in both directions.  This is due to an ill-advised line I had added in the ground-truth tool, which translated everything by 0.5 pixels before rendering.  I presumably wanted (0,0) to render in the middle of the bottom-left pixel, but in retrospect, this is stupid.&lt;/p&gt;

&lt;p&gt;Added option &#39;halfPixelCorrection&#39; for process_gt_2.  Left half-pixel offset in ground-truth tool for now; with intent to fix all existing ground truth files and then remove the offending line of code.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Starting to get some real stairstep patterns in the index spacing.  Almost certainly this means points from different views want to change order.  Do this next&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Index optimization, end-to-end</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/02/work-log"/>
   <updated>2013-12-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/02/work-log</id>
   <content type="html">&lt;p&gt;Troubleshooting ML optimization.&lt;/p&gt;

&lt;p&gt;Found a few bugs in the optimization code.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;math bug in variable transformation formula.  Applying chain rule to account for transformation used wrong formula.&lt;/li&gt;
&lt;li&gt;math bug in new version of gradient function.  Used Uc where Uc&#39; was called for (transpose of cholesky)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;As a result of bug #2, decided to re-implement end-to-end test (which hadn&#39;t been run for some time; was running a unit test on synthetic data).  Refactored test to use the functional style in &lt;code&gt;test/test_ml_deriv.m&lt;/code&gt;, added hessian computation.&lt;/p&gt;

&lt;p&gt;Fixed gradient looks good.  Hessian is way off.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;found some bugs in hessian&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;forgot to include terms from linear covariance function.&lt;/li&gt;
&lt;li&gt;forgot to scale second derivitives by variance parameters.&lt;/li&gt;
&lt;li&gt;used H1 + H2 instead of H1 - H2&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;All of these weren&#39;t caught in the unit test, because everything was tested in isolation, with scaling constants omitted.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Looks&lt;/em&gt; a lot better now, still some error on the order of 1e-2.  Most troubling isn&#39;t the magnitude, but the fact that the error seems to be structured, as opposed to random:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-02-hessian_error.png&quot; alt=&quot;hessian error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We&#39;re comparing against numerical approximation, so the error might be in the approximation, not in the math.  For now we&#39;ll proceed, but there&#39;s probably room for further investigation in the future.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Ran index optimization, and results for curve #10 are much improved.  Curve #7 still overshoots, but beyond that, no significant nastiness.&lt;/p&gt;

&lt;p&gt;Initially, we were getting errors from the gradient checker.  Swtiching the central differences fixed it.&lt;/p&gt;

&lt;p&gt;Interestingly, the optimization algorithm takes 10X to 30x more iterations to complete.  (update: Hessian isn&#39;t being transformed correctly)&lt;/p&gt;

&lt;p&gt;Recall that we increased the position-perturb variance significantly.  This seems to improve ground-truth reconstructions; setting it too small causes bizarre curves and over-extensions.  Below, we see the old method, followed by the new method with large position perturbation variance, and the new method with small perturbation variance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-02-old_recons.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above: old method&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-02-new_recons_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above: new method, large position perturbation variance&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-12-02-new_recons_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above: new method, small position perturbation variance&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tried fixing hessian, results are worse.  Probably got the transformation math wrong.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;per-view ordering constraints&lt;/li&gt;
&lt;li&gt;fix hessian transformation&lt;/li&gt;
&lt;li&gt;investigate curve #7

&lt;ul&gt;
&lt;li&gt;possibly larger position perturbation variance will help?&lt;/li&gt;
&lt;li&gt;maybe fixing hessian will help?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/12/01/work-log"/>
   <updated>2013-12-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/12/01/work-log</id>
   <content type="html">&lt;p&gt;Implemented first cut on index optimization using ML maximization.  Initial results aren&#39;t great; we currently enforce index ordering, but we probably need to allow re-ordering of indices between curves, while preserving ordering within curves.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Implementing, testing Hessian</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/29/work-log"/>
   <updated>2013-11-29T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/29/work-log</id>
   <content type="html">&lt;p&gt;Spent most of the week implementing and testing the Hessian, now done.&lt;/p&gt;

&lt;p&gt;The biggest issue was correcting the math error mentioned in the previous post.  Developed a general equation for the second derivitive of the kernel matrix, K, and updated all formulas &lt;a href=&quot;/ksimek/research/2013/11/23/reference/&quot;&gt;in the writeup&lt;/a&gt; to incorporate it.&lt;/p&gt;

&lt;p&gt;Finished hessian test program, &lt;code&gt;test/test_ml_derivs.h&lt;/code&gt;, which compares the analytical solution to one obtained by finite differences.  The error that arises in the Hessian during finite differences was shockingly large and unstable -- lowering the delta below 1e-4 caused very large errors (&gt; 1%) in finite differences.  This was a significant source of confusion and frustration during testing.  In order to track down the source of errors, performed numerical and analytical first and second derivatives of all intermediate values, and tracked growth of error through the equations.  In the end, error appeared to grow slowly through each computation, with the largest error arising at the beginning, in K&#39;.&lt;/p&gt;

&lt;p&gt;It also took a long time to find a reasonable way to compare the analytical and numerical gradients.  Absolute differences were deceiving, because small absolute diferences can actually be large in terms of percentages.  On the other hand, when the true gradient is near zero, the percent error skyrocketed.  Ultimately followed the lead of Rasmussen&#39;s &lt;a href=&quot;http://learning.eng.cam.ac.uk/carl/code/minimize/checkgrad.m&quot;&gt;checkgrad.m&lt;/a&gt;, which divides the determinant of the difference by the determinant of the sum.  Was able to confirm results to an error of 1e-3.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Implemented Hessian in &lt;code&gt;curve_ml_gradient_3.m&lt;/code&gt;, refactoring existing computation to share as much as possible with the hessian.  New function runs 4x slower than the gradient alone in a 1500-dimensional problem, which isn&#39;t bad considering the numerical Hessian would take 1500&lt;sup&gt;2&lt;/sup&gt; = 2.25 million times more running time!&lt;/p&gt;

&lt;p&gt;Still need to test against a few random elements of the numerical Hessian.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Next step is to run &lt;code&gt;fminunc&lt;/code&gt; with the hessian to optimize indices after calling &lt;code&gt;corr_to_likelihood&lt;/code&gt;.  If that fixes our problems, we should roll it into &lt;code&gt;corr_to_likelihood&lt;/code&gt; and which could simplify the code significantly.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Covering Vision Course</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/25/work-log"/>
   <updated>2013-11-25T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/25/work-log</id>
   <content type="html">&lt;p&gt;Make sure to get through slides, because homework relies on the later stuff&lt;/p&gt;

&lt;p&gt;Arrive at 1:30, start at 1:35, eand at 2:45 (can run over slightly).  Location: Shantz 242E&lt;/p&gt;

&lt;p&gt;Monday topic: Hough transform, robust estimators.&lt;/p&gt;

&lt;p&gt;Must do ransac, 15 minutes will do.&lt;/p&gt;

&lt;h2&gt;Testing hessian formulae&lt;/h2&gt;

&lt;p&gt;Realized I got the formula for \(\frac{\partial \delta_i}{\partial x_j}\) wrong for the case where \(i == j\).  It&#39;s actually a dense vector, not a sparse one, which changes all of my computations.  Should only affect on-diagonal elements of the hessian.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Marginal likelihood gradient (part 2)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/25/reference"/>
   <updated>2013-11-25T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/25/reference</id>
   <content type="html">&lt;p&gt;Today, I&#39;ll cover some additional issues not covered in the &lt;a href=&quot;/ksimek/research/2013/11/10/reference/&quot;&gt;earlier reference post on this topic&lt;/a&gt;.  First, the original gradient derivation was missing a term corresponding to the normalization constant (which isn&#39;t constant as a function of the index set).  Second, the previous write-up assumed 1-dimensional data; today we&#39;ll talk about generalizing the formulae to three dimensions.&lt;/p&gt;

&lt;h2&gt;Normalization constant&lt;/h2&gt;

&lt;p&gt;Recall the gaussian equation, as a function of indices:&lt;/p&gt;

&lt;div&gt;
\[
\frac{k}{|\Sigma(x)|^{\frac{1}{2}}} \exp\left\{- \frac{1}{2} (y - \mu)^\top \Sigma^{-1}(x) (y - \mu) \right\}
\]
&lt;/div&gt;


&lt;p&gt;Taking the log gives&lt;/p&gt;

&lt;div&gt;
\[
\log(k) - \frac{1}{2} \log(|\Sigma(x)|) + \left( -\frac{1}{2} (y - \mu)^\top \Sigma^{-1}(x) (y - \mu) \right )
\]

When taking the derivative, the first term vanishes, and the third term was handled [in the last writeup](/ksimek/research/2013/11/10/reference/) as \(\nabla g\).  We need to find the derivative of the second term.  Let \(Z(x) = \frac{1}{2} \log(|\Sigma(x)|) \). Also, let \(C_{(i)} = S \, \delta_i \, S_i^\top \), so \(U&#39;_{(i)} = C_{(i)} + C_{(i)}^\top\)
&lt;/div&gt;


&lt;p&gt;According to equation (38) of The Matrix Cookbook, the derivative of the log determinant is given by:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \frac{\partial Z(x) }{\partial x_i} &amp;= \frac{1}{2} \text{Tr}\left[ U^{-1} U&#39; \right] \\
                &amp;= \frac{1}{2} \text{Tr}\left[ U^{-1} (C + C^\top )\right] \\
                &amp;= \frac{1}{2} \text{Tr}\left[ U^{-1} C \right]  + \text{Tr}\left[ U^{-1}  C^\top \right] \\
                &amp;= \frac{1}{2} \text{Tr}\left[ U^{-1} S \delta_i S_i^\top \right]  + \text{Tr}\left[ U^{-1}  S_i \delta_i^\top S^\top \right] \\
                &amp;= \frac{1}{2} \text{Tr}\left[ S_i^\top U^{-1} S \delta_i \right]  + \text{Tr}\left[ \delta_i^\top S^\top U^{-1}  S_i \right] \\
                &amp;= \frac{1}{2} 2 \text{Tr}\left[ S_i^\top U^{-1} S \delta_i \right]  \\
                &amp;= \frac{1}{2} 2 \text{Tr}\left[ S_i^\top U^{-1} S \delta_i \right]  \\
                &amp;= S_i^\top U^{-1} S \delta_i \\
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Since this inner product gives us a single element of the gradient, we can get the entire gradient using matrix multiplication.&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \nabla Z(x) &amp;= \text{diag}(S^\top U^{-1} S \Delta^\top \\
    &amp;= \sum_i (S \odot U^{-1} S \Delta^\top )_{(i:)} \tag{1}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Note that we conly care about the diagonal elements of the matrix product.  The second expression avoids computing the off-diagonal elements by taking only the dot product of matrix rows/columns that result in the diagonal elmeents.  To do this, we use the Hadamard product, \(\odot\), and then sum over rows.&lt;/p&gt;

&lt;h2&gt;Generalizing to three dimensions&lt;/h2&gt;

&lt;p&gt;We replace several matrices with their 3D version.&lt;/p&gt;

&lt;p&gt;The 3D version of \(\delta_i\) is:&lt;/p&gt;

&lt;div&gt;
\[
\delta_i^{(3)} = P \left ( \begin{array}{ccc}\delta_i &amp; 0 &amp; 0 \\ 0 &amp; \delta_i &amp; 0 \\ 0 &amp; 0 &amp; \delta_i \end{array}\right) 
\]
&lt;/div&gt;


&lt;p&gt;Here, P is a permutation matrix such that PM converts the rows of M from \((x_1, x_2, ..., y_1, y_2, ..., z_1, z_2, ...)\) to \((x_1, y_1, z_1, x_2, y_2, z_2, ...)\).&lt;/p&gt;

&lt;p&gt;Similarly, the 3D version of \(\Delta\) is&lt;/p&gt;

&lt;div&gt;
\[
\Delta^{(3)} = P \left ( \begin{array}{ccc}\Delta &amp; 0 &amp; 0 \\ 0 &amp; \Delta &amp; 0 \\ 0 &amp; 0 &amp; \Delta \end{array}\right)  P^\top
\]

The vector \(S_i\) becomes a three-column matrix, \([ S_{x_i} S_{y_i} S_{z_i}]\), corresponding to the noise-covariance of the i-th 3D point.
&lt;/div&gt;


&lt;p&gt;The expression for \(\frac{\partial Z(x)}{\partial x_i}\) is no longer a dot product, but the trace of a 3x3 matrix.  In practice, this is easy to implement, by replacing all matrices in eq (1) with their 3D equivalent, and then suming each (xyz) block in the resulting vector.  In matlab, we can do this cheaply by reshaping the vector into a 3x(N/3) matrix and summing over rows. If the old expression was&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grad_Z = sum(S .* inv(U) S Delta&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the new expresssion becomes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grad_Z_3d = sum(reshape(sum(S .* inv(U) S Delta_3d&#39;), 3, []));
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Applying to \(\nabla g\)&lt;/h2&gt;

&lt;p&gt;We can apply a similar transformation to the other term of the gradient (which we called \(\nabla g\) &lt;a href=&quot;/ksimek/research/2013/11/10/reference/&quot;&gt;in this post&lt;/a&gt;.  Recall the old expression for a single elements of the graident was&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \frac{\partial g}{\partial x_i} = \frac{1}{2} z^\top K&#39;_{(i)} z \tag{2}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Recall that \(K&#39;\) in 1D is sparse, having the form&lt;/p&gt;

&lt;div&gt;
\[
    K&#39; = 
    \left( \begin{array}{c} 
        0 &amp; 0 &amp; \cdots &amp; \delta_i &amp; \cdots &amp; 0
    \end{array}\right )
    +
    \left( \begin{array}{c} 
        0 &amp; 0 &amp; \cdots &amp; \delta_i &amp; \cdots &amp; 0
    \end{array}\right )^\top
\]

Generalizing to the 3D equivalent, \(K&#39;^{(3)} \), the equation becomes:
&lt;/div&gt;




&lt;div&gt;
\[
\begin{align}
    K&#39;^{(3)} &amp;= 
    P \left( \begin{array}{c} 
        0 &amp; 0 &amp; \cdots &amp; \delta_i^{(3)} &amp; \cdots &amp; 0
    \end{array}\right ) 
    +
    \left( \begin{array}{c} 
        0 &amp; 0 &amp; \cdots &amp; \delta_i^{(3)} &amp; \cdots &amp; 0
    \end{array}\right )^\top P^\top
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;In other words, the \(\delta_i\) in \(K&#39;\) is replaced with a permuted block-diagonal matrix of three \(\delta_i\)&#39;s.
The dot product in equation (2) then becomes the sum of the three individual dot products for x, y, and z coordinates.&lt;/p&gt;

&lt;p&gt;We can use this observation to apply this generalization to the full gradient equation.  Recall the 1D equation for the full gradient,&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \nabla g = z \odot (\Delta&#39; z) \tag{4}
\end{align}
\]

Like in the case of a single element of the gradient, we can generalize to 3D by simply taking the sum of the result for each of the x, y, and z dimensions.  We can accomplish this in a vectorized way by replacing \(\Delta\) with it&#39;s 3D equivalent \(\Delta^{(3)}\), and then sum each block of (xyz) coordinates in the resulting vector, like we did for \(\nabla Z\).  (Note that here, we assume \(z\) is computed using the 3D prior covariance, \(K^{(3)}\), and needs no explicit lifting to 3D).  In matlab, this looks like

&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;grad_g = sum(reshape(z .* (Delta_3d&#39; * z), 3, []))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mathematically, we can represent this summation by right-multiplying by permuted stack of identity matriceces.&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \nabla g^{(3)} = \left [ z \odot (\Delta^{(3)\top} z) \right ] P \left ( \begin{array}{c} I \\ I \\ I \end{array} \right )
\end{align}
\]
&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/23/work-log"/>
   <updated>2013-11-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/23/work-log</id>
   <content type="html">&lt;p&gt;Need to re-focus my efforts on getting WACV datasets working by December 10.  My recent efforts have shown good progress on the hard problem of inferring the index set under difficult conditions, but I only have about 2 weeks to get results and write the paper, so I should be focusing on a few quick-and-dirty fixes first (improved camera calibration, naive correspondence matching).  These fixes only apply to the super-clean WACV datasets, so they won&#39;t help for ECCV, but they&#39;ll get the paper out, which is what matters at the moment.&lt;/p&gt;

&lt;p&gt;TODO today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;save wacv results from old cameras&lt;/li&gt;
&lt;li&gt;supplant camera files with recalibrated ones.&lt;/li&gt;
&lt;li&gt;re-run with new cameras&lt;/li&gt;
&lt;li&gt;compare results (hopefully much improved)&lt;/li&gt;
&lt;li&gt;investigate possible bug with continuous index-correction, i.e. FIX_WORLD_T(or omit entirely?, or replace with quasi-newton?)

&lt;ul&gt;
&lt;li&gt;remember, this doesn&#39;t need to be fast&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;h2&gt;next&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;better data curve subdivision&lt;/li&gt;
&lt;li&gt;invsetigate crap curve #2&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Investigating bad reconstruction of curve 2&lt;/h2&gt;

&lt;p&gt;it was reversed. fixed.&lt;/p&gt;

&lt;p&gt;Wait, now several views of several curves are missing.  Also, several corrections I remember making are now missing.  What is going on?  Did I somehow overwrite an old file?  Can I recover it from time machine?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Okay, recovered from time machine.  Still not sure what caused this.&lt;/p&gt;

&lt;h2&gt;Running WACV w/ linear-map correspondences&lt;/h2&gt;

&lt;p&gt;Getting NaN when estimating branch distance.  Coincident points aren&#39;t handled sensibly.  Fixed; also added handling of degenerate curves (single point repeated multiple times).&lt;/p&gt;

&lt;p&gt;Running succeeded (FIX_WORLD_T is off).&lt;/p&gt;

&lt;p&gt;The reconstructed result for dataset 8 looks good (for the first time ever), but the base point moves from view to view, even though we&#39;ve removed the offset component from the kernel.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Found the problem:  had commented out some lines during debugging; caused offset perturbations to remain in the reconstruction.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Enabling FIX_WORLD_T.  Rerun succeeded, result looks reasonable; seems 98% identical to the version with FIX_WORLD_T disabled.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Getting some attachment issues.  Deteched reconstruction shows a few curves were badly reconstructed. time to try new cameras&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;new cameras seem to be reconstructing everything in reverse???  is it an axis-flipping issue?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;It turned out to be an error in my calibration routine.  I had x and y swapped in my 3D coordinates.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Everything is now working.  Result: New results aren&#39;t qualitatively different from results using old cameras.  Back to the drawing board.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;The failing curves seem to be caused by at least one view where the curve is view on-end.  The resulting reconstructed points have lots of variability as to where on the 3D curve they arise from; in some cases, the chosen point is well in to the &quot;extrapolation region&quot; of the curve, i.e. past the end of where the other well-localized points stop.&lt;/p&gt;

&lt;p&gt;On the plus side, these badly localized points have lots of variance in the direction of error, so there is hope to correct them.  On the down side, the inferred index of these points forces them to be far from their true position.&lt;/p&gt;

&lt;p&gt;This is an issue of poor indexing not being corrected.  Is it time to bring out the big guns and try maximizing the index set w.r.t. the marginal likelihood?  It seems like we&#39;ve put this off long enough...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Imlemented, but slooooooow.  Profiling shows the gradient calculation is the culprit, specifically, a dot product that is called once per dimension, per gradient evaluation. We should be able to replace that with a matrix multiplication to get a significant vectorization speedup.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done, still slow.  It looks like the gradient itself is pretty fast (~100ms) but the first iteration of Quasi-Newton takes several hundred evaluations (probably to build up a good estimate of the hessian).  We should implement the hessian directly, which should be cheap using the cubic-time derivation, given the fact that we&#39;ve already performed inversion.  Need to write-up this derivation, then implement and test.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Fix cameras

&lt;ul&gt;
&lt;li&gt;done. no significant improvement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why is root point moving in the reconstruction?

&lt;ul&gt;
&lt;li&gt;fixed; debugging &quot;comment-out&quot; bug&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try FIX_WORLD_T=true

&lt;ul&gt;
&lt;li&gt;done. not great.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Consider running index point optimization

&lt;ul&gt;
&lt;li&gt;in the works&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;run with denser point subdivision

&lt;ul&gt;
&lt;li&gt;done. slower, not significantly better results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gradient write-up: generalization from 1d to 3d&lt;/li&gt;
&lt;li&gt;Gradient write-up: normalization component&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Hessian of Marginal Likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/23/reference"/>
   <updated>2013-11-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/23/reference</id>
   <content type="html">&lt;p&gt;Recall  the expression for the i-th element of the gradient of ML w.r.t. indices.&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
g&#39;_i(x) &amp;= \frac{1}{2} z(x)^\top K&#39;(x) z(x) - Z&#39;_i(x) \\
        &amp;= z_i(x) \delta_i^\top(x) z(x) - Z&#39;_i(x) \\
        &amp;= f_i(x) - Z&#39;_i(x)
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Where \(\delta_i = k&#39;(x_i, x_j)\), and \(Z&#39;_i\) is the derivitive of the normalization constant.&lt;/p&gt;

&lt;p&gt;The goal today is to derive the second derivitive, H.  Like the first derivitive, it will have two terms,&lt;/p&gt;

&lt;div&gt;
    \[
    H = H_1 - H_2
    \]
&lt;/div&gt;


&lt;p&gt;Ultimately, we&#39;ll split the second term \(H_2\) into two sub-terms:&lt;/p&gt;

&lt;div&gt;
    \[
    H = H_1 - H_{2,A} - H_{2,B}
    \]
&lt;/div&gt;


&lt;h2&gt;Prerequisite: \(\frac{\partial \delta_i}{\partial x_j}\)&lt;/h2&gt;

&lt;p&gt;Recall that the elements of \(\delta_i\) are the partial derivatives of the kernel function w.r.t. its first input.  The second derivatives \(\delta_i\) will be given by the second partial derivatives of the kernel function.&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\frac{\partial^2 k(x_i, x_j)}{\partial x_i^2} &amp;= 
        x_j - \min(x_i, x_j)  \\
\frac{\partial^2 k(x_i, x_j)}{\partial x_i \partial x_j} &amp;= 
        \min(x_i, x_j) 
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;The derivative of the j-th element of \(\delta_i\) is derived below.&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\frac{\partial \left(\delta_i \right)_j}{\partial x_k} 
    &amp;= \frac{\partial^2 k(x_i, x_j)}{\partial x_i \partial x_k} \\
    &amp;= (x_j - \min(x_i, x_j))\mathbb{1}_{k = i} + \min(x_i, x_j) \mathbb{1}_{k = j} 
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Note that this handles the special cases where k = i = j and \(k \neq i, k \neq j\).&lt;/p&gt;

&lt;p&gt;We can generalizing to the full vector \(\delta_i\)&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\frac{\partial \delta_i }{\partial x_k}  &amp;= A_{i} \mathbb{1}_{k = i}  + B_{ik}\\
A_{i} &amp;= (x_1 - \min(x_i, x_1), \dots, x_j - \min(x_i, x_j), \dots)^\top  \\
B_{ik} &amp;= (0, \dots, \underbrace{\min(x_i, x_k)}_\text{k-th element}, \dots, 0)^\top
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;The A term handles the on-diagonal hessian terms, whereas B is included in all terms.&lt;/p&gt;

&lt;h2&gt;First term, \(H_1 = f_i&#39;\)&lt;/h2&gt;

&lt;p&gt;We use the product rule to take the derivitive of \(f_i = z_i \delta_i \cdot  z\).&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\frac{\partial f_i(x)}{\partial x_j} &amp;=
            \left ( \frac{\partial}{\partial x_j} \, z_i(x) \right ) \delta_i^\top(x) z(x)  +
            z_i(x) \left ( \frac{\partial}{\partial x_j}\delta_i^\top(x) \right ) z(x) +
            z_i(x) \, \delta_i^\top(x) \left ( \frac{\partial}{\partial x_j} z(x) \right ) \\
&amp;=
            z_i&#39; (x) \, \delta_i^\top(x) \, z(x)  +
            z_i(x) \, (A_{i}\mathbb{1}_{j = i} + B_{ij})^\top \, z(x) + 
            z_i(x) \, \delta_i^\top(x) \, z&#39;(x) \\
&amp;=
            z_i&#39; (x) \, \delta_i^\top(x) \, z(x)  +
            \mathbb{1}_{j = i} z_i(x) \, A_{i}^\top \, z(x) + z_i(x) \, \min(x_i, x_j) \, z_j(x)  +
            z_i(x) \, \delta_i^\top(x) \, z&#39;(x)
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
z&#39; = z&#39;_{(j)} = \frac{\partial z(x)}{\partial x_j} &amp;= \frac{\partial}{\partial x_j} S^\top U^{-1}(x) S y \\
        &amp;= S V&#39; S^\top y \\
        &amp;= -S^\top U^{-1} U&#39; U^{-1} S y \\
        &amp;= -(S^\top U^{-1} S) K&#39; (S^\top U^{-1} S) y
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Our goal is the generalize this to a single expression for the entire hessian matrix.
Note that when \(i \neq j\), the third term disappears, so that term will become a diagonal matrix in the hessian expression.
Let \(\mathcal{Z}&#39; \) be the jacobian of \(z\).  We can express the hessian asWe can express the hessian as&lt;/p&gt;

&lt;div&gt;
\[
H_1 = \mathcal{Z}&#39; \odot \left[ \Delta \, z \, (1 \, 1 \, ...) \right] + M \odot \left(z z^\top \right ) + \text{diag}\left\{ z(x) \odot \Delta&#39; z(x) \right\} +  \left[ z \, (1 \, 1 \, ...) \right] \odot \left[ \Delta \, \mathcal{Z}&#39; \right]
\]
&lt;/div&gt;


&lt;p&gt;where \(M\) is a matrix whose elements \(m_{ij} = \min(x_i, x_j)\),&lt;/p&gt;

&lt;p&gt;\(\Delta\) is a matrix whose rows are composed of the \(\delta_i\) vectors,&lt;/p&gt;

&lt;p&gt;\(\Delta&#39;\) is the matrix whose i-th row is the vector \(A_i\),&lt;/p&gt;

&lt;p&gt;\(\odot\) is the Hadamard matrix product,&lt;/p&gt;

&lt;p&gt;and diag() is an operator that converts a vector into a diagonal matrix.&lt;/p&gt;

&lt;h2&gt;Second term, \(Z_i&#39;&#39;(x) = H_{2,A} + H_{2,B} \)&lt;/h2&gt;

&lt;p&gt;Below are the expressions for the zeroth, first, and second derivitives of Z;&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
Z &amp;= 0.5 \log(\det(S K S^\top + I)) \\
\frac{\partial Z}{\partial x_i} &amp;= 0.5 \text{Tr} \left[ U^{-1} U&#39; \right] \\
\frac{\partial^2 Z}{\partial x_i \partial x_j} &amp;= 0.5 \text{Tr} \left[ \frac{\partial U^{-1}}{\partial x_j} U&#39; + U^{-1} \frac{\partial U&#39;}{\partial x_j} \right] \\
        &amp;= 0.5 \text{Tr} \left[ V&#39;_{(j)} U&#39;_{(i)} + U^{-1} U&#39;&#39;_{(ij)} \right] \\
        &amp;= 0.5 \left \{ \text{Tr} \left[ V&#39;_{(j)} U&#39;_{(i)} \right] + \text{Tr} \left[ U^{-1} U&#39;&#39;_{(ij)} \right] \right\} \\
        &amp;= 0.5 \text{Tr}[A] +0.5 \text{Tr}[B]
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Where&lt;/p&gt;

&lt;div&gt;
\[
        A =  V&#39;_{(j)} U&#39;_{(i)} \\  
        B = U^{-1} U&#39;&#39;_{(ij)} 
\]
&lt;/div&gt;


&lt;p&gt;These two terms correspond to the elements to the two hessian terms, \(H_{2,A}\) and \(H_{2,B}\).&lt;/p&gt;

&lt;p&gt;We&#39;ll begin by finding \(Tr[A]\) and \(H_{2,A}\).&lt;/p&gt;

&lt;p&gt;Observe that we can rewrite \(U_{(i)}&#39;\) as&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
U&#39;_{(i)} &amp;= S  K&#39;  S^\top \\
U&#39;_{(i)} &amp;= S  (B + B^\top)  S^\top \\
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;where B is the sparse matrix,&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
B = \left( \begin{array}{ccccc}
        \mathbf{0} &amp; \cdots &amp; \delta_i &amp; \cdots &amp; \mathbf{0}
    \end{array}\right)
\end{align}.
\]
&lt;/div&gt;


&lt;p&gt;We can exploit this sparsity to further expand \(U&#39;\) to&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
U&#39;_{(i)} &amp;= \left(S \, \delta_i \right) S_i^\top  + S_i \left( S \, \delta_i \right)^\top \\
         &amp;= C_{(i)} + C_{(i)}^\top
\end{align}
\]

where \(C_{(i)} = S \, \delta_i \, S_i^\top \).
&lt;/div&gt;


&lt;p&gt;We can use this identity to expand \(\text{Tr}[A]\).&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \text{Tr}[A] &amp;= \text{Tr}[V&#39;_{(j)} U&#39;_{(i)}] \\
          &amp;= \text{Tr}[-U^{-1} U&#39;_{(j)}U^{-1} U&#39;_{(i)}] \\
          &amp;= -\text{Tr}[U^{-1} \left( C_{(j)} + C_{(j)}^\top \right) U^{-1} \left( C_{(i)} + C_{(i)}^\top \right) ] \\
          &amp;= -\text{Tr}[U^{-1} \left( C_{(j)} + C_{(j)}^\top \right) U^{-1} \left( C_{(i)} + C_{(i)}^\top \right) ] \\
          &amp;= -\text{Tr}[\left( U^{-1}  C_{(j)} + U^{-1}C_{(j)}^\top \right) \left(U^{-1}  C_{(i)} + U^{-1}C_{(i)}^\top \right) ] \\
          &amp;= -\text{Tr}\left [U^{-1}  C_{(j)} U^{-1}C_{(i)} + U^{-1}  C_{(j)} U^{-1}C_{(i)}^\top + U^{-1}C_{(j)}^\top U^{-1}C_{(i)} + U^{-1}C_{(j)}^\top U^{-1}C_{(i)} ^\top\right] \\
          &amp;= -2 \text{Tr}\left [U^{-1}  C_{(j)} U^{-1}C_{(i)}\right]  - 2 \text{Tr}\left[U^{-1} C_{(j)} U^{-1}C_{(i)}^\top \right ] \\
          &amp;= -2 \text{Tr}\left [U^{-1} \left( S \delta_j S_j^\top \right) U^{-1} \left ( S \delta_i S_i^\top \right)\right]  - 2 \text{Tr}\left[U^{-1} \left( S \delta_j S_j^\top \right) U^{-1}\left( S_i \delta_i^\top S^\top \right) \right ] \\
          &amp;= -2 \text{Tr}\left [S_i^\top U^{-1} S \delta_j S_j^\top U^{-1} S \delta_i \right]  - 2 \text{Tr}\left[ \delta_i^\top S^\top U^{-1} S \delta_j S_j^\top U^{-1} S_i \right ] \\

\end{align}
\]

The last identity exploits the fact that Traces are invariant under cyclic permutations.  Note that both expressions inside the trace operator are scalar products, which makes the trace operator redundant.  

\[
\begin{align}
    \text{Tr}[A]
          &amp;= -2 S_i^\top U^{-1} S \delta_j S_j^\top U^{-1} S \delta_i  - 2  \delta_i^\top S^\top U^{-1} S \delta_j S_j^\top U^{-1} S_i \\
          &amp;= -2 \left( S_i^\top U^{-1} S \delta_j \right) \left(S_j^\top U^{-1} S \delta_i \right)  - 2  \left(\delta_i^\top S^\top U^{-1} S \delta_j \right) \left( S_j^\top U^{-1} S_i \right) \\
\end{align}
\]

Here, we&#39;ve regrouped the dot-products in each term to be a product of two dot-products.  We can generalize this for the full hessian as follows:

\[
\begin{align}
    H_{2,A} &amp;= -\left( S^\top U^{-1} S \Delta^\top \right)^\top \odot \left( S^\top U^{-1} S \Delta^\top \right)  - \left(\Delta S^\top U^{-1} S \Delta^\top\right) \odot \left(S^\top U^{-1} S \right) 
\end{align}
\]

&lt;/div&gt;


&lt;p&gt;Next is the second term, \(\text{Tr}[B]\).&lt;/p&gt;

&lt;p&gt;First lets derive \(U&#39;&#39;\).&lt;/p&gt;

&lt;div&gt;
\begin{align}
U&#39;&#39;_{(ij)} = \frac{\partial U_{(i)}&#39;}{\partial x_j} &amp;= 
            \frac{\partial}{\partial x_j} \left \{ 
            \left(S \, \delta_i \right) S_i^\top  +
            S_i \left( S \, \delta_i \right)^\top 
            \right \} \\
            &amp;= \left(S \, \frac{\partial \delta_i}{\partial x_j} \right) S_i^\top  +
            S_i \left( S \, \frac{\partial \delta_i}{\partial x_j} \right)^\top  \\
            &amp;= S \, \left(A_i \mathbb{1}_{i = j} + B_{ij} \right) S_i^\top  +
            S_i \left(A_i^\top \mathbb{1}_{i = j} + B_{ij}^\top \right) S^\top   \\
            &amp;= \mathbb{1}_{i = j} S \, A_i \, S_i^\top + S\, B_{ij} S_i^\top +
             \mathbb{1}_{i = j} \, S_i \, A_i^\top \, S^\top + S_i \, B_{ij} \, S^\top \\
            &amp;= \mathbb{1}_{i = j} S \, A_i \, S_i^\top + \mathbb{1}_{i = j} S_i \, A_i^\top \, S^\top + S_j \min(x_i, x_j) S_i^\top + S_i \min(x_i, x_j) S_j^\top \\
            &amp;= \mathbb{1}_{i = j} \left ( S \, A_i \, S_i^\top + S_i \, A_i^\top \, S^\top \right ) + \min(x_i, x_j) \left( S_i S_j^\top + S_j  S_i^\top \right)
\end{align}
&lt;/div&gt;


&lt;p&gt;Now we can derive \(\text{Tr}[B]\).&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \text{Tr}[B] &amp;= \text{Tr}[U^{-1} U&#39;&#39;_{(ij)}] \\
            &amp;= \text{Tr}[U^{-1} \left \{\mathbb{1}_{i = j} \left ( S \, A_i \, S_i^\top + S_i \, A_i^\top \, S^\top \right ) +  \min(x_i, x_j) \left( S_i S_j^\top + S_j S_i^\top \right) \right \} ] \\
            &amp;= 
            \mathbb{1}_{i = j} \text{Tr}\left [ U^{-1} S A_i S_i^\top \right ] +
            \mathbb{1}_{i = j} \text{Tr}\left [ U^{-1} S_i A_i^\top S \right ] +
            \min(x_i, x_j) \text{Tr}\left [ U^{-1} S_i  S_j^\top \right ] + \min(x_i, x_j) \text{Tr}\left [ U^{-1} S_j  S_i^\top \right ] \\
            &amp;= 
            \mathbb{1}_{i = j} \text{Tr}\left [ S_i^\top U^{-1} S A_i \right ] +
            \mathbb{1}_{i = j} \text{Tr}\left [ A_i^\top S U^{-1} S_i \right ] +
            \min(x_i, x_j) \text{Tr}\left [S_j^\top  U^{-1} S_i \right ] + \min(x_i, x_j) \text{Tr}\left [ S_i^\top U^{-1} S_j \right ] \\
            &amp;= 
            2 \mathbb{1}_{i = j}  S_i^\top U^{-1} S A_i +
            2 \min(x_i, x_j) S_j^\top  U^{-1} S_i  \\
\end{align}
\]

&lt;/div&gt;


&lt;p&gt;This is for a single term of the Hessian.  We can rewrite it to compute the entire Hessian using matrix arithmetic:&lt;/p&gt;

&lt;div&gt;
\[
    H_{2,B} = M \odot S^\top U^{-1} S + ( S^\top \left ( U^{-1} S \mathcal{A} \right ) ) \odot I
\]
&lt;/div&gt;


&lt;p&gt;Here, M is defined as, \(m_{ij} = \min(x_i, x_j)\), and  \(\mathcal{A}\) is the matrix whose i-th column is \(A_i\).  Note that only the diagonal elements of the second term are preserved; in implementation, this can be implemented as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;diag(sum(S .* (inv(U) * S * A)))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Full Hessian&lt;/h2&gt;

&lt;p&gt;The full Hessian is the sum of the three parts above&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    H =&amp; H_1 - H_{2,A} - H_{2,B} \\
      =&amp; \mathcal{Z}&#39; \odot \left[ \Delta \, z \, (1 \, 1 \, ...) \right] + M \odot \left(z z^\top \right ) + \text{diag}\left\{ z(x) \odot \Delta&#39; z(x) \right\} +  \left[ z \, (1 \, 1 \, ...) \right] \odot \left[ \Delta \, \mathcal{Z}&#39; \right] +  \\
      &amp; \left( S^\top U^{-1} S \Delta^\top \right)^\top \odot \left( S^\top U^{-1} S \Delta^\top \right)  + \left(\Delta S^\top U^{-1} S \Delta^\top\right) \odot \left(S^\top U^{-1} S \right)  - \\
      &amp; M \odot S^\top U^{-1} S - ( S^\top \left ( U^{-1} S \mathcal{A} \right ) ) \odot I
\end{align}
\]
&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Chicken and Egg</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/20/work-log"/>
   <updated>2013-11-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/20/work-log</id>
   <content type="html">&lt;p&gt;Despite all of the elaborate scaffolding in the pipeline, we&#39;re essentially solving a chicken-and-egg problem, trying to find:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Ideal point-depth for likelihood approximation&lt;/li&gt;
&lt;li&gt;Ideal point-indices&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;We currently use a multi-step approach to estimate both using elaborate heuristics, and the discussion over the last week or shows that this approach is less than satisfying.  The current approach (roughly) is&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;recursively apply dynamic time warping (DTW) to find a decent point correspondence&lt;/li&gt;
&lt;li&gt;get 3D points by triangulating (omitting untriangulatable points)&lt;/li&gt;
&lt;li&gt;estimate indices by chord_length_parameterization&lt;/li&gt;
&lt;li&gt;smooth&lt;/li&gt;
&lt;li&gt;(optional) repeat 3 % 4 until convergence&lt;/li&gt;
&lt;li&gt;get index of untriangulated points by backprojection&lt;/li&gt;
&lt;li&gt;Re-index points in 2D against reconstructed curve in 2D (DTW)&lt;/li&gt;
&lt;li&gt;(optional) use newton&#39;s method to improve indices&lt;/li&gt;
&lt;li&gt;backproject all points against new index to get point-depth&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Steps 1 and 2 get initial 3D estimate.&lt;/p&gt;

&lt;p&gt;Steps 3, 4, 5, 6 try to optimize the posterior reconstruction&lt;/p&gt;

&lt;p&gt;Finally, in step 7, we get our first estimate of point-indices.
Step 8 gives us point-depths.&lt;/p&gt;

&lt;p&gt;Forcing a point correspondence is unnatural, since points lie on a continuous index-set, don&#39;t correspond exactly.  Jagged-tails cause correspondence failures, which prevent triangulation, so we need special code to handle these after the fact.&lt;/p&gt;

&lt;p&gt;Index estimates don&#39;t account for between-view perturbations; this rigidity causes bad index estimates when perturbations are large.&lt;/p&gt;

&lt;p&gt;Index estimation in 2D can be ambiguous; nearby points in 2D can be far in 3D.&lt;/p&gt;

&lt;p&gt;Index estimation using DTW algorithm doesn&#39;t handle smoothness.&lt;/p&gt;

&lt;p&gt;Each stage is full of hacks.&lt;/p&gt;

&lt;p&gt;Is there a better way?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Revised algorithm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Point-depth:  Backproject against XXX.&lt;/li&gt;
&lt;li&gt;point-indices: optimize marginal likelihood w.r.t. indicies.&lt;/li&gt;
&lt;li&gt;repeat until convergence.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;In the first pass, XXX can be a &quot;scene center&quot; point, or some heuristic (steps 1 and 2 above).  In future passes, the maximum posterior can be used.&lt;/p&gt;

&lt;p&gt;Getting the indices right is essential, more important than getting the depths early on, since the depths have infinite variance in the backprojection direction.&lt;/p&gt;

&lt;p&gt;Index optimization will use Quasi-newton optimization, a local minimizer.  Thus, a reasonable initial estimate is needed.  Hopefully, a simple linear map between endpoints should suffice, otherwise one of the existing heuristics could be used.  Currently have a gradient for the Newton algorithm, but need to implement hessian.  Hessian should be implementable in \(O(n&lt;sup&gt;3&lt;/sup&gt;)\).&lt;/p&gt;

&lt;p&gt;For speed, this procedure could be applied incrementally to get reasonable initial estimates when adding an extra curve to the track, just skip re-estimating depth and indices of existing points.  If accepted, re-estimate all points and re-accept.&lt;/p&gt;

&lt;h2&gt;Hessian of Marginal Likelihood&lt;/h2&gt;

&lt;p&gt;After some playing around, I&#39;m nearly certain the Hessian can be computed in cubic time (i.e. same as gradient)!&lt;/p&gt;

&lt;p&gt;The first derivative (gradient) has two terms: the exponential term and the partition function term.  Thus, the hessian has two terms, and a cubic-time formula for the first is relatively easy to derive.&lt;/p&gt;

&lt;p&gt;The hessian of the partition function is much messier, but we can boil it down to the following statement: each element of the hessian is the trace of a vector outer-product, which is equivalent to a dot product of the same two vectors.  Assuming we can obtain these vectors in ammortized linear time, we can compute all elements of the hessian in cubic time.&lt;/p&gt;

&lt;p&gt;It remains to show that we compute all the vectors in ammortized linear time.  Each vector takes O(n&lt;sup&gt;2&lt;/sup&gt;) to compute (multiplication of a nxn matrix and a nx1 vector).   There are O(n) different vectors in total which can then be cached, for a total of O(n&lt;sup&gt;3&lt;/sup&gt;) precomputation.  Divided over the n&lt;sup&gt;2&lt;/sup&gt; different times we use these vectors, the per-iteration ammortized running time is O(n).&lt;/p&gt;

&lt;p&gt;It&#39;s going to be a bear implementing this, but the payoff will be huge.  First, we&#39;ll get efficient local optimization with each iteration costing the same speed as an MCMC iteration (implementeed naively) would take.  Second, we can finally properly (approximately) integrate out the index set in our marginal likelihood computation, using the hessian and the Candidate&#39;s estimator.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>2 Days: debugging WACV reconstruction</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/18/work-log"/>
   <updated>2013-11-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/18/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Now that gradient is working, lets try using fminunc to optimize indices.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting nonsense results.  Looking into curve data.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Inspecting the data, it&#39;s clear that our method of linearly sampling points along the bezier curve is resulting in very jagged curves.  Example in dataset 8, curve 7:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-dataset8_curve7_view9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;view 9&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-dataset8_curve7_view4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;view 4&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It&#39;s not totally clear how best to resolve this.  Ideally, we would sample at a finer grain, but this caused big slow-downs for the longer curves.  Could use coarse-grain sampling for long curves, but some curves are long in some views and short in others, and nonuniform sampling breaks some assumptions we make in the library.  Furthermore, associations are unknown at the time of samplingz&lt;/p&gt;

&lt;p&gt;It&#39;s possible&lt;/p&gt;

&lt;p&gt;It&#39;s possible the bad reconstruction we&#39;re seeing from this curve isn&#39;t due to bad correspondence, but a bad indexing estimation (a later stage of inference).  We see that although the correspondence places c7v9 toward the end of the 3D curve, our re-indexing code places it more spread out, but unevenly: the first point has index 4, while the subsequent points have indieces [21, 23, 27, 27].  We usually prevent large amounts of index-skipping during re-indexing, but possibly the second-pass refinement is destroying this.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After double-checking, realized view #5 is the biggest problem child, not #7.&lt;/p&gt;

&lt;p&gt;Interestingly, #5 has relatively reasonable looking correspondence:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-curve7_corrs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But the reconstruction (both attached and free) is terrible:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-curve7_reconstr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;ll_means has really bad points at the beginning and end of the curve.  It looks like tails might be handled poorly in corr_to_likelihood_2.m&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Attempting to run with &quot;FIX_WORLD_T = false&quot; in corr_to_likelihood.  Early probes suggest this improves things; however, now getting a crash when calling &lt;code&gt;construct_attachment_covariance.m&lt;/code&gt;.  Getting NaN&#39;s from curve #8.  I&#39;ve observed that curve #8&#39;s indices start with two zeros.  Maybe this is causing our covariance algorithm to choke?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Got it: when we don&#39;t correct endpoints (i.e. FIX_WORLD_T = false), endpoints can get repeated, which makes our &quot;initial direction&quot; computation fail.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Find out why duplicate endpoints occur (doesn&#39;t triangulation make this improbable?)&lt;/li&gt;
&lt;li&gt;Handle duplicated points gracefully when computing start point.&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Spun off &lt;code&gt;../correspondence/corr_to_likelihood_2.m&lt;/code&gt;.  Added some plotting code to see data vs. smoothed reconstruction.  Reconstruction is particularly ugly for curve #5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-curve5_reconstr_rough.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At first glance, it looks like end points are poorly localized, and then are preserved through the smoothing pass.   But the last 12 points which are poorly localized hive quite a few correspondences, according to the correspondence table.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-curve5_reconstr_rough.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With pre-smoothed reconstructrion in green:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-18-reconstr_w_data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tried using GP smoothing (implemented in &lt;code&gt;reconstruction/gp_smooth_curve.m&lt;/code&gt;) instead of matlab&#39;s &lt;code&gt;csaps()&lt;/code&gt;, but I&#39;m getting weird tails.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-19-gp_reconstr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Must be a bug in how I&#39;m computing the covariance (which I&#39;m doing by hand, since the necessary fields aren&#39;t constructed at that stage of the pipeline).  I&#39;ve been over it a few times... time to sleep and look again in the morning.&lt;/p&gt;

&lt;p&gt;Eventually, chicken/egg approach might be the solution: optimize points, optimize indices, repeat.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It&#39;s clear that smarter smoothing doesn&#39;t improve reconstruction of curve #5.&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;plot projected reconstruction vs. data

&lt;ul&gt;
&lt;li&gt;Terrible&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;plot camera centers&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;is it all bad cameras?  why do others reconstruct okay?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Test: incrementally reconstruct, adding one curve each time&lt;/p&gt;

&lt;p&gt;Reverse mode: started with views 9 &amp;amp; 8, then added 7, 6, etc..  Goes to pot at curve 4, gets progressively worse through curve 1&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The purpose of &lt;code&gt;corr_to_likelihood&lt;/code&gt; is to correct position and indexing errors made during naive triangulation.  Let&#39;s test if that&#39;s  happinging by viewing post-correction results.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Dramatic improvement over pre-correction results.  Still significant change after adding curve 2, and esp. after curve 1.&lt;/p&gt;

&lt;p&gt;Last 2 points and first 4 (?) are problematic.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;trying camera recalibration. wrote a quick 7-point calibration: &lt;code&gt;cal_cam_dlt.m&lt;/code&gt;.  Also wrote a camera visualization routine: &lt;code&gt;visualization/draw_camera.m&lt;/code&gt;.  The newly calibrated results are clearly different, but not obviously better, at least by inspection.&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;touch up cameras for views 3 and 7.&lt;/li&gt;
&lt;li&gt;back up old cameras, copy-in new cameras, re-run wacv dataset 8.  Is curve 5 improved?&lt;/li&gt;
&lt;li&gt;Try chicken/egg approach,

&lt;pre&gt;&lt;code&gt;  * note that maximizing indices is easier than quasi-newton: just arc-length parameterize it.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Troubleshoot FIX_WORLD_T issue that&#39;s causing everything to break&lt;/li&gt;
&lt;li&gt;Need to adapt gp-smooth to handle perturbations&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Friday - iPlant Reading Group</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/15/work-log"/>
   <updated>2013-11-15T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/15/work-log</id>
   <content type="html">&lt;h2&gt;iPLant Reading Group Meeting&lt;/h2&gt;

&lt;p&gt;Decided to create a latex file for recording all of our reading.&lt;/p&gt;

&lt;p&gt;Shared readings&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Kyle:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hyptrace&lt;/li&gt;
&lt;li&gt;HyDe&lt;/li&gt;
&lt;li&gt;Miller et al&lt;/li&gt;
&lt;li&gt;Neuron tracing paper&lt;/li&gt;
&lt;li&gt;(see work log from a few days ago for full list, details)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Kate:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Geotrace -&lt;/li&gt;
&lt;li&gt;RooTrack - Pridmore lab (2012)&lt;/li&gt;
&lt;li&gt;RootNav - Uses EM + others?  pridmore lab, (2013)&lt;/li&gt;
&lt;li&gt;two survey apers on segmentation&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Did a a writeup on iPlant wiki that details my reading from last week.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Debugging ML Gradient (part 2)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/14/work-log"/>
   <updated>2013-11-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/14/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Resuming debugging of analytical gradient.&lt;/p&gt;

&lt;p&gt;I noted yesterday that the error we observed in K&#39; was small -- on the order of 1e-5 -- implying that it wasn&#39;t sufficient to explain the end-to-end error we&#39;re seeing.  But we should put that into context.  The delta from the numerical gradient is 1e-7, so the error in K&#39; is around 1%, i.e. in the noticible range.&lt;/p&gt;

&lt;p&gt;Also remember that running analytical grdient using the K&#39; from the numeric algorithm showed no noticible change to resutls.  So K&#39; probably &lt;em&gt;isn&#39;t&lt;/em&gt; the issue.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Today&#39;s strategy: implement a legitimate test-bed that we can run for several deltas and get all intermediate values for both numeric and analytical gradient.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Implemented, examining results.&lt;/p&gt;

&lt;p&gt;Okay, this is weird.  All of the partial derivatives have low error (1e-5), but the final derivative of g(x) is huge (0.1).&lt;/p&gt;

&lt;p&gt;Consider the analytical expression we&#39;re using for the gradient of g(x)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;g&#39; = -0.5 * y^\top * S^\top * V&#39; * S * y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plugging-in the numerical estimate for V&#39; above should give something close to the numerical estimate for g&#39;.  But they&#39;re nowhere close.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; -0.5 * y&#39; * S&#39; * dV_hat * S * y

ans =

   -0.1015

&amp;gt;&amp;gt; g_hat

ans =

    0.0522
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stated differently, if we propagate the error in dV_hat through the expression for g&#39;, this theoretical error in g&#39; is far smaller than the true error in the analytical expression for g&#39;.&lt;/p&gt;

&lt;p&gt;This strongly suggests there&#39;s a problem in our expression for g&#39;, that the error is not due to approximation or precision issues.  But if g&#39; is so wrong, why are so many elements of the gradient so close to being correct?  Recall the plot of gradients from yesterday, shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-gradient_test.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Element 22 (the element under scrutiny in the tests above) has large error, but several other elements are nearly perfect.  This suggests that if there is a problem in our expression.&lt;/p&gt;

&lt;p&gt;Is it possible there&#39;s a bug in the test logic?  Maybe we&#39;re using the wrong field for y or S?  Or maybe when we perturb x, we aren&#39;t updating all the intermediate fields?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Got it!&lt;/strong&gt;  We&#39;re neglecting the change in the normalizing &quot;constant&quot;, Z. In the usual scenario, Z is constant w.r.t. the Gaussian distribution input y, but it is most certainly a function of the values of the Sigma matrix.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Debugging ML gradient</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/13/work-log"/>
   <updated>2013-11-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/13/work-log</id>
   <content type="html">&lt;p&gt;Implemented end-to-end ML gradient in &lt;code&gt;curve_ml_derivative.m&lt;/code&gt;.  Now testing.&lt;/p&gt;

&lt;p&gt;Fixed some obvious math errors, changes reflected in &lt;a href=&quot;/ksimek/research/2013/11/10/reference&quot;&gt;writeup&lt;/a&gt;.  Now getting close results, but still getting some noticible error, see below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-gradient_test.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Green is reference, blue is testing).&lt;/p&gt;

&lt;p&gt;Results are qualitatively close, but enough error to suggest a bug&lt;/p&gt;

&lt;p&gt;Debugging so far:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;two different impementations for analytical gradient&lt;/li&gt;
&lt;li&gt;two different implementations for numerical gradient&lt;/li&gt;
&lt;li&gt;one-sided and two-sided numerical gradient.&lt;/li&gt;
&lt;li&gt;several delta sizes for numerical gradient {0.0001, 0.0001, ..., 0.1, 1.0}&lt;/li&gt;
&lt;li&gt;Using emperical \(\Delta\)&#39; (from finite differences) for analytical gradient.&lt;/li&gt;
&lt;li&gt;using both cholesky and direct method for matrix inversion (testing for numerical issues).&lt;/li&gt;
&lt;li&gt;sanity check: used intermediate values from gradient computation to compute function output.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To try:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;use numerical gradient for K&#39; instead of from \(\Delta&#39;\).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Is it possible we&#39;re not handling XYZ independence properly?&lt;/p&gt;

&lt;p&gt;Noticed that using a really large delta (~1.0) actually improves resuls.  Is it possible we&#39;re seeing precision errors being exacerbated somewhere in the end-to-end formula?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Strategy:  pick gradient element with mode error and run the following test.  For each derivative component (dK, dU, dV, dg), compare against reference to determine where the error is being introduced.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Index #22&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;dK/dt&lt;/strong&gt; has 1e-4 error in on-diagonal.  Off diagonals max out at 1e-10.&lt;/p&gt;

&lt;p&gt;   delta: 0.01
   on-diagonal error ~ 1e-3
   below-and-right &amp;lt; 1e-4&lt;/p&gt;

&lt;p&gt;   delta: 0.001
   on-diagonal error ~ 1e-4
   other error &amp;lt; 1e-10&lt;/p&gt;

&lt;p&gt;   delta: 0.0001
   on-diagonal error ~ 1e-5
   below and right ~ 1e-6&lt;/p&gt;

&lt;p&gt;   delta: 0.00001
   on-diagonal error ~ 1e-6
   below and right ~ 1e-4&lt;/p&gt;

&lt;p&gt;Decreasing delta improves on-diagonal, makes below-and-right worse.&lt;/p&gt;

&lt;p&gt;This is weird that we&#39;re even getting error in dK/dt, because it passed our unit test.  Well, \(\Delta&#39;\) passed our unit test, but that&#39;s basically the same thing...&lt;/p&gt;

&lt;p&gt;However, reduced error at delta of 1e-3 &lt;em&gt;seems&lt;/em&gt; to agree with our end-to-end test.  So maybe this is the culprit.&lt;/p&gt;

&lt;p&gt;It&#39;s also surprising that there&#39;s so much fluctuation as delta changes.  The computation for K isn&#39;t that involved, and we shouldn&#39;t be hitting the precision limit yet.  However, the values do get pretty large, so maybe that&#39;s a factor.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Magnitude of the original matrix does seem to be a factor.  Look at this slice of the error matrix (dK_test - dK_ref):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-error_trace.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Compare that to the diagonal of the matrix we did finite differences on.  This is basically a plot of the cubed index values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-K_trace.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The error seems to increases in lockstep with the magnitude of the original values (note the jumps occur at similar positions).  I guess this is to be expected, but I was surprised at the magnitude.&lt;/p&gt;

&lt;p&gt;I&#39;m still curious why the error starts to climb exactly at index #22, i.e. the index we&#39;re differentiating with respect to.&lt;/p&gt;

&lt;p&gt;This plot should drive home the relationship between index valuea end error.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-13-error_regress_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Definitely a linear relationship after index #22.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A back of the envelope error analysis suggests that below index 22, the analytical derivative&#39;s approximation error is not a function of X, but above index 22, it&#39;s a linear function of X.  This is a pretty reasonable explanation, although I couldn&#39;t get the exact numbers to explain the slope of the line ( the slope seems high).  But at this hour I wouldn&#39;t trust my error analysis as far as I could throw it, quantitatively speaking.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We can attempt to place an upper bound on the error estimate by propagating the error in K&#39; through the differential formula for g&#39;.  Assume every nonzero element of K has error of 3e-5 (the maximum we observed emperically).  Let this error matrix be \(\epsilon\), and it has the same banded structure as \(K&#39;\).  Then we can replace \(K&#39;\) with \(\epsilon\) in the formula for \(g&#39;\) (formula (1) in the &lt;a href=&quot;/ksimek/research/2013/11/10/reference&quot;&gt;writeup&lt;/a&gt;) to get the upper bound error on our data-set.&lt;/p&gt;

&lt;div&gt;
\[
    \text{max error} = \frac{1}{2}z^\top \epsilon z \tag{1}\\
\]
&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; 0.5 * z&#39; * Epsilon * z

ans =

  -1.1758e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can conclude that the error we&#39;re observing is coming from somewhere else.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To conclude for tonight, we&#39;re seeing some error in dK/ds, but probably nothing out of the ordinary, and it has low enough error that we can hopefully ignore it.&lt;/p&gt;

&lt;p&gt;Lets look a the outher sources of error tomorrow, i.e. U&#39; and V&#39;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/12/work-log"/>
   <updated>2013-11-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/12/work-log</id>
   <content type="html">&lt;p&gt;Implementing index optimization.&lt;/p&gt;

&lt;p&gt;Main accopmlishment of the day: Derived an approach for finding the entire gradient with a single matrix multiplication.  Added this to &lt;a href=&quot;/ksimek/research/2013/11/10/reference&quot;&gt;reference post on ML gradient&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Diversion: updating Ruby&lt;/h2&gt;

&lt;p&gt;Struggling with blog engine, need to upate rdiscount to get new feature, but gem install fails because gcc-4.2 is missing since upgrading to Mavericks.  Need to recompile/reinstall ruby, so the gem system uses clang.  &lt;code&gt;rvm xxx instal&lt;/code&gt; is failing, because gcc-4.2 is missing.  Forcing clang; building causes segfault. Research says upgrade to HEAd of rvm, install ruby 2.0.0.  Tried, but still segfaulting... out of ideas.&lt;/p&gt;

&lt;p&gt;New plan: install gcc-4.2 manually.  Luckilly homebrew has it in the repo.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After some struggle, just gave up.  Annoyed at having wasted over an hour on this.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>WACV Reconstruction issues; iPlant Reading; Index optimization</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/11/work-log"/>
   <updated>2013-11-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/11/work-log</id>
   <content type="html">&lt;h2&gt;WACV Reconstruction issues&lt;/h2&gt;

&lt;p&gt;I&#39;ve been struggling getting good results on the Columbia-strain images from the WACV dataset.  I was hoping that manually re-tracing the ground trough would improve results, but problems still remain.  MY current theory is that these datasets exhibit significant shifting of the plants over time.  This is causing the dynamic programming algorithm for finding point-correspondences to give bad correspondences, which are never fixed later in the pipeline.&lt;/p&gt;

&lt;p&gt;This is best illustrated using the point-correspondence tables below.  These tables describe the point-corerspondences between views of a curve.
Each row represents a 2D curve from a different view; values in the table represent the index of a point along the 2D curve.
Each column represents a position along the underlying 3D curve, and the values in a column are a set of corresponding 2D points from each view.
An &#39;x&#39; represents &#39;no match&#39;; x&#39;s only occur at the beginning and end of a row.&lt;/p&gt;

&lt;p&gt;An ideal point-correspondence table looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;              3D curve position
           +--------------------+
view   1   | 1 2 2 3 ...  50 50 |  
index  2   | 1 1 2 2 ...  50 51 |
      ...  |         ...        |
       n-1 | 1 2 3 3 ...  48 48 |
       n   | 1 1 1 2 ...  45 46 |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the plant-stem exhibits drift over time, we get problems where each subsesquent curve is shifted left in the correspondence table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                3D curve position
           +--------------------------------------+
view   1   | 1 2 2 3 ...  50 50 x  x  x  x  x  x  |  
index  2   | x x 1 1 ...  48 49 50 51 x  x  x  x  |
       ... |         ...                          |
       n-1 | x x x x ...  45 46 47 47 48 48 x  x  |
       n   | x x x x ...  43 43 44 44 45 45 46 47 |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a result, we get 3D curves that are longer and tend to exhibit loopy curvature.&lt;/p&gt;

&lt;p&gt;My current theory assumes this occurs because the point-correspondence algorithm optimizes a local score, not taking into account smoothness or per-view perturbations that our full model allows.  I&#39;m working on implementing a post-processing step that does local optimization on the index set w.r.t. the full marginal likelihood, which I&#39;m hoping will fix these problems when they occur.&lt;/p&gt;

&lt;p&gt;I&#39;ve derived an efficient method for computing the analytical gradient of the index set w.r.t. the marginal likelihood, which &lt;a href=&quot;/ksimek/research/2013/11/10/reference/&quot;&gt;I wrote up yesterday&lt;/a&gt;.  I&#39;ve implemented some of the pieces, but finish the end-to-end code.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Finished testing the function that computes dK/dt; accurate to within 1e-8 (which is probably due to the numerical approximation used for comparison).&lt;/p&gt;

&lt;h2&gt;iPlant Reading&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Hypocotyl Tracing&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Read three papers on Hypocotyl tracing from plant biology.  Methods are mostly straighforward: threshold, extract curve using {distance transform, morphological skeleton, gaussian tracing}, terminate using hand-built criterion.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hypotrace, Want et al.&lt;/li&gt;
&lt;li&gt;HyDE, Cole et al.&lt;/li&gt;
&lt;li&gt;Miller et al.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Root Tracing&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Read several papers on root tracing&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;RootRead3D&quot; Clark  et al. 2011

&lt;ul&gt;
&lt;li&gt;Zhu 2006 - hough-transform / space carving to find voxels; skeletonize using &quot;minumum cross section&quot;,  fit with NURBS curve.&lt;/li&gt;
&lt;li&gt;&quot;Smart-root&quot; Lobet et. al 2011 - Most cited Clark descendant; Semi-auto, &lt;em&gt;2D&lt;/em&gt;; hand-build algorithm, trace bright regions with consistent radius.  Grayscale features, strong GUI.

&lt;ul&gt;
&lt;li&gt;&quot;EZRhizo&quot; Armengaud et al. 2009 - Manual tracing? (To Read)&lt;/li&gt;
&lt;li&gt;&quot;DART&quot; Le Bot et al., 2010 - Manual Tracing? (To read)&lt;/li&gt;
&lt;li&gt;Iyer-Pascuzzi et a., 2010 - Automatic.  Multiple angles, but &lt;em&gt;2D analysis&lt;/em&gt;.  preprocess: Adaptive threshold.  Medial axis: distance transform&lt;/li&gt;
&lt;li&gt;&quot;Root-trace&quot; Nadeem et al. 2009 - auto; (To Download)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fang et al. 2009  - Laser scanner, skeleton using hough transform method.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Neuron Analysis&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Reading papers of the &lt;a href=&quot;http://diademchallenge.org/algorithms.html&quot;&gt;teams that won the Diadam Challenge&lt;/a&gt; for tracing neurons.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Wang et al. A Broadly Applicable 3-D Neuron Tracing Method Based on Open-Curve Snake&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Preprocessing: eigenanalysis of image hessian, eigenvalues are used to determine &quot;vesselness&quot; of each pixel.
Presegmentation: Graph cut on &quot;vesselness image&quot;.  Actually &quot;Vessel Cut&quot;;  &quot;tubular structures are further enhanced and close-lying axons are thinned in the vesselness image&quot;.  A bit vague here.  See Freiman et al. (2009).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;To Read:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Freiman et al. (2009). vessel-cut
Narayanaswamy et al. (2011) curvelets and scalar voting&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Applications to TULIPS:  datasets, evaluation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This paper uses a handfull of branching curve datasets that we could use for our project.&lt;/p&gt;

&lt;p&gt;We also get an evaluation metric for comparing centerlines.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Possible Bisque analysis&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Run graph-cut or grab-cut to do foreground segmentation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/10/work-log"/>
   <updated>2013-11-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/10/work-log</id>
   <content type="html">&lt;h2&gt;Correspondence Misalignment&lt;/h2&gt;

&lt;p&gt;WACV ground truth reconstruction errors are due to correpondence misalignment that occurs when different views don&#39;t line up (esp because of drift in the plant position over time).&lt;/p&gt;

&lt;p&gt;Ultimately, this causes index set estimates to be suboptimal.  To correct this, we can either focus on fixing the point-correspondence stage, or fixing the index set directly, after the correspondence stage.&lt;/p&gt;

&lt;p&gt;Two possible approaches come to mind.&lt;/p&gt;

&lt;p&gt;The first approach attempts to fix the point correspondences.  Do some keypoint matching and use this to constrain certain parts of the correspondence-matching (i.e. force the dynamic programming to pass through certain entries in the cost matrix).  Currently unclear how to deal with incorrect keypoint matches, or many-to-many correspondenes.&lt;/p&gt;

&lt;p&gt;The second approach is to do some optimization on the index points to improve the marginal likelihood.&lt;/p&gt;

&lt;h2&gt;Optimizing index points&lt;/h2&gt;

&lt;p&gt;The following assumes that the problem is the index set obtained by greedy point correspondence is sub-optimal under the marginal likelihood, and the indices at the true optimium would be immune to misalignment, because our kernel is robust to trnalsation and scaling errors.&lt;/p&gt;

&lt;p&gt;Doing 350-dimensional optimization seems like a terrible pain, but we might be able to take some shortcuts.&lt;/p&gt;

&lt;p&gt;First, we observe that the analytical derivative of the ML w.r.t. any change in the index set will require a computation on the same order as evaluating the marginal likelihood (nxn matrix inversion, multiplication of several nxn matrices).&lt;/p&gt;

&lt;p&gt;However, if we cache some values, we can avoid matrix inversion when computing subsequent elements of the gradient.  This still leaves nxn matrix multilication, but this becomes sparse, since most elements of K don&#39;t change when a single index changes.&lt;/p&gt;

&lt;p&gt;Alternatively, we can choose a few good directions and optimize in those directions only.  For example, when (a) all values are shifted together, (b) one endpoint is unchanged, and each subsequent index is changed more and (c) the complement of b.&lt;/p&gt;

&lt;p&gt;Regardless of the gradient approach, we still have to do a matrix inverse at every iteration.  We might be able to do this blockwise and save computation for indices that don&#39;t change.  Are there other approximations that we can do?  This doesn&#39;t need to be exact, but good enough to improve the indices&lt;/p&gt;

&lt;p&gt;Lets keep in mind, we&#39;re dealing with relatively small matrices, here.&lt;/p&gt;

&lt;h2&gt;Other thoughts&lt;/h2&gt;

&lt;p&gt;It&#39;s possible a long, curvy curve (like the one&#39;s we get when misalignment occurs) are actually the result best supported under the marginal likelihood.  In that case, we need to think more deeply about our overall approach.&lt;/p&gt;

&lt;h2&gt;Camera calibration&lt;/h2&gt;

&lt;p&gt;There is likely some room for improvement in the camera calibration.  Maybe this is the best approach overall?&lt;/p&gt;

&lt;h2&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Improve camera calibration&lt;/li&gt;
&lt;li&gt;Write function to get derivatives of covariance matrix.&lt;/li&gt;
&lt;li&gt;Try index optimization.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Gradient w.r.t. Indices</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/10/reference"/>
   <updated>2013-11-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/10/reference</id>
   <content type="html">&lt;p&gt;To optimize indices, we&#39;ll need to compute the derivative of the marginal log-likelihood w.r.t. changing indices.&lt;/p&gt;

&lt;p&gt;I first tried to derive this using the generalization of the chain rule to matrix expressions (see matrix cookbook, section 2.8.1), but the computation exploded.  Since ultimately, the derivative is a simple single-input, single output function, we can use differentials to derive the solution.&lt;/p&gt;

&lt;p&gt;Let the marginal likelihood as a function of indices be \(g(x)\):&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial g(x)}{\partial x_i} = \frac{\partial}{\partial x_i} 
        \frac{1}{2}(-y^\top S^\top ( I + S K(x) S^\top)^{-1} S y)
\]

Let \(U = I + S K(x) S^\top\), and \(V = U^{-1}\).  Working inside out, lets find \(\frac{\partial U}{\partial x_i}\).

\[
\begin{align}
    U + dU  &amp;= I + S (K + dK) S ^\top \\
            &amp;= I + S K S^\top + S dK S ^\top \\
        dU  &amp;= S \, dK\, S^\top \\
        U&#39;  &amp;= S K&#39; S^\top
\end{align}
\]

Where \(M&#39;\) is the derivative of the elements of \(M\) w.r.t. \(x_i\).  Next, \(\frac{\partial V}{\partial x_i}\), which comes from the matrix cookbook, equation (36).

\[
    dV = -U^{-1} \, dU \, U^{-1} \\
    V&#39; = -U^{-1} U&#39; U^{-1}
\]

Finally,  \(\frac{\partial g(x)}{\partial x_i}\):
    
\[
\begin{align}
    g + dg  &amp;= -\frac{1}{2}y^\top S^\top (V + dV) S y \\
    g + dg  &amp;= -\frac{1}{2}y^\top S^\top \, V \, S y - \frac{1}{2} y^\top S^\top \,dV \,S y \\
        dg  &amp;= -\frac{1}{2}y^\top S^\top \,dV \,S y \\
        g&#39;  &amp;= -\frac{1}{2}y^\top S^\top \, V&#39; \,S y \\
\end{align}
\]

Expanding \(V\) gives the final formula:
\[
\begin{align}
        g&#39;  &amp;= \frac{1}{2}y^\top S^\top U^{-1} S K&#39; S^\top U^{-1} S y \\
        g&#39;  &amp;= \frac{1}{2}y^\top M K&#39; M y \\
        g&#39;  &amp;= \frac{1}{2}z^\top K&#39; z \tag{1}\\
\end{align}
\]

&lt;p&gt;
Here, \(M = S^\top U^{-1} S \), (which is symmetric), and \(z = M y\).  
&lt;/p&gt;

&lt;p&gt;
This equation gives us a single element of the gradient, namely \(d g(x)/dx_i\).  However, once \(z\) is computed, we can reuse it  when recomputing (1) for all other \(x_j\)&#39;s.  The cost of each subsequent gradient element becomes \(O(n^2)\), making the total gradient \(O(n^3)\), which is pretty good. (This assumes the K&#39;s can be computed efficiently, which is true; see below.)  However, we also observe that \(K&#39;\) is sparse with size \(O(n)\), so we can do sparse multiplication to reduce the running time to linear, and &lt;strong&gt;the full gradient takes \(O(n^2)\)&lt;/strong&gt;, assuming \(z\) is precomputed.  Cool! 
&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;&lt;/p&gt;


&lt;h2&gt;Derivatives of K(x)&lt;/h2&gt;

&lt;p&gt;First, we&#39;ll layout the general form of K&#39;, whose elements are the full derivative of the kernel w.r.t.  \(x_k\).&lt;/p&gt;

&lt;div&gt;
\[
\frac{\partial K_{ij}}{\partial x_k} = \frac{\partial k(x_i, x_j)}{\partial x_i} \frac{d x_i}{d x_k} + \frac{\partial k(x_i, x_j)}{\partial x_j} \frac{d x_j}{d x_k}\\
\]
&lt;/div&gt;


&lt;p&gt;The first term is nonzero only on the i-th row of K&#39;, and the second term is nonzero on the i-th column of K&#39;.  This suggests the following convenient sparse representation  for K&#39;.&lt;/p&gt;

&lt;p&gt;Let the vector \(\delta_i\)  be the vector whose j-th element is \( \frac{\partial k(x_i, x_j) }{\partial x_i} \).  Using this notation, we can rewrite \(K&#39;\) as&lt;/p&gt;

&lt;div&gt;
\[
    \frac{\partial K}{\partial x_i} = K&#39; = C + C^\top  \tag{4}
\]
&lt;/div&gt;


&lt;p&gt;where \(C = \left(0 \, \dots \, \delta_i \, \dots \, 0 \right)  \).&lt;/p&gt;

&lt;p&gt;Below we derive the derivative \(\frac{\partial k(x_i, x_j)}{\partial x_i}\) for each of the three covariance expresssions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cubic covariance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Recall the cubic covariance expression:&lt;/p&gt;

&lt;div&gt;
\[
k(x_i, x_j) = (x_a - x_b) x_b^2 / 2 + x_b^3/3
\]

Where \(x_b = min(x_i, x_i)\) and \(x_a = max(x_i, x_i)\).
&lt;/div&gt;


&lt;p&gt;Taking the derivative w.r.t. (x_i) gives:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\frac{\partial k(x_i, x_j)}{\partial x_i} &amp;= 
    \begin{cases}
         x_j^2 / 2 &amp; \text{if } x_i &gt;= x_j \\
         x_i x_j - x_i^2/2 &amp; \text{if } x_i &lt; x_i
    \end{cases} \\
            &amp;= 
    \begin{cases}
         x_b^2 / 2 &amp; \text{if } x_i &gt;= x_j \\
         x_a x_b - x_b^2/2 &amp; \text{if } x_i &lt; x_j
    \end{cases} \\
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;Or equivalently&lt;/p&gt;

&lt;div&gt;
\[
\frac{\partial k(x_i, x_j)}{\partial x_i} = 
         x_b \left ( x_j  - x_b/2 \right ) \tag{2}
\]
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Linear Covariance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Recall the linear covariance expression:&lt;/p&gt;

&lt;div&gt;
\[
k(x_i, x_j) = x_i x_j
\]

The derivative w.r.t. \(x_i\) is simply \(x_j\).

&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Offset Covariance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Recall the offset covariance expression:&lt;/p&gt;

&lt;div&gt;
\[
k(x_i, x_j) = k
\]

The derivative w.r.t. \(x_i\) is zero.
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Implementation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Implemented end-to-end version in &lt;code&gt;kernel/get_model_kernel_derivative.m&lt;/code&gt;; see also components in &lt;code&gt;kernel/get_spacial_kernel_derivative.m&lt;/code&gt; and &lt;code&gt;kernel/cubic_kernel_derivative.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;These functions return all of the partial derivatives of the matrix with respect to the first input.   The i-th row of the result make up the nonzero values in \(\frac{\partial K}{\partial x_i}\).  Below is example code that computes all of the partial derivative matrices.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;N = 100;
% construct indices
x = linspace(0, 10, N);
% construct derivative rows
d_kernel = get_model_kernel_derivative(...);
d_K = eval_kernel(d_kernel, x, x);
% construct dK/dx_i, for each i = 1..N
d_K_d_x = dcell(1,N);
for i = 1:N
    tmp = sparse(N, N);
    tmp(i,:) = d_K(i,:);
    tmp(:,i) = d_K(i,:)&#39;;
    d_K_d_x{i} = tmp;
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Directional Derivatives&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I think we can get directional derivatives of \(K\) by taking the weighted sum of partial derivatives, where the weights are the component lengths of the direction vector.  I have yet to confirm this beyond a hand-wavy hunch, and in practice, this might not even be needed, since computing the full gradient is so efficient.&lt;/p&gt;

&lt;h2&gt;Full gradient&lt;/h2&gt;

&lt;p&gt;As we saw earlier, \(\frac{\partial K}{\partial x_i}\) is sparse, and has the form in equation (4).  We can use the sparsity to ultimately compute the entire gradient in a single matrix multiplication.&lt;/p&gt;

&lt;p&gt;First we&#39;ll rewrite \(g&#39;\) in terms if \(\delta_i\)&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
g&#39; &amp;= \frac{1}{2} z^\top K&#39; z \\
   &amp;= \frac{1}{2} z^\top C z + z^\top C^\top z \\
   &amp;= \frac{1}{2} \left \{ (0 \, \dots \, z^\top \delta_i \, \dots \, 0) z + z^\top (0 \, \dots \, \delta_i^\top z \, \dots \, 0)^\top \right \} \\
   &amp;= z_i (\delta_i \cdot z)
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;We can generalize this to the entire gradient using matrix operations:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \nabla g = z \odot (\Delta z) \tag{4}
\end{align}
\]

&lt;/div&gt;


&lt;p&gt;Where \(\Delta\) is the matrix whose ith row is \(\delta_i\), and \(\odot\) denotes element-wise multiplication.&lt;/p&gt;

&lt;p&gt;To handle multiple dimensions, simply apply to each dimension independently and sum the results.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>iPlant Literature Review Planning Meetings</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/08/meeting-notes"/>
   <updated>2013-11-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/08/meeting-notes</id>
   <content type="html">&lt;h2&gt;Document discussion&lt;/h2&gt;

&lt;p&gt;Some random notes about issues to bring up in the final document.&lt;/p&gt;

&lt;p&gt;Talk about representation vs. &quot;features&quot; (characteristics).  Clarify &quot;features&quot;, don&#39;t use indescriminitively (call them high-level features, topological features, as opposed to image features).&lt;/p&gt;

&lt;p&gt;Two ways to go about finding &quot;features&quot;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;look directly for charactersitics/features&quot;&lt;/li&gt;
&lt;li&gt;&quot;look for model, find the other stuff&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;i.e. looking for representations vs. characteristics&lt;/p&gt;

&lt;p&gt;Bisque vs. Nonbisque?  Unclear what the final application will be, but if Bisque is the goal, some discussion of it&#39;s feasibility is probably warranted.  For example, interactive systems like the Clark paper might not be feasible for Bisque.&lt;/p&gt;

&lt;h2&gt;Taxonomy of curve-extraction methods&lt;/h2&gt;

&lt;p&gt;Kobus mentioned the different &quot;dimensions&quot;  that the problem can be split into.  Some might be&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3D vs 2D&lt;/li&gt;
&lt;li&gt;imaging system type (microscope, MRI, camera, etc)&lt;/li&gt;
&lt;li&gt;temporal vs. nontemporal&lt;/li&gt;
&lt;li&gt;branching vs non branching&lt;/li&gt;
&lt;li&gt;biological vs. nonbiological&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Research Topics&lt;/h2&gt;

&lt;p&gt;We can split the background reading into several areas, both in image processing/CV and biological application.
Kobus recommended splitting time between these broad areas.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image Processing Topics&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;snakes&quot; - active contours&lt;/li&gt;
&lt;li&gt;Medial axis filtering&lt;/li&gt;
&lt;li&gt;curve modelling (polynomial/splines, GP, level-set methods)&lt;/li&gt;
&lt;li&gt;finding branches&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Application Areas&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pollen tube tracking&lt;/li&gt;
&lt;li&gt;root tracking&lt;/li&gt;
&lt;li&gt;Blood vessel literature / vascular segmentation&lt;/li&gt;
&lt;li&gt;Neuron tracing&lt;/li&gt;
&lt;li&gt;Alternaria / Fungus?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Other stuff&lt;/h2&gt;

&lt;p&gt;Other topics that might be worth looking into, but maybe of speculative value.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rhizotron - nondestructive underground root imaging system&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Data Inventory &lt;/h2&gt;

&lt;p&gt;We currently have data sets from a handful of different applications/domains.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pollen tubes&lt;/li&gt;
&lt;li&gt;&quot;Clark&quot; root image (2D, one image)&lt;/li&gt;
&lt;li&gt;&quot;Max Plank&quot; root images&lt;/li&gt;
&lt;li&gt;Neurons&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Any questions for Martha?&lt;/p&gt;

&lt;p&gt;Reading for next week: look into citations from Clark and Hypotrace&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kate - related work, Clark Paper&lt;/li&gt;
&lt;li&gt;Andrew - related work, Hypotrace paper&lt;/li&gt;
&lt;li&gt;Kyle image processing&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As we read, lets try to place each paper somewhere in the &quot;Taxonomy&quot; above.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>WACV reconstruction (revisited)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/07/work-log"/>
   <updated>2013-11-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/07/work-log</id>
   <content type="html">&lt;p&gt;Addressing issues that came up last time we ran WACV.&lt;/p&gt;

&lt;p&gt;Currently working on dataset #8, which has some real problems.&lt;/p&gt;

&lt;p&gt;Spent some time tweaking the ground truth, adding missing curves, and correcting one or two obvious mistakes.&lt;/p&gt;

&lt;p&gt;Still getting terrible results; one problem might be the &quot;rough&quot; reconstruction, which doesn&#39;t take into account anisotropic data uncertainty.  Fixed.&lt;/p&gt;

&lt;p&gt;False-positive reversal has been corrected, but still getting terrible results; it seems like the camera calibration is waaaay off.&lt;/p&gt;

&lt;p&gt;I dont have the calibration data for this dataset available, locally re-syncing full dataset.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Are these calibrated cameras ordered in counter-clockwise direction? No.  but the calibration_data.mat file in for this dataset is.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Inspected correspondence diagrams.  Possibly some of the ground truth curves are reversed?  Dataset #8, curve #7, view #9, only corresponds to very end of other points:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-07-bad_correspondence.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tweaking ground truth tracing program to show start-point of curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Ground truth program doesn&#39;t compile.  Clang++ for mountain lion is complaining.&lt;/p&gt;

&lt;p&gt;Apparently clang no longer uses gnu stdc++ library, and some library components use proprietary gnu symbols (I admit, it was my code).  Fixed.&lt;/p&gt;

&lt;p&gt;Getting some new errors regarding some forward declarations of STL pair.  Was hard to debug, because errors weren&#39;t local to the problematic code, and errors were cryptic.  Removed forward declarations; fixed.&lt;/p&gt;

&lt;p&gt;Found a template function I hand&#39;t ported from my experimental branch of KJB.&lt;/p&gt;

&lt;p&gt;Linker errors -- libXmu and libXi not found.  Tweaked init_compile&#39;s logic for OpenGL on Macs. Hopefully I didn&#39;t break anyone else&#39;s builds...&lt;/p&gt;

&lt;p&gt;More linker errors. Apparently i need to recompile &lt;strong&gt;everything&lt;/strong&gt; to use clang&#39;s c++ libraries?? rebuilt boost, next is casadi, but I can&#39;t find the source on my machine.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;recalibrate cameras&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>CVPR cleanup, documentation</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/06/work-log"/>
   <updated>2013-11-06T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/06/work-log</id>
   <content type="html">&lt;p&gt;Cleaned up blog, added &lt;a href=&quot;/ksimek/research/projects&quot;&gt;project pages&lt;/a&gt;, &lt;a href=&quot;/ksimek/research/events&quot;&gt;events page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Added to &lt;a href=&quot;/ksimek/research/events/CVPR2014/&quot;&gt;CVPR 2012 page&lt;/a&gt;, created &lt;a href=&quot;/ksimek/research/events/CVPR2014/summary.html&quot;&gt;summary of &quot;done&quot; and &quot;TODO&quot;&lt;/a&gt; post-CVPR.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;WACV - extend to multi-view/tracking; fix reconstruction errors.&lt;/p&gt;

&lt;p&gt;Working, but need to remove translation and scale perturb component.&lt;/p&gt;

&lt;h2&gt;Testing branch index and start index&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;negative start index causes gaps&lt;/li&gt;
&lt;li&gt;positive start index causes overshot (pre-tails)&lt;/li&gt;
&lt;li&gt;tree base is shifting between view&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO (new) &lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Testing branch index and start index, and reversal&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>iPlant </title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/05/work-log"/>
   <updated>2013-11-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/05/work-log</id>
   <content type="html">&lt;p&gt;Doing background research inre: forwarded email from Kobus, &quot;Fwd: What we really need from your students&quot;.&lt;/p&gt;

&lt;h1&gt;Literature review&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Paper 1:&lt;/strong&gt; &lt;em&gt;Three-Dimensional root Phenotyping with a Novel Imaging and Software Platform&lt;/em&gt;, by Clark, et. al.&lt;/p&gt;

&lt;p&gt;Introduces a semi-automated method for semantic reconstruction of root structures from turntable images, using our lab&#39;s definition of semantic reconstruction, i.e. 3D curves with topology and part labels.  It looks like they&#39;re using standard voxel-carving to identify foreground and background voxels (i.e. root and non-root voxels).  This approach uses calibrated cameras  to backproject silhouette images and take the intersection--in other words, visual hull but with voxels instead of polygonal meshes.   Then the skeleton of the foreground voxels is extracted using a median filter method to extract 3D curves.  Skeleton branches are then manually labeled by domain experts as one of several root types.  There also seems to be some functionality to manually correct errors during backprojection and skeleton extraction phase.&lt;/p&gt;

&lt;p&gt;They key contribution seems to be a list of 27 features derived from the resulting 3D data.  Bushiness, centroid and volume distribution seem to be discriminative for classifying a specimen between the two different species under study.    They also  measure the amount of helical curvature and how much gravity affects growth, which has apparently not been studied in rice plants prior to this?.&lt;/p&gt;

&lt;p&gt;Extracting most of the interesting features requires full semantic reconstruction, which is very difficult to obtain using known fully-automatic methods.  Further, this approach requires a calibrated camera, which likely precludes us from using it for post-hoc analysis of existing datasets that might exist in Bisque, unless calibration data is available.&lt;/p&gt;

&lt;p&gt;The &quot;Clark Rice root&quot; image provided on the wiki is a high resolution 2D image, which appears to be different from those used in the Clark paper, so it&#39;s unclear what its relevance is in this context.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Other notes&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Its unclear how silhouettes are extracted, but likely just intensity thresholding with manually-chosen threshold value.  They&#39;re using a lightbox background, so this approach seems sensible.   There&#39;s probably some tunable parameters for the skeleton-extraction phase, but these aren&#39;t discussed.&lt;/p&gt;

&lt;!--
Bisque thoughts
===================

* 3D approaches might be unrealistic in Bisque
    1. Most datasets won&#39;t have calibrated cameras.
        * Could try auto-calibration, but I double their effectiveness in the wild.
        * Could have user click corresponding points, between views, but this leads us to...
    2. Most datasets won&#39;t have multiple views.
        * could try bayesian approaches to solving the inverse problem, but this is an open research problem.

    3. If 1. and 2. are satisfied, we still need significant user interaction to implement Clarke; unlikely to be possible in Bisque?
        * Maybe Bisque could receive the files output by Clarke&#39;s software?  But at that point, the analysis is already done, what is Bisque being used for?
* Most effective 3D systems will collect data under a process specifically tailored to 3D data.  How does Bisque fit into this scenario?




iPlant recognizes that most image-processing software developed by biologists is heavily tuned to a specific application, and doesn&#39;t generalize to other datasets.  This is a problem endemic to the entire field of computer vision; even today many approaches work well on the dataset they were trained on and fail on others.  Even the Deva/Feltzenswalb human detectors--arguably one of the most robust object detectors in the field -- performs poorly on new datasets, as we saw when we applied it to Mind&#39;s Eye.  Only after rescaling images, tuning thresholds, and retraining did we get reasonable performance.

This highlights a very CV-centric mindset, which I claim hinders the progress toward the goal of obtaining the best possible plant informatics.  The problem is a misalignment of goals.  For CV researchers, the goal is to do the best CV possible with as little human interaction possible.     In contrast, biologists&#39; goals are to do the best biology possible, which hinges on obtaining the best possible data at the least possible cost. As a result, CV researchers judge our algorithms on &quot;percent correct&quot; assuming zero human interaction, whereas biologists are more likely to judge an approach by the number of human-hours needed to get to 100% accuracy or close to it.   The Clark paper is a good example, where humans manually make corrections to the computer vision algorithm and manually label the parts of a model; only then do they obtain the 27 features of plant roots that make their research novel.  We should recognize that our roles as iPLant &quot;consultants&quot; may conflict with our natural instincts as CV researchers and try to embrace strategies that leverage our CV expertise but ultimately serve the goals of biologists.

One such strategy is to relax the &quot;fully automatic&quot; constraint and look toward semi-automatic systems that use human input to improve robustness and achieve broad applicability across datasets.  Clark is a perfect example of such a method that uses some human interaction in exchange for drastically better results.  I&#39;ve seen similar successes in the area of plant modeling, where interactive CAD interfaces are combined with CV algorithms from 20 years ago (e.g. snakes) to create a method that is efficient and provides precise models.

This not to say that advanced CV methods have not place in iPlant, but if Martha says our current charges is to make recommendations as to broadly-applicable methods that are currently &quot;doable,&quot; my answer is &quot;semi-automatic methods&quot;.  Putting this in the context of CV, even the best fully automatic methods aren&#39;t really fully automatic, because they require training on the part of the user, which requires some form of interactive system.

So the overall theme of my argument is using CV to construct better interactive GUIs.  At the moment, the greatest challenge to this strategy are the constraints posed by the current Bisque system, which provides only a minimal amount of HTML-esqe interactivity (e.g. click points, type in &quot;tags&quot;), in a painfully &quot;modal&quot; framework (e.g. submit user&#39;s response to server, wait for server do some analysis, repeat).  One solution is to push hard in improviing the Bisque framework, for example exploiting HTML5 and javascript to provide rich client-side user interfaces.  However, not only would this be an expensive undertaking, but the most effective GUIs will need to be specific to the task at hand, and building an interactive system for building interactive system would be a herculean task in both design and implementation.  

Maybe the answer lies outside of Bisque itself.  Bisque&#39;s real value is as a database of images and metadata.  Let&#39;s recognize this and push more toward promoting scientists adding Bisque-compatible &quot;export&quot; functionality to existing data-analysis systems (like Clark, et al.), so Bisque can become the world-standard repository for rich image metadata.  

Alternatively, if iPlant wants to continue to push in the direction of performing analysis inside the system, it needs to put significant effort into interfaces that are dynamic, responsive, and powerful.  This means leaning heavily on javascript, using canvas, webGL, and client-side image analysis so users can respond in real-time to the results of their interactions.  For example, imagine an interface in which users can &quot;paint&quot; some foregound and background pixels to train a classifier, quickly see the result of the classifier, and re-train by painting misclassified pixels.   Or an automatic curve-tracing algorithm where users can drag incorrect curves onto their correct path.  These are approaches that are proven and can provide excellent results, but require human-hours to get there.  Minimizing human hours will be a combination of good UI to fix CV errors quickly and good CV to minimize the UI time.  



The alternative is to focus on the features we can extract in the absence of near 100% accuracy.  

--&gt;

</content>
 </entry>
 
 <entry>
   <title>Post-CVPR-deadline; 2-part likelihood efficiency, 2-pass sampling</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/11/04/work-log"/>
   <updated>2013-11-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/11/04/work-log</id>
   <content type="html">&lt;p&gt;First day after CVPR deadline.&lt;/p&gt;

&lt;p&gt;Comp exam - committee, planning.&lt;/p&gt;

&lt;p&gt;Computer issues - upgrade, crashing, keyboard?&lt;/p&gt;

&lt;h1&gt;2-Pass sampling.&lt;/h1&gt;

&lt;p&gt;Evaluating the second likelihood term is still very slow, even after a 100x speedup.&lt;/p&gt;

&lt;p&gt;First do MH using single-term likelihood.  Then treat that as the proposal for the two-term likelihood.  Since the first step satisfies detailed balance, we have:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \hat p(\theta) q(\theta&#39; | \theta) &amp;=
    \hat p(\theta&#39;) q(\theta | \theta&#39;) \\
    \frac{\hat p(\theta)}{\hat p(\theta&#39;)} &amp;=
    \frac{q(\theta | \theta&#39;)}{q(\theta&#39; | \theta)} 
\end{align}

\]
&lt;/div&gt;


&lt;p&gt;Where \(\hat p(\theta)\) is the surrogate posterior, using only the single-term likelihood.  Substituting this identity into the full MCMC acceptance term, we get:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
\alpha &amp;= \left \{ \frac{p(\theta&#39;) q(\theta | \theta&#39;)}{p(\theta)q(\theta&#39; | \theta)}  \right \} \\
    &amp;= \left \{ \frac{p(\theta&#39;) \hat p(\theta)}{p(\theta)\hat p(\theta&#39;)}  \right \} \\
    &amp;= \frac{L_2(\theta&#39;)}{L_2(\theta)}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;This obviously isn&#39;t applicable for traditional gibbs sampling, but gibbs could be used for proposal, and MH used to accept/reject&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>(Two day) Markov Sampling (ctd).  Implementing, testing, optimizing</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/31/work-log"/>
   <updated>2013-10-31T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/31/work-log</id>
   <content type="html">&lt;p&gt;Note: changes to &lt;a href=&quot;/ksimek/research/events/CVPR2014/params.html&quot;&gt;params&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Resuming from yesterday&#39;s discussion about Markov sampling.  I was concerned about piecewise sampling of interpolated values with insufficient data being used.  After some thought, realized there&#39;s a better approach: construct blocks from input indices, not output indices.&lt;/p&gt;

&lt;p&gt;Secondly: only use the markov approach when there&#39;s too much data to eat at once.  Those cases are also the ones with the least probability of poor-data issues.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Re-implemented (&lt;code&gt;curve_tree_ml_5.ml&lt;/code&gt;), geting weird results.  Huge negative eigenvalues&lt;/p&gt;

&lt;p&gt;symptom: 30 output indices, 1000 output indices
symptom: K has 26k elements!&lt;/p&gt;

&lt;p&gt;This code is a mess, proving hard to debug.  I&#39;m going to roll back to version 4, use ideas developed in v5 to implement Markov sampling.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Done.  No errors, but results aren&#39;t great.  Is it possible the markov blanket is wrong, or maybe we&#39;re misusing previous data?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;I think I found it... wasn&#39;t conditioning on previous sampled values.&lt;/p&gt;

&lt;p&gt;Seems to be fixed now.  Next on to timing and tuning&lt;/p&gt;

&lt;h2&gt;Profiling / Optimizing&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;attachment covariance&lt;/li&gt;
&lt;li&gt;markov order&lt;/li&gt;
&lt;li&gt;block size&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;strong&gt;Constructing Attachment covariance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;construct_attachment_covariance_3 is still a bottleneck.  Need to investigate some of the suggestions from a few days ago.&lt;/p&gt;

&lt;p&gt;Only computing &lt;code&gt;Cov_star_star&lt;/code&gt; once (exploiting stationarity in temporal GP) helps somewhat.&lt;/p&gt;

&lt;p&gt;Grouping &quot;sibling&quot; object construction should help a lot too.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Markov order&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Markov order as low as 10 doesn&#39;t seem to negatively affect results.&lt;/p&gt;

&lt;p&gt;Need to crop observations before the earliest sampled point we&#39;re conditioning on.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Timing
Before: 17.0s (total,  7.5s on inversion)
After: 16.5 (7.2s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tiny improvement, but at no cost so who&#39;s complaining?&lt;/p&gt;

&lt;h2&gt;Block Size&lt;/h2&gt;

&lt;p&gt;Not much to say here...  100 seems to be optimal.&lt;/p&gt;

&lt;h2&gt;Markov Order&lt;/h2&gt;

&lt;p&gt;Plotted maximum posterior for various markov orders to compare reconstruction quality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-11-01-mls_vs_mo.png&quot; alt=&quot;maximum liklehood vs. markov order&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;range: 1.40e3
(unweighted) mean: 4.0156
std deviation: 510.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the mean should be taken with a grain of salt,
Some chaos below 500, but the scale of the fluctuations is actually relatively small, 0.035%.  We should note that in MCMC, its the absolute fluctuations that are significant, so percent error can be deceptive.  But even so, I think these results are pretty good.&lt;/p&gt;

&lt;p&gt;It&#39;s interesting that markov order of 10 is only slightly better than markov order of zero!    Also surprising that between 10 and 500, error increases.&lt;/p&gt;

&lt;p&gt;Should compare variance here vs. variance of log-likelihood w.r.t. posterior.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Result of 20 posterior samples (low markov order)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markov order = 10
block size =  100
ll std deviation = 2.33e3
ll mean: 4.0153e6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Result of 20 posterior samples (high markov order)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markov order = 500
block size =  100
ll std deviation = 2.17e3
ll mean: 4.0132e6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The log-likelihood standard deviation are high relative to the error range in the graph above.   This is weak evidence that the variance in the graph above is dues to small-scale instabilities in the pixel likelihood.  That is to say, tiny insignificant changes to the 3D model can cause noticible fluctuations to the pixel likelihod, due to it&#39;s nonlinearities.  To some extent, what we really want is the expected value of the likelihood over the entire set insignificant 3D perturbations.  So if the deviations we see due to markov-order are simply observations of this phenomenon, they can be safely ignored, because they are in any scenario.&lt;/p&gt;

&lt;p&gt;Let&#39;s test by re-running the low-markov order test with lots of samples (200 instead of 20) and see if the ll mean approaches that of the high-markov-order test.  This is also a decent stress test for the likelihood server.&lt;/p&gt;

&lt;p&gt;Result of 200 posterior samples (low markov order)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;markov order = 10
block size =  100
ll std deviation = 2.5603e3
ll mean: 4.0133e6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep, we&#39;re getting closer to the high markov order results for ll mean.  I think we can drop this issue for now and be okay with a markov order of 10-20.  It is still surprising that markov order of zero seems to work so well.  I guess future data doesn&#39;t add too much if the present data is sufficient.&lt;/p&gt;

&lt;h2&gt;Bottlenecks&lt;/h2&gt;

&lt;p&gt;Inversion: 7.1s
buildling covariance:  2.5s
one_d_to_three_d: 1.5s
blkdiag: 1.2s
other: ~3.5s&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inversion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Only need to invert once for all views... Actually, not true because inversion contains view-specific sample data.  But a large block of the matrix is unchanged between views.  Implementing optimization...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Down to &lt;strong&gt;14.7 seconds&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Using sparsity and suppressing output, down to &lt;strong&gt;11.5s&lt;/strong&gt;. (8s when not in profile mode)&lt;/p&gt;

&lt;p&gt;However, getting some numerical issues (small-magnitude negative eigenvlaues); probably will do better if we use a symmetric equation and invert using cholesky.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;construct_attachment_covariance is now the bottlneck, taking a full 35% of run time.&lt;br/&gt;
Some options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;cache intermediate values here and return them as described in the previous post.&lt;/li&gt;
&lt;li&gt;group together calls to &quot;build_sibling_object&quot;&lt;/li&gt;
&lt;li&gt;avoid computing self-covariance when its available in prior_K&lt;/li&gt;
&lt;li&gt;implement one-pass version for symmetric matrices&lt;/li&gt;
&lt;li&gt;precompute self-covariance for all views in one call.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Implemented 2, 3, and 4, reduced running time to &lt;strong&gt;8.7s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Implemented 1, reduced to  &lt;strong&gt;8.4s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Implemented 5, now construct_attachment_covariance is not a bottleneck.  &lt;strong&gt;8.3s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Not many strong bottlenecks now.  Some opportunities  to avoid build_cov_ and one_d_to_three_d like we did in &lt;code&gt;curve_tree_ml_5_bak&lt;/code&gt;.  Also, using a symmetric formula for posterior might help here.&lt;/p&gt;

&lt;p&gt;Tweaked build_cov_ to skip an inner loop if it would be a no-op.  Down to &lt;strong&gt;7.8s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Set block size to 100 (had forgotten to change back yesterday, was 120)   &lt;strong&gt;7.6s&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Rearranged matrix multiplication.  &lt;strong&gt;7.4s&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mex&#39;d &lt;code&gt;one_d_to_three_d&lt;/code&gt; utility function.  &lt;strong&gt;6.9s&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Problem.  Samples look bad.  Overall shape is retained, but rough, with discontinuities.&lt;/p&gt;

&lt;p&gt;Switched off block matrix inversion, results look good, but back to &lt;strong&gt;8.3s&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found bug.  Indexing problem was causing previous samples to be ignored.  Fixed, and its better than before, but still getting some non-negligible eigenvalues and occasional discontinuities in samples.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Originally took 275s, now 8.3s in profile mode, 5.x s in regular mode.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;running time vs. number of samples to determine affect of likelihood server bottleneck (use GPU likelihood for real-world estimate)&lt;/li&gt;
&lt;li&gt;profile/optimize likelihood server directly?&lt;/li&gt;
&lt;li&gt;Symmetrize formula and use cholesky for inversion.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Optimizing posterior-sampling for pixel likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/30/work-log"/>
   <updated>2013-10-30T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/30/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting issues in &quot;optimized&quot; code in &lt;code&gt;curve_tree_ml_4&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Biggest change was in how K, K_star, K_star_star were computed.
Comparing against the reference implementation &lt;code&gt;curve_tree_m_3&lt;/code&gt; shows the &quot;parent&quot; indices aren&#39;t correct.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Solved.  When incorporating noise-free sampled values, was using indices and covariance from noisy observed values.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Timing

Old speed:    20.7 s
New runtime:  75.7 s
Speedup: 3.6x
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Still rendering black.  why?&lt;/p&gt;

&lt;p&gt;Apparently this is an issue with OSX&#39;s AMD driver -- geometry shaders fail after returning from sleep.  Restarting program solves it.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Profiling v4&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Only 19 calls to &lt;code&gt;construct_attachment_covariance_3&lt;/code&gt;, down from ~500.&lt;/p&gt;

&lt;p&gt;78% bottleneck in matrix multiplication / inversion.&lt;/p&gt;

&lt;p&gt;Main problem is the 2500x2500 matrix from the main stem.  Other curves are in the 200 to 500 dimension range, and are fairly fast.&lt;/p&gt;

&lt;p&gt;Nystrom (2x) on curves with dimension greater than 1000 brings bottlenect from 18s to 11s, but as before, results are highly sensitive to the nystrom factor and using it without careful tuning / heursitics is risky.&lt;/p&gt;

&lt;p&gt;Probably expoiting the Markov nature of the curve GP  is the answer.  Linear runtime vs. cubic.&lt;/p&gt;

&lt;h2&gt;Markov within-curve sampling&lt;/h2&gt;

&lt;p&gt;break output indices into blocks.  For each block, get observation markov blanket.  Combine observed data and ancestor data into covarance matrix and data vector, then sample.&lt;/p&gt;

&lt;p&gt;Needed three covariance matrices: data vs. data, model vs. model, and model vs. data.&lt;/p&gt;

&lt;p&gt;Opportunity for optimizing construct_attachment_covariance_3 - sample all simpling-pair covariances at once.  289 total combinations (i.e. calls to eval_kernel) per dataset, could reduce to one.&lt;/p&gt;

&lt;p&gt;Seems like we can save computation in model covariance matrices by (a) exploiting the fact that it&#39;s always between points in the same view, and (b) only view changes between calls, spacial indices stay the same.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(A) compute K_star_star only once.&lt;/li&gt;
&lt;li&gt;(B) cache prior_K between constructions of K_star.&lt;/li&gt;
&lt;li&gt;(*) Cache &lt;code&gt;obj&lt;/code&gt;s and &#39;status&#39; between calls to construct_attachment_covariance.  K_star reuses the lower triangular elements of K, K_star_star simply mirrors the upper-triangular  elements of K_star.&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Implementing...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Theoretical issues with the within-curve markov assumption.  If the markov blanket is chosen crudely, your samples could drift aimlessly until they reach observations in an unexpected place.  In other words, extrapolation, or sampling from weak data is a mistake.  Need a good criterion for when to stop takin on more evidence.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ancestral Sampling from poserior (Markov-aware sampling)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/29/work-log"/>
   <updated>2013-10-29T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/29/work-log</id>
   <content type="html">&lt;p&gt;Realization: nystrom may be tricky, because the same thinning factor probably won&#39;t be appropriate for all scenarios (e.g. short curves).&lt;/p&gt;

&lt;p&gt;Possible approach: always take first and last inidices of a curve; thin longer curves more aggressively.&lt;/p&gt;

&lt;p&gt;Better idea: use markov structure.&lt;/p&gt;

&lt;h2&gt;Exploiting Markov structure for Sampling plants &lt;/h2&gt;

&lt;p&gt;Main ideas: Use inheritence sampling to sample fully tree.  Two rules (1) Parent curve posterior should only depend on it&#39;s observed points and a small number of points on its children.  (2) Child curves depend also on the sampled parent points.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parent points&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The main question here is how many and which observed points of  child-curves should be included to compute posterior?  Heuristic approach: start with child points nearest to branch-point and work outward until information gain is negligible.&lt;/p&gt;

&lt;p&gt;Information gain is the change in entropy of the branch-point posterior, conditioned on the active set.  Entropy is proportional to the determinant of the covariance matrix.&lt;/p&gt;

&lt;p&gt;Let bp be the branch point, and I be the active set.&lt;/p&gt;

&lt;div&gt;
\[
Entropy(bp | I) \propto K(bp, bp) - K(bp, np) [K(np, np)  + \Sigma_y(np)]^{-1} K(np, bp)
\]
&lt;/div&gt;


&lt;p&gt;Where Sigma_y is the observation covariance.  We may choose to use the prior conditional entropy, which is a simpler stand-in for the posterior conditional entropy.&lt;/p&gt;

&lt;div&gt;
\[
Entropy(bp | I) \propto K(bp, bp) - K(bp, np) [K(np, np)]^{-1} K(np, bp)
\]
&lt;/div&gt;


&lt;p&gt;Note that the entropy is now only a function of the indices of the observations, not the observations themselves.  Thus, each observation only contribues one dimension to the matrix inversion, instead of three, making the computation 3&lt;sup&gt;3&lt;/sup&gt; times faster.  This is only sensible if the observation variance is relatively uniform between observations, which isn&#39;t necessarilly true, but it may be servicable heuristic if thresholds are set appropriately high.&lt;/p&gt;

&lt;p&gt;Opportunities exist to do rank-one updates to inverse, but probably not necessary, as the matrices should be small.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Child curves&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Note that child-curves can also be parent-curves, so the previous approach for determining the active set should be followed.&lt;/p&gt;

&lt;p&gt;We need to estimate the conditional prior probability for the curve, conditioned on its sampled parent.  If the sampled parent contains the branch point, bp, the conditional prior covariance is already available in the track&#39;s pre-computed prior_K field, and the mean is bp.  In general, bp isn&#39;t sampled, so we must use the nearby points on the parent to find the conditional prior of bp first and sample it explicitly.&lt;/p&gt;

&lt;p&gt;Let bp be a branch point, cp be the child point, and nbp be the &quot;neighbors&quot; of bp on the parent curve, i.e. the markov blanket of bp on the parent.&lt;/p&gt;

&lt;p&gt;Given bp, the child points are distributed by&lt;/p&gt;

&lt;div&gt;
\[
cp \mid bp \sim \mathcal{N}(bp, K_{prior})
\]
&lt;/div&gt;


&lt;p&gt;The branch point is distributed by&lt;/p&gt;

&lt;div&gt;
\[
bp \mid pnp \sim \mathcal{N}(\mu, \Sigma)
\]
&lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt;
\[
\mu = k(bp, nbp) k(nbp, nbp)^{-1} nbp \\
\Sigma = k(bp, bp) - k(bp, pnp) K(pnp, pnp)^{-1} K(pnp, bp)
\]
&lt;/div&gt;


&lt;p&gt;The a neighborhood size of four points around bp should be sufficient, since this model is piecewise cubic, but it may be worthwhile to experiment with more, since we aren&#39;t in bottleneck territory.&lt;/p&gt;

&lt;p&gt;Now that we have conditional priors, we can construct the conditional posterior using the standard formula.&lt;/p&gt;

&lt;h2&gt;Mixing noisy and noise-free data in Posterior&lt;/h2&gt;

&lt;p&gt;Ran into a snag when trying to incorporate the noise-free conditional values that arise during ancestral sampling.  Derived an elegant solution; see the &lt;a href=&quot;/ksimek/research/2013/10/29/reference&quot;&gt;writeup here&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Implementing markov-aware pixel likelihood&lt;/h2&gt;

&lt;p&gt;For now, using a constant number of points on parent, instead of using the entropy method above.  Could add later; basic approach: precompute all covariances and add/remove one at a time.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Seems to be working, but slow as hell (slower than nystrom method). profiling&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Construct_attachment_covariance_3 is called 486 times!&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Construct non view-specific matrices outside of view loop.&lt;/li&gt;
&lt;li&gt;nystrom for large curves?&lt;/li&gt;
&lt;li&gt;do all covariances at once?  saves the computation of sibling and parent objecst?&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;implemented a first attempt at speedup; producing faulty results.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish optimized ancestral sampling&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Mixing Noisy and Noise-free values in GP Posterior</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/29/reference"/>
   <updated>2013-10-29T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/29/reference</id>
   <content type="html">&lt;p&gt;When doing ancestral sampling of the posterior, each curve&#39;s conditional posterior depends on (a) it&#39;s relevant (noisy) observation and (b) the sampled (noise-free) values of the parent.  We can incorporate both in the posterior by treating the noise-free values as observations with zero variance in the likelihood.&lt;/p&gt;

&lt;p&gt;In practice, theres a minor issue implementing this.  Recall that because 3D observations are degenerate in one direction (namely the backprojection direction), we prefer to work with the precision matrix, \(\Lambda\), instead of a covariance matrix.  Under this representation, noise-free values have infinite precision, so operations on \(\Lambda\) are invalid.  Instead, we take a hybrid appraoch, using both precisions and covariances.&lt;/p&gt;

&lt;p&gt;Recall the standard formulation for the posterior mean:&lt;/p&gt;

&lt;div&gt;
\[
    \mu = K_* \left[ K + \Sigma \right]^{-1} y
\]
&lt;/div&gt;


&lt;p&gt;Without loss of generality, assume zero-noise observations appear after noisy observations.  We can rewrite the posterior in terms of the precision matrix \(\Lambda\) of the noisy values:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
    \mu &amp;= K_* \left[ K + \left( 
                \begin{array}{cc}
                    \Lambda^{-1} &amp; 0 \\
                    0 &amp; 0
                \end{array}
            \right) \right]^{-1} y \\
        &amp;= K_* \left[
        
            \left( 
                \begin{array}{cc}
                    \Lambda &amp; 0 \\
                    0 &amp; I
                \end{array}
             \right) 
        K + 
            \left( 
                \begin{array}{cc}
                    I &amp; 0 \\
                    0 &amp; 0
                \end{array}
             \right) 

            \right]^{-1} 
            \left( 
                \begin{array}{cc}
                    \Lambda &amp; 0 \\
                    0 &amp; I
                \end{array}
             \right) 
            y \\
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;We can implement this by slightly modifying our code for the precision-matrix formulation of posterior.  First, give all noise-free values a precision of 1.0 in \(\Lambda\), and then modify the \(I\) inside the parentheses by zeroing-out elements corresponding to noise-free values.  Using prime symbol to denote the modified matricies, the result is&lt;/p&gt;

&lt;div&gt;
\[
    \mu = K_* \left[ \Lambda&#39; K + I&#39; \right]^{-1} \Lambda&#39; y
\]
&lt;/div&gt;


&lt;p&gt;The expression for posterior covariance is derived in the same way, and has similar form.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Implementing Nystrom method; bugs in posterior sampling code</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/28/work-log"/>
   <updated>2013-10-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/28/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Re-ran with method 2.  Still getting negative eigenvalues.&lt;/p&gt;

&lt;p&gt;Possibly non-symmetry issue.&lt;/p&gt;

&lt;p&gt;Change gears -- work on fast implementation, then resume debugging.&lt;/p&gt;

&lt;h2&gt;Reduced rank approximation of data-covariance&lt;/h2&gt;

&lt;div&gt;
&lt;p&gt;
Goal: approximate \(\left(K + \Sigma_D \right)^{-1}\), using a low-rank approximation of K;  where \(\Sigma_D\) is the likelihood covariance.
&lt;/p&gt;&lt;p&gt;
In our case \(\Sigma_D\) has infinite covariance, so this inverse doesn&#39;t exist. However, if we work with precision matrix \(\Lambda = \Sigma_D^{-1}\), and use the decomposition \( S&#39; S = \Lambda \) we can replace this expression with the equivalent

\[
    S&#39; \left( S K S&#39; + I \right)^{-1} S
\]

Even though S is rank-deficient, the inverse in this expression does exist, thanks to the finite positive values being added to the diagonal.  
&lt;/p&gt;&lt;p&gt;
We approximate the above expression as follows.  Let \(K\) be an \(N\) by \(N\) matrix. We can take the eigendecomposition \(K = V D V&#39;\) and approximate it with \(\tilde K = \tilde V \tilde D \tilde V&#39;\), where \(\tilde V\) and \(\tilde D\) consist of the first \(n\) eigenvalues and eigenvectors of \(K\) respectively.   We can use this low-rank approximation with the [Woodbury matrix identity](http://en.wikipedia.org/wiki/Woodbury_matrix_identity) to approximate the above inverse in \(O(n^3)\) time, rather than \(O(N^3)\).  The Woodbury identity is
        
\[
    (A + U C V )^{-1} = A^{-1} - A^{-1} U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}
\]

Setting \(A = I\), \(U = V&#39; = S \tilde V \) and \(C = \tilde D \), we get:
    
\[
    \left( S K S&#39; + I \right)^{-1} = I - S \tilde V (\tilde D^{-1} + \tilde V&#39; S&#39; S \tilde V)^{-1} \tilde V&#39; S&#39;
\]
&lt;/p&gt;&lt;p&gt;
It remains to find \(\tilde V\) and \(\tilde D\) efficiently.  Naive eigenvalue decomposition takes \(O(N^3)\) time, which isn&#39;t any better than direct inversion.  Sections 4.3.2 and 8.1 of Williams and Rasmussen show how to approximate \(n\) eigenfunctions and eigenvalues from \(n\) data points in \(O(n^3)\) time.  Eigenvectors \(\tilde V\) arise by evaluating the approximate eigenfunctions at the appropriate indices.
&lt;/p&gt;
&lt;p&gt;

Substitutuing back into the original expression by surrounding with (\(S&#39;\) and \(S\)), we get the final expression:
    
\[
\begin{align}
    \approx &amp; S&#39; \left ( I - S \tilde V (\tilde D^{-1} + \tilde V&#39; S&#39; S \tilde V)^{-1} \tilde V&#39; S&#39; \right) S \\
    =&amp;\Lambda - \Lambda \tilde V (\tilde D^{-1} + \tilde V&#39; \Lambda \tilde V)^{-1} \tilde V&#39; \Lambda
    \end{align}

\]
&lt;/p&gt;
&lt;/div&gt;


&lt;h2&gt;Work log&lt;/h2&gt;

&lt;p&gt;Implemented two different implementations of &quot;nystrom solve&quot; (&lt;code&gt;tools/nystrom_solve.m&lt;/code&gt;).  Crude testing shows big speedup, but stalling on a big eigenvalue decomposition.  Replacing with a call to &quot;chol&quot;, waiting on visual results.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&quot;chol&quot; gives much faster results. (no need to symmetrize)  But results are junk -- blank screen.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, bug in call to chol; fixed.&lt;/p&gt;

&lt;p&gt;still getting black.  Posterior covariance is not positive definite; eigenvalues are negative.&lt;/p&gt;

&lt;p&gt;When I force covariance to zero (to view the mean value), it&#39;s still in the ballpark but wonky (same as before).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Clue&lt;/strong&gt;:   When re-using (thinned) input covariance matrix and indices, result looks sensible.  Clearly, we have a problem with how K_star is computed (and likelihood K_star_star, too).&lt;/p&gt;

&lt;p&gt;Comparing different K_star&#39;s...&lt;/p&gt;

&lt;p&gt;Looks like on-diagonal elements differ, off-diagonals are okay.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Got it!  Wacv was trained using no-perturb model, but is being tested with OU-perturb.&lt;/p&gt;

&lt;p&gt;TODO: edit get_all_Wacv and/or get_wacv_result to receive a model-type... done.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;reducing test to block (1,1)&lt;/p&gt;

&lt;p&gt;possible sources of disparity: indices, function&lt;/p&gt;

&lt;p&gt;checked indices -- same.&lt;/p&gt;

&lt;p&gt;check params -- model-type doesnt match.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;First view now looks good in matlab.&lt;/p&gt;

&lt;p&gt;Running all views in likelihood_server.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Means look great!.  Now on to random perturbed.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Cholesky claims posterior covariance isn&#39;t positive definite.&lt;/p&gt;

&lt;p&gt;Possibly a bug in construct_attachment_covariance_3?&lt;/p&gt;

&lt;p&gt;Re-running with K_star_star from original covariance matrix.  Still gives &quot;not positive definite&quot; errors.&lt;/p&gt;

&lt;p&gt;Resorting to eigenvalue decomposition method.&lt;/p&gt;

&lt;p&gt;Results look good!  perturbation from mode is &lt;em&gt;TINY&lt;/em&gt;, at least in the ground truthed WACV dataset.  If this is true for hyptoheses in the full system, we can probably get away with simply taking the mean and saving significant computation of the covariacne matrix.&lt;/p&gt;

&lt;h2&gt;Tuning Nystrom factor&lt;/h2&gt;

&lt;p&gt;nystrom factor: 50&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-28-nystrom_50.png&quot; alt=&quot;nystrom factor = 50&quot; /&gt;&lt;/p&gt;

&lt;p&gt;nystrom factor: 20&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-28-nystrom_20.png&quot; alt=&quot;nystrom factor = 20&quot; /&gt;&lt;/p&gt;

&lt;p&gt;nystrom factor: 10&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-28-nystrom_10.png&quot; alt=&quot;nystrom factor = 10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nystrom factor apparently has a huge affect on reconstruction quality.  Possibly better choice of thinned index set (compared to uniform sampling) will improve sensitivity, but a brief attempt at this (grouping xyz indices ) had net negative effect.&lt;/p&gt;

&lt;p&gt;Needs more thought, possibly should resort to subset of data (SD) or subset of regressors (SR) method, or exploit markov structure.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Nystrom method is implemented and provides huge speedups without obvious in rendered results.&lt;/li&gt;
&lt;li&gt;Spent many hours debugging issues with covariance matrix.  Turned out to not be bug in theory or implementation of the math, but from an unexpected hard-coded value in wacvin wacv results.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Nystrom thinning: 10
ll2 spacing: 3
Unclear&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;what is best thinning amount for Nystrom method?&lt;/li&gt;
&lt;li&gt;what is best spacing for output index set&lt;/li&gt;
&lt;li&gt;are perturbations always virtually nil?&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;refactor construct_attachment_covariance_* into a single implementation.&lt;/li&gt;
&lt;li&gt;optimize &lt;code&gt;curve_tree_ml_2.m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Testing likelihood #2 (2-day)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/26/work-log"/>
   <updated>2013-10-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Client send/receive timeout &lt;/h2&gt;

&lt;p&gt;Tried to get send/receive to timeout if the server didn&#39;t respond.  It turns out, although this feature exists in native Unix sockets, the boost library abstractions render them useless.  To get this, I&#39;ll need to use asynchronous IO, which I&#39;m not ready to jump to, yet (also not sure if callbacks will work in mex, since the callback code might not exist after the constructor function returns).&lt;/p&gt;

&lt;p&gt;Let&#39;s just take the approach that the server must always respond quickly or disconnect.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Testing &lt;code&gt;curve_tree_ml_2&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Need a reliable way to get a testing Trackset.  Write something for wacv -- &lt;code&gt;get_wacv_trackset&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Modified run_all_wacv to save Tracks;  re-running on all datasets.  &lt;code&gt;get_wacv_results&lt;/code&gt; now returns Tracks as well as means.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tweaked semantics&lt;/strong&gt; so this now &lt;em&gt;only&lt;/em&gt; computes the pixel likelihood.  The full likelihood is the sum of this and &lt;code&gt;curve_tree_ml&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Tweaked &lt;code&gt;construct_attachment_covariance_2.m&lt;/code&gt; so the self-covariance matrices are computed in the function (if needed) instead of being passed in.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Spin-off &lt;code&gt;construct_attachment_covariance_3.m&lt;/code&gt; a fully general version that receives input and output indices, and optionally a pre-computed self-covariance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TODO: &lt;/strong&gt; make this the &quot;official&quot;/dispatch version, make other versions call this (or eliminate altogether)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;data matrix is HUGE.  8913 dimensions.  Is this right?  Also, we can move inversion outside of the loop (maybe).&lt;/p&gt;

&lt;p&gt;Consider &#39;subset of data&#39; method, or other dimensionality-reduction method&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;&lt;strong&gt;Issue&lt;/strong&gt;: Can&#39;t do cholesky decomposition, because points branching from the base are redundant.  Using SVD instead.&lt;/h2&gt;

&lt;p&gt;Okay, it appears to be successfully running end-to-end.  Haven&#39;t confirmed results yet, but one thing is clear... its REALLLLLY slow (3 minutes and counting)&lt;/p&gt;

&lt;p&gt;Can do ancenstral sampling to exploit tree structure to speed it up.  can further use markov property to break up curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, not quite end-to-end success.  Some indexing, reshaping issues.   Other bugs found&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;wasn&#39;t adding posterior mean to sampled results&lt;/li&gt;
&lt;li&gt;missing transpose in posterior covariance equation.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Debugging sampled curves&lt;/h2&gt;

&lt;p&gt;Message looks okay under inspection, but getting -inf (due to exception).  Dumping...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found a recent bug in code that builds the Gl_curve.  When I chnaged an assert to an if/throw, I forgot to negate the conditional.&lt;/p&gt;

&lt;p&gt;Getting finite values now.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Result looks in the ballpark, but some perturbations look questionable (considering the tight constraints on the WACV dataset).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-27-perturb-comparison.png&quot; alt=&quot;perturbed vs. original&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Doind a full dump-mode run...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found a bug in a recent refactor of dump-mode, causing segfaults.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;All views drift very far from their true values.  Possibly a math bug in the magnitude of the posterior covariance?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Refactored server&#39;s &quot;dump mode&quot;  to continuously dump each message as it&#39;s received, instead of running in offline mode and dumping only the passed-in model.&lt;/p&gt;

&lt;p&gt;Trying method2.  Got similar results, qualitatively; per-view curves still have some bizzarre features.  Interestingly, the perturbations between successive samples of method 2 (and between method2 and method 1) are relatively small, suggesting this is an issue with the mean, not the variance.&lt;/p&gt;

&lt;p&gt;Recall that we only tested the no-perturb model for wacv reconstruction; not the per-view reconstruction.&lt;/p&gt;

&lt;p&gt;Re-running WACV dataset 2 with OU-perturb model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Looks sensible.  So that rules out the parameter settings causing bad mu&#39;s.  We should be getting exactly WACV results;  can we get there?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;use K_star = K&lt;/li&gt;
&lt;li&gt;use zero covariance matrix (always sample at the mean)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Running...  (slow, because matrix multiplication is so much larger)&lt;/p&gt;

&lt;p&gt;I&#39;m guessing the bug is in the full-tree covariance .&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Getting &quot;degenerate curve&quot; error.  Thinning points fixed.&lt;/p&gt;

&lt;p&gt;Dumped results look good.  This suggests that mean math is likely correct, as long as K_star is okay.  So K_star math is probably wrong.&lt;/p&gt;

&lt;p&gt;Need to write a test to confirm and start debugging.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done. Results match on symmetric and non-symmetric index sets. Which means no progress made on this bug...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Re-ran, but using non-zero Sigma.  Resulting covariance was not positive definite; SVD was complex-valued.&lt;/p&gt;

&lt;p&gt;Possibly this is a result of the same bug?&lt;/p&gt;

&lt;h2&gt;Covariance matrix Rank-reduction&lt;/h2&gt;

&lt;p&gt;While previous test was running, read-up on reduced-rank approximations to K.  Nystrom method seems sensible to speed up matrix inversion.  Doesn&#39;t avoid the matrix multiplication  with K_star, or cholesky decomposition of the posterior covariance...  but using a smaller output index set seems like a reasonable approach to mitigating both of those.&lt;/p&gt;

&lt;p&gt;Curious how many non-negligible eigenvalues we have; how many data points we should use.  Recalling a plot of eignevalues from a few days ago, it looked like less than 0.1% of the dimensions are significant, but need to get a concrete number.&lt;/p&gt;

&lt;p&gt;Alternative approach is to use the problem&#39;s unique markov structure to sample each section piecewise.  Needs some thought for how to do in the case of the posterior; probably need to estimate a markov blanket for each, along with sampling each branch point and conditioning on it.  Upside: better chance at asymtotic running time improvement. Down-side: not general, not as re-usable within the project.&lt;/p&gt;

&lt;h2&gt;To Be Continued&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Bug:&lt;/strong&gt; Samples generated from &lt;code&gt;curve_tree_ml_2_debug.m&lt;/code&gt; are wonky.
&lt;strong&gt;Theory&lt;/strong&gt;: Bug is related to posterior covariance.  Posterior mean looks sensible.
&lt;strong&gt;File&lt;/strong&gt;: &lt;code&gt;curve_tree_ml_2_debug.m&lt;/code&gt; &amp;lt;-- alternative version, where K is used for K_star and K_star_star.  Currently giving non-positive-definite results for posterior covariance using method 1.  Currently Stumped&lt;/p&gt;

&lt;p&gt;Fix sampling sigma bug.&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;get likelihood from multiple samples -- what is the variance of the MC estimator?&lt;/li&gt;
&lt;li&gt;re-run wacv with multi-view data, but no-perturb output&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>params, CVPR 2014</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/26/params-cvpr-2014"/>
   <updated>2013-10-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/26/params-cvpr-2014</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;/ksimek/research/events/CVPR2014/params.html&quot;&gt;See CVPR 2014 parameters page&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Refactor: one-model-per-view likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/24/work-log"/>
   <updated>2013-10-24T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/24/work-log</id>
   <content type="html">&lt;p&gt;Implementing one-object-per-view in likelihood_server&lt;/p&gt;

&lt;p&gt;Overloaded bd_mv_likelihood&#39; evaluate and dump  functions to receive a sequence of renderables.&lt;/p&gt;

&lt;p&gt;Write a &quot;wrap_all_As_silhouette&quot; to, well, wrap all renderables in silhouette renderers.&lt;/p&gt;

&lt;p&gt;re-wrote message-to-curve function to decode the message to a vector-vector-vector, and convert that to a vector-of-gl_curves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Compiling work from last 24 hours.&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li&gt;Generate a multi-model message from matlab (dummy)&lt;/li&gt;
&lt;li&gt;test message in likelihood_server_2.cpp

&lt;ol&gt;
&lt;li&gt;generate random samples from wacv data.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;Doing dummy message test.&lt;/p&gt;

&lt;p&gt;At first, accidentally sent to old implementation... and it didn&#39;t barf!  This is unsettling, because the message format has changed significantly.  Did I ever recompile the mex files?&lt;/p&gt;

&lt;p&gt;Nope.  got some mex compile errors to deal with.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Friday&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;okay, sending a random model using the new one-per-view message system isn&#39;t crashing.&lt;/p&gt;

&lt;p&gt;Questions&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the likelihood looks high, considering it&#39;s a ranomd model .  is a null model better?  is the ground-truth model better?&lt;/li&gt;
&lt;li&gt;visualize the received message;  is each view different?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Trying the null model from 1., the mex file crashed matlab.  Empty curvesset isn&#39;t handled.&lt;/p&gt;

&lt;p&gt;trying all-zeros.  Server crashed -- coincident points aren&#39;t handled gracefully.  Fixed (now it returns -inf for log likelihood).&lt;/p&gt;

&lt;p&gt;issues
server: error creating gl_curves causes client disconnect. fix exception handling
client: doesn&#39;t handle server crashes gracefully.  adding timeout
client: doesn&#39;t handle empty curveset&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;random sample from multi-view posterior (instead of single max posterior)&lt;/li&gt;
&lt;li&gt;get likelihood from multiple samples -- what is the variance of the MC estimator?&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Implementing Two-term likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log"/>
   <updated>2013-10-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/23/work-log</id>
   <content type="html">&lt;p&gt;Working on Matlab integration.&lt;/p&gt;

&lt;p&gt;Quick test shows we can get about 20 evaluations per second.  Not bad, considering each evaluation consists of 9 image likleihoods.  There&#39;s some room for improvement here, but probably not worth pursuing at the moment:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;leapfrog rendering: render (a) while evaluating on (b).&lt;/li&gt;
&lt;li&gt;cleaning up geometry and fragment shaders (fewer branches, less storage)&lt;/li&gt;
&lt;li&gt;try other GPU blurring routines.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;New Likelihood&lt;/h1&gt;

&lt;p&gt;Scenario: we have a decent likelihood that is linear-gaussian (and a gaussian prior), so we can compute the marginal likelihood in closed-form.  However, we&#39;d like to incorporate additional sources of evidence whose likelihoods aren&#39;t gaussian.  We&#39;ll see how we can estimate the joint marginal likelihood with simple Monte-Carlo sampling (no MCMC needed, no gradient).&lt;/p&gt;

&lt;h2&gt;Derivation&lt;/h2&gt;

&lt;p&gt;The old marginal likelihood looked like this:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1) &amp;= \int p(x) p(D_1 | x) dx
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;After introducing the extra likelihood term, the joint probability is no longer linear-gaussian, so the exact marginal likelihood involves an intractible integral.  However, by  re-arranging, we see we can get a good monte-carlo approximation:&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;= \int p(x) p(D_1 | x) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= \int p(x | D_1) p(D_1) p(D_2 | x) dx &amp; \left(\text{Bayes thm (see below)}\right) \\
p(D_1, D_2) &amp;= p(D_1) \int p(x | D_1) p(D_2 | x) dx \\
p(D_1, D_2) &amp;= p(D_1) \frac{1}{N} \sum p\left(D_2 | x^{(*)}\right) &amp; \text{(Monte Carlo)}
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;In the second line, I&#39;ve replaced the first two terms using Bayes theorem.  In the last line, the x-stars are samples from \(p(x | D_1)\), which we have in closed-form due to linear-Gaussian prior and likelihood for \(D_1\).&lt;/p&gt;

&lt;p&gt;Thus, we see if at least one source of data yields a linear-gaussian likelihood, we can incorporate additional data with arbitrary likelihoods  in a principled way.  In many cases, \(p(x | D_1) \) has low variance, so a small number of Monte-Carlo samples are sufficient for a good estimate -- even a single sample could suffice.  Even if the estimates are bad, they are unbiased, so any MCMC involving the marginal likelihood will converge to the target distribution.&lt;/p&gt;

&lt;h2&gt;Importance Sampling version&lt;/h2&gt;

&lt;p&gt;We can alternatively derive this in terms of importance sampling, setting the proposal probability q(x) to \(p(x | D_1) \):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
p(D_1, D_2) &amp;\approx 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{1}{q(x)} \\
            &amp;= 1/N \sum p(x) p(D_1 | x)p(D_2 | x) \frac{p(D_1)}{p(x) p(D_1 | x)} \\
            &amp;= 1/N \sum p(D_2 | x) p(D_1) \\
            &amp;= p(D_1) 1/N \sum p(D_2 | x) 
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File:&lt;/strong&gt; curve_tree_ml_2.m&lt;/p&gt;

&lt;p&gt;Basic idea&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;construct a thinned output index set (optional, but smart)&lt;/li&gt;
&lt;li&gt;construct a posterior distribution over the thinned set&lt;/li&gt;
&lt;li&gt;Add perturbation variance to the posterior&lt;/li&gt;
&lt;li&gt;take average over n trials: sample curveset and evaluate pixel likelihood&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Step 2 required updating to construct_attachment_covariance, which only constructs symmetric covariance matrices.  We need the covariance between indices with observations and the desired output indices.  Fully refactored that function into &lt;code&gt;construct_attacment_covariance_2.m&lt;/code&gt;; confirmed correctenss in the case of the self-covariance by using an existing test for version 1 of that function.  If non-symmetric, the upper-triangular blocks are processed in a second pass, swapping indices so we can re-use existing code.&lt;/p&gt;

&lt;p&gt;Need to try view-specific sampling, i.e. sample 9 different curves from 9 different views.  This refactor affects likelihood server, client, and message format.  I&#39;m worried about the performance hit, but probably not worth worrying about (or futile).  Coarse sampling of indices could mitigate.    In any case, view-specific sampling is probably necessary, because we&#39;re using such low blurring levels in the Bd_likelihood, so the reconstruction needs to fall near the data.  We&#39;ve seen how plant motion and camera miscalibration cause &quot;good&quot; 3D curves to reproject up to 10 pixels away from the data in some views.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Side-note&lt;/strong&gt; - the tcp connection is working very reliably so far!  Even after the machine sleeping/resuming several times, and suspending the server job for serveral hours, the socket is still valid and communicating flawlessly!&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Implementing per-view sampling&lt;/h2&gt;

&lt;p&gt;need to refactor send_curves.m to send &lt;code&gt;num_views&lt;/code&gt; curves instead of one.&lt;/p&gt;

&lt;p&gt;Now receive as num_curves x num_views cell array.&lt;/p&gt;

&lt;p&gt;Coded vector&lt;sup&gt;3&lt;/sup&gt; to/from message.&lt;/p&gt;

&lt;p&gt;todo: rewrite likelihood server to receive vector3, somehow pass per-view models to likelihood&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;multi-view likelihood is out - it&#39;s one-model, multi-view.  We need multi-model, multi-view.

no, MV likelihood is okay, just add an extra operator() that receives a sequence of renderables 
whose size is equal to the number of views
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;p&gt;Some protocol for starting and loading the likelihood server from matlab code
Stress test likelihood server&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>GPU debugging</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log"/>
   <updated>2013-10-22T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/22/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still searching for inf bug in libcudcvt blurred_difference_likelihood.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found it:  my GPU-based log_sum routine had a bug.  instead of subtracting the maximum value before exp-summing, it subtracted an arbitrary value.  The find-max loop looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Real pi = v[0];

#pragma unroll
for(unsigned int i = 1; i &amp;lt; N; ++i)
{
    pi = pi &amp;lt; v[0] ? v[0] : pi;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously, the zeros should be i&#39;s.&lt;/p&gt;

&lt;p&gt;A great bug to have found and fixed, but really frustrating that I let it slip through in the first place.  So stupid!  But it didn&#39;t affect correctness in the common case, so my tests didn&#39;t catch it.  It makes a good case for randomized testing (as if a good argument was lacking...).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(semi-obvious) note to self: conditional gaussian mixture is not equal to mixture of conditional gaussians!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;okay, a few more cleanup tasks, then on to real goals:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cleanup and commit version 0.1 of the likelihood server -- done&lt;/li&gt;
&lt;li&gt;split training into it&#39;s own directory. -- done&lt;/li&gt;
&lt;li&gt;Get likelihood server connecting reliably with matlab on different computers.&lt;/li&gt;
&lt;li&gt;implement likelihood importance sampling into maltlab&#39;s curve_ml procedure&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Thought some on birth moves.  It seems like there are some possibilities for births larget than two by using the adjacency graph created by pair candidates.&lt;/p&gt;

&lt;p&gt;for now, start with greedy&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Linux NVidia/Cuda/X11 erorrs; Cuda server; matlab integration</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log"/>
   <updated>2013-10-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/21/work-log</id>
   <content type="html">&lt;p&gt;Getting my (cuda-equipped) desktop system running with likelihod_server.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Trouble building makefiles. Csh issue with new makefiles.  Kobus fixed&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still can&#39;t build makefiles. Probilems in the ./lib subdirectory is apparently blocking kjb_add_makefiles from finishing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Some makefiles were missing in ./lib.  Adding them was problematic, because (like before) kjb_add_makefiles wasn&#39;t finishing, because dependency directories were missing makefiles.  Also had some issues in kjb /lib, out of date from svn and some compile bugs I introduced in my last commit.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Compiling now, but shader is giving errors at runtime (opengl can&#39;t compile it, but no error message is given.&lt;/p&gt;

&lt;p&gt;Rebooting...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;No desktop.  Missing disk issue?  Tweaked fstab, still no X11.  Must be a driver issue.  Couldn&#39;t find driver from NVidia&#39;s I downloaded from web site months ago.  Using driver &quot;Ubuntu-X&quot; PPA.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still no X.  /var/log/Xorg.0.log says driver and client have different versions.  Got a tip online:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo dpkg --get-selections | grep nvidia
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lots of extra crap there (old versions, experimental drivers, etc).  &#39;sudo apt-get purged&#39; all the non-essentials.  Now we&#39;re getting to login screen, but login is failing (simply returns to login screen).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found solution:&lt;/p&gt;

&lt;p&gt;   sudo  rm ~/.Xauthority&lt;/p&gt;

&lt;p&gt;Booting successfully.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running likelihood_server...  shader is now compiling successfully!   Previous issues must have been driver issues as a result of the recent &#39;sudo apt-get upgrade&#39; without restart.&lt;/p&gt;

&lt;p&gt;Getting segfault when dumping pixels, though.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Caused by trying to read from cuda memory as if it were main memory.  Should be dumping if we&#39;re using GPU (or use a different dump function).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Fixed;  if using gpu, copy from cuda to temp buffer, then call normal routine.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting nan&#39;s from likelihood.  Need to dump from GPU likleihood, which means digging into libcudcvt.  Got some build errors resulting from a boost bug from 8 months ago (which arose because I updated GCC).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;cudcvt updated to grab blurred frames for (some method names changed to better match the analogous CPU likelihood in KJB.).&lt;/p&gt;

&lt;p&gt;Moved &quot;dump&quot; routines from Bd_mv_likelihood_cpu to the base class Bd_mv_likelihood as pure virtual function, and implemented a version in Bd_mv_likelihood_gpu to call cudcvt&#39;s new frame-grabbing code.  So &quot;dump&quot; mode now works on both cpu and gpu version.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Dumped data from CUDA likelihood; looks fine.  So I&#39;m still clueless why we&#39;re getting nan values.&lt;/p&gt;

&lt;p&gt;Sanity check -- check that tests in libcudcvt are still passing&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;test is failing on an assert.  Looks like an overzealous assert with a low percent-error threshold.&lt;/p&gt;

&lt;p&gt;Lowered threshold, test finished; GMM model doesn&#39;t match between gpu and cpu versions...&lt;/p&gt;

&lt;p&gt;what&#39;s changed?  Rolling back to first releast to see if tests pass.  Is it possible I never validated this?  Or maybe different GGM&#39;s being used between cpu and gpu?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Old version crashed and burned hard.  GPU is returning &#39;inf&#39;.  No help here...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, svn version has lots of uncommitted changes, including bugfixes.  No wonder it was no help.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;found issue: introduced a regression when troubleshooting last bug.  Grrr.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Still getting &#39;inf&#39;.  Checked:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GPU buffers contain valid data for data and model&lt;/li&gt;
&lt;li&gt;CPU version gives finite results.&lt;/li&gt;
&lt;li&gt;GPU can give finite results (e.g. in test application)&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;

&lt;p&gt;Modified libcudcvt test to receive arbitrary data and model images.  It&#39;s giving finite results, so it must be something about how I&#39;m calling it in the likelihood server program.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;issue:&lt;/strong&gt; Bd likelihood in GMM mode gives lower values for ground truth model than random model.&lt;/p&gt;

&lt;p&gt;Bug. fixed.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Digging deeper into &#39;inf&#39; issue.&lt;/p&gt;

&lt;p&gt;Dug all the way into the thrust::inner_product call.  Probed both buffers -- look okay.  mixture components look okay&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Got it!&lt;/strong&gt;  Was able to reproduce the &#39;inf&#39;s in the likelihood test program in libcudcpp.  It was so hard to reproduce because it only occurred when&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;old GMM values were used&lt;/li&gt;
&lt;li&gt;&lt;em&gt;unblurred&lt;/em&gt; model &lt;em&gt;and&lt;/em&gt; data were used&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Regarding 2, previosuly I just used the unblurred model, since it was at hand from a cuda dump, but used the blurred data from a training dump.&lt;/p&gt;

&lt;p&gt;Blurring doesn&#39;t seem to matter; both 2.0 and 5.0 give inf.&lt;/p&gt;

&lt;p&gt;Interestingly, the perfect model doesn&#39;t oveflow, but the random (almost null) model does.&lt;/p&gt;

&lt;p&gt;Also, for all the &#39;inf&#39; cases, the cpu results aren&#39;t terribly high, so float overflow seems unlikely.&lt;/p&gt;

&lt;p&gt;Most likely it&#39;s the conditioning, where the joint pdf is divided by the marginal.  Maybe we&#39;re getting some bizzare model values that are off the charts?  But using the model image with the blurred data image is no problem, so that suggests the model values aren&#39;t a problem.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Interesting:&lt;/strong&gt; Tried removing border edges and the problem disappeared.  Is it possible that the blurring routine is not robust to near-edge pixels?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;idea: dump image after scale-offset, but before reduce&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;p&gt;Probability maps look sensible when a 2-pixel margin is added (--data-margin=2).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_2.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 2.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf.tiff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(above: 5.0 blurring, 2 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice what happens when we disable the data margin (rendered dimmer and with blue padding to emphasize effect):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-22-gmm_pdf_no_margin.tiff.jpg&quot; alt=&quot;&quot; /&gt;
(2.0 blurring, 0 pixel margin)&lt;/p&gt;

&lt;p&gt;Notice the white pixels around the border, which presumably correspond to inf values.&lt;/p&gt;

&lt;p&gt;Inspecting the blurred data image, it looks like these pixels are, indeed, the brightest in the image.  It&#39;s possible we were lucky enough to have found the maximum range of this gmm.&lt;/p&gt;

&lt;p&gt;It looks like evaluating the bivariate normal is underflowing.  It could have been brought back down to size during conditioning, but we never made it that far.  could refactor by computing conditional covariance matrix beforehand, instaed of taking a ratio of computed values&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Pre-test all incoming data images: evaluate against an empty image and a full image and check for NaN and inf.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>KJB EM GMM</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log"/>
   <updated>2013-10-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/20/work-log</id>
   <content type="html">&lt;p&gt;Still struggling with the EM GMM algorithm.  Found/fixed one bug where responsibilities aren&#39;t initialized if NULL is passed to the function.&lt;/p&gt;

&lt;p&gt;Ran overnight and it hadn&#39;t finished at the end.  Found/fixed a bug where the entire @M element responsibility matrix is rescaled 2M times (once per point).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting rank-deficient errors.  Looks like this is a result of the huge dataset; covariance matrix is divided by 1e6 (the number of soft-members of the cluster).&lt;/p&gt;

&lt;p&gt;For now, hack by trying to thin the dataset.  Long-term, adding a minimum offset to the emperical covariance seems to be the common solution.&lt;/p&gt;

&lt;p&gt;Possibly normalizing the data would be good.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Added offset to covariance (command-line options and file-static variable already existed, just needed to add it to the full_GMM logic).&lt;/p&gt;

&lt;p&gt;Added &lt;code&gt;read_gmm.m&lt;/code&gt; matlab routine, and plotted along with scatter plot of data.  Results look pretty good:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-20-kjb-gmm-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Moving trained gmm to ~/data/arabidopsis/training/bd_likelihood_gmm/blur_2.0.gmm&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;re-train GMM on held-out dataset.&lt;/li&gt;
&lt;li&gt;implement matlab likelihood sampling&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;how to sync datasets on matlab and c++ server?&lt;/li&gt;
&lt;li&gt;Wacv multi-view reconstruction for training&lt;/li&gt;
&lt;li&gt;saving wacv to files for training&lt;/li&gt;
&lt;li&gt;fixing issues in wacv reconstructions&lt;/li&gt;
&lt;li&gt;evaluation methodology&lt;/li&gt;
&lt;li&gt;split/merge&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Likelihood Training</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/19/work-log"/>
   <updated>2013-10-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/19/work-log</id>
   <content type="html">&lt;h2&gt;Training likelihood&lt;/h2&gt;

&lt;p&gt;Fitting a three-component GMM resulted in a null model having a higher likelihood than the ground truth model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Spent a lot of time troubleshooting this and finally realized I had scaled all values up by 100x so Matlab wouldn&#39;t choke, but I wasn&#39;t accounting for this when evaluating.  Blame it on this blasted cold, killing my focus!&lt;/p&gt;

&lt;p&gt;Ground truth model looks much better than null model now.&lt;/p&gt;

&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;Trying different blurring levels.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1.0 is to small.&lt;/li&gt;
&lt;li&gt;2.0 seems okay&lt;/li&gt;
&lt;li&gt;5.0 gives a wide variance for true-positives.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In the end, running max likleihood on this will be best, but this gives us a ballpark.&lt;/p&gt;

&lt;p&gt;--&lt;/p&gt;

&lt;p&gt;Trying to get KJB EM algorithm working on my 2M element dataset.  Getting NaN errors at the moment.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - Uninformative likelihood?</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/17/work-log"/>
   <updated>2013-10-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15701&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Likelihood sanity check (ctd)&lt;/h2&gt;

&lt;p&gt;An empty model should have a significantly worse likelihood than the ground truth model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ground truth model:  -3.38064e+08
empty model: -3.28279e+08
full model: -1.20572e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both empty and full are better than ground truth.  Why??&lt;/p&gt;

&lt;p&gt;At least possiblities here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Noise level is so high that fitting is impossible (remember the paper &quot;Fundamental limits of Bayesian Inference: Order Parameters and Phase Transitions for Road Tracking&quot;)&lt;/li&gt;
&lt;li&gt;Bug.  maybe renderings are just waaaay off?&lt;/li&gt;
&lt;li&gt;Bad calibration.  Looking at the GMM plot from yesterday (see below) it looks like the one-sigma contour is very very large.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-bd.gmm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Case 1 is what&#39;s left over once 2 and 3 are ruled out.&lt;/p&gt;

&lt;h2&gt;Rendering sanity check.&lt;/h2&gt;

&lt;p&gt;This seems to be a culprit.&lt;/p&gt;

&lt;p&gt;Here&#39;s first data view:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-data-view-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Not a good match to the actual first data image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-ler_2_36_0.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But a good match to the second data image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-ler_2_36_1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Must be an off-by-one error when reading data.&lt;/p&gt;

&lt;p&gt;The cameras are not off-by one.  Here&#39;s the rendering of the model under the first camera:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-model-view-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Good match to the first data image above.&lt;/p&gt;

&lt;p&gt;Bug is in the config file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;view-indices=1:4:36  # bug here
view-indices=0:4:35  # should be this
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Good news, bad news.&lt;/p&gt;

&lt;p&gt;Good news: after fixing config file, data and rendering look good (use slider to swap between images)&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-10-17-data.jpg&quot;,
        &quot;/ksimek/research/img/2013-10-17-model.jpg&quot;
        ]

    construct_animation($(&quot;#data-model-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;data-model-anim&quot; style=&quot;width:530px&quot;&gt; &lt;/div&gt;


&lt;p&gt;Bad news: likelihood gets &lt;strong&gt;worse&lt;/strong&gt;!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fixed likelhood: -3.38298e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like the rendered model is lighter than the data model.  This might account for the issues we&#39;re seeing.  We&#39;re in the region to the right of the diagonal &quot;true positive&quot; component in the GMM (see contour plot above).  In this region, the noise model has a stronger pull than the true positive model, which partially explains why we get better when fewer model pixels are on the screen.   It doesn&#39;t explain why the model gets better when we shift left and right -- maybe it fits better in the tails of the blurred data, but we&#39;d expect it to eventually get worse.  Lets test it.&lt;/p&gt;

&lt;p&gt;Recall that this GMM was trained on a manipulated version of the data image, not a rendered 3D model image.  The process taken was (a) removing all background pixels from the data image and (b) perturbing the foreground pixels. We may have removed a few foreground pixels to add false positives, but the amount was small, if memory serves.  Now we have far fewer model pixels, so we expect the &quot;true positive&quot; region to be slanted more upward.&lt;/p&gt;

&lt;h2&gt;Optimality test (redux)&lt;/h2&gt;

&lt;p&gt;Code:
    x_values=&quot;-80 -40 -20 -10 -5 -2 -1 0 1 2 5 10 20 40 80&quot;
    for x in $x_values; do
        echo -n &quot;$x &quot;;
        ./likelihood_server \
            --config=test.conf  \
            --image-bounds &quot;0 530 397 0 200 2000&quot;  \
            --cam-convention &quot;1 0 0&quot;  \
            --dbg_save_frames  \
            --dbg_load_message  \
            --dbg-ppo-x-offset=$x 2&gt; /dev/null | grep ^3 | sed -e &#39;s/3 //&#39; -e &#39;s/,//&#39;;
    done&lt;/p&gt;

&lt;p&gt;Output (delta-x vs log-likelihood):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-80 -3.36866e+08
-40 -3.36987e+08
-20 -3.37434e+08
-10 -3.37864e+08
-5 -3.38169e+08
-2 -3.38279e+08
-1 -3.38294e+08
0 -3.38298e+08
1 -3.38292e+08
2 -3.38275e+08
5 -3.38172e+08
10 -3.37912e+08
20 -3.37479e+08
40 -3.36958e+08
80 -3.36991e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-likelihood_plot_vs_x.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;&quot;Full&quot; model issue&lt;/h2&gt;

&lt;p&gt;One of the strangest things is the fact that a rendering full of &quot;1.0&#39;s&quot; performs far better than the ground truth or empty model.  This area lies along the x-axis in the GMM contour plot, where there is no support.&lt;/p&gt;

&lt;p&gt;Lets debug this.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found an insane bug.  Below is the loop that compares all the pixels of the data and rendered model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    for(size_t i = 0; i &amp;lt;= size_; ++i)
    {
        kjb::Vector2 x;
        x[0] = model[i];
        x[1] = data_[i] + 1;

        ...  // compute p(x[1] | x[0])
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have no idea why we&#39;re adding 1.0 to &lt;code&gt;data_[i]&lt;/code&gt;, but this explains everything.  In the &quot;full&quot; model, it puts us right on the diagonal of the GMM.  In the empty model, we&#39;re right on the x-axis.  Both are well-supported regions.  In the ground truth model, we&#39;re lying between these extremes, which is a no-man&#39;s land of near-zero support.&lt;/p&gt;

&lt;p&gt;Spending 5 minutes to determine how this got added...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;It was added between November 11 and December 13 last year.   No obvious reason.  Oh well.&lt;/p&gt;

&lt;p&gt;New sanity check numbers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;full model -5.2217e+08
ground truth model 4.03273e+06
empty model: 4.01252e+06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much more sensible.  Missing data looks terrible, noise looks okay but not great,  ground truth looks best.  Lets do an offset plot just to be safe.&lt;/p&gt;

&lt;h2&gt;Optimality test (redux&lt;sup&gt;2&lt;/sup&gt;)&lt;/h2&gt;

&lt;p&gt;Results:
    -80 3.1039e+06
    -40 3.27557e+06
    -20 3.58156e+06
    -10 3.77333e+06
    -5 3.93159e+06
    -2 4.01459e+06
    -1 4.02845e+06
    0 4.03273e+06
    1 4.02704e+06
    2 4.01209e+06
    5 3.93021e+06
    10 3.78782e+06
    20 3.60557e+06
    40 3.26585e+06
    80 3.26894e+06&lt;/p&gt;

&lt;p&gt;Plot&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-likelihood_plot_vs_x_fixed.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Oh Yeah!!&lt;/p&gt;

&lt;h2&gt;BD Likelihood Data Dump Mode.&lt;/h2&gt;

&lt;p&gt;add a mode by which pixel likelihood will save pixel data to files for analysis or debugging&lt;/p&gt;

&lt;p&gt;changes will appear in:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;lib/evaluate/bd_likelihood.h - Bd_pixel_likelihood_cpu&lt;/li&gt;
&lt;li&gt;lib/evaluate/bd_likelihood.h - Bd_mv_likeliood_cpu&lt;/li&gt;
&lt;li&gt;./likelihood_server.cpp - Options&lt;/li&gt;
&lt;li&gt;./likelihood_server.cpp - main()&lt;/li&gt;
&lt;/ol&gt;


&lt;hr /&gt;

&lt;p&gt;Added dump mode.  Probably should move this into a separate directory, or at least a different program, since it has almost nothing to do with the likleihood server.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;rename matlab &quot;render_client&quot; code to &quot;likelihood_client&quot;&lt;/li&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;remove edges around image border.&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;train likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open issues&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;how to sync datasets on matlab and c++ server?&lt;/li&gt;
&lt;li&gt;Wacv multi-view reconstruction for training&lt;/li&gt;
&lt;li&gt;saving wacv to files for training&lt;/li&gt;
&lt;li&gt;fixing issues in wacv reconstructions&lt;/li&gt;
&lt;li&gt;evaluation methodology&lt;/li&gt;
&lt;li&gt;split/merge&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - silhouettes, training likelihood, evaluating likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/16/work-log"/>
   <updated>2013-10-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Enabled glPolygonOffset and it improved results somewhat.  Still getting stippling 10% of the time.&lt;/p&gt;

&lt;p&gt;Adding edge-detection code to show stippled edges helps, but adds some internal edges for some reason.&lt;/p&gt;

&lt;p&gt;Apparently, we have point spacing too close together.  This introduces lots of little ridges, which makes silhouette edges appear on the interior of the object incorrectly.  We should pro-process curves in matlab to force spacing to be greater than or equal to to the curve radius.&lt;/p&gt;

&lt;h2&gt;Likelihood rendering bugs&lt;/h2&gt;

&lt;p&gt;Image dumps from our likelihood show all edges are being rendered (not just silhouette edges).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;seems to be an issue with Camera_render_wrapper.  Possibly the y-axis flipping?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Yep.  Reverses the handedness of the projected model.&lt;/p&gt;

&lt;p&gt;Shader assumes orientation of forward-facing triangles isn&#39;t affected by projection.  Thus, cross-product can be used to determine whether a face is forward-facing (a fundamental operation for classifying silhouette edges).&lt;/p&gt;

&lt;p&gt;Can we solve by doing visibility test in world coordinates, which avoids the projection matrix altogether?&lt;/p&gt;

&lt;p&gt;Yes, but there&#39;s a bug causing a few faulty silhouette edges to appear.  Why?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It turns out, we were assuming the normal vector of opposite faces are parallel, and used this to .  Apparently not always the case, especially when sharp corners occur (which happens routinely when point spacing is close).&lt;/p&gt;

&lt;p&gt;During testing, we somehow corrupted the GPU&#39;s memory.  System sprites (e.g. cursor) are starting to be corrupted, and program keeps crashing  from GPU errors.  Probably constructed too many shaders in a single session.  Will reboot and continue.&lt;/p&gt;

&lt;p&gt;Summary: found silhouette rendering bug caused by projection matrix that didn&#39;t preserve handedness.  Rewrote silhouette pass 2 geometry shader to handle this case better.  Now renders correctly in both cases.&lt;/p&gt;

&lt;h2&gt;Next: determine cause of likleihood NaN&#39;s; train the likelihood.&lt;/h2&gt;

&lt;h2&gt;Blurred-difference (bd) likelihood issues&lt;/h2&gt;

&lt;p&gt;Getting &quot;NaN&quot; when evaluating.  Checking the result of blurring the model shows inf&#39;s everywhere.  Weird, because I can confirm that the input data is succesfully blurring:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-blurred_data.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Same blurrer is used for data and model&lt;/li&gt;
&lt;li&gt;It isn&#39;t the input (tried substituting data in for model and still got corrupt stuff back).&lt;/li&gt;
&lt;li&gt;re-initializing blurrer doesn&#39;t seem to help&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Stepping through in GDB...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After convolution, values are on the order of 1e+158...  clearly invalid.  Converting to float causes inf.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Aside:  the padding added before convolution isn&#39;t zeros!  Fixing bug in lib/evaluate/convolve.h:fftw_convolution class.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Aside:  Need to remove edges around image border:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-border_edges.jpg&quot; alt=&quot;border edges bug&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Aside: different blurrer for each view?  unnecessary!&lt;/p&gt;

&lt;p&gt;Wrong, same blurrer, different internal buffers.  Caused by calling blurrer.init_fttw_method() every time we add a view to the multi-view likelihood.  Removed that call.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Apparently, calling blurrer_.init_fftw_method() is wreaking all kinds of havok on the blurring results.  I have idea why.  Possibly calling init_fftw_method() more than once is simply not supported.  But that call simply destroys and object and creates a new one, so why is that different from the first time we called it?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Theory: fftw wisdom is messing with us?&lt;/p&gt;

&lt;p&gt;Results: nope&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We have a working case and a non-working case.  Can we walk from one case to the other and find the exact change that causes the error?&lt;/p&gt;

&lt;p&gt;Aside: Here&#39;s a bizarre result wer&#39;e getting during failure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-weird_results.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kind of cool!&lt;/p&gt;

&lt;p&gt;Actually, there might be a hint here... Notice the eliptical shapes that are about the same size as the turntable, but shifted and screwed up.&lt;/p&gt;

&lt;p&gt;It&#39;s possible what we&#39;re seeing is convolution with a random mask.  the two main ellipses we&#39;re seeing are two random bright pixels in the mask, and the miscellaneous roughness might be ca combinariton of small positive and negative values causing the texture we see.  Next step: inspect mask&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bingo!  We&#39;re working with a random mask.&lt;/p&gt;

&lt;p&gt;Let&#39;s step back to the original failing case and see if the mask is still random.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Confirmed.  Here&#39;s an eample mask:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-tmp.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here it is dimmer:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-bunk_maks_dimmer.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Anything look familiar?  It&#39;s derived from our plant image (notice the ellipses from the turntable).&lt;/p&gt;

&lt;p&gt;Theory: somehow the mask is getting overwritten with our blurred model.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Solved.  Simple bug but was obscured by several issues.&lt;/p&gt;

&lt;p&gt;The fundamental issue was that when the blurring mask was being padded, the padding wasn&#39;t being filled with zeros.  Thus, the padded mask consisted of a small section of real mask, surrounded by huge amounts of uninitialized memory.&lt;/p&gt;

&lt;p&gt;This was a bug that I discovered and fixed 12 months ago, but it got rolled back when Andrew revamped the FFTW convolution class.  Ultimately, I was able to restore my changes to the FFTW class without much trouble, but for the six hours that followed, these changes weren&#39;t being compiled, because they were in a header file, and the build system only rebuilds object files if the &quot;cpp&quot; file is changed.&lt;/p&gt;

&lt;p&gt;Further, there was a red-herring that appeared as &quot;re-initializing the blurrer causes the errors&quot;.  While this was true, it was only because re-initializing also re-initialized the blurring mask, and it was only during re-initialization did the uninitialized mask padding have junk -- on the first initialization, the memory just happened to be blank.  Go figure.&lt;/p&gt;

&lt;p&gt;This was an absolutely essential bug to find and fix.  Glad its fixed, just wish it hadn&#39;t regressed in the first place.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Okay time to reset.  Let&#39;s confirm that the rendered models are blurring correctly.  Then confirm non-zero log-likleihood result.    Then on to training.&lt;/p&gt;

&lt;p&gt;Model blurring: CHECK
Finite likelihood: CHECK  (result: -3.38064e+08)
Likelihood is near-optimal: &lt;strong&gt;FAIL&lt;/strong&gt;. see below&lt;/p&gt;

&lt;h2&gt;Optimality test&lt;/h2&gt;

&lt;p&gt;Added debugging option to shift rendering left/right/up/down in the image: &lt;code&gt;--dbg-ppo-x-offset&lt;/code&gt; and &lt;code&gt;--dbg-ppo-y-offset&lt;/code&gt;.   Ideally, the likleihood should be optimized at zero offset&lt;/p&gt;

&lt;p&gt;Log likelihood vs x-offset
     0  -3.38064e+08
    +1  -3.38069e+08
    +2  -3.38068e+08
    +5  -3.38033e+08
    +10 -3.37881e+08
    -2  -3.38039e+08
    -5  -3.37968e+08
    -10 -3.37786e+08&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-likleihood_plot_vs_x.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weird... Why is it at a local &lt;em&gt;minimum&lt;/em&gt; at the center?&lt;/p&gt;

&lt;p&gt;Log likelihood vs y-offset&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 0  -3.38064e+08
-2  -3.38062e+08
-5  -3.38033e+08
-10 -3.3789e+08
-20 -3.37321e+08
-40 -3.36139e+08
-80 -3.33693e+08
 2  -3.38052e+08
 5  -3.38025e+08
 10 -3.37983e+08
 20 -3.37955e+08
 40 -3.3785e+08
 80 -3.37523e+08
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-likelihood_plot_vs_y.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that as y decreases, the top of the plant starts to fall off the screen. This suggests that having fewer model points might be over-preferred.  Need to sanity-check the GMM model we&#39;re using -- I trained it months ago and never really leaned on it hard, so it&#39;s the first place to look for problems.&lt;/p&gt;

&lt;p&gt;Also, might be a scaling issue.  Double-check that blurred model and blurred data have similar scale.&lt;/p&gt;

&lt;h2&gt;GMM sanity check&lt;/h2&gt;

&lt;p&gt;Plotting first and second standard deviation of the joint distribution of model-pixel-intensity and data-pixel-intensity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-17-bd.gmm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This looks sensible.  The first component is the diagonal one, representing true postives (in the presence of random perturbations).&lt;/p&gt;

&lt;p&gt;The second component is at the origin -- true negatives.&lt;/p&gt;

&lt;p&gt;The third component is along the y-axis, representing false positives in the data, aka noise.&lt;/p&gt;

&lt;p&gt;The first component has roughly 10x less weight than the second and third.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;No component for the false negatives (aka missing data).  As I recall, running EM on a GMM with four or more components always resulted in redundant components.  Possibly hand-initializing the components in each of the four positions might help.&lt;/p&gt;

&lt;p&gt;I also recall some sort of GPU limitation that prevented me from evaluating more than three components in hardware.  That seems unusual though, and I may be misremembering.&lt;/p&gt;

&lt;p&gt;Does this provide any insight as to why we prefer our model to drift off of the screen?  This introduces more &quot;negative&quot; model pixels, pushing toward the well-supported y-axis are of our model.  Possibly this is the result of poor calibration?  Or wrong blurring sigma?&lt;/p&gt;

&lt;p&gt;TODO: re-run the x/y offset experiment with smaller blurring sigma.&lt;/p&gt;

&lt;h2&gt;Data Sanity check&lt;/h2&gt;

&lt;p&gt;To do.  Approach: instead of evaluating likelihood pixels against data, dump model/data pixel pairs into a list and (a) plot them or (b) train on them.  Maybe just dump to a file and do it in matlab?&lt;/p&gt;

&lt;h2&gt;Blooper reel&lt;/h2&gt;

&lt;p&gt;While writing miscellaneous blocks of memory to disk as images, got some interesting but totally wrong results.&lt;/p&gt;

&lt;p&gt;The following is partially due to rendering an array of doubles as floats.  Interesting banding pattern.  A mini-Mondrian!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-fun-mess-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next is just uninitialized memory.  Iteresting patterns :-)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-16-fun-mess-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;increase spacing between reconstructed points&lt;/li&gt;
&lt;li&gt;remove edges around image border.&lt;/li&gt;
&lt;li&gt;share blurrer between views&lt;/li&gt;
&lt;li&gt;get this running on cuda server&lt;/li&gt;
&lt;li&gt;why is it crashing on exit?&lt;/li&gt;
&lt;li&gt;Data sanity check - does it roughly match the GMM&#39;s distribution?&lt;/li&gt;
&lt;li&gt;train likelihood&lt;/li&gt;
&lt;li&gt;add &quot;dump&quot; mode to bd-likelihood which saves all image pairs to disk for analysis or debugging.&lt;/li&gt;
&lt;li&gt;test empty model likelihood&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Work Log - Finishing Likelihood Server, integrating to sampling engine</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/15/work-log"/>
   <updated>2013-10-15T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/15/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Old silhouette rendering technique is failing.  Point spacing is much tighter than before.  Our old algorithm relied on segment-ids matching between the first-pass and second pass to work.&lt;/p&gt;

&lt;p&gt;Have a replacement algorithm that works reasonably well.  Basically edge-detection on the depth image.  The down side is we&#39;re getting double-edges in some areas, and you have to adjust a threshold.  It&#39;ll have to do for now, hopefully we can train a the likelihood to be robust to these issues.&lt;/p&gt;

&lt;h2&gt;Training Bd_likelihood&lt;/h2&gt;

&lt;p&gt;added &#39;train_bd_likelihood()&#39; function to &#39;lib/evaluate/bd_likelihood.h&#39;.  Uses &#39;kjb_c::get_full_GMM()&#39;  to fit a three-component gaussian mixture model to model the relationship between model and data pixels.&lt;/p&gt;

&lt;h2&gt;Refactoring&lt;/h2&gt;

&lt;p&gt;Shader objects from &#39;./shader.h&#39; are now in the project library, under &#39;lib/ogl/shader_2.h&#39;.  Not the greatest name, but shader.h already exists and conflicts.  Consder refactoring them later.&lt;/p&gt;

&lt;p&gt;Silhouetter rendering from &#39;./render_util.h&#39; are now in &#39;lib/graphics/silhouette_renderer.h&#39;.  Much better encapsulation now, so we can use it in other projects.&lt;/p&gt;

&lt;p&gt;In the process of updating &#39;./likelihood_server.cpp&#39; to reflect these changes.  At the moment, getting an &quot;invalid operation&quot; when rendering.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It was a stray &lt;code&gt;glUseProgram(notzero)&lt;/code&gt; in the shader loading code.&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - Troubleshooting silhouette rendering</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/14/work-log"/>
   <updated>2013-10-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/14/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Silhouettes are rendering badly&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-13-dump_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looks like internal edges are rendering.  Likely an issue with shader_src/silhouette2.gs.glsl.&lt;/p&gt;

&lt;p&gt;Need a quick way to tweak shader and re-run.&lt;/p&gt;

&lt;h2&gt;Building shader debugging mode&lt;/h2&gt;

&lt;p&gt;if enabled, results will display in viewer instead of being passed to likelihood.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log - Four days of OpenGL Debugging</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/11/work-log"/>
   <updated>2013-10-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/11/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Status&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Server is now running, connecting, receiving and decoding messages successfully.&lt;/li&gt;
&lt;li&gt;Matlab client is constructing, sending messages, and destroying successfully.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Since we&#39;ve sent our first message and saved it to a file, we no longer need matlab to debug the server, using the --dbg-load-message flag.&lt;/p&gt;

&lt;h2&gt;OpenGL crashing &lt;/h2&gt;

&lt;p&gt;In render_util.cpp -&gt; render_silhouettes(), crash is occurring somewhere in these lines.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;39      GLenum draw_buffers[2] = {GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1};
(gdb) list
40      glDrawBuffers(2, draw_buffers);
41
42      glClearColor(0.0,0.0,0.0,0.0);
43      glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
44      glEnable(GL_DEPTH_TEST);
45      DBG_GL_ETX();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The dreaded &quot;invalid operation&quot;&lt;/p&gt;

&lt;p&gt;I&#39;m guessing opengl isn&#39;t in a valid rendering state yet; we may not have bound the fbo, or forgot to enable a rendering program.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;yep, fbo wasn&#39;t bound.&lt;/p&gt;

&lt;p&gt;now running to completion, but result is NaN.&lt;/p&gt;

&lt;p&gt;Probably because we&#39;re using random numbers for our curves.  Lets grab some real curve data and re-run.  We&#39;ll probably need to visualize the silhouette output soon.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;added function &lt;code&gt;wacv-2012/get_wacv_result.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Code to get curves and send them to server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp = get_wacv_result(2);
tmp = cellfun(@(x) [x; ones(1,size(x,2))], tmp, &#39;UniformOutput&#39;, false);
socket = construct_render_client(&#39;localhost&#39;, &#39;12345&#39;)&#39;
send_curves(socket, tmp);
result = send_curves(socket, tmp);
destroy_render_client(socket)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Got first dumps.  issues&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;all black, no content&lt;/li&gt;
&lt;li&gt;reversed wrong aspect ratio&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Possible causees for 1.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;bad camera files&lt;/li&gt;
&lt;li&gt;bad code for converting intrinsic matrix to opengl&lt;/li&gt;
&lt;li&gt;bad curves?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;2 seems most likely.  Can debug 1 and 2 by trying old camera object and see if they work;  if so, compare matrices with new matrices.&lt;/p&gt;

&lt;h2&gt;Possibly the mods I made to the multi-view likelihoods are the culprit.  I&#39;m using raw matrices now instead of cmara objects in obj.add_view() and Camera_view_wrapper.&lt;/h2&gt;

&lt;p&gt;Added command-line option &quot;--dbg-save-frames&quot;.  Writes all rendered views to disk as &quot;dump_1.png, dump_2.png,...&quot;.  Only the most recent 10 are kept at any time.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;re-running using turntable camera; passing intrinsic and extrinsic matrices to likelihood.&lt;/p&gt;

&lt;p&gt;Renders are still black.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Forgot to set white color?&lt;/p&gt;

&lt;p&gt;No; its in the render_silhouette function.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found something: wacv results were aritficially centered on zero.  fixed.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Did matlab test on modelview and projection matrices and curve points -- z coordinate falls outside of [-1 1].  Beyond far plane.&lt;/p&gt;

&lt;p&gt;reason: not negating z-coordinate in test code.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;changing gears.  running matlab test on original input modelview and projection matrices.&lt;/p&gt;

&lt;p&gt;Modelview looks okay. Projection is way off.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;intrinsic matrix?&lt;/li&gt;
&lt;li&gt;NDC matrix?&lt;/li&gt;
&lt;li&gt;bounds?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Intrinsic looks reasonable.&lt;/p&gt;

&lt;p&gt;probably NDC/bounds issue&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bug in ndc math&lt;/p&gt;

&lt;p&gt;projecting outside from far plane&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;needed to flip axes to handle convention mismatch.&lt;/p&gt;

&lt;p&gt;Now getting something, but the positions are looking weird.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;was re-negating ppo; already flipped during convention resolution.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;now getting decent results, but off-center and smaller:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-12-dump_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;rough original to rendering offsets:  up by 56, left by 115.&lt;/p&gt;

&lt;h2&gt;non-uniform scaling; width is smaller than height.&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;Projecting plant base using
(a) intrinsic and extrinsic matrices from matlab
(b) modelview and projection matrices pulled from opengl

results:
(a) projects to the right place (tested using image and pixelstick)
(b) NDC coordinates look okay.  I manually remapped (scale and offset), and they look okay.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since NDC look okay, the remapping must be wrong.  glViewport issue?  I&#39;m manually setting it, but maybe it needs to be set for each shader?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Or maybe it&#39;s a geometry shader issue?  I&#39;m using different modelview and projection matrices now.&lt;/p&gt;

&lt;p&gt;=--&lt;/p&gt;

&lt;p&gt;Found it (Sunday night) -- Somehow the viewport transformation is getting reset. Possibly viewport is a shader-specific state that needs to be set each time?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;now centered, need to diagnose silhouette issues:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-10-13-dump_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;TODO (new)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;rename matlab &quot;render_client&quot; code to &quot;likelihood_client&quot;&lt;/li&gt;
&lt;li&gt;rename likelihood_server/sampler2.cpp to something not nonsense.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Implementing Likelihood server </title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/09/work-log"/>
   <updated>2013-10-09T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/09/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Spent most of the day finishing coding and compiling likelihood server.&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;build config file and make first run.

&lt;ul&gt;
&lt;li&gt;build camera files&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;write matlab TCP client mex code&lt;/li&gt;
&lt;li&gt;send ground truth model from matlab to server&lt;/li&gt;
&lt;li&gt;debug:

&lt;ul&gt;
&lt;li&gt;check rendering: silhouettes look okay?&lt;/li&gt;
&lt;li&gt;check y-axis flipping issue&lt;/li&gt;
&lt;li&gt;check likelihood vs. perturbed model vs. null model vs. overexpressive model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Issue: stem radius&lt;/h2&gt;

&lt;p&gt;Until now, our model and data have been infinitesimally thin curve (medial axis).&lt;/p&gt;

&lt;p&gt;Need to consider how to add width.  Options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Train?&lt;/li&gt;
&lt;li&gt;Fixed?  (pass as CLI parameter to server)&lt;/li&gt;
&lt;li&gt;Marginalize over?&lt;/li&gt;
&lt;li&gt;Optimize?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Since we&#39;re on short time, I&#39;m leaning toward 2 for simplicity.&lt;/p&gt;

&lt;h2&gt;Building config file&lt;/h2&gt;

&lt;p&gt;Done, but need to double-check which dataset we&#39;re using in matlab at the moment.&lt;/p&gt;

&lt;h2&gt;Issues&lt;/h2&gt;

&lt;p&gt;When running batches of several datasets, need a way to ensure matlab and likelihood server are running on the same dataset.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>New undertaking: C++ render server, pixel-based likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/08/work-log"/>
   <updated>2013-10-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Likelihood Server&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;likelihood_server&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;blockquote&gt;&lt;p&gt;Note on November 6, 2013: During the 5-day gap from the previous entry, I decided to go with &quot;Option 1&quot; proposed that day, namely construct a pixel likelihood that uses the rendered model, which should rule out bad candidates by penalizing missing data.  Since OpenGL rendering this isn&#39;t possible  in Matlab, realized I will need to implement rendering in C++ and somehow communicate to Matlab using some inter-process communication.   To minimize latency, avoid polling, deciced on using synchronous TCP over localhost.  Boost makes this pretty easy, and have implemented a proof of concept both in MEX and in KJB.   The Matlab/MEX version constructs a socket on the heap, and returns its address as a &lt;code&gt;uint64&lt;/code&gt; to matlab.&lt;/p&gt;

&lt;p&gt;At the point when the following was written, I had just begun implementing a render-server in opengl, which would receive a curve-tree and return the rendered pixels.  I would later abandon this approach in favor of computing an edge-based likelihood in C++ and return the log-likelihood value to matlab.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Working on compiling render server.&lt;/p&gt;

&lt;p&gt;Do I want to compute likelihood in C++ too?&lt;/p&gt;

&lt;p&gt;Code exists for it.  Maybe best to just re-use code from curve_sampling.cpp and abandon render server.&lt;/p&gt;

&lt;p&gt;Lets do that.&lt;/p&gt;

&lt;h2&gt;Reviewing old curve sampling code&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Pixel store&lt;/li&gt;
&lt;li&gt;renderer&lt;/li&gt;
&lt;li&gt;multi-view likelihood&lt;/li&gt;
&lt;li&gt;pixel likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Pixel likelihood receives a pointer to linear array of reals that represent the pixel matrix.&lt;/p&gt;

&lt;p&gt;Multi-view likelihood receives a renderable and handles all the rendering and pixel-retreival, and dispatches to pixel likelihood objects for computing.  is constructed with one or more cameras.  On evaluation it receives a renderable object, projects it under each camera, retreives the pixels, and passes to a pixel likelihood.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Refactor&lt;/strong&gt;: Separate rendering into a &quot;renderer&quot; object, which can be split into its own process if needed.  It will also make leapfrog pixel-reading simpler.&lt;/p&gt;

&lt;p&gt;End product will&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;receive message from matlab containing curves&lt;/li&gt;
&lt;li&gt;decode curves into vectors&lt;/li&gt;
&lt;li&gt;convert into gl_curves (renderables)&lt;/li&gt;
&lt;li&gt;pass to likelihood object&lt;/li&gt;
&lt;li&gt;convert log-posterior to message&lt;/li&gt;
&lt;li&gt;transmit back to matlab&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;all functionality is already implemented, just need to weed out code rot and put it together.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Getting weird link errors.  For some reason, the build system is using gcc instead of g++ to link my main program.  Will try re-running kjb_add_makefiles
&lt;strong&gt;Refactor&lt;/strong&gt;: specify all parameters at command-line (width, height, cameras, etc)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Meanwhile, I&#39;m planning the program&lt;/p&gt;

&lt;p&gt;inputs:
    * dimensions
    * cameras
    * likelihood parameters
    *&lt;/p&gt;

&lt;p&gt;Steps
    * initialize likelihood
    * initialize TCP server&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Abandoning GL_context from sampler_cpp.  writing one-off offscreen-buffer class.  writing new GL_context struct with just the stuff we need&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Reducing dependence on perspective_camera object in favor of using modelview and projection matrices.  We&#39;ll be working with matrices from matlab.&lt;/p&gt;

&lt;p&gt;Updated Camera_render_wrapper to use matrices instead of camera.&lt;/p&gt;

&lt;p&gt;Updated Abstract_mv_likelihood to use matrices in addition to cameras (for backward compatibility).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work Log</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/03/work-log"/>
   <updated>2013-10-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Thought more on split/merge.  Could make merge a special case of split, and the move becomes a split/split process.  At every step, pick a track, pick a subset of observations from that track, then reassign them all to an existing curve or a new curve.  It&#39;s symmetric, which is nice, but the probability of a merge being selected becomes vanishingly small very quickly.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Looking into issues with offline pair-candidate generation.  So many bad candidates are coming up, and their ML values look better than the good candidates.&lt;/p&gt;

&lt;p&gt;A big issue is that we aren&#39;t penalizing missing data (gaps/tails), and we don&#39;t enforce multiple-view consistency (only two views need to match).&lt;/p&gt;

&lt;p&gt;Option 1: use a stronger likelihood to rule out background curves.  Use per-pixel foreground/background classifier?  Use color consistency metric?  Sample from posterior, project, check pixels, repeat -- gets a monte-carlo of marginal likelihood.&lt;/p&gt;

&lt;p&gt;Option 2: use foreground/background classifier to classify fragments; misusing a background fragment in a foreground pair results in some penalty.&lt;/p&gt;

&lt;p&gt;Option 1 is nice theoretically, but has lots of moving parts (new features, training, monte-carlo issue, need to know how thick to make branches.)  Unlikely to get working in two weeks.  Also isn&#39;t clear what the role of the detector curves are.  Are they data?  data-driven proposals?&lt;/p&gt;

&lt;p&gt;Options 2 is a bit weird, but doesn&#39;t introduce dimensionality issues.  But also doesn&#39;t specifically address the issue of bizzare candidates being introduced.  For example, a bad pair could still be proposed as long as they&#39;re both foreground curves.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Split/merge toy (ctd)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/10/02/work-log"/>
   <updated>2013-10-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/10/02/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Debugging split/merge toy sampler.&lt;/p&gt;

&lt;p&gt;Split from 5 to 6 groups:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; split_alpha = -p_merge + p_split - q_split + q_merge - log(psplitmv) + log(0.5);
 K&amp;gt;&amp;gt; p_merge

 p_merge =

   -1.4437e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;-1.4437e+03 + -1.6060e+03 - -125.6828 + -2.7081&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; K&amp;gt;&amp;gt; p_split

 p_split =

   -1.6060e+03

   K&amp;gt;&amp;gt; q_split

   q_split =

    -125.6828

    K&amp;gt;&amp;gt; q_merge

    q_merge =

       -2.7081

       K&amp;gt;&amp;gt; log(psplitmv)

  ans =

     -0.6931

     K&amp;gt;&amp;gt; log(0.5)

  ans =

     -0.6931

     K&amp;gt;&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Merge from 6 to 5 groups:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;merge_alpha = p_merge - p_split + q_split - q_merge - pmergemv + psplitmv ;
K&amp;gt;&amp;gt; p_merge

p_merge =

  -1.4437e+03

K&amp;gt;&amp;gt; p_split

p_split =

  -1.6060e+03

K&amp;gt;&amp;gt; q_split

q_split =

 -208.8604

K&amp;gt;&amp;gt; q_merge

q_merge =

   -2.7081

K&amp;gt;&amp;gt; pmergemv

pmergemv =

   -0.6931

K&amp;gt;&amp;gt; psplitmv

psplitmv =

   -0.6931
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that q_split doesn&#39;t match between the two moves.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After reflection, this shouldn&#39;t match, because we&#39;re comparing two different cases.  Specifically, the post-split results in two small groups, where as the pre-merge model has 6 groups of equal size.  Smaller groups have smaller number of ways to split, so the proposal probability is higher.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Killed myself today thinking about this problem and getting exactly nowhere.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Problems with WACV Ground-truth reconstruction; Disasterous results with Split/merge toy problem</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/30/work-log"/>
   <updated>2013-09-30T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/30/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Have some problems with offline correspondence pair_matching.&lt;/p&gt;

&lt;p&gt;Is it a bug introduced in the reconstruction code?  Lots of refactors there, recently.&lt;/p&gt;

&lt;p&gt;Test by re-running wacv results.&lt;/p&gt;

&lt;h2&gt;WACV Reconstruction (revisited)&lt;/h2&gt;

&lt;p&gt;Now that datasets six through eleven have been re-ground-truthed, should confirm them by running wacv reconstruction code on them.  Also this should confirm whether we&#39;ve introduced any reconstruction bugs in the last week or two.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Problem with dataset six:  curve #4 isn&#39;t connected to its parent.&lt;/p&gt;

&lt;p&gt;Problem with dataset 1: really crooked.  Inspecting the ground-truth doesn&#39;t show any obvious problems that would cause this.  Maybe reversed curves? (which should be handled, but maybe aren&#39;t)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset1.png&quot; alt=&quot;Dataset 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dataset 6: base curve attaches from wrong side (red curve below):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset6.png&quot; alt=&quot;Dataset 6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dataset 7: Wacky top curves, missing connections at top, no base curves:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset7.png&quot; alt=&quot;Dataset 7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dataset 8: missed connections, bad top curves&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset8.png&quot; alt=&quot;Dataset 8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dataset 9: problems...&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset9.png&quot; alt=&quot;Dataset 9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Datset 10:  Good!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset10.png&quot; alt=&quot;Dataset 10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Datset 11: Bad top-curve associations&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-30-dataset11.png&quot; alt=&quot;Dataset 11&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Revenge of the Pre-curves&lt;/h2&gt;

&lt;p&gt;When investigating issues from dataset 6, discovered that results improved when ensuring that lateral curves don&#39;t go past the parent curve.  For example&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   |   /      |   /     |   /
   |  /       |  /      |  /
   | /        |_/      _|_/
   |          |         |
   |          |         |
   |          |         |
 Good       Best       Bad
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Priors and proposal distributions&lt;/h2&gt;

&lt;p&gt;Thinking about split proposal distributions.&lt;/p&gt;

&lt;p&gt;I&#39;ve been thinking about the combinitorics of partitions and realized I&#39;v been counting wrong.  Given M curves and N observations, I was counting the number of partitions as \(M&lt;sup&gt;N&lt;/sup&gt;\).  This doesn&#39;t account for the fact that order of cluster id&#39;s doesn&#39;t matter, so a better count would be \(M&lt;sup&gt;N&lt;/sup&gt; / N! \), which divides by the number of permutations of the cluster id&#39;s.    However, this is still wrong, because it counts assignments with empty clusters (which is indistinquishible from a model with N-1 clusters.  The real number of partitions is given by &lt;a href=&quot;http://en.wikipedia.org/wiki/Stirling_number_of_the_second_kind&quot;&gt;Stirling numbers of the second kind&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The same goes for the partitioning that occurs during a split move.&lt;/p&gt;

&lt;p&gt;Implemented &lt;code&gt;tools/build_stirling2.m&lt;/code&gt;, which builds an NxN matrix of the log of Stirling numbers of the second kind.&lt;/p&gt;

&lt;p&gt;Want to know that probability of accepting a merge move in various configurations, assuming the likelihood is constant.  Here&#39;s the test code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% number of observations: 918
% number of tracks: n
% number of observations in track i: k_i

p_merged  = @(n,k) -sm(918,n-1);
p_split = @(n,k) -sm(918,n);
q_split = @(n,k) log(1/(n-1)) - sm(k, 2);
q_merge = @(n,k) -log(nchoosek(n, 2));
alpha = @(n,k) p_merged(n,k) - p_split(n,k) + q_split(n,k) - q_merge(n,k);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running in several scenarios from plant-modelling problem:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% small number of curves; L,M,S number of observations
&amp;gt;&amp;gt; alpha(5, 50)

ans =

  169.0363

&amp;gt;&amp;gt; alpha(5, 20)

ans =

  189.8307

&amp;gt;&amp;gt;  alpha(5, 5)

ans =

  200.2747

% medium number of curves; L,M,S number of observations
&amp;gt;&amp;gt; alpha(10, 50)

ans =

   60.8395

&amp;gt;&amp;gt; alpha(10, 20)

ans =

   81.6339

&amp;gt;&amp;gt; alpha(10, 5)

ans =

   92.0779

% huge number of curves; L,M,S number of observations
&amp;gt;&amp;gt; alpha(100, 50)

ans =

  -26.7355

&amp;gt;&amp;gt; alpha(100, 20)

ans =

   -5.9411

&amp;gt;&amp;gt; alpha(100, 5)

ans =

    4.5029
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: These numbers are slightly off, due to a bug in the stirling number computation (now fixed).  The resulting alphas aren&#39;t substantially different.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In most realistic-model scenarios, the probability of accepting a merge move (ignoring the likelihood; assuming constant) is 100% (i.e. positive alpha).&lt;/p&gt;

&lt;p&gt;When the number of curves is huge (e.g. early in the sampler, when we need merges the most), the probability of acceptance is non-negligible only if the number of observations in the resulting curve is relatively small.  This is usually the case, and continuously merging short curves will result the acceptance probability getting larger and larger.&lt;/p&gt;

&lt;p&gt;It is interesting that in the presence of a constant likelihood, the sampler would ever prever fewer tracks to more.&lt;/p&gt;

&lt;p&gt;In these cases, the sampler is preferring to split up curves with abnormally large number of observations (compared to other pertitions).&lt;/p&gt;

&lt;p&gt;Would it be interesting to sample over partitions and see how the partition numbers and sizes evolve over time?  Heck yes!&lt;/p&gt;

&lt;h1&gt;Toy problem: sampling partitions&lt;/h1&gt;

&lt;p&gt;Prior: Uniform over number of groups.  Given a group, uniform over partitions.&lt;/p&gt;

&lt;p&gt;Likelihood: uniform.  For simplicity, I&#39;d like the sampler&#39;s exploration to not be influenced by which split or merge we pick.&lt;/p&gt;

&lt;p&gt;We implement a split-merge move.  When splitting, a group is picked at random, and its members are randomly split into two groups.  On merge, two groups are picked at random and merged.&lt;/p&gt;

&lt;p&gt;The prior of a model with N observations and k groups is \(1/s(N,k)\) where s the number of ways to partition N into k groups.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;When initialized with a single monolithic group (k=1), the sampler jumps between k=1 and k=2, rejecting about 50% of the time.  &lt;strong&gt;The sampler never jumps to k=3.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a problem, because we&#39;d expect to spend the same amount of time in the k=3 state as any other, since the target distribution is uniform over number of tracks.&lt;/p&gt;

&lt;p&gt;Either this is a bug, a flaw in my assumptions, or &quot;simply&quot; a mixing problem (a huge practical issue, but theoretically still correct).&lt;/p&gt;

&lt;p&gt;After exhausting myself bug-hunting, I decided to explore the mixing issue.&lt;/p&gt;

&lt;p&gt;Instead of initializing to k=1, I initialized to k=3.  Both the split and merge move have an acceptance probability of exp(-51.0), meaning it accept once in every 7.1e23 samples.  This sampler &lt;strong&gt;definitely has bad mixing&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Aside&lt;/em&gt;. It seemed notable that the split and merge move had the exact same acceptance probability for k=3.  At first this seems sensible, since the sampler should spend as much time in a two-group model as in a four-group. It turns out not to be true for any other \(k \neq 3\) I tried, but all exhibited poor mixing.  In fact, this asymmetry is natural, as the cardinality of the split proposals doesn&#39;t grow at exactly the same rate as the cardinality of the data-partitions.  This is balanced by the fact that you might transition to a lower model less often than the higher model, but you might spend more time in the lower model once you&#39;re there.&lt;/p&gt;

&lt;p&gt;Note that although we&#39;ve used a uniform likelihood, this mixing problem likely can&#39;t be fixed by using a different likelihood.  The likelihood could drive the sampler in a specific direction, but the reverse direction would be even harder to explore.  So the likelihood only makes matters worse.&lt;/p&gt;

&lt;p&gt;Q: Can I compute the expected length spent in each state?&lt;/p&gt;

&lt;p&gt;A: yes.  1/p_leave, where p_leave = (0.5 * split_alpha + 0.5 * merge_alpha)&lt;/p&gt;

&lt;p&gt;Q: Can I compute the expected direction of transition between states?
A: yes.  split_alpha / (split_alpha + merge_alpha);&lt;/p&gt;

&lt;p&gt;Q: Can I compute the expected ratio between an N-group model and an N+1 group model?
A: Do MCMC sampling.  When you reach a state, add its expected number of rejected samples to its tally, then transition up or down according to the direction distribution.&lt;/p&gt;

&lt;h2&gt;New toy sampler: Expected ratio between model probabilities&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;% compute expected samples
% compute up probability
% (state, tallies)
% random walk between states, 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Fix dataset six: connect curve 4 to parent&lt;/li&gt;
&lt;li&gt;Add confirmation code to check for multiple connected components in ground truth.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Ground truth finished.</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/28/work-log"/>
   <updated>2013-09-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/28/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finished re-doing Liu&#39;s ground-truth.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sampling - implemeting offline pair candidates</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/26/work-log"/>
   <updated>2013-09-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Problem with revamped index-estimation code:  it fails miserably on poorly-triangulatable hypotheses.&lt;/p&gt;

&lt;p&gt;The heuristic triangulation explodes in size, probably because triangulated points are all over the map (need to confirm this).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Adding error codes; if any part of the pipeline can&#39;t complete for any reason, set error code and return.  On error, the MCMC move will simply reject the proposal.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Issue with &lt;code&gt;build_full_correspondence&lt;/code&gt;: input track is destroyed and replaced.&lt;/p&gt;

&lt;p&gt;Refactored to replace make_correspondence with init_track.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make_correspondence -&amp;gt; init_track
make_trivial_correspondence -&amp;gt; init_trivial_corespondence
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Running &lt;code&gt;offline_pair_candidates_2&lt;/code&gt;.  Seems to be running without crashing.&lt;/p&gt;

&lt;p&gt;Takes about 10 minutes;  seems fast considering all the new logic we&#39;ve added (re-triangulation, 2D DTW). Probably the mex&#39;d DTW is helping.  Some profiling will likely identify some low-hanging optimization fruit here, too.&lt;/p&gt;

&lt;p&gt;Need to inspect triangulated curves when done.&lt;/p&gt;

&lt;p&gt;Need to see if background subtraction classifier will help prune.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Oops, spoke too soon.  Crash at about 75% completion.&lt;/p&gt;

&lt;p&gt;Random bug in candidate-proposal generator.  wasn&#39;t handling the &quot;no candidates&quot; case, resulted in index-out-of-bounds error.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>todo</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/26/todo"/>
   <updated>2013-09-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/26/todo</id>
   <content type="html">&lt;ul&gt;
&lt;li&gt;TESTS

&lt;ul&gt;
&lt;li&gt;Three levels of branching

&lt;ul&gt;
&lt;li&gt;visual test: reconstruction&lt;/li&gt;
&lt;li&gt;numeric test: fast ML vs reference&lt;/li&gt;
&lt;li&gt;numeric test: fast Covariance matrix vs. reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finish ground truthing&lt;/li&gt;
&lt;li&gt;implement recursive update&lt;/li&gt;
&lt;li&gt;end-to-end sampler

&lt;ul&gt;
&lt;li&gt;test birth/death move&lt;/li&gt;
&lt;li&gt;finish swap move&lt;/li&gt;
&lt;li&gt;implement attachment moves&lt;/li&gt;
&lt;li&gt;re-order move (see &lt;a href=&quot;/ksimek/research/2013/09/24/todo/&quot;&gt;here&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;evaluation&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Track stages</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/26/reference"/>
   <updated>2013-09-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/26/reference</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; The information that follows may be out of date.  The most recent version of this information is available &lt;a href=&quot;/ksimek/research/project/tulips/pipeline&quot;&gt;in the TULIPS documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A track&#39;s &quot;stage&quot; is a description of where it is in the processing pipeline.&lt;/p&gt;

&lt;p&gt;Each stage has one or more track fields associated with it.  No stage may modify fields from the previous stages.&lt;/p&gt;

&lt;h2&gt;Stage 0&lt;/h2&gt;

&lt;p&gt;Track is ready to be processed.   At this stage, the only valid fields are those set by the user, namely:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Track.assoc
Track.reversed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To construct a stage-zero track:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;init_track
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 1&lt;/h2&gt;

&lt;p&gt;A stage 1 track has passed the correspondence stage.&lt;/p&gt;

&lt;p&gt;Three functions can prepare a track for stage 1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;init_trivial_track
merge_correspondence
build_full_correspondence
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated fields are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;corr
means
precisions
cov_error
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 2:&lt;/h2&gt;

&lt;p&gt;In stage 2, raw curve likelihood fields have been constructed, and are ready for post-processing.&lt;/p&gt;

&lt;p&gt;The transition from stage 1 to stage 2 is relatively expensive, as every point requires backprojection, a pass of dynamic time warping, and a few iterations newton&#39;s method to find the index set.&lt;/p&gt;

&lt;p&gt;Associated fields&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ll_means
ll_precisions
ll_distances
sm_lambda
curve_sm
curve_sm_t
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated functions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;corr_to_likelihood
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 3&lt;/h2&gt;

&lt;p&gt;Stage three consists of inexpensive post-processing of the likelihood fields.  Curve reversal is handled here; flattening and sorting is lumped into this stage too.  Reversing and re-evaluating curves is a common use-case, and this can be done efficiently by keeping stage 3 separate from stage 2.&lt;/p&gt;

&lt;p&gt;This is also where the likelihood covariance blocks are computed; since this is somewhat time-costly, it may be moved into stage 2 in the future.&lt;/p&gt;

&lt;p&gt;Associated fields&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ll_views_flat
ll_means_flat
ll_precisions_flat
ll_distances_flat
ll_S
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated functions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flatten_sort_and_reverse
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 4: Attachment&lt;/h2&gt;

&lt;p&gt;Stage 4 is where topology is handled.  The predictive distribution of the branch point is computed and stored for efficient computation of the marginal likelihood later.&lt;/p&gt;

&lt;p&gt;Stage 4 needs to be applied recursively to all children.&lt;/p&gt;

&lt;p&gt;Associated fields&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;parent_ci
start_index
prior_K
branch_distance
mu_b
Sigma_b
branch_mu
branch_K
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated functions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;attach
detach
att_set_branch_distance (called from attach)
att_set_start_index (called from attach/detach)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Stage 5&lt;/h2&gt;

&lt;p&gt;Marginal likelihood has been computed for this track, conditioned on its parent.&lt;/p&gt;

&lt;p&gt;Note: in the current implementation, ml field is not set, and curve_ml simply returns the ml value.  This will change in the near future to comply with the multi-stage model described in this post.&lt;/p&gt;

&lt;p&gt;Associated fields&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Associated function&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curve_ml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Running all stages&lt;/h2&gt;

&lt;p&gt;In many cases, it&#39;s not necessary to construct tracks from scratch.   Reversing a curve only requires re-running stages 3 through 5.  Attaching or detaching a curve only requires re-running Stages 4 and 5.&lt;/p&gt;

&lt;p&gt;However, some cases require a full end-to-end running of stages 1 through 5.  The function that does this is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;build_track
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function is also a nice reference of how to run each stage.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sampling - birth/death</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/25/work-log"/>
   <updated>2013-09-25T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/25/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Coding swap and birth/death moves.&lt;/p&gt;

&lt;p&gt;Lots of refactoring.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;new function &lt;code&gt;attachment/detach.m&lt;/code&gt;; moved some code from attach to detach function.&lt;/li&gt;
&lt;li&gt;Reworked &lt;code&gt;kernel/&lt;/code&gt; directory.

&lt;ul&gt;
&lt;li&gt;added function: &lt;code&gt;kernel/get_base_covariance.m&lt;/code&gt;.  Used to implement detach.&lt;/li&gt;
&lt;li&gt;new function: &lt;code&gt;kernel/get_model_temporal_kernel.m&lt;/code&gt;.  Returns ind, ou, or sqexp kernel&lt;/li&gt;
&lt;li&gt;Eliminated &lt;code&gt;kernel/*_perturb_model.m&lt;/code&gt;.  &lt;code&gt;Get_model_kernel&lt;/code&gt; now constructs these directly using &lt;code&gt;get_model_temporal_kernel&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Reworked &lt;code&gt;kernel/get_model_kernel.m&lt;/code&gt; to use get_model_temporal_kernel.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Birth/Death Move&lt;/h2&gt;

&lt;p&gt;Implemented birth/death move.  New concept: birth and death candidate sets; on birth, candidate is moved from one to the other.&lt;/p&gt;

&lt;h2&gt;Migrating legacy sampling code&lt;/h2&gt;

&lt;p&gt;Reworking old sampler code&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&quot;sampling context&quot; object; (see &lt;code&gt;inference/ctx_*&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;offline construction of curve-pair (see &lt;code&gt;inference/offline_*&lt;/code&gt;);&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Refactoring Tracks (previously Corrs)&lt;/h2&gt;

&lt;p&gt;Continuing to update the codebase to use the new &quot;Tracks&quot; structure instead  of &quot;Corrs&quot;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;New function &lt;code&gt;correspondence/build_empty_curveset.m&lt;/code&gt;.  This is useful in two ways: (a) constructing an initial object before sampling begins, and (b) constructing a single track, which is now defined as a trackset of length 1.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;To be continued...&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;need to test &lt;code&gt;mcmc_birth_death.m&lt;/code&gt;.

&lt;ul&gt;
&lt;li&gt;need to construct context &lt;code&gt;ctx_init.m&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;need to construct offline pair candidates: &lt;code&gt;offline_pair_candidates_2&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Current task: constructing offline pair candidates.  Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data_2 = offline_pair_candidates_2(data_, params, 0, 1, 3);
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Discussion: maintaining attachment during sampling</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/24/work-log"/>
   <updated>2013-09-24T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/24/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Struggling with re-assigning associations.&lt;/p&gt;

&lt;p&gt;Yesterday, I was planning to re-sample attachments after every change to association.  The bookkeeping necessary to ensure reversibility of this is proving to be a nightmare, and it&#39;s questionable whether it actually improves results.  The counter-argument is that MH moves should be cheap to propose, but re-sampling attachments has significant cost.  All that cost is wasted if the proposed move is a bad one, which it often will be.  Also, most curves only change slightly when association changes, so changing the branching is probably a bad idea.&lt;/p&gt;

&lt;p&gt;Two special cases need to be handled.  First is when a change to association causes a curve to become shorter, and an attachment point disappears.  In this case, we allow it, but during evaluation, clamp to the endpoint.&lt;/p&gt;

&lt;p&gt;However, when birth/death occurs, re-doing attachment is absolutely necessary.  In this case, we take a simpler apprach than yesterday&#39;s strategy:  on death, child curves become root curves; on birth, new curve gathers children from root curves.  Or alternatively, birth/death is illegal if curve has children.&lt;/p&gt;

&lt;p&gt;The preceeding dicusson about avoiding re-attachment is only valid for re-assignment moves.  For split/merge moves, we must update attachment.  But this is easy too: on merge, child sets are merged; on split, sample A vs. B based on distance.&lt;/p&gt;

&lt;h2&gt;Approach #2&lt;/h2&gt;

&lt;p&gt;When re-assigning associations, if it results in a dangling curve, reject&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>todo</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/24/todo"/>
   <updated>2013-09-24T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/24/todo</id>
   <content type="html">&lt;h2&gt;Track order issue&lt;/h2&gt;

&lt;p&gt;There are situations where ties between tracks are broken by giving priority to the track with lower index. For this reason, we should have a move where tracks randomly change order.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;p&gt;Quick&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;test three-levels of branching

&lt;ul&gt;
&lt;li&gt;reconstruction&lt;/li&gt;
&lt;li&gt;ML vs. reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cleanup Corr fields

&lt;ul&gt;
&lt;li&gt;group fields by processing stage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;map-out processing pipeline&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Medium&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;finish ground-truthing (Friday night, Saturday)&lt;/li&gt;
&lt;li&gt;implement recursive update&lt;/li&gt;
&lt;li&gt;code for inferring branching parameters.&lt;/li&gt;
&lt;li&gt;Finish training

&lt;ul&gt;
&lt;li&gt;infer branching parameters&lt;/li&gt;
&lt;li&gt;re-write training ML&lt;/li&gt;
&lt;li&gt;re-train prior parameters with full ML

&lt;ul&gt;
&lt;li&gt;jointly train FG and BG using same&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MCMCDA sampler

&lt;ul&gt;
&lt;li&gt;association moves&lt;/li&gt;
&lt;li&gt;split/merge moves&lt;/li&gt;
&lt;li&gt;re-order moves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;evaluation code&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Misc maintenance & "Thinking" on sampling, priors</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/23/work-log"/>
   <updated>2013-09-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/23/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Updating data structures to Version 2&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;* Refactored several variable names, some functions may need updating.
* Corrs was changed from cell-of-structs to a cell-array.  Some functions may still need updating.
* Removed old, unused variables
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Thinking on sampling&lt;/h2&gt;

&lt;p&gt;Did a writeup on MCMCDA with attachments, see associated &quot;strategy&quot; entry&lt;/p&gt;

&lt;h2&gt;Thinking on priors&lt;/h2&gt;

&lt;p&gt;Did a writeup on association priors, see associated &quot;reference&quot; entry&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Split/Merge moves and Association Priors</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/23/splitmerge-moves-and-association-priors"/>
   <updated>2013-09-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/23/splitmerge-moves-and-association-priors</id>
   <content type="html">&lt;p&gt;Thinking again about split/merge moves, and the problem of the exploding number of split moves as the nubmer of observations increases.&lt;/p&gt;

&lt;p&gt;Part of the problem is the fact that we&#39;re treating all associations as equal under the prior.  This has the side-effect that the prior strongly prefers more curves in a model as opposed to fewer.  Consider a scenario with N observations, and M curves.  There are M&lt;sup&gt;N&lt;/sup&gt; number of ways to assign these observations to M curves.  Compare this with a model with one curve -- there&#39;s only one possible assigment.  Since all assignments are equal under the prior, the model with M curves is more favored by the prior by a factor of M&lt;sup&gt;N&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;For this reason, I propose that a more sensible prior is one that is uniform over the number of curves in the scene.  Then, given the number of curves, the prior over associations is uniform.  In other words, given an association, it&#39;s prior should be 1/M&lt;sup&gt;N&lt;/sup&gt;, where M is the number of curves represented in the association.&lt;/p&gt;

&lt;p&gt;When running a split move, the number of ways to split is 2&lt;sup&gt;K&lt;/sup&gt;, where K is the number of observations associated with the original curve.  If chosen uniformly, the Metropolis-Hastings proposal probability is 2&lt;sup&gt;K&lt;/sup&gt;.  This should cancel nicely with the prior term in the MH acceptance ratio.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>MCMCDA Sampling with Attachment</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/23/mcmcda-sampling-with-attachment"/>
   <updated>2013-09-23T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/23/mcmcda-sampling-with-attachment</id>
   <content type="html">&lt;p&gt;The old MCMCDA gibbs sampler runs into trouble now that we allow attachment.&lt;/p&gt;

&lt;p&gt;Previously, for any observation, we would compute the ML of grouping it with each candidate, and select a candidate from those weights.&lt;/p&gt;

&lt;p&gt;The problem now is that regrouping an observation sometimes causes it&#39;s old group to vanish, and the question of what to do with the group&#39;s children is unanswered.  The same problem occurs if a regrouping makes a curve shorter, and children were attached to the now-missing structure.&lt;/p&gt;

&lt;h2&gt;Metropolis Hastings&lt;/h2&gt;

&lt;p&gt;Things become much easier if we move away from Gibbs sampling.  Gibbs was nice for a proof of concept, because we never failed to pick embarrassingly good candidates, and we almost always moved toward a better result with each step.  However, it was very expensive, and now we have a question of how to compute the ML of a candidate whose attachment property is unknown.&lt;/p&gt;

&lt;p&gt;So, we&#39;ll move to Metropolis Hastings.  We resolve the attachment issue by using a simple rule:  every time a track&#39;s association changes, we re-sample its the attachment and the attachment of its former children.&lt;/p&gt;

&lt;h2&gt;Sampling Associations.&lt;/h2&gt;

&lt;p&gt;When sampling associations, we&#39;ll use two types of moves:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;swap&lt;/strong&gt;: re-assign a single observation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;split/merge&lt;/strong&gt; re-assign a group of observations&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The swap move will be similar to the existing Gibbs move, except we&#39;ll choose from candidates uniformly at random.  Split/merge proposals will be naive; we&#39;ll incorporate an association prior to counter the exploding number of split moves.&lt;/p&gt;

&lt;h2&gt;Re-sampling attachment&lt;/h2&gt;

&lt;p&gt;After sampling a new association, we&#39;ll re-sample attachment.  For this, we&#39;ll introduce a new function &lt;code&gt;sample_attachment&lt;/code&gt;, which will be responsible for constructing a list of reasonable candidates and attachment parameters (Start point and branch position), and selecting one at random (probably based on geometry, i.e. not uniform).  It returns the sampled attachment, along with the proposal probability.  It can optionally receive a &lt;code&gt;hint&lt;/code&gt; parameter, which is a list of attachment candidates, along with a guess as to the attachment parameters.  The candidate of &quot;no attachment&quot; should always be an option.&lt;/p&gt;

&lt;p&gt;We&#39;ll also need a function that returns the probabiliy of selecting a specific attachment, according to &lt;code&gt;sample_attachment&lt;/code&gt;, for computing the reverse move.&lt;/p&gt;

&lt;h2&gt;Topology moves&lt;/h2&gt;

&lt;p&gt;In addition to association moves, we&#39;ll also have topology moves.  This will consist of simply calling &lt;code&gt;sample_attachment&lt;/code&gt; without a sampling an association beforehand.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Branching ML Done</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/20/branching-ml-done"/>
   <updated>2013-09-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/20/branching-ml-done</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finally finished implementing and confirming the multi-view branching ML.  Next steps:&lt;/p&gt;

&lt;p&gt;Quick&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;re-run tests with non-zero start index&lt;/li&gt;
&lt;li&gt;test three-levels of branching

&lt;ul&gt;
&lt;li&gt;reconstruction&lt;/li&gt;
&lt;li&gt;ML vs. reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cleanup Corr fields

&lt;ul&gt;
&lt;li&gt;eliminate clean_ fields&lt;/li&gt;
&lt;li&gt;group fields by processing stage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;map-out processing pipeline&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Medium&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;finish ground-truthing (Friday night, Saturday)&lt;/li&gt;
&lt;li&gt;implement recursive update&lt;/li&gt;
&lt;li&gt;code for inferring branching parameters.&lt;/li&gt;
&lt;li&gt;Finish training

&lt;ul&gt;
&lt;li&gt;infer branching parameters&lt;/li&gt;
&lt;li&gt;re-write training ML&lt;/li&gt;
&lt;li&gt;re-train prior parameters with full ML

&lt;ul&gt;
&lt;li&gt;jointly train FG and BG using same&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gibbs sampler&lt;/li&gt;
&lt;li&gt;evaluation code&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Reach&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;split-merge moves&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>refactoring; dependencies</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/19/work-log"/>
   <updated>2013-09-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;investigate difference between ML implementations&lt;/li&gt;
&lt;li&gt;handle nonzero start-index in branching. re-run confirmation&lt;/li&gt;
&lt;li&gt;ground truth - trace datasets 7 through 10&lt;/li&gt;
&lt;li&gt;retrain using attachment&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Invstigating consistent ML implementations&lt;/h1&gt;

&lt;p&gt;Discovered issue:  was calling &lt;code&gt;curve_tree_ml&lt;/code&gt; instead of &lt;code&gt;curve_tree_ml_2&lt;/code&gt;.  Working now&lt;/p&gt;

&lt;h2&gt;Aside: common mistakeis&lt;/h2&gt;

&lt;p&gt;Losing a too much time to stupid organizational mistakes --&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;indexing,&lt;/li&gt;
&lt;li&gt;passing wrong copies of variables to code,&lt;/li&gt;
&lt;li&gt;leaving random debugging code in miscellaneous functions,&lt;/li&gt;
&lt;li&gt;running wrong debugging branch in code,&lt;/li&gt;
&lt;li&gt;running wrong version of function.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Code has gotten complex enough that managing all of these little sharp edges is too expensive.  Need to start being more disciplined.  Solutions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;indexing&lt;/strong&gt;: use functions for common indexing tasks

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;one_d_to_three_d&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;to_block_index&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;miscellaneous variables&lt;/strong&gt;: clean-up workspace every evening&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Debugging cruft&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;if code is an in-line test, refactor questionable code into functions; write a real test.&lt;/li&gt;
&lt;li&gt;if code is a &lt;code&gt;plot&lt;/code&gt;, &lt;code&gt;imagesc&lt;/code&gt;, or &lt;code&gt;printf&lt;/code&gt;, always wrap in a DEBUGGING block, even if its a two-minute test.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wrong debugging branch&lt;/strong&gt;:

&lt;ul&gt;
&lt;li&gt;if using &lt;code&gt;method = 1&lt;/code&gt; give names like &lt;code&gt;method = MARKOV&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;display warnings for non-standard methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;wrong version of function&lt;/strong&gt;: depends on future of old code

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;deprecation&lt;/strong&gt;: add block to old reference code: &lt;code&gt;fprintf(&#39;running legacy code (press enter)&#39;); pause;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reference implementation&lt;/strong&gt;: rename to *_ref.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Discussion: start_index and branch_index&lt;/h1&gt;

&lt;p&gt;In previous days, I was torn over whether I should add start_index to branch_index before evaluating.&lt;/p&gt;

&lt;p&gt;The answer is yes, because it greatly simplifies the optimization of start_index, because you don&#39;t have to re-adjust the children&#39;s branch_indices.&lt;/p&gt;

&lt;p&gt;how is markov blanket determined (pre or post offset indexing?)&lt;/p&gt;

&lt;p&gt;Do we need to recurse after attach?&lt;/p&gt;

&lt;p&gt;RAW VALUES should be stored, not derived values.  branch_index is a raw value.&lt;/p&gt;

&lt;p&gt;*Example: * C is attached to B.  We want to attach B to A, with start index of 10.  Assume branch_index is stored relative to the zero-index (as opposed to the first observed point).  After attaching, we need to update C&#39;s branch point; if we later detach B from A, B&#39;s branch point need to be updated again.  There&#39;s potential for drift to accumulate after all of these upates.&lt;/p&gt;

&lt;p&gt;To avoid confusion, replace branch_index to branch_distance; will convert to an index value before computing. Also &lt;code&gt;prior_indices&lt;/code&gt; needs updating?  Or just eliminate&lt;/p&gt;

&lt;h2&gt;recursive Updating after attachment&lt;/h2&gt;

&lt;p&gt;Consider attachment:&lt;/p&gt;

&lt;p&gt;Before&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(D -&amp;gt; C -&amp;gt; B)    (A)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(D -&amp;gt; C -&amp;gt; B -&amp;gt; A) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visually,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;             A   B C D         A B C D 
             |   | | |         | | | |
             |   | |/          | | |/
             |   | +           | | +  
             |   |/            | |/ 
             |   +       ==&amp;gt;   | +
             |                 |/         
             |                 +      
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;After attaching B to A,

&lt;ul&gt;
&lt;li&gt;C&#39;s branch index changes, so...

&lt;ul&gt;
&lt;li&gt;C&#39;s branch distribution changes, so...

&lt;ul&gt;
&lt;li&gt;D&#39;s branch distribution changes so...

&lt;ul&gt;
&lt;li&gt;D&#39;s ML changes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;C&#39;s ML changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;C&#39;s prior_K doesnt change.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As far as stored fields go, it looks like branch distributions change recursively after attachment, etc.&lt;/p&gt;

&lt;h1&gt;Dependencies&lt;/h1&gt;

&lt;p&gt;Did some thinking about dependencies; what needs to be updated when parents are changed.  Scan of the notes are available below.  Also did &lt;a href=&quot;/ksimek/research/reference/2013/09/19/dependencies/&quot;&gt;a reference writeup&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/ksimek/research/img/2013-09-19-dependency_notes.jpg&quot;&gt;Dependency hand-written notes&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;Refactoring&lt;/h1&gt;

&lt;p&gt;Changed &lt;code&gt;Corr.branch_index&lt;/code&gt; to &lt;code&gt;Corr.branch_distance&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Replaced all instances of &lt;code&gt;Corrs(i).branch_index&lt;/code&gt; with &lt;code&gt;get_branch_index(Corrs, i)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Replaced Corr.ll_indices* with Corr.ll_distances.&lt;/p&gt;

&lt;p&gt;removed Corr.prior_indices&lt;/p&gt;

&lt;p&gt;TODO: replace kernel(XX,YY) with eval_kernel(asdf)
    see att_set_start_index_2
TODO: replace Corrs cell array with Corrs structure array&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;p&gt;Minor&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;re-run tests with non-zero start index&lt;/li&gt;
&lt;li&gt;test three-levels of branching

&lt;ul&gt;
&lt;li&gt;reconstruction&lt;/li&gt;
&lt;li&gt;ML vs. reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cleanup Corr fields

&lt;ul&gt;
&lt;li&gt;eliminate clean_ fields&lt;/li&gt;
&lt;li&gt;group fields by processing stage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;map-out processing pipeline&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Medium&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;finish ground-truthing (Friday night, Saturday)&lt;/li&gt;
&lt;li&gt;implement recursive update&lt;/li&gt;
&lt;li&gt;code for inferring branching parameters.&lt;/li&gt;
&lt;li&gt;Finish training

&lt;ul&gt;
&lt;li&gt;infer branching parameters&lt;/li&gt;
&lt;li&gt;re-write training ML&lt;/li&gt;
&lt;li&gt;re-train prior parameters with full ML&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MCMCDA sampler&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Summary of Dependency relationships</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/19/dependencies"/>
   <updated>2013-09-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/19/dependencies</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;The dependency between individual variables is a bit complicated, but if we group variables together, the relationship between the groups is simple.  We group parameters into &quot;self&quot; parameters and &quot;inherited&quot; parameters.&lt;/p&gt;

&lt;p&gt;Self parameters are immune to changes to parents or children.
Inherited parameters are affected by any changes to parent.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SELF PARAMETERS

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;reconstruction fields&lt;/em&gt;: assoc, corr, ll_*,&lt;/li&gt;
&lt;li&gt;start index&lt;/li&gt;
&lt;li&gt;prior_K&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;INHERITED PARAMETERS

&lt;ul&gt;
&lt;li&gt;branch distance, branch_index&lt;/li&gt;
&lt;li&gt;&lt;em&gt;branch distribution&lt;/em&gt;:  mu_b, mu_Sigma, branch_mu, branch_Sigma&lt;/li&gt;
&lt;li&gt;Marginal likelihood.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This can be summarized as follows:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Any time a curve is changed in any way, we must recursively update all inherited parameters.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This &quot;if anything changes, update everything&quot; rule is a bit broad, and we can use a finer-grained definition to update fewer inherited fields, but in general, we only avoid updating fields that are inexpensive to update anyway.  For heavy-wieght fields (e.g. branch distribution and ML), they are affected by everything, so we&#39;re forced to update them after every change.  Thus, the simpler rule is only nominally less efficeint, and much easier to implement and understand.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Testing full-tree covariance matrix</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/18/work-log"/>
   <updated>2013-09-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/18/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Running covariance confirmation for model_type = 3.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Results don&#39;t match.&lt;/p&gt;

&lt;p&gt;Off-diagonals are okay.&lt;/p&gt;

&lt;p&gt;on-diagonal&#39;s are larger in reference impl; on order of 10.&lt;/p&gt;

&lt;p&gt;Reference implementation has all positive eigenvalues.  Likely bug is in testing implementation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Open question: Does branch_index need to be incremented by start_index?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;also, is prior_K including start_index?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Root curve is block incorrect, which means we can focus on either &lt;code&gt;build_root_object&lt;/code&gt; or &lt;code&gt;build_sibling block&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Got it&lt;/strong&gt;: prior_K was computed with model_type = 1.  It contains no perturb covariance.  But root block does contain perturb covariance.&lt;/p&gt;

&lt;p&gt;Re-attach using:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wacv_Corrs_ratt = wacv_Corrs;
for i = 1:numel(wacv_Corrs_reatt)
    C = wacv_Corrs_reatt{i};
    wacv_Corrs_reatt = attach_2(wacv_Corrs_reatt, i, C.parent_ci, C.start_index, C.branch_index,params);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now results are identical for the testing set&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;results differ on the full set.&lt;/p&gt;

&lt;p&gt;in testing set, all branches occured from the zero point on the parent.  the full set has branches from other points, and we&#39;re getting different results.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found the cause of the problem: it was leftover cruft code I added during debugging.  Caused non-symmetric matrix.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;TODO: Move &quot;confirm full covariance against reference implementation&quot; into its own test folder&lt;/p&gt;

&lt;p&gt;TODO: handle nonzero start-index in branching. re-run confirmation&lt;/p&gt;

&lt;p&gt;TODO: finish &quot;confirm curve-tree ML against reference implementation&quot;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Struggling with curve_ml5, where Chol is failing.  Looks like the problem is in att_Set_Branch_index; branch conditional covariance is not positive definite.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found: indexing bug in &lt;code&gt;attachment/attach_2.m&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Comparing three methods:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% method 1
ml1 = curve_tree_ml_ref(wacv_Corrs_reatt, params, data_)
% method 2
ml2 = curve_tree_ml(wacv_Corrs_reatt, params, data_)
% method 3
ml3 = 0;
for i = 1:numel(wacv_Corrs_reatt)
    ml = ml + curve_ml5( ...
            wacv_Corrs_reatt{i}, ...
            data_, ...
            params, ...
            get_model_kernel(params, params.model_type), ...
            params.ml_block_size, ...
            params.ml_markov_order);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All three currently differ significantly.&lt;/p&gt;

&lt;p&gt;Also compared against an indendent model, whose ML was about 300 points lower.&lt;/p&gt;

&lt;p&gt;At this point, method 2 and method 3 should be giving the same results, but aren&#39;t.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Moved covariance matrix test to &lt;code&gt;test/test_construct_attachment_covariance.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Re-ran and now it&#39;s failing :-/&lt;/p&gt;

&lt;p&gt;Modified it to save the output; re-running to debug.&lt;/p&gt;

&lt;p&gt;Needed to re-run &lt;code&gt;attach_2()&lt;/code&gt;, because prior_K was stale&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;troubleshoot covariance test&lt;/li&gt;
&lt;li&gt;investigate difference between method 2 and method 3 above.&lt;/li&gt;
&lt;li&gt;handle nonzero start-index in branching. re-run confirmation&lt;/li&gt;
&lt;li&gt;ground truth - trace datasets 7 through 10&lt;/li&gt;
&lt;li&gt;retrain using attachment&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Building reference implementation of full-tree covariance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/17/work-log"/>
   <updated>2013-09-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;




&lt;div&gt;
Building reference version of `build_attachment_covariance()`.  It does full recursion for every element, taking \(O(n^2 \log n)\) time.
&lt;/div&gt;


&lt;hr /&gt;

&lt;p&gt;Running now with model_type = 1;  estimated runtime: 45 minutes.  (compare this to less than 5 seconds in the fast version).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Results don&#39;t match.&lt;/p&gt;

&lt;p&gt;Issue 1: topology indices array was reversed; handled inheritance wrongly.
Issue 2: forgot about start_index; should add to all curve indices.&lt;/p&gt;

&lt;p&gt;Results now match when model type = 1.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Matching results suggest that the covariance matrix is correctly implemented as designed, but why are we getting negative eigenvalues?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Next: re-run with model_type = 3.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Full-tree covariance; Run on WACV dataset</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/16/work-log"/>
   <updated>2013-09-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finished testing &lt;code&gt;construct_attachment_covariance.m&lt;/code&gt; against legacy method for constructing covariance matrix.  Constructed new test &lt;code&gt;wacv_2014/run_wacv_2.m&lt;/code&gt; to compare new method against old method in &lt;code&gt;wacv_2014/run_wacv.m&lt;/code&gt;.  After significant amount of debugging, results match.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tweak to Corr.prior_K: &lt;/strong&gt; don&#39;t replicate until needed.  New function &lt;code&gt;tools/one_d_to_trhee_d&lt;/code&gt; helps with this.&lt;/p&gt;

&lt;p&gt;Now, to test new covariance matrix algorithm against the existing branching ML code to confirm that the latter is correct...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Wrote &lt;code&gt;curve_tree_ml_ref&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Having trouble running  on wacv dataset.  The covariance matrix constructed by &lt;code&gt;construct_attachment_covariance&lt;/code&gt; has several &lt;em&gt;negative eigenvalues&lt;/em&gt; (magnitude on the order of 1000).  These don&#39;t appear when attachments don&#39;t exist.&lt;/p&gt;

&lt;p&gt;Old method of constructing the covariance matrix has several negative eigenvalues, but they&#39;re on the order of 1e-9.  (were&#39;nt they identical?)&lt;/p&gt;

&lt;p&gt;Lets go back to the WACV example and run everything on those...&lt;/p&gt;

&lt;p&gt;The big question to answer is: &lt;strong&gt;is WACV ML better with attachments or worse?&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;On second look, the test and reference prior matricies &lt;strong&gt;don&#39;t&lt;/strong&gt; match.  Going back to debugging &lt;code&gt;construct_attachment_covaraince.m&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;found bug in bugfix in attach.m.   results now match&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Now covariancaes from wacv and curve_tree_ml_ref don&#39;t match.  Furthermore, curve_tree_ml_ref crashes because the matrix isn&#39;t positive definite.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Found issue: wacv program forced model_type to be 1 (no-perturb model).  This accounts for the difference between wacv reconstruction and the reference ML code.&lt;/p&gt;

&lt;p&gt;Still don&#39;t know why we&#39;re getting negative eigenvalues when model_type != 1.&lt;/p&gt;

&lt;h2&gt;Medium-term planning&lt;/h2&gt;

&lt;p&gt;Q: What is needed to get end-to-end running?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Efficient ML for branching model.&lt;/li&gt;
&lt;li&gt;inference of branch points, given attachment (part of proposal mechanism?)&lt;/li&gt;
&lt;li&gt;training with attachment

&lt;ul&gt;
&lt;li&gt;updated training ML&lt;/li&gt;
&lt;li&gt;updated training procedure&lt;/li&gt;
&lt;li&gt;joint training of foreground and background, with shared noise parameter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;re-write sampling infrastructure

&lt;ul&gt;
&lt;li&gt;pair candidates&lt;/li&gt;
&lt;li&gt;gibbs moves&lt;/li&gt;
&lt;li&gt;merge/split (using Swendsen Wang?)&lt;/li&gt;
&lt;li&gt;attach/detach&lt;/li&gt;
&lt;li&gt;HACKS

&lt;ul&gt;
&lt;li&gt;background subtraction?&lt;/li&gt;
&lt;li&gt;&quot;cheating&quot; (nonreversible) merge/split&lt;/li&gt;
&lt;li&gt;forced attachment?&lt;/li&gt;
&lt;li&gt;heuristic initialization?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish debugging reference ML: &lt;code&gt;curve_tree_ml_ref.m&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;why are we getting negative eigenvalues in prior matrix when model_type &gt; 1?&lt;/li&gt;
&lt;li&gt;compare against long-hand version of matrix (slow, full recursive version).  Does it still have negative eigenvalues?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Confirm that WACV dataset has better ML when attachments are modeled.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Branching prior covariance; implementing</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/12/work-log"/>
   <updated>2013-09-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/12/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Goals:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ground truth - trace datasets 7 through 10&lt;/li&gt;
&lt;li&gt;build new attachment covariance&lt;/li&gt;
&lt;li&gt;reconstruct ground truth using new attachment covariance&lt;/li&gt;
&lt;li&gt;work attachment covariance function into ml&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Building Full Attachment Covariance Matrix&lt;/h2&gt;

&lt;p&gt;See &lt;code&gt;attachment/construct_attachment_covariance.m&lt;/code&gt; (filename still in flux).&lt;/p&gt;

&lt;p&gt;Issue: The recursive definition of the attached curve kernel that we outlined yesterday is too expensive to implement naively.  Can implement by re-using precomputed values, but the branch point isn&#39;t necessarilly one of the observed points, so it&#39;s not necesarilly represented in the curve&#39;s &lt;code&gt;prior_K&lt;/code&gt;.  This means we need to construct covariance entries between each of the attachment points and their parent and child curves.  Then we can run the &quot;memoized&quot; recursive function.&lt;/p&gt;

&lt;h2&gt;Base self covariance&lt;/h2&gt;

&lt;p&gt;Key idea: each child curve has a &quot;base self covariance&quot; that is extra covariance inherited from the branch point on parent that is added to all points of the curve self-covariance matrix.  That curve, in turn, has a base covariance, that &lt;em&gt;it&lt;/em&gt; inherits, all the way to the root, whose base covariance is the &quot;position variance&quot; parameter.  Thus, the leaves have a base covariance that is the sum of all the incremental variance of all the branches to the root.&lt;/p&gt;

&lt;p&gt;The base covariance is an NxN matrix, where N is the number of views.  We construct each in isolation, and then cumulatively sum them from root to leaf.&lt;/p&gt;

&lt;p&gt;Every element of the curve&#39;s observed-point covariance matrix is increased by the base_covariance relative to it&#39;s entry; e.g. if element (k_{ij}) is the covariance between a point in view 3 and a point in view 5, you&#39;ll add the (3,5) element of the base covariance matrix.&lt;/p&gt;

&lt;h2&gt;Base Parent covariance&lt;/h2&gt;

&lt;p&gt;Each child curve also hase a &quot;parent covariance&quot; (for lack of a better term), which is the set of covariances between the branch point and each of the parent points.  Again, this is inherited recursively; just as the curve&#39;s covariance has it&#39;s self-covariance added, the parent covariance has the paren&#39;ts self-covariance added.&lt;/p&gt;

&lt;p&gt;This will be used for the off-diagonal blocks of the full attachment covariance matrix.&lt;/p&gt;

&lt;h2&gt;Implementation Misc&lt;/h2&gt;

&lt;p&gt;Issues during implementation and testing of &lt;code&gt;construct_attachment_covariance&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;replacing Cell-of-structs with struct-array:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Corrs = cell2mat(Corrs);
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;The trivial code path of calling &lt;code&gt;blkdiag&lt;/code&gt; takes 500ms. Wow!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Running on the WACV dataset is giving bad results.&lt;/p&gt;

&lt;p&gt;Looks like the covariance matrix is different from the one in the reference implementation.&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Need to convert from &quot;index of branch index&quot; to simply &quot;branch index&quot;.&lt;/li&gt;
&lt;li&gt;More debugging.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Branching prior covariance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/10/work-log"/>
   <updated>2013-09-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/10/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;In the past, we&#39;ve formulated the branching prior covariance matrix by starting with an independent-curve block-diagonal covariance matrix and multiplying by an &quot;attachment matrix.&quot;  This works when the branching point appears in the index set, but we&#39;d like to allow branching from any continuous point along the curve.  Stated differently, we currently model branching using a discritized model, but we&#39;d prefer to express branching using the underlying continuous gaussian process.  This means modelling branching using the covariance function, rather than by manipulating the covariance matrix.&lt;/p&gt;

&lt;div&gt;
&lt;p&gt;
The key observation here is that in addition to each point&#39;s smooth curvature variance, it inherits the covariance of its parent&#39;s branch point.   In order to model correlation between curves, we extend the definition of &quot;index&quot; to add the curve index, \(c_i\), in addition to point index and time/view index.  Thus, index is now a triple: (c_i, p_i, t_i).  Note that we&#39;ve changed our notation; previously point index was \(t_i\) and time/view index was \(v_i\).  The new notation uses \(p_i\) and \(t_i\) respectively, to avoid confusion between spacial and temporal dimensions.  
&lt;/p&gt;

&lt;p&gt;
Let&#39;s introduce two functions, \(\text{branch_curve}(c_i)\) and \(\text{branch_point}(c_i)\), which return the curve index and point index of curve \(c_i\)&#39;s branch point, resepectively, or zero if \(c_i\) is a root curve.  We&#39;ll also use the convenience function \(\text{branch}(i)\), which is shorthand for full branch index, i.e. the tuple \(\text{branch_curve}(c_i), \text{branch_point}(p_i), t_i)\).  Note that the time-index \(t_i\) is preserved when getting the branch index.
&lt;/p&gt;

&lt;p&gt;
Without loss of generality, we&#39;ll assume curves are indexed in topological order, with parents appearing occurring before descendants.  If there are multiple connected components in the graph, it will be important that all root nodes have a lower index than non-root nodes, so we can add a sentinal node with index \(c_i = 0\), and attach all root curves to it before ordering. (Note to self: breadth-first search order might be simpler to describe).   The branching covariance function is now:
&lt;/p&gt;

\[
\begin{align}
k\left( i, j \right) 
     &amp;= 
    k\left( (c_i, p_i, t_i), (c_j, p_j, t_j) \right)  \\
     &amp;=  
     \begin{cases}
     k(j, i) &amp; \text{if } c_i &gt; c_j = 0 \\
     k_\text{base} ( i, j ) + k_o(ti, tj)  &amp; \text{if } \text{isroot}(c_i) \text{ and } \text{isroot}(c_j) \text{ and } c_i = c_j &amp;&amp;  \\
     k_\text{base} ( i, j ) + 0 &amp; \text{if } \text{isroot}(c_i) \text{ and } \text{isroot}(c_j) \text{ and } c_i \neq c_j \\
     k_\text{base} ( i, j ) + k(i, \text{branch}(j)) &amp; \text{otherwise} 
    \end{cases}
\end{align}
\]



&lt;p&gt;
For convenience, we&#39;ve used the \(k(i,j) \) to stand-in for \(k\left((c_i, p_i, t_i), (c_j, p_j, t_j)\right)\).
  Here, \(k_\text{base}(i,j)\) is the independent curve covariance (e.g. cubic spline covariance w/ perturbations).   In our case, \(k_\text{base}(\dot)\) is zero if \(c_i \neq c_j\), but this isn&#39;t necessarilly required.
  &lt;/p&gt;

&lt;p&gt;
Here, \(k_o\) is to marginal covariance of the first curve point.  All points in the plant are measured relative to this point, so this covariance ends up in every element of the covariance matrix.  Since it only measures covariance of a specific point with respect to itself, curve and point index are irrelevant to \(k_o\).  Only the temporal is received, which allows us to model how the point moves over time.

In the case of the Ornstein Uhlenbech process, \(k_o\) is

\[
k_o = \sigma_o + \exp(|t_i - t_j|) \sigma_{o,b})
\]
&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;Lets examine each case in the piecewise function.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first case just simplifies the definition by ensuring that if i and j are related, the ancestor always appears in the first position.&lt;/li&gt;
&lt;li&gt;The second case handles the offset variance that allow root curves to translate away from the origin.&lt;/li&gt;
&lt;li&gt;The third case ensures zero covariance between disconnected trees.&lt;/li&gt;
&lt;li&gt;The fourth case inherits covariance from the second parameter&#39;s parent curve.  Note that \(c_j\) is always a non-root curve; if it were, one of the first three cases would handle it.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Testing &lt;/h2&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish tracing wacv datasets 7 through 11.&lt;/li&gt;
&lt;li&gt;Test the new covariance function against a matrix-based approach.&lt;/li&gt;
&lt;li&gt;Test the new covariance function by reconstructing 9-view sequence using WACV datasets.&lt;/li&gt;
&lt;li&gt;implement attachment-based ml using new covariance function&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>WACV results</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/06/work-log"/>
   <updated>2013-09-06T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/06/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;WACV deadline has passed, but still working on reconstructing ground truth, because it&#39;s  a good test on the end-to-end system.&lt;/p&gt;

&lt;p&gt;Open issues:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;weird issue with branching&lt;/li&gt;
&lt;li&gt;clear up math issue with branching&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Branching bug&lt;/h1&gt;

&lt;p&gt;Fixed!  It was a bug in &lt;code&gt;infer_branch_points()&lt;/code&gt; function.  I was assuming Corr.curve_sm used the same  index set as Corr.ll_means_flat, which it was not.&lt;/p&gt;

&lt;p&gt;I wanted to use &lt;code&gt;Corr.curve_sm&lt;/code&gt;, because it is precomputed in &lt;code&gt;corr_to_likelihood()&lt;/code&gt;, and it would save me an extra semi-expensive call to the cubic spline smoother, &lt;code&gt;csaps()&lt;/code&gt;.  In the end, I modified &lt;code&gt;corr_to_likelihood&lt;/code&gt; to save the smoothing amount and used it to call csaps inside &lt;code&gt;run_wacv()&lt;/code&gt; to re-smooth the curves.  At least &lt;code&gt;csaps&lt;/code&gt; is faster (linear time) than computing the posterior mean (quadratic).&lt;/p&gt;

&lt;h2&gt;Test Results&lt;/h2&gt;

&lt;p&gt;Results on dataset 2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-06-result_dataset_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looks good!&lt;/p&gt;

&lt;h2&gt;All Results&lt;/h2&gt;

&lt;p&gt;Running on all 11 datasets...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Matlab is running out of memory on the second dataset (apparently)&lt;/p&gt;

&lt;p&gt;When running directly on second dataset, it&#39;s fine.  Is the pass on the first dataset leaking memory or something?&lt;/p&gt;

&lt;p&gt;Maybe one of the mex files?&lt;/p&gt;

&lt;p&gt;It&#39;s weird, because we&#39;ve run on dataset #2 tens of times without this problem...&lt;/p&gt;

&lt;p&gt;Now running directly on dataset #2 is resulting in a curve of over 1 million points.  investigating...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Solution: the image dimensions were reversed (should be nrows X ncols, not width x height).  The triangulations were nutso, and after projecting and resampling, the number of points was in the millions.  It&#39;s interesting that it&#39;s so easy to make this algorithm blow up; are there other; will need to keep an eye out for more benign ways to trigger this kind of explosion.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Ground truth issues&lt;/h2&gt;

&lt;p&gt;Encountered this error message when running on all ground truth datasets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Warning: file wacv-2012/datasets/3/ground_truth_2d.gt2
Missing curves in view 6: 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 2: 2, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 3: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 4: 2, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 6: 2, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 8: 1, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 10: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 15: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 19: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 23: 2, 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 27: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 31: 3,

Warning: file wacv-2012/datasets/6/ground_truth_2d.gt2
Missing curves in view 35: 3,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dataset 6 looks to be totally crap; need to investigate.&lt;/p&gt;

&lt;h2&gt;Found a bug in &lt;code&gt;corr_to_likelihood&lt;/code&gt; that led to out-of-bound indexing.&lt;/h2&gt;

&lt;p&gt;Looks like Liu never finished ground-truthing dataset &lt;code&gt;2010-01-26/arab_1_36&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Skipping for now; lets see if others are okay...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Dataset 9 is broken. investigating...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Dataset 9 seems not terrible, actually.  A few missing views on the hard-to-trace views of a few curves.&lt;/p&gt;

&lt;p&gt;Some cuves were so short that they contained only one point, which violated an assert that requires all curves to be at least 2 points long.  Adjusted the bezier-to-sampled-curve code so that the final point is always included.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Finished running all.  issues&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dataset 1 is very rough&lt;/li&gt;
&lt;li&gt;datasets 6 was skipped&lt;/li&gt;
&lt;li&gt;datasets 7 - 11 are worthless.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Need to re-trace 6 through 11.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Finished re-trace of 6.  Whew, 90 minutes... that sucked.  Reconstruction looks good now.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish tracing datasets 7 through 11.&lt;/li&gt;
&lt;li&gt;Work on attached-curve-multi-view prior and reconstruction.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>WACV Deadline</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/05/work-log"/>
   <updated>2013-09-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/05/work-log</id>
   <content type="html">&lt;p&gt;Late-hour realization: not sure how to handle attachment matrix when multiple-views are allowed.  Deriving the correct equations will take all afternoon; no time.  Decided to remove the tracking part from the paper.&lt;/p&gt;

&lt;h1&gt;Reconstructions&lt;/h1&gt;

&lt;p&gt;code in &lt;code&gt;wacv-2012/run_wacv.m&lt;/code&gt;.  Example call:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gt_paths = arrayfun(@(x) sprintf(&#39;wacv-2012/datasets/%d&#39;, x), 1:11, &#39;UniformOutput&#39;, false);
gt_cam_fmt = [gt_paths{i} &#39;/cal/calib_%d.txt&#39;];
gt_fname = [gt_paths{i} &#39;/ground_truth_2d.gt2&#39;];
[mu, lengths] = run_wacv(gt_fname, gt_cam_fmt, 1:4:36, [397,530]&#39;, params, 1);
mu = reshape(mu, 3, []);
mu = mat2cell(mu, 3, lengths);
colors = lines(numel(mu));
for i = 1:numel(mu);
    c = mu{i};
    col = colors(i, :);
    plot3(c(1,:), c(2,:), c(3,:), &#39;-o&#39;, &#39;Color&#39;, col);
    hold on
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Dataset #1&lt;/em&gt;: curve #1 has some weird shape.  Probably and error in GT&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dataset #2&lt;/em&gt;: again weirdness in curve #1.  Now I&#39;m guessing it&#39;s an attachment issue:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-05-bad_result_c2_pass1.png&quot; alt=&quot;bad result&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Run again without attachments.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dataset #2&lt;/em&gt;: curve 1 issue is better; now exhibiting some weird stray points.  few views, bad indices?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-09-05-c2-pass2.png&quot; alt=&quot;result 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like the base curves are trying to attach to the &lt;em&gt;end&lt;/em&gt; of curve #1, instead of the beginning.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Branching ML, debugging, training</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/09/03/work-log"/>
   <updated>2013-09-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/09/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Blockwise precision decomposition issue&lt;/h1&gt;

&lt;p&gt;Bug: when attaching, was getting incorrect results for estimated location of attachment.  Our copmutation used a subset of data points on the parent curve nearby the branch point to estimate the distribution of the branch point position.  We were simply taking a submatrix of the full (decomposed) precision matrix, which wasn&#39;t correct, due to the way the full decomposed matrix was constructed.&lt;/p&gt;

&lt;h2&gt;Submatrix of decomposition != decomposition of submatrix&lt;/h2&gt;

&lt;p&gt;When using subset of variables, need to be able to get submatrix of noise precision and prior.&lt;/p&gt;

&lt;p&gt;Let \((s&#39; s) = S\), and let \(P\) be a matrix that selects rows of \(S\).  if you want to decompose the submatrix \(P S P&#39;\), in general, you cannot simply take the submatrix \(P s P&#39;\).  There are at-least two special case where this is valid:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;\(s\) is a Cholesky decomposition of \(S\).  But since \(S\) is degenerate in our case, we can&#39;t use Cholesky.&lt;/li&gt;
&lt;li&gt;\(S\) is block-diagonal, and \(P\) doesn&#39;t split-up the blocks.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I thought that condition 2 was being satisfied, because I foolishly assumed the eigenvector matrix produced by eigenvalue decomposition was preserving block structure.  I had to modify &lt;code&gt;correspondence/flatten_sort_and_reverse.m&lt;/code&gt; to perform eigenvalue decomposition on each block individually.  I had previously determined that operating on groups of five blocks at once is faster than operating individually, but we&#39;ll have to sacrifice this speed-up for now.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Okay, fixed the precision problem.  After attaching stem 2 to stem 1, Corrs{2}.mu_b now looks reasonable.&lt;/p&gt;

&lt;p&gt;Still getting really low ML&#39;s...&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bingo!  Bug in &lt;code&gt;curve_tree_ml_2.m&lt;/code&gt; that was introduced when we started allowing non-zero means when evaluating our conditional normal distribution.   Recall the equation for marginal likelihood (reproduced from &lt;a href=&quot;/ksimek/research/2013/07/12/marginal-likelihood/&quot;&gt;this post&lt;/a&gt;):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
     p(y) &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z}  \exp\{x^\top S(S + S K S)^{-1} S x \} \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z}  \exp\{x^\top s^\top(I + s K s^\top)^{-1} s x \} \\
     &amp;= \frac{Z_{l,3d} }{Z_l} \mathcal{N}\left (sx ; 0, (I + s K s^\top) \right )
\end{align}
\]
&lt;/div&gt;




&lt;div&gt;Aside: in the second line, we&#39;ve substituded the decomposition, \(s^\top s = S\), which we use in practice.  &lt;/div&gt;


&lt;p&gt;However, this equation assume zero mean.  If we include a nonzero mean, the \(s\) matrix distributes to both \(y\) and \(\mu\):&lt;/p&gt;

&lt;div&gt;
\[
\begin{align}
     p(y) &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z}  \exp\{(x-\mu)^\top s^\top(I + s K s^\top)^{-1} s (x - \mu) \} \\
     &amp;= \frac{Z_{l,3d} }{Z_l} \mathcal{N}\left (sx ; s \mu, (I + s K s^\top) \right )
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;In our implementation, we weren&#39;t multiplying \(\mu\) by \(s\).  Fixing this seems to fix the low ML issue:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Corrs = attach(Corrs, 2,1,12, -12, params);
curve_tree_ml_2(Corrs, params, data_)

    ans =

       2.0903e+04

Corrs_ind = attach(Corrs, 2,0,0,0, params);
curve_tree_ml_2(Corrs_ind, params, data_)

    ans =

       2.0804e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the case above, ML is roughly maximized when branch index is -12 and start index is 12, which seems to agree with the data, visually.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Next:
* get attachment, reversal, and branch points from ground truth
* store ml_2d with corr. update on merge. use during ml computation instead of data_
* branching in training ML
* traing with branching
* branch-wise reconstruction.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Misc.</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/26/work-log"/>
   <updated>2013-08-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15229&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Monday IVILAB infrastructure meeting&lt;/h1&gt;

&lt;p&gt;I&#39;ll be organizing the Computational Intelligence seminar this semester.&lt;/p&gt;

&lt;p&gt;Will need to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Arrange volunteers&lt;/li&gt;
&lt;li&gt;send announcements&lt;/li&gt;
&lt;li&gt;send reminders to speakers&lt;/li&gt;
&lt;li&gt;Set up seminar web page&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Building efficient curve-tree ML.&lt;/h1&gt;

&lt;p&gt;Significant rework.&lt;/p&gt;

&lt;p&gt;New field: branch_K and branch_mu to store mean and covariance of all points in curve.
Prior_k now does not include offset covariance; stored in branch_K.
mu_b and Sigma_b now store the branch point means and covariances for all views&lt;/p&gt;

&lt;p&gt;fix kernel to not include offset index&lt;/p&gt;

&lt;h1&gt;Issues&lt;/h1&gt;

&lt;p&gt;Finished implementation.  Has bugs.&lt;/p&gt;

&lt;h2&gt;Start-point has no effect&lt;/h2&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;start_pt = [0 100];
ml = [];
for i = 1:2
    Corrs = attach(Corrs, 2, 1, start_pt(i), 0, params);
    ml(i) = curve_tree_ml_2(Corrs, params, data_);
end
assert(ml(1) == ml(2));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Fixed&lt;/em&gt; - stupid bug in &lt;code&gt;attachment/attach.m&lt;/code&gt; - was index offset was hard-coded to zero due to a refactoring mishap.&lt;/p&gt;

&lt;h2&gt;Optimal Branch point isn&#39;t correct&lt;/h2&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;start_pt = [0 10 50 100 1000 5000];
ml = [];
for i = 1:2
    Corrs = attach(Corrs, 2, 1, start_pt(i), 0, params);
    ml(i) = curve_tree_ml_2(Corrs, params, data_);
end
assert(all(diff(ml) &amp;gt; 0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Solved&lt;/em&gt; - curves were reversed.&lt;/p&gt;

&lt;h1&gt;Handling reversed curves&lt;/h1&gt;

&lt;p&gt;Should add a &lt;code&gt;reversed&lt;/code&gt; flag, which reverses indices before building likelihood and prior.  currently, constructing the likelihood occurs during the &quot;backproject and re-index&quot; phase, in file &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt;.  Should refactor likelihood construction into its own function, so we don&#39;t have to re-backproject when we don&#39;t need to.&lt;/p&gt;

&lt;p&gt;Let&#39;s review the data-flow so we can see more clearly where everything happens.&lt;/p&gt;

&lt;h1&gt;Overview: end-to-end curve construction&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;propose association and attachment

&lt;ul&gt;
&lt;li&gt;By sampling (no code yet)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;train/labels_from_ground_truth&lt;/code&gt; - propose from ground truth.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: assoc {}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Construct track

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correspondence/make_correspondence&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Corrs {};  Corr.ml_2d&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Correspondence and triangulation

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correspondence/build_full_correspondence.m&lt;/code&gt; - build from scratch&lt;/li&gt;
&lt;li&gt;&lt;code&gt;correspondence/merge_correspondence_2.m&lt;/code&gt; - merge two pieces&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;corr&lt;/code&gt;, &lt;code&gt;means&lt;/code&gt;, &lt;code&gt;precisions&lt;/code&gt;, &lt;code&gt;cov_error&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;backproject and estimate curvature

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt; -&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;ll_{means, precisions, indices}&lt;/code&gt;, &lt;code&gt;curve_sm*&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;construct likelihood (flatten, sort, and reverse if needed)

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correspondence/flatten_sort_and_reverse.m&lt;/code&gt; - (doesn&#39;t exist yet)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;ll_{means_flat, precisions_flat, indices_flat, S}&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;handle attachment recursively

&lt;ol&gt;
&lt;li&gt;compute conditional prior

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_start_index.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;start_index&lt;/code&gt;, &lt;code&gt;prior_K&lt;/code&gt;, &lt;code&gt;prior_indices&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;compute branch point posterior

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_branch_index.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: &lt;code&gt;branch_index&lt;/code&gt;, &lt;code&gt;mu_b&lt;/code&gt;, &lt;code&gt;sigma_b&lt;/code&gt;, &lt;code&gt;branch_K&lt;/code&gt;, &lt;code&gt;branch_mu&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Compute (ML, argmax, etc)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Consider renaming step 4.  backproject against rough triangulation; estimate curvature at each point; determine index set.  At this point, the order of points don&#39;t matter, because the index set hasn&#39;t been put to use.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Action: reverse curve&lt;/strong&gt; - detach?, rerun step 5, 6.1 &amp;amp; 6.2 on self, update branch point &amp;amp; rerun 6.2 for children.&lt;/p&gt;

&lt;h1&gt;Test: optimize branch point and start index&lt;/h1&gt;

&lt;p&gt;(TODO)&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;optimize test&lt;/li&gt;
&lt;li&gt;get attachment, reversal, and branch points from ground truth&lt;/li&gt;
&lt;li&gt;store &lt;code&gt;ml_2d&lt;/code&gt; with corr.  update on merge.  use during ml computation instead of data_&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Attachment ML Math (ctd); Implementing Attach()/Detach()</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/22/work-log"/>
   <updated>2013-08-22T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/22/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15169&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Note new working path: &lt;code&gt;data_association_3&lt;/code&gt;&lt;/strong&gt;&lt;br/&gt;
Fixed corrupt svn working copy.&lt;/p&gt;

&lt;h1&gt;Computing \(\mu_b\) and \(\Sigma_b\)&lt;/h1&gt;

&lt;div&gt; Since \(S\) is usually singular, we can&#39;t invert it, so we need to tweak yesterday&#39;s equations.  In what follows, the precision matrix \(S\) is decomposed into \(s s^\top\), which will allow us to keep the covariance matrix symmetric. 

&lt;div&gt;
The updated forumula for \(\mu_b\) is:&lt;/div&gt;
\[
    \begin{align}
    \mu_b &amp;= K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} y_{MB}  \\
    \tag{1}
          &amp;= K_*^\top s^\top \left(s K_{\mathcal{MB}} s^\top + I\right)^{-1} s y_{MB}  \\
    \end{align}
\]

The formula for \(\Sigma_b\) is:

\[
    \Sigma_b = K_b - K_*^\top s^\top \left(s K_{\mathcal{MB}} s^\top + I\right)^{-1} s K_*
\]

Recall that \(K_b\) is the prior covariance of the branch point.
&lt;/div&gt;


&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2&gt;Predictive covariance question&lt;/h2&gt;

&lt;div&gt;
&lt;p&gt;
I realized during implementation that it isn&#39;t clear what the predictive covariance, \(K_*\), should be.  I know it&#39;s the covariance between the true branch point and the observed points in the markov blanket, but the observed points have associated view-indices, while the branch point does not.  After some thought, I realized that the covariance arising from view-indices is essentially *likelihood* variance in this context (i.e. imaging noise), and according to Williams and Rasmussen, these parts of the likelihood should be omitted when computing the covariance between data and true unobserved points.  (See equation (2.21), notice that off-diagonal elements don&#39;t include the noise variance, \(\sigma_n I\)).
&lt;/p&gt;
&lt;p&gt;
To accomodate the use-case where you want to compute the covariance while ignoring view-index, I tweaked `kernel/get_model_kernel`; now, if you pass-in a `model_index` of zero, it returns a two-parameter no-perturb kernel where only spacial-indices are received.
&lt;/p&gt;
&lt;/div&gt;


&lt;h2&gt;&lt;code&gt;ll_indices_flat&lt;/code&gt; vs. &lt;code&gt;prior_indices&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;There&#39;s some ambiguity in these two fields of a curve-track.  The difference is simply that in &lt;code&gt;ll_indices_flat&lt;/code&gt;, indices &lt;em&gt;always&lt;/em&gt; start at zero, whereas in &lt;code&gt;prior_indices&lt;/code&gt; the offset &lt;code&gt;start_index&lt;/code&gt; is added to all values.  In practice, there are rules for where these two fields should be used:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;when passing indices to a kernel, always use &lt;code&gt;prior_indices&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;When dealing with geometry (e.g. referring to a point along a curve), always use ll_indices.  That way, the position of the referred point is unchanged if &lt;code&gt;start_index&lt;/code&gt; ever changes.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Task progress&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Finish &lt;code&gt;attachment/att_set_branch_index.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;rough test of attachment stuff - (does it parse? does it run?)

&lt;ul&gt;
&lt;li&gt;It runs, but with buggy output for \(\mu_b\) and \(\Sigma_b\).&lt;/li&gt;
&lt;li&gt;Found bug - incorrect re-indexing (xxyyzz to xyzxyz)&lt;/li&gt;
&lt;li&gt;Now giving apparently good results.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Implement efficient tree ML&lt;/li&gt;
&lt;li&gt;implement naive ML&lt;/li&gt;
&lt;li&gt;write tests

&lt;ol&gt;
&lt;li&gt; compare against &lt;code&gt;curve_ml5.m&lt;/code&gt; for independent curves&lt;/li&gt;
&lt;li&gt; compare against naive ML using full-covariance-matrix.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;invalidate children after calling attach/detach&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Misc notes&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;should place a condition on \(\Sigma_b\).  If it&#39;s variance is too high, need to expand markov-blanket.&lt;/li&gt;
&lt;li&gt;Add &lt;code&gt;model_type to params&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Have &quot;make_correspondence&quot; initialize start_index, parent_index, branch_index.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;chain of affects&lt;/h1&gt;

&lt;p&gt;How are children affected when properties of the parent change?&lt;/p&gt;

&lt;div&gt;
&lt;table border=&quot;1&quot;&gt;
&lt;tr&gt;
&lt;th width=&quot;33%&quot;&gt;parent property&lt;/th&gt;
&lt;th width=&quot;33%&quot;&gt;self affects&lt;/th&gt;
&lt;th&gt;child affects&lt;/th&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Assoc/Correspondence&lt;/td&gt;
&lt;td&gt;start_index (and inherited)&lt;/td&gt;
&lt;td&gt;Branch_index (and inherited.  easy to re-estimate deterministically), marginal likelihood, (not start index)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Start index&lt;/td&gt;
&lt;td&gt;prior_K, prior_indices, ML&lt;/td&gt;
&lt;td&gt;\(\mu_b\)*, \(\Sigma_b\)*, ML*&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Branch index&lt;/td&gt;
&lt;td&gt;\(\mu_b\), \(\Sigma_b\), &lt;/td&gt;
&lt;td&gt;none.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;
* Updating these might not be necessary
&lt;/div&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Implement Tree ML (2 ways, &quot;naive&quot; and &quot;fast&quot;)&lt;/li&gt;
&lt;li&gt;Test&lt;/li&gt;
&lt;li&gt;Get attachment for ground truth&lt;/li&gt;
&lt;li&gt;Re-train using attachment&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Clique-tree math (ctd)</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/21/work-log"/>
   <updated>2013-08-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/21/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Conditional clique node&lt;/h1&gt;

&lt;p&gt;Continuing from yesterday, let&#39;s convert our conditional gaussian distribution to a gaussian function over \(x_c\) and \(\mu_b\).&lt;/p&gt;

&lt;p&gt;Let \(II\) represent a stack of identity matrices:&lt;/p&gt;

&lt;div&gt;
\[
II = \left( \begin{array}{c} I\\I\\...\\I \end{array}\right)
\]

The exponential expression for the conditional Gaussian distribution is

    \[
    -\frac{1}{2} (x_C - II \mu_b)^\top \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}(x_C - II \mu_b)
\]

Let&#39;s convert this to a linear function over \((x_C, \mu_b)\), instead of just \(x_C\):
        
    \[
    -\frac{1}{2} \left(\begin{array}{c}x_C \\ \mu_b \end{array} \right )^\top \left ( \begin{array}{cc}I &amp; -II\end{array}\right )^\top \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}\left ( \begin{array}{cc}I &amp; -II\end{array}\right )\left(\begin{array}{c}x_C \\ \mu_b \end{array}\right )
\]
&lt;/div&gt;


&lt;p&gt;Recall that \(\mu_b\) is a linear function of the data-points in the Markov-blanket, \(y_\mathcal{MB}\) (reproduced from &lt;a href=&quot;/ksimek/research/2013/08/19/work-log/&quot;&gt;yesterday&#39;s post&lt;/a&gt;)&lt;/p&gt;

&lt;div&gt;
\[
    \begin{align}
    \mu_b &amp;= K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} y_{MB}  \\
        &amp;= K_*^\top K^{-1}_{y_\mathcal{MB}}  y_\mathcal{MB}  \\
    \end{align}
\]

Rewriting the expression as a function of \((x_C, y_\mathcal{MB})\):

    \[
    \begin{align}
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\ K_* K^{-1}_{y_\mathcal{MB}}  y_\mathcal{MB} \end{array} \right )^\top
    \left ( \begin{array}{cc}I &amp; -II\end{array}\right )^\top 
    \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}
    \left ( \begin{array}{cc}I &amp; -II\end{array}\right )
    \left(\begin{array}{c}x_C \\ K_* K^{-1}_{y_\mathcal{MB}}  y_\mathcal{MB} \end{array}\right )
    \\
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\   y_\mathcal{MB} \end{array} \right )^\top
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}} \end{array}\right )^\top 
    \left( \Sigma_C + II \Sigma_b II^\top \right )^{-1}
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}}\end{array}\right )
    \left(\begin{array}{c}x_C \\ y_\mathcal{MB} \end{array}\right )
    \end{align}
\]

And expanding \(\Sigma_b\):
 
\[
    \begin{align}
    \Sigma_b &amp;= K_b - K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} K_* \\
             &amp;= K_b - K_*^\top K^{-1}_{y_\mathcal{MB}} K_*

    \end{align}
\]

The final expression is:

    \[
    \begin{align}
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\   y_\mathcal{MB} \end{array} \right )^\top
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}} \end{array}\right )^\top 
    \left( \Sigma_C + II \left(K_b - K_*^\top K^{-1}_{y_\mathcal{MB}} K_* \right)  II^\top \right )^{-1}
    \left ( \begin{array}{cc}I &amp; -II K_* K^{-1}_{y_\mathcal{MB}}\end{array}\right )
    \left(\begin{array}{c}x_C \\ y_\mathcal{MB} \end{array}\right ) \\
     &amp;=
    -\frac{1}{2} \left(\begin{array}{c}x_C \\   y_\mathcal{MB} \end{array} \right )^\top
    \Lambda_{C \mid \mathcal{MB}}
    \left(\begin{array}{c}x_C \\ y_\mathcal{MB} \end{array}\right )
    \end{align}
\]

the normalization constant is:
    
\[
    Z = 
    (2 \pi)^\frac{k}{2} \left | \Sigma_C + II \left(K_b - K_*^\top K^{-1}_{y_\mathcal{MB}} K_* \right)  II^\top \right |^\frac{1}{2}
\]
&lt;/div&gt;


&lt;h2&gt;Test experiment&lt;/h2&gt;

&lt;p&gt;See  &lt;code&gt;exp_2013_08_21_clique_tree_test.m&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;sample N points for curve 1, \(C_1\)&lt;/li&gt;
&lt;li&gt;sample N points for curve 2, \(C_2\)&lt;/li&gt;
&lt;li&gt;offset curve 1: \(C_2 = C_2 + C_1(:,5)\)&lt;/li&gt;
&lt;li&gt;add noise to \(C_1\) and \(C_2\) to get data \(y_1\),\(y_2\)&lt;/li&gt;
&lt;li&gt;add noise to \(C_1\) and \(C_2\) to get data \(y_1\), \(y_2\).&lt;/li&gt;
&lt;li&gt;Construct full prior, \(\Sigma\) (see below for definition).&lt;/li&gt;
&lt;li&gt;Evaluate ML directly from \(p(y_1, y_2) = \mathcal{N}(0, (\Sigma + \sigm_n I))\).&lt;/li&gt;
&lt;li&gt;Construct ML decomposed: \(p(y_2 | y_1) p(y_1) \)&lt;/li&gt;
&lt;li&gt;(didn&#39;t implement) Construct clique tree using

&lt;ul&gt;
&lt;li&gt;Node 1: \((0, \Sigma_1)\)&lt;/li&gt;
&lt;li&gt;Node 2: \((0, inv(\Lambda_{C\mid \mathcal{MB}}))\)&lt;/li&gt;
&lt;li&gt;multiply by corresponding noise nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(didn&#39;t implement) Marginalize clique tree.  compare against result in 7.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I originally thought the result from 8 was only an approximation (mainly because I hadn&#39;t originally written it out that way).  In fact, it&#39;s an exact computation, but it isn&#39;t very useful in this form, because deep trees still exhibit linear growth of the condition-set, meaning cubic growth in running time.  In practice, we can replace \(y_1\) to the data markov-blanket, \(y_{1,\mathcal{MB}}\).  The data markov-blanket is naturally larger than the prior m.b., which is a single point, but if the noise is low relative to the point-spacing, the data m.b. should still be relatively small.  Thus, we can approximate the ML with a decomposed form that avoids cubic growth.&lt;/p&gt;

&lt;p&gt;The alternative is to use the clique tree from step 9., but the former is simpler to implement, because we don&#39;t have to use the crazy linear form we developed above, and we don&#39;t have to do any message-passing.  We just need \(\Sigma_b\) and \(\mu_b\).&lt;/p&gt;

&lt;h2&gt;Computing max posterior with attachments&lt;/h2&gt;

&lt;p&gt;Since this operation won&#39;t be run during production, we can just implement it the naive way.&lt;/p&gt;

&lt;p&gt;But if we wanted a fast version, we could to a forward-pass approximation, i.e. find the max for the root curve, and then pass that as data to the child curves.&lt;/p&gt;

&lt;p&gt;A full forward-backward algorithm probably wouldn&#39;t be too hard, but probably not worth the trouble at the moment.&lt;/p&gt;

&lt;h1&gt;Build Tasks&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;apply attacment to trackset&lt;/li&gt;
&lt;li&gt;guess branch spacing&lt;/li&gt;
&lt;li&gt;construct independent cliques, given branch spacing.&lt;/li&gt;
&lt;li&gt;Guess branch point&lt;/li&gt;
&lt;li&gt;construct markov-blanket, \(\mathcal{MB}\).&lt;/li&gt;
&lt;li&gt;construct \(sigma_b\) and \(\mu_b\) from parent \(\mathcal{MB}\).&lt;/li&gt;
&lt;/ol&gt;


&lt;h1&gt;TODO &amp;amp; In-progress&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/attach.m&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_start_index.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;attachment/att_set_branch_index.m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Next

&lt;ul&gt;
&lt;li&gt;pre-flatten and pre_sort ll_*.

&lt;ul&gt;
&lt;li&gt;remove all calls to flatten_inputs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;precompute likelihood covariance matrix.

&lt;ul&gt;
&lt;li&gt;remove logic from &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;compute full branching ML using cached values.

&lt;ul&gt;
&lt;li&gt;identify connected components&lt;/li&gt;
&lt;li&gt;topological sort&lt;/li&gt;
&lt;li&gt;root-to-leaf evaluation over each CC.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>cleanup</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/21/cleanup"/>
   <updated>2013-08-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/21/cleanup</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_3&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Note new working path: &lt;code&gt;data_association_3&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Started realizing that there are a huge number of dead/obsolete files in the &lt;code&gt;data_association_2&lt;/code&gt; directory.  It&#39;s time to migrate to a clean code-base, &lt;code&gt;data_association_3&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Reorganization&lt;/h1&gt;

&lt;h2&gt;Setups&lt;/h2&gt;

&lt;p&gt;Realized that &lt;code&gt;tmp_setup_workspace.m&lt;/code&gt; is very valuable, but will probably need to change and evolve over time.  Created new directory called &lt;code&gt;setups/&lt;/code&gt;, where setup scripts will be stored, organized by date.  Related files that the setup scrip needs will be stored in the same directory, with a similar name to the script itself.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tmp_setup_workspace.m&lt;/code&gt; is now in &lt;code&gt;setups/setup_workspace_2013_08_21.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Format is: &lt;code&gt;setup_&amp;lt;type&amp;gt;_&amp;lt;date&amp;gt;.m&lt;/code&gt;.  Currently only type is &quot;workspace&quot;.
Related &quot;load&quot; files have format: &lt;code&gt;setup_&amp;lt;type&amp;gt;_&amp;lt;date&amp;gt;.mat&lt;/code&gt;   or if multiple files, &lt;code&gt;setup_&amp;lt;type&amp;gt;_&amp;lt;date&amp;gt;.&amp;lt;N&amp;gt;.mat&lt;/code&gt;, where N is an increasing integer starting at 1.&lt;/p&gt;

&lt;h2&gt;Mex files&lt;/h2&gt;

&lt;p&gt;Propose creating a new file &lt;code&gt;compile_mex_scripts.m&lt;/code&gt;.  If called with no arguments, it will compile all scripts that are uncompiled.  Optional &quot;force_recompile&quot; parameter.&lt;/p&gt;

&lt;h1&gt;Misc Notes/Issues&lt;/h1&gt;

&lt;h2&gt;split_correspondence&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;split_correspondence.m&lt;/code&gt; is no longer available.  It became obsolete some time ago, around when we moved from &lt;code&gt;clean_correspondence.m&lt;/code&gt; to &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When we return to end-to-end sampling, we&#39;ll need something like it.  Alternatively,  maybe full-rebuilding of the split curves is fast enough, now that we&#39;ve mexed the bottlenecks?&lt;/p&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Not totally done, but we can at least run the workspace setup file.&lt;/p&gt;

&lt;p&gt;Will save the current workspace, re-open matlab, and see what issues arise as we continue to work toward the short-term goal of implementing branching-curve marginal likelihood.&lt;/p&gt;

&lt;p&gt;Workspace saved to &lt;code&gt;tmp/workspace_2013_08_21.mat&lt;/code&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Branching curve clique-tree</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/19/work-log"/>
   <updated>2013-08-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Thinking about attachments.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Organize into connected components.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Reviewing ML code, with considerations to generalizing for attachments.&lt;/p&gt;

&lt;p&gt;Can we exploit the structure in the branching-prior covariance matrix to speed-up inversion in the marginal likelihood?  No obvious way.&lt;/p&gt;

&lt;p&gt;Why did we abandon the clique-tree method?  Because it wasn&#39;t clear that the perturb-models would be compatible with it.  Now, it seems like it might be, as long as we set the markov-order high enough.  More importantly, it might be &lt;em&gt;necessary&lt;/em&gt;, because after adding attachment, the covariance matrix is growing too large to allow direct evaluation.&lt;/p&gt;

&lt;p&gt;Construct clique tree per-track, conditioned on parent point.  In case of base-curves, the parent point is the prior.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;New fields&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;branch parent

&lt;ul&gt;
&lt;li&gt;curve index&lt;/li&gt;
&lt;li&gt;point index&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;branch distance

&lt;ul&gt;
&lt;li&gt;i.e. starting index of self&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gaussian structure

&lt;ul&gt;
&lt;li&gt;in canonical form&lt;/li&gt;
&lt;li&gt;no position variance&lt;/li&gt;
&lt;li&gt;no branching variance&lt;/li&gt;
&lt;li&gt;yes branch distance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;Inferring branch distance&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The reconstructed lateral branch curves are usually have gaps of several millimeters from their parent branch.  Assigning an index of zero to the first point will almost certainly result in a worse marginal likelihood, because these gaps are not well-modelled.&lt;/p&gt;

&lt;p&gt;Instead, we should attempt to infer the branch distance(or better yet, try to marginalize over it).&lt;/p&gt;

&lt;p&gt;Spent some time trying to derive a &quot;fast update&quot; for covariance matrices for inferring branch distance.  In this scheme, the matrices are computed assuming branch distance is zero, and then a easily computed delta matrix is added to account for non-zero branch distance.&lt;/p&gt;

&lt;p&gt;After some work, it looks like the smoothing variance kernel is too complicated to permit such a method.  It&#39;s just easier to recompute the entire covariance matrix.&lt;/p&gt;

&lt;h2&gt;Constructing branching clique-tree&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Construct individual cliques trees, assuming zero position variance (but including branch distance).  Call this the &quot;raw prior.&quot;&lt;/li&gt;
&lt;li&gt;Given a branch-point-index, compute the markov-blanket of the data points on the parent.&lt;/li&gt;
&lt;li&gt;Given raw prior and markov-blanket, construct conditional clique  node from raw clique node.&lt;/li&gt;
&lt;li&gt;Given clique tree, multiply and marginalize from tips to root.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;All cliques will now be stored in track structure (currently named &lt;code&gt;Corrs&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to construct conditional prior?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\mathcal{MB}\) - indices of the parent curve markov blanket, centered at branch point.&lt;/li&gt;
&lt;li&gt;\(y_\mathcal{MB}\) - observations in the markov blanket&lt;/li&gt;
&lt;li&gt;\(x_b\) - branch point&lt;/li&gt;
&lt;li&gt;\(\Sigma_b\) - branch point predictive covariance, conditioned on markov blanket&lt;/li&gt;
&lt;li&gt;\(x_c\) - child curve points, relative to branch point.&lt;/li&gt;
&lt;li&gt;\(x_C\) - child curve points, relative to world origin&lt;/li&gt;
&lt;li&gt;\(N\) - number of points in child curve.&lt;/li&gt;
&lt;li&gt;\(\Sigma_c\) - curve predictive covariance, conditioned on branch point&lt;/li&gt;
&lt;li&gt;\(\Sigma_C\) - curve predictive covariance, conditioned on markov blanket&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;div&gt;
The branch point distribution is given by the GP predictive distribution:
    
\[
p(x_b | y_\mathcal{MB}) = \mathcal{N}(\mu_b, \Sigma_b) \]

where 

\[
\begin{align}
    \mu_b &amp;= K_* \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} y_{MB} \\
    \Sigma_b &amp;= K_b - K_*^\top \left(K_{\mathcal{MB}} + S^{-1}\right)^{-1} K_*

\end{align}
\]

Here, \(K_*\) is the kernel of the branch index vs. the markov blanket indices, \(k(t_\mathcal{MB}, t_b)\).
&lt;/div&gt;


&lt;p&gt;The conditional child curve distribution is&lt;/p&gt;

&lt;div&gt;
\[ p(x_C \mid x_b) = p(x_c) = \mathcal{N}(x_b, \Sigma_c) \]

Here, \(Sigma_c\) is the raw curve covariance, whatever it may be.
&lt;br /&gt;
The marginal over the child curve arises due to the linear combination of random variables \(x_c\) and \(x_b\):

\[
x_C = x_c + \left ( \begin{array}{c}I\\I\\ \ldots \\I \end{array}\right) x_b
\]

The corresponding distribution is:

\[
p(x_C | y_\mathcal{MB}) = \mathcal{N}(\mu_C, \Sigma_C)
\]

where

\[
\begin{align}
\mu_C &amp;= \left( \begin{array}{c}I\\I\\ \ldots \\I \end{array}\right) \mu_b \\
\Sigma_C &amp;= \Sigma_c + \left( \begin{array}{c}I\\I\\ \ldots \\I \end{array}\right) \Sigma_b \left ( I \; I \ldots I \right )
\end{align}
\]
&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>Saturday Thoughts - Enabling Non-gaussian models by using Gaussian models as proposal distributions</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/17/work-log"/>
   <updated>2013-08-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/17/work-log</id>
   <content type="html">&lt;p&gt;The different between the models we&#39;re fitting and the models we&#39;d really like to use is that our model doesn&#39;t have a way to prefer certain branch angles or internode-distance.&lt;/p&gt;

&lt;p&gt;Unfortunately, adding those features would break the Gaussian-ness of our model.  So we couldn&#39;t use it to evaluate marginal likelhoods, which we need to evaluate curve-fragemnt correspondences, i.e. triangulation.&lt;/p&gt;

&lt;p&gt;However, what&#39;s notable is that is that our models are invriably more permissive than the models we&#39;d prefer.  So any model allowed under our preferred models would certainly be permitted by our &lt;em&gt;worse-but-Gaussian&lt;/em&gt; model.&lt;/p&gt;

&lt;p&gt;So even though we can&#39;t distinguish between different species with our weaker model, we can triangulate with it.  And once we have a triangulation, we can switch to a stronger model to do classification.  Since the data is so strong, the posterior under the weak model is still extremely peaked.  We can do importance sampling to marginalize the strong model, using the weak model as a proposal distribution.&lt;/p&gt;

&lt;p&gt;This also gives us an approach to using non-gaussian likelihoods (e.g. pixel-based likelihoods).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;use matlab to construct gaussian model.&lt;/li&gt;
&lt;li&gt;save and read into C++ code&lt;/li&gt;
&lt;li&gt;Use C++ code for sampling full (non-Gaussian) model&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>Improved indexing; Retraining; Distinguishing between camera and plant motion</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/16/work-log"/>
   <updated>2013-08-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Improved Indexing (ctd)&lt;/h1&gt;

&lt;p&gt;Finished debugging changes to &lt;code&gt;corr_to_likelihood&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Recap: since projected model curves are discretely sampled at coarse intervals, multiple observed points may correspond to the same model point.  The results below show this. The projected model curve is sampled every 2 pixels (&lt;code&gt;index_delta_2d&lt;/code&gt;), so each model point has between 2 and 3 corresponding data points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-16-bad-indexing.png&quot; alt=&quot;old indexing results in aliasing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The new scheme post-processes the indexes by linearly interpolating the model curve and projecting the data point onto the neighboring line segments.  Resulting indices are much improved:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-16-better-indexing.png&quot; alt=&quot;new indexing scheme permits continuous (between-point) correspondences, which results in better indexing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that viewing angle distorts the correspondence angles somewhat.  Non-perpendicular correspondence lines may be simply due to non-orthogonal viewing direction.&lt;/p&gt;

&lt;p&gt;Since coarse sampling is no longer an issue, we can increase the 2D sample period and still get good results.  Below is the result after increasing 2D sampling period from 2 to 5:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-16-better-indexing-in-spite.png&quot; alt=&quot;new indexing scheme permits continuous (between-point) correspondences, which results in better indexing&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Improved training&lt;/h2&gt;

&lt;p&gt;This has implications on training results.  Re-running training using &lt;code&gt;exp_2013_08_11_train_all&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0030
            noise_variance: 1.0805
         position_variance: 1.6270e+04
             rate_variance: 0.2904
perturb_smoothing_variance: 1
     perturb_rate_variance: 1
 perturb_position_variance: 1
             perturb_scale: 2.5000

Final ML: -9.094636e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Ind-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0034
            noise_variance: 0.3472
         position_variance: 1.6458e+04
             rate_variance: 0.2605
perturb_smoothing_variance: 1.4186e-06
     perturb_rate_variance: 3.0555e-04
 perturb_position_variance: 0.5467
             perturb_scale: 2.5000

Final ML: -6.203953e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;OU-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0035
            noise_variance: 0.3486
         position_variance: 1.6440e+04
             rate_variance: 0.2587
perturb_smoothing_variance: 1.4874e-06
     perturb_rate_variance: 3.6269e-04
 perturb_position_variance: 0.7241
             perturb_scale: 2.3364

Final ML: -6.156721e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;SqExp-Perurb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0035
            noise_variance: 0.3479
         position_variance: 1.6246e+04
             rate_variance: 0.2745
perturb_smoothing_variance: 1.5495e-06
     perturb_rate_variance: 4.1614e-04
 perturb_position_variance: 0.6613
             perturb_scale: 0.9654

Final ML: -6.159716e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Awesome news:  perturb smoothing variance is now non-negligible!&lt;/strong&gt;  There must have been so much IID noise resulting from bad indexing that it totally masked the perturb smoothing variance.&lt;/p&gt;

&lt;p&gt;The totally validates our efforts to fix indexing.  Before, the model was fundamentally broken; bad indexing was preventing us from making any correct inferences beyond a certain level of granularity.  By fixing indexing, we&#39;re suddenly able to everything clearly, whereas before we were squinting through a noisy haze.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other observations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ML is much lower compared to the badly indexed results, which were one the order of -8000 (&lt;a href=&quot;/ksimek/research/2013/08/14/work-log/&quot;&gt;according to this post&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Noise variance dropped from 0.68 to 0.35.&lt;/li&gt;
&lt;li&gt;Smoothing variance has increased, probably because we attribute fewer deviations to IID noise.  Great!&lt;/li&gt;
&lt;li&gt;Global rate variance is lower, while perturb rate variance roughly tripled.&lt;/li&gt;
&lt;li&gt;Perturb scale dropped slightly.  Since noise variance can&#39;t explain independent deviations, the perturb-model takes over, becomes closer to independent.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Lets see if anything interesting comes out of our reconstructions...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;OU-perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;az = 24;
el = 16;
axis_ = [ 70.0000  110.0000   50.0000  110.0000   47.8040  224.0467 ]

exp_2013_08_11_reconstruct_for_web(test_Corrs_ll_2, retraining_results{3}, 3, axis_, el, az, num_views, &#39;/Users/ksimek/src/research_blog/img/2013-08-16-ou-model-%d.png&#39;, &#39;/ksimek/research/img/2013-08-16-ou-model-%d.png&#39;, &#39;ou-reconstruct-anim&#39;, true)
&lt;/code&gt;&lt;/pre&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-16-ou-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-anim&quot; style=&quot;width:137px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;SqExp-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-sqexp-model-9.png&quot;
        ]

    construct_animation($(&quot;#sqexp-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;sqexp-reconstruct-anim&quot; style=&quot;width:136px&quot;&gt; &lt;/div&gt;


&lt;h2&gt;Removing camera-based motion&lt;/h2&gt;

&lt;p&gt;We can remove perturbations that arise from poor camera calibration by assuming it is captured in the linear and offset perturbations; under this assumption, the remaining cubic-spline smooth perturbations capture the true plant motion.&lt;/p&gt;

&lt;div&gt;Removing linear and offset perturbations is as simple as removing their contributions to \(K^*\) in yesterday&#39;s equation for the mean of the predictive distribution.&lt;/div&gt;


&lt;p&gt;Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;reverse = false(1,num_tracks);
reverse([1 2 4 5 6 8 9 10 11 12 14 15]) = true;
% error above, should omit 11:
% reverse([1 2 4 5 6 8 9 10 12 14 15]) = true;
exp_2013_08_16_visualize_smooth_perturbations( ...
        test_Corrs_ll_2, ...
        retraining_results{3},  ...
        3, axis_, el, az, num_views,  ...
        &#39;/Users/ksimek/src/research_blog/img/2013-08-16-ou-model-smooth-%d.png&#39;, ...
        &#39;/ksimek/research/img/2013-08-16-ou-model-smooth-%d.png&#39;, ...
        &#39;ou-reconstruct-smooth-anim&#39;, reverse)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;removing-camera-perturbation&quot;&gt;&lt;/a&gt;
&lt;em&gt;OU-Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-16-ou-model-smooth-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-smooth-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-smooth-anim&quot; style=&quot;width:250px&quot;&gt; &lt;/div&gt;


&lt;p&gt;It&#39;s notable that the little curves at the top don&#39;t move.  Attaching them to the large main step will allow them to move, which should improve ML.&lt;/p&gt;

&lt;p&gt;It should also significantly affect training if we train with attachments in place.  Perturb_position_variance should be responsible for less of the variance, and perturb_smoothing_variance should explain more.&lt;/p&gt;

&lt;h1&gt;Attachments&lt;/h1&gt;

&lt;p&gt;Tasks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Create new attachments structure.  Indicates which curve and index the curve is attached to.&lt;/li&gt;
&lt;li&gt;Algorithm to convert N attachment to M track-sets, where M is the number of connected components in the attachment structure.  (in practice, attachment structure might be irrelevant, given the track-set structure).&lt;/li&gt;
&lt;li&gt;New function to construct prior matrix for track-sets, as opposed to individual tracks.&lt;/li&gt;
&lt;li&gt;New function to evaluate ML over track-sets.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Add attachments before training.&lt;/li&gt;
&lt;li&gt;Complete end-to-end training

&lt;ul&gt;
&lt;li&gt;input: path of training and curves&lt;/li&gt;
&lt;li&gt;automatically add attachments&lt;/li&gt;
&lt;li&gt;automatically reverse curves as needed&lt;/li&gt;
&lt;li&gt;save trained parameters somewhere&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Index Refinement; Mean-curve Reconstruction</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/15/work-log"/>
   <updated>2013-08-15T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/15/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing from yesterday.&lt;/p&gt;

&lt;p&gt;Tried changing index_delta from 0.5 to 0.25.  ML suddenly dropped a &lt;em&gt;lot&lt;/em&gt;.  Realized that oversampling the smoothed curves causes significant degree of many-to-one, and our code prevents skipping more than one or two points during correspondences.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main Idea:&lt;/strong&gt; Sampling period should be equal to (or close to) the data point-spacing.&lt;/p&gt;

&lt;h1&gt;Index-refinement&lt;/h1&gt;

&lt;p&gt;Allow indices to correspond to continuous values between the samples.&lt;/p&gt;

&lt;p&gt;Modified &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt; to post-process indices to search the neighboring line segments for a better value. Re-ran &lt;code&gt;tr_prep_likelihood&lt;/code&gt; to reconstruct Corrs collection.  ML improves after fix.&lt;/p&gt;

&lt;p&gt;Need to confirm improved indexing by visualizing results.  See &lt;code&gt;experiments/exp_2013_08_15_visualize_indices.m&lt;/code&gt;, still in progress.  Some apparent bugs in aforementioned changes, causing bad results.  Still investigating...&lt;/p&gt;

&lt;h1&gt;Mean-curve reconstruction&lt;/h1&gt;

&lt;p&gt;Last week, I derived the equation for finding the maximum posterior per-view reconstruction.  Now we have a formula for the unobserved mean curve, i.e. the curve that each view is perturbed from.  Iterestingly, they both have the same form:&lt;/p&gt;

&lt;div&gt;
\[
    \mu = K^* (S K + I)^{-1} S y
\]
&lt;/div&gt;


&lt;p&gt;Note that this is slightly different from the form used in a previous post.  This form is equivalent, and better reflects the form used by Williams and Rasmussen.&lt;/p&gt;

&lt;div&gt;Here, \(K\) is the prior covariance, \(S\) is the likelihood precision matrix, and \(y\) are the virtual observations in 3D.  \(K^*\) is the covariance between the observed points (columns) and the points to be predicted (rows). The difference between the per-view and the mean reconstruction is the form of \(K^*\):  for the per-view reconstruction, \(K^*\) uses the full prior covariance, whereas with the mean reconstruction, the perturb covariances are set to zero.&lt;/div&gt;


&lt;h2&gt;Usage&lt;/h2&gt;

&lt;p&gt;Modified &lt;code&gt;curve_max_posterior_2.m&lt;/code&gt; by adding an extra optional parameter, &lt;code&gt;kernel_2&lt;/code&gt;.  If set, it assumes you want a mean reconstruction instead of a per-view reconstruction, and uses &lt;code&gt;kernel_2&lt;/code&gt; to build \(K*\).&lt;/p&gt;

&lt;p&gt;Added an extra flag to &lt;code&gt;experiments/exp_2013_08_11_reconstruct_for_web.m&lt;/code&gt;, which if set to true, will also plot the mean curve.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Background ML bugs; Why is Foreground Noise Variance so large?</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/14/work-log"/>
   <updated>2013-08-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/14/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;ML Validity Testing&lt;/h1&gt;

&lt;p&gt;In yesterday&#39;s test, I hadn&#39;t realized that the training ML didn&#39;t include all of the curves, only the foreground curves were included.  Rerunning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference ML: -5.1552e+04
Training ML: -5.0786e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Okay, we&#39;re back in the ballpark -- Within 1.5% of the reference.&lt;/p&gt;

&lt;p&gt;Found the other issue: the roundabout way I was using to generate the training covariance matrices was ignoring the user-specified position_variance_2d.  Results now match:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference ML: -5.1552e+04
Training ML: -5.1552e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, none of this explains why training is causing noise variance to collapse so low.  All discovered problems were merely bugs in the validation logic.  At least we know the training ML logic is valid.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Quick inspection shows that 2D curves are, indeed, pre-smoothed.  This means that noise variance can collapse to near-zero when fitting 2D curve.&lt;/p&gt;

&lt;p&gt;The new question becomes: why doesn&#39;t it occur in the foreground (3D) model?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Try re-indexing...&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% update params related to smoothing variance
params_2 = tbc_.params;
params_2.smoothing_variance = training_results{1}.smoothing_variance;
params_2.noise_variance = training_results{1}.noise_variance;
% re-construct likelihood (including indicies)
test_Corrs_ll_2 = tr_prep_likelihood(test_Corrs, data_, params_2);
% re-run training
train_params_done = tr_train(test_Corrs_ll_2, training_results{2}, 400, 3);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0011
            noise_variance: 0.6846
         position_variance: 1.3597e+04
             rate_variance: 0.3042
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.3013e-06
 perturb_position_variance: 0.6621
             perturb_scale: 2.9795

Final ML: -7840.140322
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare against results prior to re-indexing: version:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654

Final ML: -8049.9e+03
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we&#39;ve improved, but nowhere near the background model&#39;s ML of 4611.9.  Note that curves got smoother and less noisy.  More correlation, more variance pushed into the perturbations.  (why the f*** perturb_smoothing_variance is just sitting there like an idiot is still beyond me).&lt;/p&gt;

&lt;h2&gt;Miscellaneous thoughts&lt;/h2&gt;

&lt;p&gt;Need to visualize ll_means against smoothed curve.  The perturbations should be correlated.  Maybe even plot them?  Note the perturbations need to be considered only in the directions parallel to the image plane.  &lt;strong&gt; This has never been done, and is necessary to validate the index-estimation in &lt;code&gt;correspondence/corr_to_likelihood&lt;/code&gt;.&lt;/strong&gt; Can we visualize after removing rate and offset components?  Yes: difference between ll_means and per-view reconstructed curve.&lt;/p&gt;

&lt;p&gt;Consider smarter smoothing in &lt;code&gt;corr_to_likelihood&lt;/code&gt; -- using posterior max instead of &lt;code&gt;csaps&lt;/code&gt;.    Could give better index estimation if the visualization test shows problems.&lt;/p&gt;

&lt;p&gt;What if we were using a 3D likelhood for background curves too? Could we still expect the BG ML to be insanely high, and the noise variance to be insanely peaked?  Backproject, estimate 3d noise variance, estimate index set.  The farther away it gets, the more variance in the correlated points.  Which means lower ML, right?  But training will push variance lower.&lt;/p&gt;

&lt;p&gt;Note that larger noise variance in FG model will explain away bad triangulations.  Perturb variances also explain to some extent, but maybe they aren&#39;t sufficient to explain enough of it.&lt;/p&gt;

&lt;p&gt;Why is perturb_smoothing_variance basically zero?  If it was higher, it could explain more of the traingulation error, and allow noise variance to drop.  Should we be using a different pertubation model?  Maybe Brownian motion instead of integrated brownian motion?  Visualizing perturbations would be informative here.&lt;/p&gt;

&lt;p&gt;Do smooth perturbations follow a different dynamics model than linear and offset perturbations?  Can we force it to be larger?  what if it was the only option for modelling perturbations?   It&#39;s true that a small amount of smoothing variance can result in a huge amount of marginal point variance, and large variances kill ML&#39;s.  Probably a mean-reverting model is more sensible -- Ornstein Ulenbeck process, perhaps?  Or SqExp?  I avoided these in the past, because it changes the form of the marginal curve covariances -- they&#39;re no longer purely cubic-spline processes.  But I never considered the fact that we need to model triangulation error.&lt;/p&gt;

&lt;p&gt;Observations: setting perturb_smoothing_variance to exactly zero has no change in ML.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Consider tying foreground and background noise variance during training.&lt;/strong&gt;  &amp;lt;---  This is the most pragmatic solution.  Avoids getting mired in details, and acknowledges what we know to be true: image noise arises from the same process in foreground and background models.&lt;/p&gt;

&lt;p&gt;Possibly the fact that we allow a nonzero position_mean in 2D but not in 3D is the issue?&lt;/p&gt;

&lt;h1&gt;Finer-grained index estimation&lt;/h1&gt;

&lt;p&gt;Got it!  In &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;, we have two parameters that determine how fine-grained the sampling is along the smoothed curve.  Each observed curve is then matched against the sampled curve.  The sampling period is 2 pixels, which means there&#39;s an average error of about 1 pixel in each index estimate.&lt;/p&gt;

&lt;p&gt;Reducing the sampling period to 1 pixel and re-training gives:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0014
            noise_variance: 0.3986
         position_variance: 1.3555e+04
             rate_variance: 0.3100
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 3.3992e-04
 perturb_position_variance: 0.6874
             perturb_scale: 2.3823

Final ML: -6357.585946
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Although we only slightly changed the sampling period, the final ML improved significantly.  The noise variance dropped from 0.7 to 0.4, too.  Perturb rate variance changed by 2 orders of magnitude!&lt;/p&gt;

&lt;p&gt;Reducing sampling period to 0.5:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0018
            noise_variance: 0.3060
         position_variance: 1.3499e+04
             rate_variance: 0.3098
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 4.3411e-04
 perturb_position_variance: 0.8152
             perturb_scale: 2.4095

Final ML: -5664.35
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The upward ML trend continues, but the noise variance appears to be flattening out.  The perturb_position_variance jumped upward unexpectedly.&lt;/p&gt;

&lt;p&gt;This might explain all of the disparity between the 2D and 3D noise variances.  Unfortunately, we can&#39;t reduce the sampling period to 0.0004, because the runtime complexity of the matching  is O(N&lt;sup&gt;2&lt;/sup&gt;), where N is the number of sampled points.&lt;/p&gt;

&lt;p&gt;Better idea: after finding the optimal region in the matching algorithm, improve it by projecting the point onto the line segments neighboring the matched point.  Constant-time, and significantly better!&lt;/p&gt;

&lt;p&gt;Another thought: if the rasterization error was approx. 1 pixel, the sampling error could be reduced to the point where the rasterization error dominated (possibly a sampling period of  0.5 or 0.01 would achieve this).  That way, both 2D and 3D noise sigma would be dominated by rasterization error, and would train to similar values.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;implement post-match index improvement.&lt;/li&gt;
&lt;li&gt;Plot reconstruction residuals, look for correlation model.

&lt;ul&gt;
&lt;li&gt;Goal: determine if residuals are truely independent, and belong in the noise bucket.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Try training BG and FG together, with the same noise variance.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Theoretical Rate variance bug; Training background curve model</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log"/>
   <updated>2013-08-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/13/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Curve reversing thoughts&lt;/h1&gt;

&lt;p&gt;The reversed curve issue only really matters during training.  Our tests show that curve-flip moves will mix well, even if the maximum isn&#39;t always correct.  Adding connections between parent and child curves should resolve these issues.&lt;/p&gt;

&lt;p&gt;During training, derive curve direction from ground-truth.&lt;/p&gt;

&lt;h1&gt;Theoretical Rate Variance (take 2)&lt;/h1&gt;

&lt;p&gt;Realized that my last attempt at this had two bugs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I used &lt;code&gt;rand()&lt;/code&gt; instead of &lt;code&gt;randn()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;I was normalizing by the &lt;em&gt;squared&lt;/em&gt; vector magnitude.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Fixing this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir = randn(3,10000000);
dir = bsxfun(@times, dir, 1./sqrt(sum(dir.^2)));
var(dir(:))

    ans =
        0.3332
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare this to earlier theoretical results of ~0.23.&lt;/p&gt;

&lt;p&gt;This new result is interesting, because it is 25% higher than the emperical results we&#39;ve been getting.  I&#39;m guessing that the fact all of the curves point upward reduces the variance. To prove, we&#39;ll force all points to be in the top hemishphere:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; dir = randn(3,10000000);
 dir(3,:) = abs(dir(3,:));
 dir = bsxfun(@times, dir, 1./sum(dir.^2));

 var(dir(:))

    ans =
        0.3149
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep.  And in practive, our values take on an even smaller range of directions.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Repeating for the 2D case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dir = randn(2,10000000);
dir = bsxfun(@times, dir, 1./sqrt(sum(dir.^2)));
var(dir(:))

    ans =
        0.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This strongly suggests a pattern of variance being 1/D.&lt;/p&gt;

&lt;h1&gt;Connection test&lt;/h1&gt;

&lt;p&gt;does connecting each of the curves result in better ML?  Do we need to marginalize?&lt;/p&gt;

&lt;h1&gt;training background model&lt;/h1&gt;

&lt;p&gt;construct training ML for background ML
construct mats for bg curve models.&lt;/p&gt;

&lt;p&gt;Result: &lt;code&gt;train/tr_train_bg.m&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 1.7837e+04
     rate_variance_2d: 0.5000
    noise_variance_2d: 9.4597e-04
smoothing_variance_2d: 0.0157
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that noise_variance_2d is so low.  We expected it to be on the order of 1 pixel.  More discussion on this later.&lt;/p&gt;

&lt;p&gt;I retrained the BG model for &lt;em&gt;only&lt;/em&gt; the foreground curves, and evaluated the ML under it.&lt;br/&gt;
         position_mean_2d: [2x1 double]
     position_variance_2d: 5.8116e+03
         rate_variance_2d: 0.5000
        noise_variance_2d: 4.8105e-04
    smoothing_variance_2d: 0.0053&lt;/p&gt;

&lt;p&gt;Smaller noise variance, smaller smoothing variance.  This shrinking of variance with smaller training set is typical overfitting behavior.  Not of much concern.&lt;/p&gt;

&lt;p&gt;Here&#39;s the comparison against the ML for the trained foreground model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bg model = 4611.886746
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not good.  The FG model on true foreground curves should have a better marginal likelihood than the same curves under the BG model.&lt;/p&gt;

&lt;p&gt;Some questions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Why is bg noise variance so low??

&lt;ul&gt;
&lt;li&gt;did we smooth the detected curves before storing them?&lt;/li&gt;
&lt;li&gt;If so, why isn&#39;t the foreground model lower?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why is the background model so much better than the foreground model?

&lt;ul&gt;
&lt;li&gt;We expect foreground curves to have a higher marginal likelihood under the foreground model than the background.&lt;/li&gt;
&lt;li&gt;could it be an indexing issue?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This warrants further investigation.&lt;/p&gt;

&lt;h2&gt;Other observations&lt;/h2&gt;

&lt;p&gt;If I force the noise variance to be equal to that of the fg model (0.72), the ML drops significantly (fg results reprinted for convenience):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; bg model = -9413.1e+03
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&#39;re back in business.  This is a good sanity check, but it doesn&#39;t explain why we can&#39;t get similar noise variances for both models when training.&lt;/p&gt;

&lt;p&gt;Possibly the smoothing variance would need to change in this case.  Retraining, with noise_variance forced to 0.72:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     position_mean_2d: [2x1 double]
 position_variance_2d: 5.8116e+03
     rate_variance_2d: 0.5000
    noise_variance_2d: 0.7204
smoothing_variance_2d: 2.7394e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Smoothing variance dropped dramatically.  ML comparison (fg results reprinted for convenience):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bg model = -9214.632874
fg model = -8049.097873
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that bg ML didn&#39;t significantly change (-2%) after optimizing noise variance (which did change a lot).&lt;/p&gt;

&lt;p&gt;Might be worthwhile visualizing the optimal fits with these parameters.  Are we oversmoothing?  undersmoothing?  These would suggest a bug.&lt;/p&gt;

&lt;h2&gt;ML Validity Testing&lt;/h2&gt;

&lt;p&gt;Running reference ml:
    data_2 = init_data_curves_ml(data_, bg_train_params_done_force)
    sum([data_2.curves_ml{:}])&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    ans =
      -5.1552e+04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Very different from the training implementation.  Need to dig deeper to determine the cause.&lt;/p&gt;

&lt;h2&gt;Misc Thoughts&lt;/h2&gt;

&lt;p&gt;Do we need to re-estimate the index set during training of the FG model?&lt;/p&gt;

&lt;p&gt;Iterate: train, re-index, repeat.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;investigate disparity between training ML and reference ML for background curves.&lt;/li&gt;
&lt;li&gt;Further investigate the FG vs. BG marginal-likelihood issue.&lt;/li&gt;
&lt;li&gt;test the re-indexing approach to FG model training.&lt;/li&gt;
&lt;li&gt;re-build end-to-end sampler.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Re-run training, Re-reconstruction, Curve-Flipping</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log"/>
   <updated>2013-08-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/11/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Re-training&lt;/h1&gt;

&lt;p&gt;Re-ran training after several bug-fixes.&lt;/p&gt;

&lt;h2&gt;New Files:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;train/tr_train_all.m&lt;/code&gt;  - Utility method for training all four models.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;experiments/exp_2013_08_11_train_all.m&lt;/code&gt;  - end-to-end training example; recreates results here.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;All results generated by &lt;code&gt;exp_2013_08_11_train_all.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0024
            noise_variance: 1.2308
         position_variance: 1.6072e+04
             rate_variance: 0.2743
perturb_smoothing_variance: 1
     perturb_rate_variance: 1
 perturb_position_variance: 1
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7192
         position_variance: 1.6132e+04
             rate_variance: 0.2451
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1292e-06
 perturb_position_variance: 0.4849
             perturb_scale: 2.5000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;OU-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0019
            noise_variance: 0.7204
         position_variance: 1.6111e+04
             rate_variance: 0.2465
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1296e-06
 perturb_position_variance: 0.5931
             perturb_scale: 2.4654
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: 0.0018
            noise_variance: 0.7207
         position_variance: 1.6117e+04
             rate_variance: 0.2480
perturb_smoothing_variance: 7.1854e-19
     perturb_rate_variance: 1.1355e-06
 perturb_position_variance: 0.5172
             perturb_scale: 0.9202
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised to see that noise-variance only changed by a factor of 10, not 100.  However, the resulting noise_variance is right in the range that you&#39;d expect arising from pixel-grid rasterization error.&lt;/p&gt;

&lt;p&gt;OU perturb-scale is lower than in the last case, and perturb position and rate variance is lower, too.&lt;/p&gt;

&lt;p&gt;SqExp perturb-scale is higher than in the last case, and perturb rate variance is lower.&lt;/p&gt;

&lt;p&gt;Lower position and rate variance makes sense after correcting curve-reversals.&lt;/p&gt;

&lt;p&gt;However, since we trimmed the pre-tails, a higher global and perturb position variance should result.  The result we&#39;re seeing is a combination of these competing effects.&lt;/p&gt;

&lt;h1&gt;Reconstructions&lt;/h1&gt;

&lt;p&gt;Some curves are flipped; need to an approach that will detect and correct flipped curves.&lt;/p&gt;

&lt;p&gt;Images and javascript generated by &lt;code&gt;../experiments/exp_2013_08_11_reconstruct_for_web.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ind-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-ind-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ind-model-9.png&quot;
        ]

    construct_animation($(&quot;#ind-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ind-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;OO-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-ou-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-ou-model-9.png&quot;
        ]

    construct_animation($(&quot;#ou-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;ou-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;p&gt;&lt;em&gt;SqExp-perturb model&lt;/em&gt;&lt;/p&gt;

&lt;script&gt;
$(function(){
    var urls = [
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-1.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-2.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-3.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-4.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-5.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-6.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-7.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-8.png&quot;,
        &quot;/ksimek/research/img/2013-08-11-sqexp-model-9.png&quot;
        ]

    construct_animation($(&quot;#sqexp-reconstruct-anim&quot;), urls);
});
&lt;/script&gt;


&lt;div id=&quot;sqexp-reconstruct-anim&quot; style=&quot;width:264px&quot;&gt; &lt;/div&gt;


&lt;h1&gt;Detecting and Flipping Curves&lt;/h1&gt;

&lt;p&gt;Experiment: &lt;code&gt;../experiments/exp_2013_08_11_flip_curves.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Result: doesn&#39;t really work.  Lots of false negatives.&lt;/p&gt;

&lt;p&gt;Algorithm output: 2     4     5     6     8    10    12
Ground Truth: 1 2 4 5 6 8 9 10 11 12 14 15&lt;/p&gt;

&lt;p&gt;Not really sure why this is failing.  After flipping, most of these curves are closer to the origin, which is promoted by position_variance.  And any tip-perturbations should be better modelled after flipping.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Think more on curve-reversing&lt;/li&gt;
&lt;li&gt;Central curve extraction&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;li&gt;end-to-end correspondence sampling&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Pre-tails fix</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/10/work-log"/>
   <updated>2013-08-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/10/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Pre-tails issue&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Bug&lt;/strong&gt;: Smallest index of reconstructed curves is significantly greater than 0.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp_setup_workspace
min([test_Corrs_ll{1}.ll_indices{:}])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This results in long pre-tails, as seen in this image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-10-pretails.jpg&quot; alt=&quot;pretails&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Recall that reconstruction occurs by doing (1) rough triangulation, (2) smoothing, then (3) re-triangulating against the smoothed curve.  The initial triangulation usually results in a very bad index set, with spacing far larger than it should be, due to poor localization by maximum likelihood.  The subsequent smoothing causes the curve to stretch out longer than it should, so when re-triangulation occurs, ends are cut off.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Solution &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After re-triangulating, re-index so the minimum index is zero.&lt;/p&gt;

&lt;p&gt;Change to &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;145  % re-index by subtracting minimum index
146  min_index = min([Corr.ll_indices{:}]);
147  Corr.ll_indices = cellfun(@(x) x - min_index, Corr.ll_indices, &#39;UniformOutput&#39;, false);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt; Fallout &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This likely had an effect on training results, because marginal prior variance of the initial point was over-estimated, because it&#39;s index was over-estimated.&lt;/p&gt;

&lt;h1&gt;Cleanup&lt;/h1&gt;

&lt;p&gt;I&#39;m afraid I made a mess of things yesterday when I was addressing the noise_variance issue in training.  Need to review the end-to-end systems for training, reconstruction and marginal likelihood evaluation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is training world-variance scaled everywhere?&lt;/li&gt;
&lt;li&gt;is training world-variance projecting to 1.0?&lt;/li&gt;
&lt;li&gt;is training-ml equal to inference-ml?&lt;/li&gt;
&lt;li&gt;are the reconstructed results sensible?&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;handle reversed curves&lt;/li&gt;
&lt;li&gt;Retrain all models since the following changes

&lt;ul&gt;
&lt;li&gt;reversal fixes&lt;/li&gt;
&lt;li&gt;noise variance fix&lt;/li&gt;
&lt;li&gt;&quot;pre-tail&quot; index fix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;central curve extraction&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;li&gt;end-to-end sampling system&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Refactoring, cleanup, bug fixes</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/09/work-log"/>
   <updated>2013-08-09T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/09/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;15169&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Goals&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Move &lt;code&gt;test/tmp*&lt;/code&gt; to &lt;code&gt;experiment/&lt;/code&gt; directory.&lt;/li&gt;
&lt;li&gt;Overlay reconstruction onto images.&lt;/li&gt;
&lt;li&gt;Test effect of reversing curves on ML.&lt;/li&gt;
&lt;li&gt;Train background curve model.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Git Blog Mess&lt;/h1&gt;

&lt;p&gt;Got sidetracked after screwing up a git commit of the research blog.  Not sure the cause, but several files were deleted from the &quot;source&quot; branch.  Changed the &quot;preview&quot; rake target so it builds to /tmp/research_blog_site, instead of the source directory.  Hopefully this will avoid these issues in the future.&lt;/p&gt;

&lt;h1&gt;End-to-end experiment&lt;/h1&gt;

&lt;p&gt;Created an experiment file that recreates yesterday&#39;s results from scratch: &lt;code&gt;exp_2013_08_09_animated_reconstruction.m&lt;/code&gt;.  Since it runs training, it takes about 5 minutes to run.&lt;/p&gt;

&lt;p&gt;Also broke out reconstruction code into function in &lt;code&gt;reconstruction/reconstruct_views.m&lt;/code&gt;.&lt;/p&gt;

&lt;h1&gt;Overlay reconstruction onto images&lt;/h1&gt;

&lt;p&gt;See &lt;code&gt;test/tmp_vis_overlay.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I&#39;m seeing some weirdness in the reconstructions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Curves are too stiff&lt;/li&gt;
&lt;li&gt;long pre-tails on a couple of curves.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;Stiff Curves Solved&lt;/h1&gt;

&lt;p&gt;Figured out what was causing stiff curves.  I forgot that during training, all precisions are stored with noise_variance fixed at 1.0, and then are scaled on the fly.  However, during visualization, that scaling doesn&#39;t occur; the precisions are assumed to be stored at the desired scale.  i.e. a dumb bug.&lt;/p&gt;

&lt;p&gt;The new reconstructions now show a moderate amount of curvature, compared to their pre-bug stiff counterparts.&lt;/p&gt;

&lt;p&gt;Found another bug:  when constructing the &quot;unscaled&quot; precisions in &lt;code&gt;tr_prep_likelihood.m&lt;/code&gt;, I called &lt;code&gt;corr_to_likelihood&lt;/code&gt; with &lt;code&gt;params.noise_variance&lt;/code&gt; instead of 1.0.  Thus, if I understand correctly, the reported training value for noise-variance is 100x lower than it should be.&lt;/p&gt;

&lt;p&gt;That means the no-perturb model has a noise standard deviation on the order of 3.4 pixels and the perturb models are around 2.7.  This is closer to the range I was expecting, but I was hoping the perturb model stddev would be closer to 0.5, because it should arise only from pixel rasterization.  However, other sources of noise could be the curve detector, and also the cubic spline model might not be expressive enough to capture the model variance.&lt;/p&gt;

&lt;p&gt;I think I need to re-run end-to-end training and reconstruction to make sure there aren&#39;t any side-effects of these fixes.&lt;/p&gt;

&lt;p&gt;Committed to revision 15169&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Determine cause of long pre-tails.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Visualizing Results; New training method</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/08/work-log"/>
   <updated>2013-08-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h1&gt;Visualization&lt;/h1&gt;

&lt;p&gt;I reconstructed the curves using the models I trained yesterday.  I was able to recover both the overall structure and track it&#39;s motion over 9 views.  For each of these results, use the slider below to change between each of the 9 views (the changes are subtle).&lt;/p&gt;

&lt;p&gt;I&#39;m still struggling with smoothing variance being too low, which causes curves to be too straight.  For all of these results, I manually changed &lt;code&gt;smoothing_variance&lt;/code&gt; to be 0.1.&lt;/p&gt;

&lt;script&gt;
$(function(){
    var ind_urls = [
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ind-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#ind-reconstruction&quot;), ind_urls);


    var ou_urls = [
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-ou-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#ou-reconstruction&quot;), ou_urls);

    var sqexp_urls = [
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v1.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v2.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v3.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v4.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v5.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v6.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v7.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v8.png&quot;,
        &quot;/ksimek/research/img/2013-08-08-sqexp-reconstruction_v9.png&quot; 
        ]

    construct_animation($(&quot;#sqexp-reconstruction&quot;), sqexp_urls);

});
&lt;/script&gt;


&lt;h2&gt;Ind-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Below is the reconstruction for the independent-perturbation model.  The curves in each view are independently perturbed versions of central mean curves (not shown).&lt;/p&gt;

&lt;div id=&quot;ind-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;p&gt;It is unclear how much of this motion is due to camera miscalibration, and how much is actual plant motion.  Nevertheless, this shows that we can use the perturbation models to simultaneously triangulate and track over time.&lt;/p&gt;

&lt;p&gt;First, although each &lt;em&gt;view&#39;s&lt;/em&gt; perturbation is independent of the others, the perturbation is correlated between nearby points within the same view.  In other words, perturbations don&#39;t violate the smoothness constraint.&lt;/p&gt;

&lt;h2&gt;OU-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Next is the Ornstein-Ulenbeck perturbation model.  As opposed to the previous model, which modeled each view&#39;s perturbations as white noise, this model assumes Brownian motion over time.  Thus, we see a more naturally evolving time-series.&lt;/p&gt;

&lt;div id=&quot;ou-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;p&gt;The OU process models brownian motion&lt;/p&gt;

&lt;h2&gt;SQEXP-Perturb Reconstruction&lt;/h2&gt;

&lt;p&gt;Finally we have the squared-exponential perturbation model.  Now Brownian motion has been replaced by smooth motion.  I&#39;m doubtful that this is a natural motion model for these plants.  The scale parameter is so low, I question whether it has any significant effect.&lt;/p&gt;

&lt;div id=&quot;sqexp-reconstruction&quot; style=&quot;width: 200px&quot;&gt; &lt;/div&gt;


&lt;h1&gt;New Training Method&lt;/h1&gt;

&lt;p&gt;Need to determine why learned smoothing variance is so low.&lt;/p&gt;

&lt;p&gt;Is it even valid to do max-likelihood to train the parameters of the covariance function?&lt;/p&gt;

&lt;p&gt;Should we be training the smoothness parameter using the noiseless ground-truth data?&lt;/p&gt;

&lt;h2&gt;Two-pass Learning Procedure&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;First learn using noiseless data:

&lt;ol&gt;
&lt;li&gt;Set rate-variance to fixed ~0.23 (see &lt;a href=&quot;/ksimek/research/2013/08/07/work-log/#optimal-rate-variance&quot;&gt;yesterday&#39;s results&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Estimate  position_variance by emperical distribution over all point positions in ground truth.&lt;/li&gt;
&lt;li&gt;Estimate smoothing_variance by maximum likelihood over ground truth.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Learn the noise variance for the no-perturb model.

&lt;ol&gt;
&lt;li&gt;Find point correspondence between detected curves and corresponding ground truth curve.&lt;/li&gt;
&lt;li&gt;Compute variance between projected points and observed points.  This should maximize \(p(D \mid \Theta_0) \), where \(\Theta_0\) are the ground truth curves.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Learn perturb model parameters: noise_variance, perturb_{smoothing_variance, rate_varaince, position_variance}

&lt;ol&gt;
&lt;li&gt;Use nonlinear optimization to maximize \(p(D \mid \Theta_0) \), as defined below.  Start with no-perturb parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;


&lt;div&gt;Let \(\theta_0\) be the ground-truth curve, and \(\{\theta_i\}\) be the set of all (unobserved) per-view curves \(\theta_i\). Let \(S_i\) be the virtual point precision matrices, assuming noise variance \(\sigma_n^2 = 1\).  The likelihood conditioned on the ground truth data \(p(D \mid \theta_0)\) is given by: &lt;/div&gt;




&lt;div&gt; 
\begin{align}
p(D \mid \theta_0) &amp;= \int_{\{\theta_i\}}  p(D_i, \{\theta_i\} \mid \theta_0)  d\{\theta_i\} \\
                   &amp;= \int_{\{\theta_i\}} \prod_{i=1}^N p(\theta_i \mid \theta_0) p(D_i | \theta_i) d\{\theta_i\} \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  p(\theta_i \mid \theta_0) p(D_i | \theta_i) d\theta_i \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  \mathcal{N}(\theta_i; \theta_0, \Sigma_p) \mathcal{N}(D_i;  \theta_i, \sigma_n^2 S_i^{-1}) d\theta_i \\
                   &amp;= \prod_{i=1}^N \int_{\theta_i}  \mathcal{N}(D_i ; \theta_0, \Sigma_p + \sigma_n^2 S_i^{-1})
\end{align}
&lt;/div&gt;




&lt;div&gt; where  \(\Sigma_p\) is the perturbation variance. &lt;/div&gt;


&lt;p&gt;This should be an improvement over the current method, where we use only ground-truth labellings (not positions) and fit all parameters simultaneously.  This method assumed too much noise variance and not enough smoothness variance.&lt;/p&gt;

&lt;p&gt;Since we actually know the noiseless curves, we should train to that, to avoid the confounding of smoothness variance and noise variance.  The ground truth curves are aso stronger sources of evidence, compared to the curves reconstructed from data.&lt;/p&gt;

&lt;h1&gt;TODO:&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;reproject per-view reconstructions and overlay with original image.&lt;/li&gt;
&lt;li&gt;obtain &amp;amp; visualize unobserved &quot;central curve&quot;&lt;/li&gt;
&lt;li&gt;investigate the low smoothness variance.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Later TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;automatic curve reversing?&lt;/li&gt;
&lt;li&gt;add branching&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Training, Reversed Curves, and Theoretical Rate Variance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/07/work-log"/>
   <updated>2013-08-07T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/07/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Visualized results after capping likelihood variance.  As expected, degree of spreading stops growing as perturb_rate_variance continues to grow.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Tasks&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;manually flip some curves and see if model changes&lt;/li&gt;
&lt;li&gt;try automatically determining which to curves need flipping&lt;/li&gt;
&lt;li&gt;try to get training and visualization to agree&lt;/li&gt;
&lt;li&gt;visualize curves moving through space over time&lt;/li&gt;
&lt;li&gt;train background curve model

&lt;ul&gt;
&lt;li&gt;is background model better than foreground?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Long term goals&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sampling framework

&lt;ul&gt;
&lt;li&gt;try using background pixel modeling to prune background curves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Visualizing curve-direction revealed a bizarre artifact: most curves start somewhere in the middle of the reconstructed curve!&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Found issue - wasn&#39;t sorting by index when reconstructing.&lt;/p&gt;

&lt;h2&gt;Breaking Change&lt;/h2&gt;

&lt;p&gt;Modified &lt;code&gt;tr_curves_ml&lt;/code&gt; to &lt;em&gt;not&lt;/em&gt; include the background curve ml into the computation.  Recall that the normal ML computation code doesn&#39;t actually return ML, but a &lt;em&gt;ratio&lt;/em&gt; of the foreground curve ML and the background curve ML.  This indicates how much the model improves over the &quot;null model&quot;.&lt;/p&gt;

&lt;p&gt;Since the background curve ml is constant during training, this shouldn&#39;t affect results.  However, if you want to confirm the correctness of &lt;code&gt;tr_curves_ml&lt;/code&gt; against the reference implementation in &lt;code&gt;curve_ml5.m&lt;/code&gt;, you&#39;ll need to manually divide by a constant.  See the documentation for &lt;code&gt;tr_curves_ml&lt;/code&gt; for more details.&lt;/p&gt;

&lt;h2&gt;Reversing Curves&lt;/h2&gt;

&lt;p&gt;Investigating the effect of reversing curves.&lt;/p&gt;

&lt;p&gt;Visually determined which curves were reversed.  See modified version of &lt;code&gt;test/tmp_visualize_test&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Hacked &lt;code&gt;train/tr_construct_matrices.m&lt;/code&gt; with hard-coded list of curves to flip.  Re-ran training for &lt;em&gt;IND-Perturb&lt;/em&gt; model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;: we should see larger values for perturb_rate_variance and/or perturb_smoothing_variance, and smaller values for perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0020
            noise_variance: 0.0720
         position_variance: 1.3414e+04
             rate_variance: 0.2378
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.5332e-06
 perturb_position_variance: 0.4662

Final ML: -95.736042
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare against old results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0718
         position_variance: 1.6706e+04
             rate_variance: 0.2135
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4942e-06
 perturb_position_variance: 0.4886

Final ML: -97.463243
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary of changes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        smoothing_variance: +2.09%
            noise_variance: 0.17%
         position_variance: -19.7%
             rate_variance: +11.3%
perturb_smoothing_variance: 0 
     perturb_rate_variance: +2.61%
 perturb_position_variance: -4.59%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As expected, global position variance dropped; perturb rate grew while perturb position variance decreased.&lt;/p&gt;

&lt;p&gt;Unexpected increase in rate_variance; expected it to stay constant.  Possibly due to random fluctuations; both old and new values (0.214 and 0.238, respectively) are near the theoretical optimum (0.23, see next section).&lt;/p&gt;

&lt;p&gt;Also unexpected small increase in global smoothing variance (expected to be constant); also possibly due to random fluctuations.&lt;/p&gt;

&lt;p&gt;Literally no change to perturb smoothing variance.  I&#39;m starting to suspect something weird is going on with this value...&lt;/p&gt;

&lt;h2 id=&quot;optimal-rate-variance&quot;&gt;Theoretical Rate Variance&lt;/h2&gt;


&lt;p&gt;Was curious what the rate variance should be, assuming the rate vectors are drawn from a uniform distribution over the unit sphere.&lt;/p&gt;

&lt;p&gt;Determined empirically that rate variance should be somewhere between 0.220 and 0.235.  Code below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% generate 10,000 3-vectors with distribution over direction
dir = rand(3,10000);
% normalize to lie on unit sphere
dir = bsxfun(@times, dir, 1./sum(dir.^2));
% Get the emperical covariance of the vectors
Sigma = cov(dir&#39;)

        ans =

            0.2105    0.0556    0.0580
            0.0556    0.2297    0.0680
            0.0580    0.0680    0.3735
% take the average of the diagonals
mean(diag(Sigma))

        ans =

            0.2290
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This strongly suggests that the global rate variances we&#39;ve seen in training are consistent with the theoretical value.  Great!&lt;/p&gt;

&lt;h2&gt;Visualizing Curve Motion&lt;/h2&gt;

&lt;p&gt;Attempting to visualize perturbations between views.&lt;/p&gt;

&lt;p&gt;First attempt: tweak &lt;code&gt;test/test_visualize_test&lt;/code&gt; to only display points from a particular view.  Doesn&#39;t work great, because only part of the plant is visible in each view, and those parts differ between views.&lt;/p&gt;

&lt;p&gt;Next attempt: tweak &lt;code&gt;curve_max_posterior.m&lt;/code&gt;  to define a canonical index set for the curve, and then reconstruct for each view.&lt;/p&gt;

&lt;p&gt;More tomorrow...&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;p&gt;Training Background model&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Singular Regions Issue; Training</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/06/work-log"/>
   <updated>2013-08-06T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/06/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Investigating the &quot;Spreading&quot; issue with increases to perturb_rate_variance.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Confirmed the same phenomenon with increase to perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Setting perturb_position_variance to 1000:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-1.png&quot; alt=&quot;perturb_position_variance = 1000&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Setting perturb_position_variance to 1000000:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-visualize-training-2.png&quot; alt=&quot;perturb_position_variance = 1000000&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spreading appears to increase monotonically with perturb_position_variance.&lt;/p&gt;

&lt;p&gt;Again, this is surprising, because you&#39;d expect them to revert to the maximum likelihood solution.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;However, recall that the per-view likelihood has infinite variance in the back-projection direction.  The spreading appears to be occuring in this direction.  The infinite variance means that any influence from the prior will overcome the likelihood.&lt;/p&gt;

&lt;p&gt;But isn&#39;t the prior centered at zero?  Why is the result drifting so far from zero?&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;One thing is clear: with high perturb&lt;em&gt;* values, the correlation between nearby views becomes negligible compared to the within-view variance.  And since the likelihood variance has an infinite component, the posterior variance grows with perturb&lt;/em&gt;*.  While we can inspect the maximum posterior curve, it is relatively meaningless because the variance is so great.&lt;/p&gt;

&lt;p&gt;Even so, why doesn&#39;t it just revert to the mean?&lt;/p&gt;

&lt;p&gt;Mean rate is zero, but it can&#39;t be exactly zero, because the likelihood requires that the curve be near the data.  But the data&#39;s position is only known in two dimensions, so the posterior is free to manipulate the third dimension so that the rate is minimized.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Model is trying to use a linear-perturbation model to explain per-view deviations from the mean model.  Since the deviations don&#39;t arise from pure scaling, it has to contort into bizarre shapes to explain it.&lt;/p&gt;

&lt;p&gt;But the bizarre shapes fit the data better, so it&#39;s worth it.&lt;/p&gt;

&lt;h1&gt;&quot;Singular Regions&quot;&lt;/h1&gt;

&lt;p&gt;GOT IT!  Recall that the likelihood variance is measured in world units, even though they are really image-based values.  As the curve moves toward the camera, the likelihood variance &lt;em&gt;should&lt;/em&gt; ideally reduce, since the same size of world perturbations result in larger image perturbations.  But in our model they don&#39;t, and so curves can stray farther from the data in the image, but still look nearby the data according to the Gaussian.&lt;/p&gt;

&lt;p&gt;In the extreme case, all the points end up near the camera pinhole, and they will be in the center of the data Gaussian.  In practice, any point within \(\sigma&lt;sup&gt;2&lt;/sup&gt;\) of the camera will be well supported, where \(\sigma&lt;sup&gt;2&lt;/sup&gt;\) is the average noise variance in 3D.  I&#39;ll call this the &quot;Singular Region&quot;, where all the of likelihood&#39;s Gaussian &quot;cylinders&quot; (degenerate cones) intersect and overlap.&lt;/p&gt;

&lt;p&gt;In terms of the marginal likelihood, this can cause counterintuitive behavior.  For example, large perturb_rate_variance might be good, because it allows the curve to wander into the &quot;singular zone&quot; near the camera.  Thinking of in spherical coordinates, there is a wedge of the sphere that points toward the camera, and as the perturb_rate_variance increases, this wedge remains relatively constant in angular size, but gets longer and longer.  The longer it gets, the more of singular zone it overlaps. Even though greater variance means there are more possible model configurations, there is a period during which the proportion of these configurations that are well-supported by the likelihood doesn&#39;t necessarily decrease, so the ML doesn&#39;t necessarily decrease, either.&lt;/p&gt;

&lt;p&gt;This explains the phenomenon we saw during training, where the plot of ML vs. perturb_rate_variance (reproduced below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-06-training-plot.png&quot; alt=&quot;ML vs. perturb_rate_variance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The lump to the left is the result of the singular region giving false support near the camera.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Need to somehow place penalty for any point that strays too far from the mean curve.  Can this be done without radically distorting the model?&lt;/p&gt;

&lt;p&gt;What if I placed a limit on the likelihood variance, instead of letting it be infinite?  It will prevent the prior of taking credit for lousy configurations during the ML marginalization.&lt;/p&gt;

&lt;h1&gt;Experiments&lt;/h1&gt;

&lt;p&gt;Modified &lt;code&gt;train/tr_construct_matrices.m&lt;/code&gt; to clamp the likelihood&#39;s maximum variance to some multiple of the largest finite eigenvalue (see local function &quot;fix_precisions&quot;).  ML shouldn&#39;t change much when using reasonable values.&lt;/p&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Reference (no cap): 2.2675e+04  (inf)
100x cap:           2.2109e+04  (1.2 mm)
1000x cap:          2.2360e+04  (3.8 mm)
10000x cap:         2.2510e+04  (12 mm)
100000x cap:        2.2596e+04  (38 mm)
1000000x cap:       2.2654e+04  (12 cm) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Capping the variance to 10000x the triangulation variance results in a standard deviation of about 12mm in practice, which seems very reasonable.&lt;/p&gt;

&lt;p&gt;We have to raise standard deviation to 12 cm for it to be accurate to three significant digits, which seems somewhat high.  Possibly, even with reasonable model parameters, we&#39;re still seeing some influence from the &quot;singular zone,&quot; so it may be a good thing that we aren&#39;t seeing the full reference value.&lt;/p&gt;

&lt;h2 id=&quot;training-results&quot;&gt;Training Results&lt;/h2&gt;


&lt;p&gt;Running training using clamped likelihoods.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;No Perturb Model&lt;/em&gt;
    Model:
            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6658e+04
                 rate_variance: 0.2207&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Final ML: 2.371 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Exactly the same result as the non-clamped version.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ind Perturb Model&lt;/em&gt;
    Model:
            smoothing_variance: 0.0019
                noise_variance: 0.0719
             position_variance: 1.6729e+04
                 rate_variance: 0.2422
    perturb_smoothing_variance: 3.3860e-41
         perturb_rate_variance: 1.4918e-06
     perturb_position_variance: 0.4801&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Final ML:  2.512 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Old non-clamped training resulted in perturb_rate_variance exploding.  The new perturb_rate_variance looks very reasonable.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;OU Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0019
            noise_variance: 0.0721
         position_variance: 1.6681e+04
             rate_variance: 0.2146
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4711e-06
 perturb_position_variance: 0.7793
             perturb_scale: 3.7353

Final ML:  2.516 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of the global variance is can be pushed into the perturb_variance, since they are now correlated.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SQEXP Perturb Model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Model:
        smoothing_variance: 0.0018
            noise_variance: 0.0720
         position_variance: 1.6689e+04
             rate_variance: 0.2122
perturb_smoothing_variance: 3.3860e-41
     perturb_rate_variance: 1.4952e-06
 perturb_position_variance: 0.5130
             perturb_scale: 0.8425

Final ML: 2.516 x 10^4
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;General observations&lt;/h2&gt;

&lt;p&gt;It&#39;s still somewhat weird that perturb_smoothing_variance is so low.  I&#39;m pretty sure there are non-negligible deformations occurring during the imaging process.  Maybe it&#39;s just the Ind-perturb model?  More likely it&#39;s because the curves that deform are reversed...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Which of the perturb-components are really correlated?  Seems like position variance is probably independent, but rate variance might not be.  Definitely smoothing_variance (i.e. nonrigid deformations) should be correlated.&lt;/p&gt;

&lt;h1&gt;TODO&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;try visualization with truncated likelihoods

&lt;ul&gt;
&lt;li&gt;does increasing rate variance eventually have no effect?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;remove perturb_scale from ind model (better inference)&lt;/li&gt;
&lt;li&gt;hand-pick parameters and fix them, to reduce dimensionality of search space.&lt;/li&gt;
&lt;li&gt;Handle &quot;flipped&quot; curves.  Try to infer direction&lt;/li&gt;
&lt;li&gt;Does the visualized max posterior look good for the trained values (I&#39;m guessing not -- too strict of variances, overfitting)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Training Bugs</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log"/>
   <updated>2013-08-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/05/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14852&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing training of OU perturb model.&lt;/p&gt;

&lt;p&gt;Yesterday, we had problems with the perturb scale exploding.  Have switched to using a logistic sigmoid instead of an exponential to map perturb_scale, which sets a lower-bound and and upper-bound during optimization.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;perturb_scale is now staying low, but stil getting weird results.&lt;/p&gt;

&lt;p&gt;perturb_position_variance wants to be 9000+ ???&lt;/p&gt;

&lt;p&gt;perturb_rate_variance wants to be ~40?&lt;/p&gt;

&lt;p&gt;perturb_smoothing_variance wants to be ~0.0.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Trying new kernel: ind_kernel.  Like OU and SQEXP kernels, but no correlation between perturbations.  It&#39;s an intermediate step between no-perturb and ou-perturb, since it allows perturbations, but doesn&#39;t need a scale-length parameter.&lt;/p&gt;

&lt;p&gt;Training results for ind_kernel:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_3 = 

            smoothing_variance: 0.0024
                noise_variance: 0.0132
             position_variance: 1.5747e+04
                 rate_variance: 0.1709
    perturb_smoothing_variance: 0.0020
         perturb_rate_variance: 26.3458
     perturb_position_variance: 0.8770
                 perturb_scale: 1.7199
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Very weird that perturb_rate variance&lt;/p&gt;

&lt;p&gt;Consider limiting number of dimensions somehow...&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Force both smoothing_variances to be the same&lt;/li&gt;
&lt;li&gt;remove perturb_scale parameter&lt;/li&gt;
&lt;li&gt;???&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;....&lt;/p&gt;

&lt;p&gt;recall that handling view-index in kernels has never been thoroughly tested...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Let&#39;s visualize results for the ind_perturb model, and see if everything looks reasonable.&lt;/p&gt;

&lt;p&gt;Created &lt;code&gt;test/tmp_visualize_test.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Interesting...  Some curves are &quot;reversed&quot;, i.e. the base is the &quot;end&quot; of the curve and the tip is the &quot;beginning&quot;.  This has some unintended consequences when applying the perturb model, because the tips are where perturbations are greatest, but when curves are reversed, the tips aren&#39;t affected by most of the modelled perturbations.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Weird.  As perturb_rate_variance increases, the per-view curves spread out radially like flower petals.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-1.png&quot; alt=&quot;high perturb_rate_variance causes radial spreading of curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would have expected that the likelihood would take over, but this is clearly not happening.  Lets look at the likelihood...&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-2.png&quot; alt=&quot;Maximum-likelihood solution.  Posterior should revert to this as variance increases asymtotically.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We must be running up against numerical precision errors.  Let&#39;s look at the equation for maximum posterior:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;When \(\Sigma_0\) has huge eigenvalues, this equation basically reduces to&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
\mu_P = (\Sigma_0 \Sigma_l^{-1})^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l 
     &amp;= \Sigma_l \Sigma_0^{-1} \Sigma_0 \Sigma_l^{-1} \mu_l
     &amp;= \Sigma_l \Sigma_l^{-1} \mu_l
     &amp;= \mu_l
\end{align}
\]
&lt;/div&gt;


&lt;p&gt;However, the effective cancellation of \Sigma_0 can&#39;t occur, because the expression \((\Sigma_0 \Sigma_l^{-1} + I)\) has a huge condition number, which makes inverting it an unstable operation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Anyways, that&#39;s visualization.  Is the same issue arising in ML computations?  Higher rate variance can result in significantly larger condition numbers, and IIRC, we don&#39;t take any special steps to handle such issues in the ML  computation anymore (for example, using the matrix inversion lemma).&lt;/p&gt;

&lt;p&gt;Maybe we should force rate variance to be within a reasonable range.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Even when perturb_rate_variance is reasonable (1.0), we still get drifting:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-08-05-visualize-training-3.png&quot; alt=&quot;perturb_rate_variance == 1.0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The degree of this phenomenon seems to be a smooth function of perturb_rate_variance.  If it was a numerical instability issue, we&#39;d see it arise abruptly, indicating we had entered the unstable regime.&lt;/p&gt;

&lt;p&gt;I&#39;m now thinking this is a real issue with the model, not simply an artifact of computation (maybe it&#39;s both?).  Need to think more about it.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Determine why spreading happens when &lt;code&gt;perturb_rate_variance&lt;/code&gt; increases.&lt;/li&gt;
&lt;li&gt;Determine why huge &lt;code&gt;perturb_rate_variance&lt;/code&gt; values are promoted by the ML.&lt;/li&gt;
&lt;li&gt;Handle &quot;reversed curves&quot; issue, where perturbation is applied to the wrong end.&lt;/li&gt;
&lt;li&gt;Training

&lt;ul&gt;
&lt;li&gt;Finish &quot;ind&quot; model&lt;/li&gt;
&lt;li&gt;Traing &quot;ou&quot; and &quot;sqexp&quot; models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log"/>
   <updated>2013-08-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/04/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Troubleshooting training of OU perturbation model (&lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Found bug in &lt;code&gt;tr_curves_ml.m::ou_kernel&lt;/code&gt; -- &lt;code&gt;smoothing_variance&lt;/code&gt; was used where &lt;code&gt;perturb_smoothing_variance&lt;/code&gt; should have been (copy-paste bug).  Also fixed in &lt;code&gt;sqexp_kernel&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Optimizer appears to be running much more smoothly now.  Probably because there previously was a strong correlation, due to &lt;code&gt;smoothing_variance&lt;/code&gt; appearing in two different parts of the equation.  The resulting ridge could have caused slow progress.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Initial params:&lt;/p&gt;

&lt;p&gt;train_params =&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.2500
                noise_variance: 100
             position_variance: 90000
                 rate_variance: 2.2500
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 2.2500
     perturb_position_variance: 90000
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Stopped after 3 iterations.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.4501
                noise_variance: 2.5982e-04
             position_variance: 6.5359e+04
                 rate_variance: 3.9645
    perturb_smoothing_variance: 3.3437e-06
         perturb_rate_variance: 0.7268
     perturb_position_variance: 3.0785e+04
                 perturb_scale: 0.5867

Final ML: 28423.438936
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final ML (2.84e4) is greater than the no-pertrub model (2.37e4).  This is expected, since this model has more parameters, so we can get a tighter fit (maybe overfitting).&lt;/p&gt;

&lt;p&gt;I like that noise variance is much smaller (close to the minimum--do I need to lower the floor?).  This is expected, because the only source of IID noise is the rasterization process.&lt;/p&gt;

&lt;p&gt;Position standard deviation is much higher than before (255 vs. 126), as is rate standard deviation (2.0 vs. 0.47).&lt;/p&gt;

&lt;p&gt;Perturb variances are harder to explain.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is low, which is nice to see, because the worst of the perturbations are arising from miscalibrations (rate and position perturbations), not deformations.  But it&#39;s still far smaller than I expected.  hOnestly, I was expecting it to be almost the same as &lt;code&gt;smoothing_variance&lt;/code&gt;, but I realize now it makes sense for it to be smaller.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Perturb position variance is much, much larger than I expected.&lt;/strong&gt;  Standard deviation is 175.4, which is basically saying each plant has 17 cm of IID perturbations from the unobserved mean plant.   I have no good explanation for this, expect maybe that the initial value was ridiculously large.&lt;/p&gt;

&lt;p&gt;Pertub scale seems reasonable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Test sensitivity to initialization.  Initial values:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 0.0500
         perturb_rate_variance: 0.1000
     perturb_position_variance: 1
                 perturb_scale: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First four were initialized from the trained no-perturb model.&lt;/p&gt;

&lt;p&gt;Note the biggest change: perturb_position_variance changed from 9000 to 1.&lt;/p&gt;

&lt;p&gt;Terminated after 46 iterations; much better than the 3 iterations from last attempt.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0019
                noise_variance: 0.0721
             position_variance: 1.6267e+04
                 rate_variance: 0.2050
    perturb_smoothing_variance: 6.7471e-10
         perturb_rate_variance: 1.3201e-04
     perturb_position_variance: 403.7082
                 perturb_scale: 2.4888e+03

Final ML: 25163.231449
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Weird that this is lower than that poorly-initialized model.  Consider initializing using a combination of Attempt #1 and Attempt #2&#39;s initial values.&lt;/p&gt;

&lt;p&gt;Smoothing variance is much smaller than before, noise variance is larger.&lt;/p&gt;

&lt;p&gt;Position variance is in the same order-of-magnitude, but about 1/5 the magnitude.&lt;/p&gt;

&lt;p&gt;Rate variance is an OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb smoothing variance is still near-zero.&lt;/p&gt;

&lt;p&gt;Perturb rate variance is 3 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb position variance is 2 OoM smaller.&lt;/p&gt;

&lt;p&gt;Perturb scale is sooooooo high.  Basically flat, so the regular and perturb variances sum.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;: Result is very sensitive to initialization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: When perturb scale is this high, there exists a ridge, due to perturb variances playing the same role as normal variances.  Note that when perturb scale is infinity, this model degenerates to the no-perturb model.  Notice that Final ML isn&#39;t much better than the no-perturb.  Possibly when perturb scale exploded, the model could no longer improve, so constraining perturb scale to some small set of values may be a good idea (sigmoid transform).&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Try smaller changes to initialization.&lt;/li&gt;
&lt;li&gt;Try sigmoid transform on perturb scale.&lt;/li&gt;
&lt;li&gt;Try training using standard deviations instead of variances.&lt;/li&gt;
&lt;li&gt;Try changing one or more trained values to sensible defaults and retrain.&lt;/li&gt;
&lt;li&gt;Work with standard deviations, not variances&lt;/li&gt;
&lt;li&gt;Fix some &#39;known&#39; values and optimize others.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log"/>
   <updated>2013-08-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Running optimizer...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issue&lt;/strong&gt;: Likelihood variance is collapsing to zero.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Guess&lt;/strong&gt;: Training ML is different from naive ML and isn&#39;t penalizing noise correctly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Test 1&lt;/strong&gt;: save params, compare training ml and naive ml&lt;/p&gt;

&lt;p&gt;Same result.&lt;/p&gt;

&lt;h2&gt;Discussion&lt;/h2&gt;

&lt;div&gt; The marginal likelihood is monotonically increasing as the noise variance \(\sigma_n^2\) approaches zero.  This shouldn&#39;t be happening, because as the likelihood variance collapses to a delta, the marginal likelihood should become equal to the prior evaluated at the (virtual) observed locations.&lt;/div&gt;




&lt;div&gt;
\begin{align}
    \lim_{\sigma_n \to 0} p(D) &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; p_l(D \mid x) dx &amp; \text{(Marginalization)}\\
         &amp;= \lim_{\sigma_n \to 0} \int p_0(x) \; f(D - x) dx &amp;   \text{(Convolution)} \\
         &amp;= \int p_0(x) \delta(D - x) dx \\
         &amp;= p_0(D) 
\end{align}
&lt;/div&gt;


&lt;p&gt;This implies there&#39;s a bug in the code causing this phenomenon; the mathematical model is not the cause.&lt;/p&gt;

&lt;h2&gt;Solved&lt;/h2&gt;

&lt;div&gt;Found the bug.  When computing the likelihood precision \(S\) from the unscaled precision \(S_0\), the noise variance \(\sigma_n^2\) was multiplied instead of divided:&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;S = S0 * sigma_n; // incorrect
S = S0 * (1/sigma_n); // corrected version
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Resuming Training&lt;/h2&gt;

&lt;p&gt;Had some trouble with noise sigmas being too low.  Solved by in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Attempts 1-3: clamping to a minimum value and then adding a penalty depending on the amount that was clamped.&lt;/li&gt;
&lt;li&gt;Attempt 4:  offset by minimum value before transforming&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = tr_train(test_Corrs_ll, train_params, data_, 400);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Attempt #1&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Handle extreme values by incuring a penalty for variances smaller than 1e-5.  Result stored in &lt;code&gt;train_params_done_1&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_1 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprisingly small smoothness sigma.  Surprisingly low rate variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the magnitude of the penalty term.  Scaled up penalty by 1000.  Results in &lt;code&gt;train_params_done_2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_2 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Attempt #3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Testing sensitivity to the threshold where the penalty term starts to be incurred.  Set MIN_NOISE_VARIANCE to 1e-3 and MIN_SMOOTHING_VARIANCE to 1e-7 (previously both 1e-5).  Results in &lt;code&gt;train_params_done_3&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done_3 = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Attempt #4 &lt;/strong&gt;
Handled small variances by offsetting before transforming:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% converting param to state variable
x(1) = log(smoothing_variance - MIN_SMOOTING_VARIANCE);

% converting state variable to param
x(1) = exp(smoothing_variance) + MIN_SMOOTING_VARIANCE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is more elegant than the penalty hack used in the last three attempts.   Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
    perturb_smoothing_variance: 1
         perturb_rate_variance: 1
     perturb_position_variance: 1
                 perturb_scale: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Summary: no change.&lt;/p&gt;

&lt;h2&gt;No-Perturb model summary:&lt;/h2&gt;

&lt;p&gt;Successfully trained model.&lt;br/&gt;
Required &lt;strong&gt;115 function evaluations&lt;/strong&gt;, taking &lt;strong&gt;112 s&lt;/strong&gt;.&lt;br/&gt;
Optimal marginal likelihood:  &lt;strong&gt;23714.937760&lt;/strong&gt;.&lt;br/&gt;
Optimal parameters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;            smoothing_variance: 0.0025
                noise_variance: 0.1231
             position_variance: 1.6676e+04
                 rate_variance: 0.2212
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised at how small noise_variance was, considering the calibration noise.  However, I guess the maximum-likelihood reconstruction looked pretty good, if unity-apect-ratio axis scaling is used.&lt;/p&gt;

&lt;h2&gt;Training OU-Perturb model&lt;/h2&gt;

&lt;p&gt;Getting weird results.  Halted after second iteration; second iteration took 250 function evaluations; perturb_smoothing_variance gradient is zero.  Results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_params_done = 

            smoothing_variance: 1.5003e-05
                noise_variance: 1.0087e-04
             position_variance: 7.9393e+04
                 rate_variance: 0.7879
    perturb_smoothing_variance: 0.2500
         perturb_rate_variance: 1.5183
     perturb_position_variance: 2.5159e+04
                 perturb_scale: 48.6163
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Recall that new model ML&#39;s aren&#39;t deeply tested; probably a bug in there (in the kernel implementation?  in the kernel theory? in the conversion from mats to kernel?).  Will continue tomorrow.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log"/>
   <updated>2013-08-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/08/02/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Morning&lt;/h2&gt;

&lt;p&gt;Researched telecommuting strategies.&lt;/p&gt;

&lt;p&gt;Set up Google hangout: &lt;a href=&quot;https://plus.google.com/hangouts/_/89759369dd280ff225c298a7a4291745134e1d6f&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Set up IRC chat room: &lt;a href=&quot;http://webchat.freenode.net/?channels=ivilab&amp;amp;uio=d4&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Probably IRC will be best for general chat, Google Hangouts for screen sharing or group video chat.&lt;/p&gt;

&lt;h2&gt;Afternoon&lt;/h2&gt;

&lt;p&gt;Setting up training minimizer using Matlab&#39;s &lt;code&gt;fminunc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Got all params in-place, but Matlab version is too old.&lt;/p&gt;

&lt;p&gt;Trying to upgrade matlab, but Java is out of date.&lt;/p&gt;

&lt;p&gt;Taking a break to move furniture...&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Downloaded and installed Java 7.&lt;/p&gt;

&lt;p&gt;Now Matlab installer isn&#39;t able to communicate with server.  Arg...&lt;/p&gt;

&lt;p&gt;Working now... downloading...  Should be done in 45 minutes.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;p&gt;Migrating settings from old Matlab...&lt;/p&gt;

&lt;p&gt;Done.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>, Week summary</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/26/work-log"/>
   <updated>2013-07-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;No work logs this week.  All week I&#39;ve been splitting time between research and packing for the move to Phoenix on Monday.  I&#39;ve made some progress on the training framework.&lt;/p&gt;

&lt;h2&gt;Miscellaneous&lt;/h2&gt;

&lt;p&gt;Due to periodic crashing of Matlab (user error!), I added a script &lt;code&gt;tmp_setup_workspace.m&lt;/code&gt;, which is intended to restore all the needed variables for whatever task I&#39;m currently working on.&lt;/p&gt;

&lt;h2&gt;Optimized likelihood for training&lt;/h2&gt;

&lt;p&gt;Spend some time thinking about how to design a version of the marginal likelihood computation that is optimized for training.&lt;/p&gt;

&lt;p&gt;Some time was spent deriving the analytical ML gradient w.r.t. the training parameters.  I believe the result won&#39;t be too complicated, but deriving it was taking up too much time.  We&#39;ll use a numerical gradient for now; will return to analytical gradient if the numerical proves to be lacking.&lt;/p&gt;

&lt;p&gt;To save some computation during training, I precomputed the component matrices for the prior and likelihood.  Constructing the prior and likelihood covariance matrices will involve scaling and summing the component parts.  These cached matrices consume about &lt;strong&gt;740 MB&lt;/strong&gt; with the current training set.&lt;/p&gt;

&lt;p&gt;However, one unavoidable bottleneck continues to be the cholesky decomposition, which isn&#39;t improved by this precomutation.  I was hoping there would be some matrix inversion tricks involving linear combinations of matrices, but my research on this came up empty.  My last hope is to run Cholesky on the GPU (Matlab makes this trivial), but if that doesn&#39;t speed things up, I&#39;ll resign myself to waiting 10 hours for training.&lt;/p&gt;

&lt;p&gt;Nevertheless, the component caching still gives a 1.5x end-to-end speedup on the no_purturb_kernel  case, compared the to direct implementation.&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;div&gt;After optimizing, the bottlenecks are split evenly three ways: (1) constructing Prior kernel; (2) constructing ML covariance matrix; and (3) evaluating ML pdf.   The latter two are dominated mostly by \(O(n^3)\) matrix multiplication.  Surprisingly, Cholesky is significant, but not dominating.&lt;/div&gt;


&lt;h2&gt;Summary:&lt;/h2&gt;

&lt;p&gt;Training-optimized Marginal Likelihood is finished; in &lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;.  Results confirmed against reference implementation: &lt;code&gt;train/tr_curves_ml_ref.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;finish Training framework:

&lt;ul&gt;
&lt;li&gt;setup local minimizer with &lt;code&gt;train/tr_curves_ml.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;determine if numerical gradient is okay&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sanity check: compare the ML&#39;s of each trained model on training data.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/21/work-log"/>
   <updated>2013-07-21T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/21/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;: The likelihood covariance for the &quot;virtual observations&quot; scales linearly with the 2D likelihood variance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt;: see &lt;code&gt;experiments/exp_2013_07_21_likelihood_covariance.m&lt;/code&gt;.  Constructs a likelihood using noise_variance of one and then scaling precisions afterward.  Compares to directly-constructed likelihood precisions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Negligible difference&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;: Practice matches theory--scaling likelihood precisions is equivalent to constructing likelihood with the scaled precision.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;: This conclusion means that we can construct the likelihood precisions exactly once during training, and simply scale them as we modify the likelihood precision.  It would make sense to always use 1.0 when computing precisions, and refactor all existing code to scale the matrix by the reciprocal of the noise variance before using it.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Build training framework

&lt;ul&gt;
&lt;li&gt;fast likelihood evalautor

&lt;ul&gt;
&lt;li&gt;custom function for evaluating ML without recomputing stuff&lt;/li&gt;
&lt;li&gt;cache the three prior component matrices (smooth, linear, and offset)&lt;/li&gt;
&lt;li&gt;cache unscaled likelihood precision.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;wrap in a lambda

&lt;ul&gt;
&lt;li&gt;scales and combines all components&lt;/li&gt;
&lt;li&gt;depending on motion model, use different expression for purturbation coefficient&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;pass to quasi-newton minimizer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat for background curves&lt;/li&gt;
&lt;li&gt;Heuristic pruning using background-subtraction.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log"/>
   <updated>2013-07-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing: ground-truth-to-data-labels.  See file &lt;code&gt;train/label_from_ground_truth.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finished.&lt;/p&gt;

&lt;p&gt;Next:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data labels to likelihood means/covariances&lt;/li&gt;
&lt;li&gt;Likelihood means/covariances to marginal likelihood&lt;/li&gt;
&lt;li&gt;training framework&lt;/li&gt;
&lt;li&gt;training&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Visualizing &lt;code&gt;labels_from_ground_truth()&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Wrote &lt;code&gt;tmp_get_max_posterior.m&lt;/code&gt;, a temporary script that computes the posterior mean from a possibly-overconstrained prior.  In this case, the posterior covariance is singular, but the mean can still be obtained.  The math behind it &lt;a href=&quot;/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance/&quot;&gt;is available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Below is a plot using the test dataset and the ground truth labels:&lt;/p&gt;

&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;//www.youtube.com/embed/foL28SUn1JM?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;


&lt;p&gt;This shows that given a good labeling, a quality 3D reconstruction can be obtained using only the fragmented curves output by the curve-detector.&lt;/p&gt;

&lt;p&gt;Notice that the curves at the base have missing parts.  There isn&#39;t sufficient edge data here, but this could probably be fixed by connecting them to the base of the main stem and using the Branching Gaussian Process prior to enforce connectivity.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Maximum posterior with singular prior covariance</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance"/>
   <updated>2013-07-19T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/19/maximum-posterior-with-singular-prior-covariance</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Consider the scenerio of Bayesian inference with a linear-Gaussian prior and likelihood.&lt;/p&gt;

&lt;p&gt;It is sometimes the case that our prior has a singular covariance matrix, indicating that our variables are embedded in a lower-dimensional hyberplane, i.e. some dimensions are redundant.  In our 3D curve triangulation application, this situation arises when  the same 3D point is observed in from multiple views.&lt;/p&gt;

&lt;p&gt;We can still find the maximum posterior arising from such a prior, as long as the likelihood is non-singular.   We can interpret this as multiple observations of the rendant dimensions, and if we are careful with our math, we can handle it the same way as the case with a non-singular prior.&lt;/p&gt;

&lt;p&gt;Given a likelihood \( \mathcal{N}(\mu_l, \Sigma_l) \) and prior \( \mathcal{N}(\mu_0, \Sigma_0) \), recall that the posterior is given by  \( \mathcal{N(\mu_P, \Sigma_P)} \), where&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
\Sigma_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} \\
\mu_P &amp;= (\Sigma_l^{-1} + \Sigma_0^{-1})^{-1} (\Sigma_l^{-1} \mu_l + \Sigma_0^{-1} \mu_0)
\end{align}

\]
&lt;/div&gt;


&lt;p&gt;However, if \(\Sigma_0 \) is singular, we must avoid inverting it when computing \( \mu_P  \).  An equivalent equation for \(\mu_P\) that satisfies this condition is:&lt;/p&gt;

&lt;div&gt; \[
\mu_P = (\Sigma_0 \Sigma_l^{-1} + I)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;




&lt;div&gt;This should be computable as long as the likelihood precision matrix \(\Sigma_l^{-1}\) has no infinite eigenvalues.  &lt;/div&gt;


&lt;p&gt;To see an example result, see &lt;a href=&quot;/ksimek/research/2013/07/19/work-log/&quot;&gt;today&#39;s Work Log&lt;/a&gt; entry.&lt;/p&gt;

&lt;h2&gt;Implementation Notes&lt;/h2&gt;

&lt;p&gt;I tried implementing a version of this that uses Cholesky decomposition and backsubstitution instead of generic matrix inversion.  I needed a symmetric matrix, so I modified the equation for \(\mu_P\):&lt;/p&gt;

&lt;div&gt; \[
\mu_P = \Sigma_0 (\Sigma_0 \Sigma_l^{-1} \Sigma_0 + \Sigma_0)^{-1} (\Sigma_0 \Sigma_l^{-1} \mu_l + \mu_0)
\]
&lt;/div&gt;


&lt;p&gt;This was not noticibly faster than general matrix inversion, because it involves two additional large matrix multiplications.&lt;/p&gt;

&lt;p&gt;Surprisingly, I &lt;em&gt;was&lt;/em&gt; able to get a significant speedup (~7x) be using backsubstitution (Matlab&#39;s &#39;\&#39; operator) &lt;em&gt;without&lt;/em&gt; Cholesky decomposition.  I always assumed that the lower-triangular form was what made backsubstitution so fast, but it is apparently also fast with dense matrices.  So we can avoid the extra expensive dens-matrix multiplications, and also avoid expensive matrix inversion.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log"/>
   <updated>2013-07-18T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/18/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bootcamp demo session&lt;/li&gt;
&lt;li&gt;Ground truth labeling of curve fragments.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Bootcamp demo session&lt;/h2&gt;

&lt;p&gt;Ran bootcamp demo session.  Final code available at&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;svn+ssh://v01/misc/svn/src/bootcamp/kjb_demos/3d_demo
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Ground truth labeling of curve fragments&lt;/h1&gt;

&lt;p&gt;Goal: use 2D ground truth to automatically label bottom-up curve fragments.&lt;/p&gt;

&lt;h2&gt;Overview&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Draw ground truth curves using curve index&lt;/li&gt;
&lt;li&gt;dilate slightly&lt;/li&gt;
&lt;li&gt;For each rendered data curve, gather all GT indices&lt;/li&gt;
&lt;li&gt;keep most-occurring GT index.&lt;/li&gt;
&lt;li&gt;add to assoc list for that curve&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Issues:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Remember how to read and process ground truth.&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; See &lt;code&gt;../ground_truth/read_gt2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Symlinked to &lt;code&gt;train/read_gt2.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Which dataset does our example data come from?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/ler_5_36_0.jpg&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Where is ground truth data?&lt;br/&gt;
&lt;strong&gt;A:&lt;/strong&gt; &lt;code&gt;~/data/arabidopsis/2010-06-03/ler_5_36/resized_50%/ground_truth_2d.gt2&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;Blah Blah&lt;/h2&gt;

&lt;p&gt;Refactored some code into new function &lt;code&gt;build_curve_maps.m&lt;/code&gt;, which renders each of the curves in a cell-array into a map containing their indices.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored as Bezier curves.  Symlinked the all bezier-related code into data_association_2.  Relevant function is &lt;code&gt;bezier/polybez_to_polyline.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Forgot that GT is stored in OpenGL-style coordinates (bottom-left origin).  Converting to top-right using &lt;code&gt;tools/flip_y.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Summary &lt;/h2&gt;

&lt;p&gt;Got through item 2 in &quot;Overview&quot; above.  Will finish next time, and start building training framework.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title> 2</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2"/>
   <updated>2013-07-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log-2</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Experimenting with &lt;code&gt;ou_perturb_kernel.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Was getting weird results, where the ML approached infinity as &lt;code&gt;noise_variance&lt;/code&gt; approached zero.&lt;/p&gt;

&lt;p&gt;Realized that the set of precision matrices needs to be updated &lt;strong&gt;every time the noise sigma changes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is an oversight that has tripped me up before.  Need to try to avoid it in the future.&lt;/p&gt;

&lt;p&gt;It is possible (likely?) that this transformation is as simple as multiplying the precision matrices by \(\sigma_n* / \sigma_n\).  This would avoid a semi-expensive Hessian calculation for each point, which could be a bottleneck during training.&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;p&gt;See previous entry&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log"/>
   <updated>2013-07-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14866&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Note: This entry marks then end of significant work on the new likelihood function.  The SVN revision is noted in the meta-box to the right.&lt;/p&gt;

&lt;h2&gt;Optimizing &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Attempting to use sparsity to speed up &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First attempt&lt;/strong&gt;: try building only the relevant elements of the prior matrix, K.&lt;br/&gt;
&lt;strong&gt;Resut&lt;/strong&gt;: Gains in multiplication are lost in construction of K.  Insignificant speedup.  Rolling back.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second attempt&lt;/strong&gt;: convert block-diagonal matrix &lt;em&gt;S&lt;/em&gt; to a sparse matrix.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Significant gains in multiplication; tolerable losses when constructing &lt;em&gt;S&lt;/em&gt;.  ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third Attempt&lt;/strong&gt;: Convert the individual blocks &lt;em&gt;S_i&lt;/em&gt; to sparse, &lt;em&gt;then&lt;/em&gt; construct &lt;em&gt;S&lt;/em&gt; from them.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: Further speedup of ~2x&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fourth Attempt&lt;/strong&gt;:  Change &lt;code&gt;(eye(size(K)) + S * K * S&#39;)&lt;/code&gt; to &lt;code&gt;(speye(size(K)) + S * K * S&#39;)&lt;/code&gt; ; i.e. changing &lt;code&gt;eye&lt;/code&gt; to &lt;code&gt;speye&lt;/code&gt;.&lt;br/&gt;
&lt;strong&gt;Result&lt;/strong&gt;: moderate speedup, ~1.5x.&lt;/p&gt;

&lt;p&gt;I think we&#39;ve squeezed all we can from this function.  It&#39;s now 10x faster than the naive method on a problem with 4000-dimensions.&lt;/p&gt;

&lt;h2&gt;Results: &lt;/h2&gt;

&lt;p&gt;Running &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6362.9 ms)
Old ML: 207.041742

Computing marginal likelihood (new way, legacy correspondence)...
Done. (7655.2 ms)
New ML (Legacy corr): 207.700616

Computing marginal likelihood (new way)...
Done. (6537.7 ms)
New ML (New corr): 793.585038

Computing marginal likelihood (new way, no MIL)...
Done. (6330.3 ms)
New ML (New corr, no MIL): 793.600835

Computing marginal likelihood (direct method)...
Done. (667.9 ms)  &amp;lt;-------------------  10x speedup over previous
New ML (direct method): 793.300407 &amp;lt;--  0.04% error!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note speedup and negligible error compared the previous method.&lt;/p&gt;

&lt;h2&gt;General Observations Regarding &lt;code&gt;curve_ml5.m&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;I&#39;ve noticed that &lt;code&gt;markov_order&lt;/code&gt; must be much larger than I expected to avoid approximation error.&lt;/p&gt;

&lt;p&gt;Recall in &lt;code&gt;curve_ml2.m&lt;/code&gt;, we use the Markov assumption to break-down the prior covariance, and then combine them with the likelihood cliques.  In that case, we could use a Markov order between 2 and 5 without significant approximation error.&lt;/p&gt;

&lt;p&gt;In &lt;code&gt;curve_ml5.m&lt;/code&gt;, using a Markov order less than 100 results in unacceptable error.&lt;/p&gt;

&lt;p&gt;There are two reasons for this.  First, we&#39;re decomposing the marginal likelihood Gaussian, not the prior.  The prior is explicitly Markovian, whereas the marginal likelihood is not.  The ML Guassian adds extra uncertainty to every point, which means we need to consider more nearby points to avoid erroneous conclusions.&lt;/p&gt;

&lt;p&gt;Second, because the new approach creates a distinct index set for each view, indices are often repeated (multiple views of the same point) and considering them doesn&#39;t tell you anything about what direction the curve is heading.&lt;/p&gt;

&lt;p&gt;The first point will likely be mitigated when we start using the new curve model, which will allow the likelihood variance to decrease dramatically.  However, the markov assumption in the prior is less likely to hold under these models, so some experimentation will be needed.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The code for the new likelihood, &lt;code&gt;curve_ml5.m&lt;/code&gt; is now stable.  It is fast and accurate in its current implementation, assuming a reasonable value for &lt;code&gt;markov_order&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Medium-term goal: calibration/training for all curve models.&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ground-truth labelling of bottom-curve curve data&lt;/li&gt;
&lt;li&gt;set-up method for evaluating ground-truth using a given parameter-set, without re-computing indices each time.&lt;/li&gt;
&lt;li&gt;Run multi-dimensional optimization to fit for all models&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log"/>
   <updated>2013-07-16T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/16/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing new direct method for marginal likelihood.&lt;/h2&gt;

&lt;p&gt;Implemented in &lt;code&gt;curve_ml5.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After initial attempt, the new method produced inconsistent results.&lt;/p&gt;

&lt;p&gt;Re-derived the result two more ways and the math comes out the same.  The theory looks right.&lt;/p&gt;

&lt;p&gt;Finally found the bug -- was computing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;instead of&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;det = 2 * log(sum(chol(Sigma)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the Cholesky decomposition is the square-root of the matrix Sigma, I was forgetting to double it&#39;s determinant to get the determinant of Sigma.&lt;/p&gt;

&lt;p&gt;Now getting the correct results; now time to make it fast.&lt;/p&gt;

&lt;h2&gt;Optimizing direct method for ML&lt;/h2&gt;

&lt;p&gt;Need to get O(n) runtime instead of O(n&lt;sup&gt;3&lt;/sup&gt;).  Will use existing code for markov-decomposed PDF evaluation.&lt;/p&gt;

&lt;p&gt;Bottleneck is 100% the Cholesky decomposition.  The direct method doesn&#39;t remove redundant dimensions, so it&#39;s slower than &lt;code&gt;curve_ml4.m&lt;/code&gt;, which does.  Recall that this fact makes &lt;code&gt;curvE_ml4.m&lt;/code&gt; not general enough to use the new foreground models with.&lt;/p&gt;

&lt;p&gt;...........&lt;/p&gt;

&lt;p&gt;After some investagation, the previous statement appears to be untrue.  The bottlenect is dense matrix multiplication, which is fixed by using sparse matrices.&lt;/p&gt;

&lt;p&gt;Now, cholesky &lt;strong&gt;is&lt;/strong&gt; the bottlenect, but not as huge as before.  The markov decompose method only gives a ~30% speedup at best, while introducing ~2.5% error and significant extra complexity. Is it worth it?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Direct Evaluation of the Marginal Likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood"/>
   <updated>2013-07-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/12/marginal-likelihood</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Previously, we&#39;ve always used hacks to evaluate the marginal likelihood, because evaluating it directly was apparently impossible because of an infinite normalization constant.  I&#39;ve discovered that this isn&#39;t actually true; the infinite normalization constant arose due to our approximation of the likelihood.  The trick in correctly computing the ML is to replace the usual normalization constant with a corrected one.&lt;/p&gt;

&lt;p&gt;Our old method required knowledge of the maximum posterior solution, which was costly to compute.  The new approach is straightforward to describe, more accurate than our previous method, and can be evaluated in linear time by exploiting its Markovian structure.&lt;/p&gt;

&lt;p&gt;Below is a rough-draft writeup of my derivation of the marginal likelihood function.&lt;/p&gt;

&lt;h2&gt;Direct Evaluation of the Marginal likelihood&lt;/h2&gt;

&lt;p&gt;The 3D marginal likelihood function arises as the sum of a 3D curve process and a 3D perturbation process.  Both are gaussian (the second is approximate), this the result is a Gaussian function, the convolution of two Gaussians.  However, the perturbation process has infinite variance in the direction of backprojection, which arises from the fact that perturbation actually occurs in 2D, we are just backprojecting it to 3D for tractibility.  In other words, the ML isn&#39;t a distribution in 3D, it&#39;s a distribution in 2D, and we need to determine the appropriate normalization constant for the 3D function to it generates 2D ML densities.&lt;/p&gt;

&lt;p&gt;The likelihood function is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y | x) &amp;= \prod p(y_i | x_i) \\
         &amp;= \prod \mathcal{N}(y_i; \rho_I(x_i), \sigma_n^2 I) \\
         &amp;\approx \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \mathcal{G}(Y_i; x_i, S_i^{-1}) \\
         &amp;= \prod \frac{1}{\sqrt{2 \pi \sigma^2}^2} \exp\{(Y_i - x_i)^\top S_i (Y_i - x_i)\} \\
         &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}^{2n}} \exp\{(Y - x)^\top S (Y - x)\} \\
         &amp;= \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\}
\end{align}
\]&lt;/div&gt;


&lt;p&gt;Where&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\rho_i(x)\) is the projection of x into the I-th view,&lt;/li&gt;
&lt;li&gt;\(Y_i\) is the estimated backprojected position of observation \(y_i\),&lt;/li&gt;
&lt;li&gt;\(S_i\) is the curvature of the 3d likelihood function w.r.t. \(x_i\) evaluated at $Y_i$,&lt;/li&gt;
&lt;li&gt;\(Y\), \(S\) and \(x\) are the concatenation of \(Y_i\), and \(x\)&lt;/li&gt;
&lt;li&gt;\(S\) is the block-diagonal matrix of \(S_i\)&#39;s&lt;/li&gt;
&lt;li&gt;\(\mathcal{G}\) is a Gaussian function (an unnormalized normal distribution).&lt;/li&gt;
&lt;li&gt;\(Z_l\) is a normalization constant.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We have transformed the likelihood into a log-linear function of x, by rewriting the PDF in terms of only 3D entities.  Note that \(S_i\) has zero curvature in the backprojection direction, resulting in infinite variance in the Gaussian function.   Also note that the normalization constant isn&#39;t the standard one for a 3D gaussian distribution, because this isn&#39;t a distribution over x.  The normalization constant is chosen so the approximate likelihood in 3D agrees with the exact 2D likelihood when perturbation is zero (i.e. when \(Y_i\) lies anywhere on the backprojection line).&lt;/p&gt;

&lt;p&gt;This approximation ignores the nonlinearity of projection; the likelihood function as a function of \(x_i\) would actually look like a cone whose axis-perpendicular slices are Gaussians, whereas our function is cylinder-shaped.  In practice, the approximation error has minimal effect on the marginal likelihood computation, assuming $Y_i$ is a good estimate of the posterior depth, and the posterior is reasonably peaked.  (A graphic to illustrate would be good here).&lt;/p&gt;

&lt;p&gt;The prior is given by&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(x) &amp;= \mathcal{N}(x; \mathbf{0}, K) \\
     &amp;= \frac{1}{Z_p} \exp\{x^\top K^{-1} x\}
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;where \(K\) is the covariance matrix arising from the Gaussian process kernel, and Z_p is the standard Gaussian normalization constant.&lt;/p&gt;

&lt;p&gt;The marginal likelihood is defined as&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \int p(y|x) p(x) dx \\
     &amp;= \int \left ( \frac{1}{Z_l} \exp\{(Y - x)^\top S (Y - x)\} \right ) \left ( \frac{1}{Z} \exp\{x^\top K^{-1} x\} \right ) \\
     &amp;= \frac{1}{Z_l Z_p}\int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     \end{align}
\]&lt;/div&gt;


&lt;p&gt;The expression within the integral is the convolution of two unnormalized zero-mean Gaussians, so the integral is a zero-mean Gaussian whose covariance is the sum of the inputs&#39; covariances.  If the input Gaussians &lt;strong&gt;were&lt;/strong&gt; properly normalized, the normalization constant would be \(1/Z\);  since they are not, the normalization constant is \((Z_1 Z_2 / Z)\), where \(1/Z_1\) and \(1/Z_2\) are the would-be normalization constants for the first and second Gaussian, respectively.  Note that the second Gaussian&#39;s normalization constant is \(1/Z_p\), so this results in the cancellation we see below&lt;/p&gt;

&lt;div&gt;\[
\begin{align}
p(y) &amp;= \frac{1}{Z_l Z_p} \int   \exp\{(Y - x)^\top S (Y - x)\}   \exp\{x^\top K^{-1} x\} \\
     &amp;= \frac{1}{Z_l Z_p} \left ( \frac{Z_{l,3d} Z_p}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \} \right ) \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z} \exp\{x^\top (S^{-1} + K)^{-1} x \} \\
     &amp;= \frac{Z_{l,3d} }{Z_l}\frac{1}{Z}  \exp\{x^\top S(S + S K S)^{-1} S x \}
     \end{align}
\]&lt;/div&gt;




&lt;div&gt;
    Where \( Z_{l,3d} \) is the would-be 3D normalization constant for our likelihood Gaussian.  The last line avoids inverting the singular matrix \(S\) when it&#39;s singular.  Since \(S\) is rank-deficient, the normalization constants based on \(S^{-1}\) will be infinite (i.e. \(Z_{l,3d}\) and \(Z\)).  However, the terms involving determinants of \(S^{-1}\) should cancel in the ratio, resulting in a gaussian with infinite variance, but non-infinite normalization constant. **(Need to show this mathematically next time)** 
    &lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log"/>
   <updated>2013-07-11T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/11/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Markov-decomposed ML&lt;/h2&gt;

&lt;p&gt;The original plan was to markov-decompose the prior and posterior, but soon remembered that I also need to compute the mean of the posterior, which isn&#39;t as straighforward to compute.&lt;/p&gt;

&lt;p&gt;After some deliberation and whiteboarding, I decided to implement Bishop&#39;s forward/backward algorithm to simultaneously marginalize and compute the maximum.  Will test this against the clique-tree implementation to ensure correctness.&lt;/p&gt;

&lt;p&gt;........&lt;/p&gt;

&lt;p&gt;During testing I realized a fundamental problem with the current likelihood: there is no obvious way to consistently handle the &quot;redundant&quot; dimensions that arise from duplicated indices.  In some kernels, these are handled naturally, namely the kernels that treat different views as different indices.  The old kernel, however, does not, and the duplicated indices result in degenerate posterior distributions unless handled using hacks.&lt;/p&gt;

&lt;p&gt;We could handle this on a kernel-by-kernel basis, but it would be unmaintainable, and could hinder further research into new kernels.  It also isn&#39;t clear that the hacks I have in mind would actually give correct results.&lt;/p&gt;

&lt;p&gt;This problem is inherent to the &quot;candidates estimator&quot; of the marginal likelihood, because it involves a ratio of the posterior and prior, both of which are degenerate in these cases.&lt;/p&gt;

&lt;p&gt;The actual marginal likelihood function has no such degeneracies, because each observation is independent, given the underlying curve.  However, until recently it was unclear how to evaluate the marginal likelihood using the approximated likelihood function.  The approximate likelihood has a rank-deficient precision matrix, and its normalization constant is non-standard due to the transformation from 2D to 3D.  However, I think I&#39;ve developed a way to evaluate it, which I describe in the next article.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log"/>
   <updated>2013-07-08T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/08/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Cleanup and speedup of curve_ml.&lt;/p&gt;

&lt;h2&gt;Task 1: Version without matrix-inversion lemma&lt;/h2&gt;

&lt;p&gt;Making a simpler version that doesn&#39;t exploit the matrix inversion lemma (MIL).&lt;/p&gt;

&lt;p&gt;The benefits of the MIL are likely mitigated by markov-decomposition (next task), and MIL
complicates the code and API design.&lt;/p&gt;

&lt;p&gt;Results (small test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (108.2 ms)
Old ML: 62.183969
Computing marginal likelihood (new way, legacy correspondence)...
Done. (120.8 ms)
New ML (Legacy corr): 63.908784
Computing marginal likelihood (new way)...
Done. (88.9 ms)
New ML (New corr): 72.304718           &amp;lt;----------  OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (97.3 ms)
New ML (New corr, no MIL): 72.304666   &amp;lt;----------  NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The difference is about 0.00007% -- seems good.  Slightly slower, possibly just noise.&lt;/p&gt;

&lt;p&gt;Results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6447.3 ms)
Old ML: 207.324848
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7665.1 ms)
New ML (Legacy corr): 209.199676
Computing marginal likelihood (new way)...
Done. (6205.6 ms)
New ML (New corr): 457529.406731  &amp;lt;---------- ?????
Computing marginal likelihood (new way, no MIL)...
Done. (5946.7 ms)
New ML (New corr, no MIL): 778723.327102  &amp;lt;---------- ?????
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both versions give CRAZY high results; significantly different from the &lt;a href=&quot;/ksimek/research/2013/07/05/work-log/&quot;&gt;same test on Friday&lt;/a&gt;.  Need to investigate...&lt;/p&gt;

&lt;h2&gt;Investigating new results&lt;/h2&gt;

&lt;p&gt;Observations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All results differ from friday, including old ML (slightly)&lt;/li&gt;
&lt;li&gt;New ML using legacy correspondence is still reasonable.&lt;/li&gt;
&lt;li&gt;Both crazy results are using new correspondence.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;TODO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Look at pieces (log posterior, likelihood, prior)&lt;/li&gt;
&lt;li&gt;Inspect new correspondence&lt;/li&gt;
&lt;li&gt;Think about what changed since Friday?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:10:19 PM&lt;/p&gt;

&lt;p&gt;Ran old test, &lt;code&gt;test_ml_end_to_end.m&lt;/code&gt; and compared to old results in new test, &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt;.  Old test still gives old results, so I&#39;ll compare the two tests to determine what has changed.&lt;/p&gt;

&lt;p&gt;12:12:48 PM&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;mix&lt;/code&gt; parameter was set to 0.5 instead of 0.0.  I had forgotten I changed it at the end if Friday.&lt;/p&gt;

&lt;p&gt;Now getting &quot;good&quot; results again.  It raises a question that needs to be answered: why is ML sooo sensitive to evalaution position when using new correspondence?&lt;/p&gt;

&lt;h2&gt;Resuming&lt;/h2&gt;

&lt;p&gt;New results (full test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6009.5 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (7286.3 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6146.6 ms)
New ML (New corr): 793.585038             &amp;lt;---------- OLD RESULT
Computing marginal likelihood (new way, no MIL)...
Done. (6008.3 ms)
New ML (New corr, no MIL): 793.600835     &amp;lt;---------- NEW RESULT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;0.002% error, slightly faster.  Good.&lt;/p&gt;

&lt;h2&gt;Investigating Anomaly in new ML &lt;/h2&gt;

&lt;p&gt;So why is new ML so sensitive to where it is evaluated?&lt;/p&gt;

&lt;p&gt;Note that it&#39;s only extreme when using new correspondence.  Old correspondence is okay.&lt;/p&gt;

&lt;p&gt;Inspect difference between max likelihood and max posterior.  Maybe it&#39;s just more extreme than with old correspondence.&lt;/p&gt;

&lt;p&gt;12:20:59 PM&lt;/p&gt;

&lt;p&gt;Idea: bug in non-0.0 case?  Nope.&lt;/p&gt;

&lt;p&gt;Tried mixes: 0.0, 0.01, 0.1. Steady and dramatic increase in ML for new correspondence.  Old correspondence is nearly constant.  What is different between these correspodnences (other than the correspondences themselves).&lt;/p&gt;

&lt;p&gt;12:57:08 PM&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;: The max likelihood in the new correspondences is waaaaay into the tails of the prior.&lt;/p&gt;

&lt;p&gt;Since no real triangulation is done to ensure agreement between views, the maximum likelihood triangulation is very rough  (infinite likelihood variance allows this to happen without penalty).  This places the curve far into the tails of the prior, where evaluation is highly unstable.  Both the prior and likelihood are then extremely low (-1e-8 in log space), which should cancel, but don&#39;t due to numerical instability.  Hence, marginal likelihoods with huge magnitude.&lt;/p&gt;

&lt;p&gt;This is great news, since it means everything is working mostly as expected.  The numerical instability issue is easilly solved by always evaluating at the posterior, not the maximum likelihood.&lt;/p&gt;

&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;The version of the new ML that doesn&#39;t use the matrix inversion lemma is accurate, so we can proceed to the markov-decomposed version next.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log"/>
   <updated>2013-07-05T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/05/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Still trying to troubleshoot the difference between old and new likelihood implementations.&lt;/p&gt;

&lt;p&gt;Reviewing what is known&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prior is same&lt;/li&gt;
&lt;li&gt;Likelihood is same&lt;/li&gt;
&lt;li&gt;Posterior evaluates same in two different ways.&lt;/li&gt;
&lt;li&gt;mean curve is different&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Try using mean curve from old alg. in new alg.&lt;/p&gt;

&lt;p&gt;10:13:35 AM&lt;/p&gt;

&lt;p&gt;SOLVED.&lt;/p&gt;

&lt;p&gt;Yesterday I noticed that setting mix to 1.0 give decent results, but 0.0 was terrible.  Now I know why: 0.0 is a special case, in which a simplified calculation is used.  There was a bug in that special case, where the normalization constant didn&#39;t take into account the redundant dimensions that we eliminated.&lt;/p&gt;

&lt;h2&gt;ML Test&lt;/h2&gt;

&lt;p&gt;11:02:29 AM&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ml_test_end_to_end_2.m&lt;/code&gt; is now running&lt;/p&gt;

&lt;p&gt;Running on subset of correspondence, results:
    &gt;&gt; test_ml_end_to_end_2([], [], [], false)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...  
Done. (96.9 ms)  
Old ML: 62.176782

Computing marginal likelihood (new way, legacy correspondence)...  
Done. (112.3 ms)  
New ML (Legacy corr): 63.859940

Computing marginal likelihood (new way)...  
Done. (83.5 ms)  
New ML (New corr): 71.964026
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Surprised the magnitude of difference between &quot;Old ML&quot; and &quot;New ML (Legacy corr)&quot;.  I guess the posterior variance is larger in this case, which means the posterior means can differ more.&lt;/p&gt;

&lt;p&gt;Running on full correspondence, results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt; test_ml_end_to_end_2([], [], [], false)

Computing marginal likelihood (old way)...
Done. (6400.1 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8328.4 ms)
New ML (Legacy corr): 207.700616
Computing marginal likelihood (new way)...
Done. (6349.2 ms)
New ML (New corr): 793.585038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Interesting that &quot;New ML (New Corr)&quot; is dramatiacally higher than the old and legacy ML&#39;s.  I suppose I should have been expected this, since this entire approach &lt;a href=&quot;/ksimek/research/2013/06/28/work-summary/&quot;&gt;was motivated by the terrible correspondences arising from the legacy correspondence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It&#39;s nice to see the ML for good correspondences getting even better.  Hopefully we&#39;ll see even more improvement as we try new curve models.&lt;/p&gt;

&lt;p&gt;Taking a break for lunch...&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Next steps:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;fast ml_curve3

&lt;ul&gt;
&lt;li&gt;linear eval&lt;/li&gt;
&lt;li&gt;try without matrix inversion lemma&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;try new models (check that they&#39;re nonsingular; don&#39;t crash)&lt;/li&gt;
&lt;li&gt;set up training framework&lt;/li&gt;
&lt;li&gt;consider version without inversion lemma&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log"/>
   <updated>2013-07-04T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/04/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Continuing debugging of new likelihood&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Test:&lt;/strong&gt; &lt;code&gt;test/test_ml_end_to_end_2.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;07:11:52 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Trouble with NaN error in &lt;code&gt;correspondence/corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;08:22:43 AM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Narrowed it down to gap-handling code-branch.&lt;/p&gt;

&lt;p&gt;08:53:34 AM&lt;/p&gt;

&lt;p&gt;Fixed. Arose from a nasty indexing scheme in the original &lt;code&gt;clean_correspondence.m&lt;/code&gt;, which I didn&#39;t handle well when adapting for &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;09:29:35 AM&lt;/p&gt;

&lt;p&gt;Nonsingular prior covariance matrix causing crash.  Apparently caused by same index appearing multiple times.&lt;/p&gt;

&lt;p&gt;In the past, we always merged such points.&lt;/p&gt;

&lt;p&gt;The new idea is different views of the same point would make the posterior covariance singular.  Either this isn&#39;t true, or there&#39;s a bug...&lt;/p&gt;

&lt;p&gt;Debugging this will take some serious coding and math brainpower and I fell debugging fatigue coming on.  Taking a brief break to refresh, then will tackle...&lt;/p&gt;

&lt;p&gt;11:56:21 AM&lt;/p&gt;

&lt;p&gt;Using a simple toy example, I (partially) confirmed my intuition that a degenerate prior matrix is okay, as long as its nullspace is spanned by the likelihood matrix.&lt;/p&gt;

&lt;p&gt;In the context of this bug, it means that two points sharing the same index value is fine as long as they arose from different views (assuming non-degenerate camera configuration, which is true in this case).&lt;/p&gt;

&lt;p&gt;Possible causes&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Numerical instability -- near-singular matrix.&lt;/li&gt;
&lt;li&gt;Error in Linear Algebra logic (seems unlikely, same code worked in old likelihood)&lt;/li&gt;
&lt;li&gt;Some bug earlier in the pipeline, causing invalid matrices here.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I&#39;m guessing its 3; if not that, then maybe 2.&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;inspect the shared indices -- same view?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;12:48:49 PM&lt;/p&gt;

&lt;p&gt;Reversing my previous stance:  Degenerate prior with shared index will always result in a degenerate posterior.  The prior has two variables that are 100% correlated, and the posterior will also be 100% correlated.  This implies a degenerate covariance matrix.&lt;/p&gt;

&lt;p&gt;In such a case, the posterior&#39;s implied dimensionality is lower than it&#39;s apparent dimensionality.  The prior and posterior will be degenerate in the same way; this symmetry is aesthetically appealing, because the Candidates estimator for the marginal likelihood involves their ratio.  This suggests that eliminating the redundant dimensions for both pdfs is a sensible thing to do.&lt;/p&gt;

&lt;p&gt;How to resolve this?  Possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detect redundant indices early, eliminate them in the prior and posterior.  USe bookkeeping to remember which observations refer to the same model point.&lt;/li&gt;
&lt;li&gt;Handle reduntancies later, after degenerate posterior is formed, before evaluating posterior or prior .  The mean will automatically contain redundancies, which will avoid bookkeeping when comparing the mean to the data points (when &lt;code&gt;mix&lt;/code&gt; is != 0).&lt;/li&gt;
&lt;li&gt;Try to determine how identical indices arose in the first place.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Decided on 2 for now.  In retrospect, approach 1 may have had slightly less code and faster.&lt;/p&gt;

&lt;p&gt;01:45:00 PM&lt;/p&gt;

&lt;p&gt;ML is now running on legacy correspondence.  Results are significantly higher than the legacy ML algorithm (~ 2.25 times).  Not yet sure why yet.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Computing marginal likelihood (old way)...
Done. (6665.8 ms)
Old ML: 207.041742
Computing marginal likelihood (new way, legacy correspondence)...
Done. (8853.1 ms)
New ML (Legacy corr): 439.273126
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Noticible slow-down, probably because of extra indices arising from not merging duplicate observations.&lt;/p&gt;

&lt;p&gt;possible causes of discrepancy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In old code, re-triangulation used simple averaging, not accounting for differing precisions of the points&lt;/li&gt;
&lt;li&gt;index sets differ between old and new code.  (update: checked, they are same)&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Plotting means for both to view differences...&lt;/p&gt;

&lt;p&gt;Investigation into ML discrepancy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Posterior means are similar, but obviously differ.&lt;/li&gt;
&lt;li&gt;both means are very non-smooth (need to re-address the smoothness issue)&lt;/li&gt;
&lt;li&gt;new mean has signficantly more points (~7%, 80 pts).  Could this alone affect ML?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;almost all difference is in posterior&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Maybe posterior kernel is different?  but prior is same
It&#39;s just the data covariance thats different.
but why??  There&#39;s more of them.&lt;/p&gt;

&lt;p&gt;New ml changes significantly when evaluation point changes.  This implies that the posterior curvature differs from the curvature of the joint distribution, which shouldn&#39;t happen.  A small amount of difference can arise naturally due to the approximation of the likelihood, but this is too large to be explained in that way.  Most noticibly, the new results look very similar to the old ones &lt;strong&gt;when evaluating at the likelihood mean&lt;/strong&gt;.  The largest difference occurs when evaluating at the posterior mean.  Not clear yet what conclusions to draw from this, or if this is just a coincidence.&lt;/p&gt;

&lt;p&gt;Maybe likelihood curvatures are differing.&lt;/p&gt;

&lt;p&gt;07:35:28 PM&lt;/p&gt;

&lt;p&gt;Was able to get identical likelihood value using matrix-form as I did with the original 2D geometric error form.  Seems like the precision matrices are good.&lt;/p&gt;

&lt;p&gt;Priors appear good too, since it evaluates to the same value as the reference implementation.&lt;/p&gt;

&lt;h2&gt;Reference implementation for new ML&lt;/h2&gt;

&lt;p&gt;Idea: direct implementation for quadform, but with hacked nomralization constant&lt;/p&gt;

&lt;h2&gt;In progress&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnosing errors in new likelihood &lt;code&gt;curve_ml_3.m&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;getting &lt;code&gt;test_ml_end_to_end_2.m&lt;/code&gt; running.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log"/>
   <updated>2013-07-03T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/03/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Implementing covariance functions&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood&lt;/li&gt;
&lt;li&gt;Implement fast generalized marginal likelihood&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implement generalized marginal likelihood&lt;/h2&gt;

&lt;p&gt;Rewriting &lt;code&gt;curve_ml3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Based on &lt;code&gt;curve_ml.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Kernel now takes two sets of indices, curve-index and view-index.&lt;/p&gt;

&lt;p&gt;Kernel is now passed-in as a lambda.&lt;/p&gt;

&lt;p&gt;Need to decide whether to use matrix inversion lemma for global offset and linear prior.  Currently being used;  not using would be simpler; try both and compare results.&lt;/p&gt;

&lt;h2&gt;Testing new marginal likelihood&lt;/h2&gt;

&lt;p&gt;Creating new version of &lt;code&gt;test/test_ml_end_to_end.m&lt;/code&gt;, named &lt;code&gt;test/test_ml_end_to_end2.m&lt;/code&gt;.  This version uses the new likelihood format, and compares against the old.&lt;/p&gt;

&lt;h2&gt;New curve_likelihood&lt;/h2&gt;

&lt;p&gt;Need to write new version of &lt;code&gt;curve_likelihood.m&lt;/code&gt; to handle new likelihood format.&lt;/p&gt;

&lt;p&gt;It&#39;s going to be difficult to compare to older version, since we&#39;ve changed the correspondence method...&lt;/p&gt;

&lt;p&gt;I&#39;ll have to add a flag that simulates the old correspondence method, but stores it in the new likelihood format...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;01:33:52 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I decided to write a separate function for the old correspondence method, called &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.  Moved &lt;code&gt;clean_correspondence3.m&lt;/code&gt; to &lt;code&gt;corr_to_likelihood.m&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The legacy function isn&#39;t exactly identical to the old function.  In particular, the index values might be slightly different, since it depends on the old function&#39;s global mean curve, which we don&#39;t compute in the new function.  If it&#39;s important, we can call the old function, and then call the new one.  This is a debugging function, so speed doesn&#39;t matter. For the moment, we&#39;ll accept the small difference, and use this function simply to confirm that we&#39;re in the ballpark.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;03:06:24 PM&lt;/em&gt;
Test is implemented.  Troubleshooting syntax errors...&lt;/p&gt;

&lt;p&gt;&lt;em&gt;04:11:17 PM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Elusive NaN error in &lt;code&gt;corr_to_likelihood_legacy.m&lt;/code&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log"/>
   <updated>2013-07-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Implementing marginal likelihood &lt;/h2&gt;

&lt;p&gt;Working on generalizing marginal likelihood for all three foreground models.&lt;/p&gt;

&lt;p&gt;Considered briefly how to use the matrix inversion lemma to improve numerical stability of all models.  Decided it could be difficult to build in a general way;  will proceed with the direct method for now, until there is evidence of numerical instability.&lt;/p&gt;

&lt;p&gt;Generalized how covariance is generated.  Wrote several new functions for generating differenct covariance matrices.  Will need to rework some due to the developments I describe later.&lt;/p&gt;

&lt;h2&gt;Theoretical Developments&lt;/h2&gt;

&lt;p&gt;Thought extensively about perturbation models I described yesterday.  I realized there is a serious problem with modeling motion using brownian motion -- the prior variance grows without bound as time approaches infinity.  Thus, the marginal prior for the curve in view 36 has much greater prior variance than the curve one in view 1.  This doesn&#39;t make sense -- ideally, they should all have the same marginal prior.&lt;/p&gt;

&lt;p&gt;This led to a reading session in Williams and Rasmussen, which led me to develop two new motion models, which I describe extensively in today&#39;s accompanying post.&lt;/p&gt;

&lt;h2&gt;Tomorrow&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Finish implementing new covariance functions.&lt;/li&gt;
&lt;li&gt;Implement generalized marginal likelihood.&lt;/li&gt;
&lt;li&gt;implement &lt;em&gt;fast&lt;/em&gt; generalized marginal likelihood (approximation)

&lt;ul&gt;
&lt;li&gt;Test against existing ML for old models.&lt;/li&gt;
&lt;li&gt;Test with new models.  Are the ML&#39;s higher? (will probably need training).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Build training data set from hand-traced ground-truth.&lt;/li&gt;
&lt;li&gt;Write training code.  Train all three candidate models.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Rethinking covariance functions</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions"/>
   <updated>2013-07-02T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/02/rethinking-covariance-functions</id>
   <content type="html">&lt;p&gt;Noticed a problem with the cubic spline perturbation model.  As the view number increases, the variance marginal prior of the curve in that view approaches infinity.  This means that the &quot;smoothness&quot; prior is ignored more and more in later views.&lt;/p&gt;

&lt;h2&gt;Squared Exponential perturbation model&lt;/h2&gt;

&lt;p&gt;A better model would have the same marginal prior for all views, but with correlation between nearby views.  This allows curves to revert to the prior as temporally correlated evidence becomes less informative.  I believe the squared exponential covariance function has this property.  Instead of adding the sq-exp covariance to the existing covariance function, we should multiply it, so self-covariance is unchanged, but pairwise correlation is non-zero.&lt;/p&gt;

&lt;p&gt;An added benefit of this is that it only has one tunable parameter.&lt;/p&gt;

&lt;p&gt;It should be easy to incorporate into our test-bed, along with the other two newly proposed foreground models.&lt;/p&gt;

&lt;h2&gt;Ornstein Uhlenbeck perturbation model&lt;/h2&gt;

&lt;p&gt;Digging deeper into Williams and Rasmussen, I found precisely the GP I was looking for a few days ago:  the OrnsteinUhlenbeck (OU) process.  This   process describes brownian motion under the influence of a regularizing force that pulls toward the mean.&lt;/p&gt;

&lt;p&gt;In other words, I can model correlation between views without affecting the marginal prior of the curve any particular view.  This is also accomplished by the squared-exponential model, but the OU process is probably more realistic, because the plant&#39;s motion looks non-smooth.&lt;/p&gt;

&lt;h2&gt;Modelling the mean or not?&lt;/h2&gt;

&lt;p&gt;I&#39;m struggling with whether or not to explicitly model a &quot;mean&quot; curve with the SE and the OU processes.&lt;/p&gt;

&lt;p&gt;If I did model the mean, each curve&#39;s covariance function would be the sum of a standard covariance plus a perturbation covariance.  The standard covariance models the &quot;mean&quot; curve, and it would be the same for all views (100% correlated).  The perturbation covariance would be partially correlated between the views, using the SE or the OU process.  The bayes net has the following structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        O   &amp;lt;-- mean curve
     / /\ \
   /  /  \  \
 /   |    |   \
O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The alternative is to model the per-view curves directly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;O---&amp;gt;O---&amp;gt;O---&amp;gt;O   &amp;lt;- per-view curves
|    |    |    |
O    O    O    O   &amp;lt;- observations
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Under this model, each view&#39;s curve has a cubic-spline marginal distribution, and the SE or UO process controls the correlation between them.&lt;/p&gt;

&lt;p&gt;What isn&#39;t clear is whether the perturbations in the latter model will be independent between points.  We need to model within-view perturbations as correlated, otherwise the marginal likelihood will drop too low.  There is no explicit description of how perturbations of adjacent points correlate.&lt;/p&gt;

&lt;h2&gt;What follows is me thinking out-loud...&lt;/h2&gt;

&lt;p&gt;Intuitively, points nearby in curve-space (have similar \(\tau\)&#39;s) can never be more correlated than they are when they appear in the same view.  Separating them in time (view) will decreases their correlation, until finally, there is no correlation; the only remaining correlation is between points in the same view.  The SE kernel modulates that correlation.  The SE kernel doesn&#39;t explicitly model correlation between perturbations, but this doesn&#39;t mean the correlation doesn&#39;t exist -- it is implicit in the original kernel.&lt;/p&gt;

&lt;p&gt;There is an analogy here with the classic krigging example.  In there models, the squared exponential over a 2D landscape (i.e. joint function over \(\mathbb{R}\)&lt;sup&gt;2&lt;/sup&gt; space) is equal to the product of 1D squared exponentials (i.e. two functions over \(\mathbb{R}\)&lt;sup&gt;1&lt;/sup&gt; space).  In other words, the 2D kernel is constructed by the product of 1D kernels.  There is no worry that the delta between nearby &quot;slices&quot; of the surface are uncorrelated, because the marginal covariance within that slice will enforce that smoothness.&lt;/p&gt;

&lt;p&gt;In our case, we also have a product of 1D covariance functions constructing a 2D covariance function.  The difference is that one of the kernels (the curve-space kernel) is a cubic-spline process, while the other (the time-dimension kernel) is squared exponential (or Ornstein-Uhlenbeck).  Despite this difference, my intuition is that the conclusions are the same - the deltas will be smooth (i.e. correlated), because they will be the difference between two smooth curves.&lt;/p&gt;

&lt;p&gt;Considering the marginals in both directions further illustrates why this works.  Obviously, a slice in the curve-direction will always look curve-like, since the marginals are all the same cubic-covariance GP prior.  In the time-direction, a single point will follow a GP or OU process, with initial conditions dictated by the first curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;...BUT...&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;...in the absence of data, each individual point will drift over time to the prior mean, I.e. &lt;strong&gt;zero&lt;/strong&gt;.  In other words, the 3D curve observed in view infinity gains no information from curve observed at time zero.&lt;/p&gt;

&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; realistic.  In reality, these points shouldn&#39;t drift far from some &quot;sensible&quot; center curve, implying that a mean curve exists.&lt;/p&gt;

&lt;p&gt;The time-slice of any particular point &lt;em&gt;should&lt;/em&gt; look like either a OU or SE process, but the mean needs to be explicit.  This implies an additive model, with distinct models for the unobserved &quot;mean curve&quot; and the deviations from it, which are summed to get the 3D curve seen in each view.&lt;/p&gt;

&lt;h2&gt;Modeling perturbation&lt;/h2&gt;

&lt;p&gt;So if we must model perturbation, how should we model it?&lt;/p&gt;

&lt;p&gt;One thing is clear: the marginal prior for the curve in any view &lt;strong&gt;must&lt;/strong&gt; still be a cubic-spline process.&lt;/p&gt;

&lt;p&gt;This implies that the perturbation must be a cubic-spline process, too.&lt;/p&gt;

&lt;p&gt;However, the variances for each component (offset, linear, and cubic) are likely to be different for the perturbation model, compared to the mean-curve model.  Most importantly, the magnitude of the offset variance must be large in the mean-curve model but will be &lt;em&gt;much&lt;/em&gt; lower (relative to linear and cubic variance) in the perturbation model.&lt;/p&gt;

&lt;p&gt;I was hoping to avoid adding four extra parameters to our model (perturbation offset, linear and cubic variance, plus perturbation scale-length).  The mean-free model only adds one parameter - scale-length.  I guess this is the price we pay for a better model -- and for higher marginal likelihoods.  Ideally, the occam&#39;s razor quality of the marginal likelihood will allow us to avoid overfitting this many parameters (7 total).  Any parameters that are superfluous should become near-zero during training.&lt;/p&gt;

&lt;p&gt;...I hope.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log"/>
   <updated>2013-07-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Background:  This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;p&gt;Also developed a new approach to evaluate marginal likelhoods thats much simpler, but need to confirm that it matches reference implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implement both new foreground models, plus old one in new &quot;kernel function&quot; way.&lt;/li&gt;
&lt;li&gt;Implement new evaluation method and test against reference.&lt;/li&gt;
&lt;li&gt;Learn parameters for all three models (needs some ground truthing).&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Implementing new models&lt;/h2&gt;

&lt;p&gt;Worked on general framework for evaluating ML under general new model framework.&lt;/p&gt;

&lt;p&gt;After some struggling with covariance degeneracies inherent in backprojection, I realized that the 3D marginal likelihood is naturally degenerate, because it isn&#39;t the true likelihood function (which is in 2D).&lt;/p&gt;

&lt;p&gt;I&#39;m kicking myself for not remembering that I struggled with this exact problem 5 months ago.  At that time, I realized the better approach is to use Candidate&#39;s estimator, which is the ratio of the unnormalized posterior to the normalized posterior.  The unnormalized posterior comes from the prior and 2D likelIhood;  the normalized posterior is obtainable from the 3D likelihood and the prior.&lt;/p&gt;

&lt;p&gt;This was already implemented in &lt;code&gt;curve_ml.m&lt;/code&gt;, but was  O(n&lt;sup&gt;3&lt;/sup&gt; ), so was all but abandoned in favor of the junction-tree method in &lt;code&gt;curve_ml2.m&lt;/code&gt;, which is O(n).&lt;/p&gt;

&lt;p&gt;However, it&#39;s recently become clear that the candidate&#39;s estimator should be evaluated in \(O(n)\)  by exploiting the Markovian nature of the covariance matrix.&lt;/p&gt;

&lt;p&gt;It should be easy to try both, by simply swapping out covariance matrices with ones arising from the new covariance functions.  Tomorrow...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Foreground Curve Models as Gaussian Process Covariance Function</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models"/>
   <updated>2013-07-01T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/07/01/foreground-curve-models</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Background&lt;/h2&gt;

&lt;p&gt; This weekend, I worked out the math for two new foreground models, which differ in how 3D perturbations between views are modelled.  The first assumes a single &quot;mean&quot; curve, and all observations are small 3D perturbations of it.  The second assumes the unobserved curve follows Brownian motion over time.&lt;/p&gt;

&lt;h2&gt;Covariance functions as models&lt;/h2&gt;

&lt;p&gt;I realized this weekend that all of the models I&#39;ve considered are achievable by using different covariance functions.&lt;/p&gt;

&lt;p&gt;The original model is given by&lt;/p&gt;

&lt;div&gt; \[
k_1(i,j) = \sigma_s k_{\text{cubic}}(i,j) + \sigma_o  k_{\text{offset}}(i,j) + \sigma_r k_{\text{linear}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where&lt;/p&gt;

&lt;div&gt; \[
\begin{align}
k_{\text{cubic}}(i,j) &amp;= \frac{|\tau_i - \tau_j| \min(\tau_i, \tau_j)^2}{2} + \frac{\min(\tau_i, \tau_j)^3}{3} \\
k_{\text{linear}}(i,j) &amp;= \tau_i \tau_j  \\
k_{\text{offset}}(i,j) &amp;= 1 \\ 
\end{align}
\] &lt;/div&gt;


&lt;p&gt;The cubic model penalizes non-zero second derivative over the length of the curve.  The offset and linear model penalize zero and first derivative initial conditions.&lt;/p&gt;

&lt;h2&gt;White noise perturbation model&lt;/h2&gt;

&lt;p&gt;Both of the two new models expand on the original by modelling how the observed curve differs between views.  That is, the model for the curves are the same as before (they are cubic spline curves), but we additionally model &lt;strong&gt;how they are perturbed between views&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The first new model I call the &quot;White Noise&quot; perturbation model, which treats each view of the curve as arising from a white noise process that perturbs a &quot;master curve&quot;, I.e. the unobserved mean curve.  Its covariance is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{white}}(i,j) = k_1(i,j) + \delta(v_i-v_j) k_{\text{w}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(v_i\) is the view that captured the \(i\)th point and \(k_w\) is&lt;/p&gt;

&lt;div&gt; \[
k_w(i,j) = \sigma_{o,w}  k_{\text{offset}}(i,j) + \sigma_{r,w} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model adds extra covariance that is independent per-view.  This treats the perterbations from the mean curve as independent.&lt;/p&gt;

&lt;p&gt;The perturbations themselves are assumed to be purely to the curve&#39;s initial conditions, i.e. its position and direction.&lt;/p&gt;

&lt;p&gt;This model is motivated by the assumption that perturbations arise due to camera mis-calibrations, which result in mostly translation and small rotational changes.&lt;/p&gt;

&lt;h2&gt;Brownian motion perturbation model&lt;/h2&gt;

&lt;p&gt;The second model treats perturbations as arising from Brownian motion, i.e. each curve is independent, conditioned on the previous view&#39;s curve.  The covariance function is:&lt;/p&gt;

&lt;div&gt; \[
k_{\text{brown}}(i,j) = k_1(i,j) + \min(\tau_i, \tau_j) k_{\text{b}}(i,j)
\] &lt;/div&gt;


&lt;p&gt;where \(k_b\) is&lt;/p&gt;

&lt;div&gt; \[
k_b(i,j) = \sigma_{s,b} k_{\text{cubic}}(i,j) + \sigma_{o,b}  k_{\text{offset}}(i,j) + \sigma_{r,b} k_{\text{linear}}
\] &lt;/div&gt;


&lt;p&gt;This model assumes perturbations arise by motion of the plant during imaging, like moving closer to a light source, or responding to a temperature gradient in the room.  &quot;View index&quot; is a surrogate for a time variable, since time between captures is roughly constant.  The use of Brownian motion means the magnitude of perturbation increases by the square root of the distance between the views (time).  \(k_b\) models the nature of the perturbation; we use the cubic-spline kernel which says that point-wise perturbations are strongly correlated and increase in magnitude the further they get from the base of the curve.&lt;/p&gt;

&lt;p&gt;This \(k_b\) is possibly overkill;  using the simpler \(k_w\) from the white-noise model might work just as well.  This simpler model implies curves drift in position and direction only, and these perturbations are correlated over time.&lt;/p&gt;

&lt;h2&gt;End stuff&lt;/h2&gt;

&lt;p&gt;After some consideration, I think implementing these models in the current system will will very simple.  Training them will be harder; some more ground truthing will be needed.&lt;/p&gt;

&lt;p&gt;Of the two new models, I suspect that the white noise model will be sufficient to get the gains in marginal likelihood we seek.  It is a much better explanation for misaligned data-points compared to the old model models all point perturbations as independent.&lt;/p&gt;

&lt;p&gt;The brownian motion model will give us something to compare against in evaluation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Work summary</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary"/>
   <updated>2013-06-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-summary</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Finished editing new likelihood function.  See &lt;code&gt;correspondence/clean_correspondence3.m&lt;/code&gt; (filename likely to change soon).  Below is a summary of results and comparison to the old approach.&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;Below is a plot of the maximum likelihood curves:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-likelihood_1.gif&quot; alt=&quot;Maximum likelihood reconstruction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each colored curve arises from a single 2D data curve.  The curve represents the mean of a gaussian process. Variance is not shown; it is infinite in the direction of backprojection.&lt;/p&gt;

&lt;p&gt;The dark-blue curve is an estimate of the posterior curve, which was used to backproject the 2D curves against.  It is estimated from the point-correspondences.&lt;/p&gt;

&lt;p&gt;Note that for clarity, x/y scale is significantly smaller than z-scale (up-direction).  When axis are equally-scaled, max-likelihood curves lie very close the to blue curve.&lt;/p&gt;

&lt;h2&gt;Improved correspondence&lt;/h2&gt;

&lt;p&gt;The old likelihood suffered from a small but non-negligible number of awful correspondences, which severely damaged both reconstruction and marginal likelihood values.  This was because the likelihood was derived from the point-to-point correspondences, which (a) is problematic at gaps, and (b) suffer bad correspondences which can&#39;t be fixed later.
The new approach uses the old approach as a starting point, but then recomputes all 2D curve correspondences against a rough reconstruction.  This dramatically improves correspondences as we see below.&lt;/p&gt;

&lt;p&gt;This is the old correspondence.  Blue points are points from the 2D data curve;  the teal line is the posterior 3D curve (projected to 2D); red lines show correspondences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug.png&quot; alt=&quot;Correspondence bug &quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next is the fixed correspondence.  Notice how the red correspondence lines are much shorter, indicating a less costly correspondence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-28-ll-bug-fixed.png&quot; alt=&quot;Correspondence bug, fixed &quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Per-data-curve likelihood&lt;/h2&gt;

&lt;p&gt;The old likelihood had a single GP curve that represented all of the different views.  Now we have a GP curve &lt;strong&gt;per data-curve&lt;/strong&gt;, which will be related by a GP prior.&lt;/p&gt;

&lt;p&gt;This will allow us to simultaneously track and triangulate, a key novelty to this approach.  More importantly, it will give us higher marginal likelihood numbers for true 3D curve observations, because we can make the independent noise component very small.&lt;/p&gt;

&lt;h2&gt;TODO - short-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt; - This new function adds an extra pass of DTW per data-curve, per iteration where previously, only one pass was needed.  This has introduced a significant performance bottleneck on the order of 10-50x.  I need to profile and optimize this function if we want runtimes that aren&#39;t measured in weeks.  This may motivate a full re-thinking of &lt;code&gt;merge_correspondence.m&lt;/code&gt;, to avoid a full DTW after every merge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Marginal Likelhood &lt;/strong&gt; - Need to build the new marginal likelihood for the foreground curve model.  This proposed version will take advantage of the new per-data-curve likelihood.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;TODO - mid-term&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end&lt;/strong&gt; - Incorporate new likelhood and ML into gibbs sampler.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt; Swendsen wang cuts &lt;/strong&gt; - design SWC split/merge move&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log"/>
   <updated>2013-06-28T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/28/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Profiling and optimizing &lt;code&gt;clean_correspondence3.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Huge bottleneck in &lt;code&gt;get_dtw_matches_()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Wrote MEX version of get_dtw_matches_(),  &lt;code&gt;get_dtw_matches_horiz.c&lt;/code&gt;.   ~8x speedup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New bottlenecks&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;projection_error_hessian&lt;/li&gt;
&lt;li&gt;&lt;code&gt;interp1&lt;/code&gt; - linear interpolation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;csaps&lt;/code&gt; - spline smoothin&lt;/li&gt;
&lt;li&gt;dtw&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Plan&lt;/h2&gt;

&lt;p&gt;can possibly eliminate some calls to interp1.&lt;/p&gt;

&lt;p&gt;could mex projecion_error_hessian.  should be a big win&lt;/p&gt;

&lt;p&gt;DTW can be done in two passes, or mex&#39;d&lt;/p&gt;

&lt;h2&gt;Mexing &lt;code&gt;projection_error_hessian&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Tried using Matlab&#39;s &quot;Coder&quot; feature to generate mex code automatically.  The result was totally bloated (14 files!) and no faster than the matlab version.  Next I tried hand-coding the mex, and its ~10x faster.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/27/work-log"/>
   <updated>2013-06-27T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/27/work-log</id>
   <content type="html">&lt;p&gt;&lt;strong&gt; Thursday Overview &lt;/strong&gt;
* Continued refactoring of likelihood
* C++ Bootcamp
* VR lab tour and new user set-up&lt;/p&gt;

&lt;h2&gt;Continued refactoring of likelihood&lt;/h2&gt;

&lt;p&gt;Implemented &quot;alternative approach&quot; to handling tail points mentioned in last entry.  The old way didn&#39;t benefit from re-indexing; this way does.  This way handles negative index points correctly, too.&lt;/p&gt;

&lt;p&gt;In process of testing and debugging.  Possibly more outcomes later tonight.&lt;/p&gt;

&lt;h2&gt;C++ Bootcamp&lt;/h2&gt;

&lt;p&gt;today&#39;s session: inline and const-correctness&lt;/p&gt;

&lt;h2&gt;VR lab tour and new user set-up&lt;/h2&gt;

&lt;p&gt;By Angus&#39;s request, I showed showed the new postdoc Javier around the lab and set him up with an admin account.&lt;/p&gt;

&lt;p&gt;Lots of things still broken on VR01; biggest problem is video card #2 not displaying anything.  Game controller not set up yet.    Showed osgviewer demo, and by Angus&#39;s request, got his Processing demo running too.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/26/work-log"/>
   <updated>2013-06-26T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/26/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Likelihood construction, &lt;code&gt;clean_correspondence3.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;new tracking GP model&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Likelihood construction&lt;/h2&gt;

&lt;p&gt;Started and finished implementation today.  Need to design a test and then debug.&lt;/p&gt;

&lt;p&gt;~ 250 lines of Matlab code.  Logic overview:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;smooth triangulated points&lt;/li&gt;
&lt;li&gt;resample smoothed 3D curve uniformly&lt;/li&gt;
&lt;li&gt;handle tails (see below)&lt;/li&gt;
&lt;li&gt;project curve into each view and resample uniformly&lt;/li&gt;
&lt;li&gt;DTW to correspond 2d data curve to projected smooth curve  (see below)&lt;/li&gt;
&lt;li&gt;map corresponding projected curve points back to 3D points and indices&lt;/li&gt;
&lt;li&gt;triangulate 2d data points against corresponding 3d point&lt;/li&gt;
&lt;li&gt;compute likelihood hessian around that point&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;New DTW&lt;/h2&gt;

&lt;p&gt;Re-implemented a specialized version of DTW with following changes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;only horizontal steps accrue cost&lt;/li&gt;
&lt;li&gt;Hard-constraint on the number of vertical steps per horizontal step.&lt;/li&gt;
&lt;li&gt;keeps track of &quot;best&quot; match along vertical runs. no need for second pass

&lt;ul&gt;
&lt;li&gt;I think this is only possible because only horizontal steps accrue cost.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Tail points&lt;/h2&gt;

&lt;p&gt;Still iffy on the &quot;tail points&quot; case. Implemented late at night and likely needs review in the morning.  Still need to handle negative index values.&lt;/p&gt;

&lt;p&gt;Alternative implementation: only inspect the tails to determine the length of the 3D curve.  Then proceeed as usual. no special cases&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;review tail points code

&lt;ul&gt;
&lt;li&gt;consider alternative implementation (see above)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;handle negative index values&lt;/li&gt;
&lt;li&gt;think about hessian and transformation Jacobian&lt;/li&gt;
&lt;li&gt;testing, debugging, profiling&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Rethinking Likelihood</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/20/rethinking-likelihood"/>
   <updated>2013-06-20T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/20/rethinking-likelihood</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Testing and debugging &lt;code&gt;clean_correspodnence2.m&lt;/code&gt; has revealed some significant problems with the current approach to constructing the likelihood function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 5%-10% of fragements have correspondences that are nonsensical. &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I hypothesize that this is due to bad correspondences early on, when there is little evidence to drive a good correspodnence.  These bad correspodnences are propagated as new curves are added that could suggest a better correspodnence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Large lateral gaps in triangulation result in large axial gaps in posterior curve.  &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The problem is that the index set is computed from the triangulation.  The current fix for this -- smooth, re-index, repeat -- is very limited in the severity it can overcome.  In practice, most gaps are only partially reduced.&lt;/p&gt;

&lt;h2&gt;Rethinking &quot;Correspondence to Likelihood&quot;&lt;/h2&gt;

&lt;p&gt;Corresondence is good for constructing a decent-quality 3d curve, but isn&#39;t good for computing fine-grained pointwise likelihood, due to sporradic terrible correspondences and gaps.&lt;/p&gt;

&lt;p&gt;Instead of continuously Band-Aiding these issues that keep arising, its time to re-think how the likelhood is constructed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 1. the mean of 3d gaussians should project to the 2D position of the corresponding data point &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; 2. the depth should be based on the corresponding position in the unobserved curve &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Two issues here.  First, how to localize the unobserved curve without having the likelihood already (chicken and egg).  Second, how to identify the corresponding point of the unobserved curve?&lt;/p&gt;

&lt;p&gt;In the old method, the answer to both was &quot;use the correspondence matrix.&quot;&lt;/p&gt;

&lt;p&gt;In the new method, we still use the correspondence matrix to triangulate, but we smooth the result using the prior and then throw away correspondence information.&lt;/p&gt;

&lt;h2&gt;Killing the correspondence grid&lt;/h2&gt;

&lt;p&gt;The corresponence matrix artificially forces points from different views to correspond to the same point.  This is out of necessity -- we need correspondence to achieve triangulation.  But we don&#39;t need to adhere to this to compute the likelhood.  Indeed, observed points may fall anywhere in the continuous index set, not into a discrete set of predefined cells.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Observations can correspond to any index in \([0 t_{end}]\).&lt;/li&gt;
&lt;li&gt;Unobserved curve is modelled at a uniform grid.&lt;/li&gt;
&lt;li&gt;Previously, the dimensionality of the unobserved curve grew with the number of observations. Now it grows with the range of the index set (i.e. the length of the curve).`&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Random thoughts&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;evaluate marginal likelihood directly?  then add extra normalization from triangulation (jacobian?)

&lt;ul&gt;
&lt;li&gt;evaluate linearly using markov conditional probabilities&lt;/li&gt;
&lt;li&gt;extend to poly model

&lt;ul&gt;
&lt;li&gt;grid-structured Bayes net&lt;/li&gt;
&lt;li&gt;topological sort for evaluation&lt;/li&gt;
&lt;li&gt;use scope variables to manage dependencies generally&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/17/work-log"/>
   <updated>2013-06-17T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/17/work-log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Continuing work on foreground curve model. Editing &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Logic overview&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;smooth triangulated points &amp;amp; estimate their indices&lt;/li&gt;
&lt;li&gt;fill-in untriangulatable points/indices&lt;/li&gt;
&lt;li&gt;resolve many-to-one data-to-3d-curve correspondences by picking best match&lt;/li&gt;
&lt;li&gt;triangulate each data point against 3d curve to get mean and covariance for point&lt;/li&gt;
&lt;/ol&gt;


&lt;h2&gt;Logic detail&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;1. estimate indices by chord-length parameterization
2. repeat N times:
    2.1 smooth triangulated points using current index estimate (posterior mean approx.)
    2.2 re-estimate indices using chord-length parameterization
3. for each untriangulatable point
    3.1 triangulate against cubic interpolation of neighboring points (Newton&#39;s method)
    3.2 store resulting point and index with smoothed points from 2.1
4. for each 2D data curve
    4.1 triangulate against corresponding 3d point in smoothed curve to get likelihood mean
    4.2 compute curvature at this point to get likelihood precision
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;03:13:30 PM&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First run, addressing syntax errors.&lt;/li&gt;
&lt;li&gt;Plotting first test: triangulated curve w/ and wo/ gap-filling

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: gap-fill at beginning maps exactly to first non-gap point&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Issue&lt;/strong&gt;: interior gap point falls exactly on non-gap point&lt;/li&gt;
&lt;li&gt;need to merge points?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;04:06:03 PM&lt;/strong&gt;
wrapping up...&lt;/p&gt;

&lt;h2&gt;TODO&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;diagnose coincident-point issue -- bug or not?&lt;/li&gt;
&lt;li&gt;Finish testing - plot per-view likelihood points&lt;/li&gt;
&lt;li&gt;Add point-merging&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/14/log-entry"/>
   <updated>2013-06-14T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/14/log-entry</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;

    &lt;tr&gt;
        &lt;th&gt;SVN Revision&lt;/th&gt;
        &lt;td&gt;14528&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;Working on foreground curve model.&lt;/p&gt;

&lt;p&gt;Begin of significant rework; SVN revision noted in infobox.&lt;/p&gt;

&lt;p&gt;Editing  &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt; which will replace &lt;code&gt;correspondence/clean_corespondence.m&lt;/code&gt;.  Drawing partially from &lt;code&gt;corr_to_bg_likelihood.m&lt;/code&gt;, esp. for &quot;best match&quot; logic.&lt;/p&gt;

&lt;p&gt;Probably will rename &lt;code&gt;correspondence/clean_correspondence2.m&lt;/code&gt; to &lt;code&gt;correspondence/corr_to_likelihood.m&lt;/code&gt; before I finish.&lt;/p&gt;

&lt;p&gt;Will use &lt;code&gt;tests/test_ml_end_to_end&lt;/code&gt; to compare old and new implementations.&lt;/p&gt;

&lt;p&gt;Work still in progress...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/13/log"/>
   <updated>2013-06-13T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/13/log</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;p&gt;After investigating the false positives from the last entry, it seems clear that bad matches look good because missing data are not penalized.  For example&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-13-bad-match.png&quot; alt=&quot;Bad match&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the overlapping region of the blue and green curves, the distance between them is relatively low (less than 3 pixels, or 1.5px radius).  But the size of their overlap is so low that it would be hard to claim that they come from the same underlying curve with any confidence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Params &lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smoothing_variance_2d: 0.2500
    noise_variance_2d: 10
     position_mean_2d: [2x1 double]
 position_variance_2d: 1.3629e+04
     rate_variance_2d: 0.4962
   smoothing_variance: 1.0000e-04
       noise_variance: 10
        position_mean: [3x1 double]
    position_variance: 62500
        rate_variance: 2.2500
      smoothing_sigma: 0.2000
    noise_variance_bg: 0.1038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I would have expected that noise_variance_bg was low enough to discount this candidate, but the log ML ratio is 71.0.  The noise model must just look really bad...&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Visualizing/Debugging BG ML</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/12/visualizingdebugging-bg-ml"/>
   <updated>2013-06-12T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/12/visualizingdebugging-bg-ml</id>
   <content type="html">

&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Experiment&lt;/h2&gt;

&lt;p&gt;We now have a new algorithm for computing background curve Marginal Likelihood.  Lowering noise sigma \(\sigma_n\) should rule out bad matches.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;: Re-run background candidate matching with new algorithm and roughly-trained \( \sigma_n \).&lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;If the threshold is set right, the results are improved, but we still have some false-positives and false negatives.&lt;/p&gt;

&lt;p&gt;It&#39;s still unclear whether we can get good results without thresholding, since we haven&#39;t computed the noise ML using the new algorithm, so absolute numbers are meaningless.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Params&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;smoothing_variance_2d: 0.2500
    noise_variance_2d: 10
     position_mean_2d: [2x1 double]
 position_variance_2d: 1.3629e+04
     rate_variance_2d: 0.4962
   smoothing_variance: 1.0000e-04
       noise_variance: 10
        position_mean: [3x1 double]
    position_variance: 62500
        rate_variance: 2.2500
      smoothing_sigma: 0.2000
       nlise_variance: 10
    noise_variance_bg: 0.1038
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Calls&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;data = offline_pair_candidates(data, params, 0, 1, 1, &#39;bg&#39;);
cands = tmp_get_cands(data);
visualize_bg_cands(data, cands, 250)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt; Plots &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Matched curves appear in white, unmatched appear in gray&lt;/p&gt;

&lt;p&gt;False negatives&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-12-bad-matches.png&quot; alt=&quot;bad matches&quot; /&gt;&lt;/p&gt;

&lt;p&gt;False positives:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2013-06-12-false-positive.png&quot; alt=&quot;bad matches&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Next Steps&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;apply new indexing and cleanup algorithm to 3d and noise curves&lt;/li&gt;
&lt;li&gt;better training of foreground/background

&lt;ul&gt;
&lt;li&gt;ground truth curve fragments&lt;/li&gt;
&lt;li&gt;automatic training of background&lt;/li&gt;
&lt;li&gt;automatic training of noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;better foreground model

&lt;ol&gt;
&lt;li&gt; compute likelihood separately&lt;/li&gt;
&lt;li&gt; add smooth GP to likelihood&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;re-run experiment with trained noise model parameters&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Index set bug fixed, marginal likelihood curves improved</title>
   <link href="http://vision.sista.arizona.edu/ksimek/research/2013/06/10/index-set-bug"/>
   <updated>2013-06-10T00:00:00-07:00</updated>
   <id>http://vision.sista.arizona.edu/ksimek/research/2013/06/10/index-set-bug</id>
   <content type="html">&lt;div class=&quot;meta-info&quot;&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Project&lt;/th&gt;
        &lt;td&gt;&lt;a href=&quot;/ksimek/research/projects/tulips.html&quot;&gt;Tulips&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Subproject&lt;/th&gt;
        &lt;td&gt;Data Association v2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Working path&lt;/th&gt;
        &lt;td&gt;projects/&amp;#8203;tulips/&amp;#8203;trunk/&amp;#8203;src/&amp;#8203;matlab/&amp;#8203;data_association_2&lt;/td&gt;
    &lt;/tr&gt;


&lt;/table&gt;

    Unless otherwise noted, all filesystem paths are relative to the &quot;Working path&quot; named above.
&lt;/div&gt;


&lt;h2&gt;Index Set Bug fixed &lt;/h2&gt;

&lt;p&gt;After fixing a problem with how index sets were estimated, the marginal likelihood (ML) curves are now much more sensible.&lt;/p&gt;

&lt;p&gt;Here are two marginal likelihood curves using the old approach with two different smoothing sigmas.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2012-06-10_ML_old.png&quot; alt=&quot;pre-bug ML curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how lowering the smoothing sigma \(\sigma_s\) causes the maximum ML to continuously improve, and results in a &lt;em&gt;huge&lt;/em&gt; improvement when noise sigma \(\sigma_n\) is low. There are two implications of this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ML continuously improves as \(\sigma_s\) approaches zero&lt;/li&gt;
&lt;li&gt;As \(\sigma_s\) approaches zeros, the optimal \(\sigma_n \) approaches zero&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;We &lt;em&gt;need&lt;/em&gt; to be able to use \(\sigma_n \) to control the cutoff for background curves, but this is not possible if ML is a monotonic function of \(\sigma_n\).&lt;/p&gt;

&lt;p&gt;The problem arose from the fact that index-set spacing grew in proportion to curve-noise, whereas it should have stayed roughly constant.  As a result, more noise made the curves look &lt;em&gt;smoother&lt;/em&gt;, because the unobserved points seemed to be farther apart.&lt;/p&gt;

&lt;p&gt;Obviously, this is the opposite of what we would want.  I rewrote the &quot;cleanup&quot; algorithm so index sets are now computed from an estimate of the posterior curve, not from the maximum likelihood curve.  This causes noise to be smoothed out of the curve before measuring the distance between points, so increasing noise will not significantly change the inter-point distance.&lt;/p&gt;

&lt;p&gt;Here are the ML curves after the change.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ksimek/research/img/2012-06-10_ML_new.png&quot; alt=&quot;pre-bug ML curves&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Some observations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The affect of changing \(\sigma_s\) is &lt;em&gt;far&lt;/em&gt; less dramatic&lt;/li&gt;
&lt;li&gt;As \(\sigma_n\) approaches zero, the ML &lt;em&gt;always&lt;/em&gt; drops below 0&lt;/li&gt;
&lt;li&gt;The position and value of the maximum is mostly unchanged, suggesting a &quot;natural&quot; noise-value that is independent of the smoothing value.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Point 2 is particularly important.  These ML values are actually log ratios between the ML under this model and the ML under the &quot;default&quot; noise model.  Values below zero indicate that the naive noise model is a better fit.  The fact that we can adjust \(sigma_n\) to control the trade-off between the two models is promising, and suggests that this new model can, indeed, be discriminitive.  Prior to these bugs, it was not clear that this was the case, because the background curve model &lt;em&gt;always&lt;/em&gt; looked better than noise.&lt;/p&gt;

&lt;h2&gt;Point merging&lt;/h2&gt;

&lt;p&gt;One additional complication that arose was that after smoothing, many curve points at appeared at nearly the same position.  As a result, the changes in the index set were very small and the resulting Gaussian Process covariance matrix became degenerate.  I added some code that merges points that are too close to each-other, and updates the likelihood function and index set accordingly.  My tests show that this causes a negligible decrease in the ML compared to the non-merged case, and eliminates the degeneracy problem in all the cases I encountered.&lt;/p&gt;

&lt;h2&gt;Smart Cleanup&lt;/h2&gt;

&lt;p&gt;During my investigations, I also rewrote the &quot;cleanup&quot; logic, which ensures that each point corresponds to the model curve exactly once.  It originally did this by naively taking the first correpsondence, and I thought that the problematic results from above were caused by this.  I wrote new logic that now chooses the &lt;em&gt;best&lt;/em&gt; correspondence, i.e. the correspondence that results in the lowest error.&lt;/p&gt;

&lt;h2&gt;Summary, Next steps&lt;/h2&gt;

&lt;p&gt;The new code is in &lt;code&gt;correspondence/corr_to_bg_likelihood.m&lt;/code&gt;, which now replaces the deprecated &lt;code&gt;clean_bg_correspondence.m&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Next steps&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Next test: can we distinguish BG curves from non-bg/foreground curves?&lt;/li&gt;
&lt;li&gt;Migrate this new logic to 3D curve model (clean_correspondence.m and maybe merge_correspondence.m)&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 
</feed>
